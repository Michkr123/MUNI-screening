[
    {
        "title": "fix: handle `load_balance_hosts` null value",
        "id": 2843011946,
        "state": "open",
        "first": "Hi, everyone! After testing the PgBouncer 1.24 and CNPG 1.25 with [this fix](https://github.com/cloudnative-pg/cloudnative-pg/pull/6630) applied I see the following errors in the pooler logs:\r\n```\r\n{\r\n  \"level\": \"error\",\r\n  \"ts\": \"2025-02-10T13:01:21.430417823Z\",\r\n  \"msg\": \"Error while executing SHOW POOLS\",\r\n  \"logger\": \"pgbouncer-manager\",\r\n  \"error\": \"sql: Scan error on column index 16, name \\\"load_balance_hosts\\\": converting NULL to int is unsupported\",\r\n  \"stacktrace\": \"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\t/root/go/pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/pgbouncer/metricsserver.(*Exporter).collectShowPools\\n\\tpkg/management/pgbouncer/metricsserver/pools.go:301\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/pgbouncer/metricsserver.(*Exporter).collectPgBouncerMetrics\\n\\tpkg/management/pgbouncer/metricsserver/pgbouncer_collector.go:143\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/pgbouncer/metricsserver.(*Exporter).Collect\\n\\tpkg/management/pgbouncer/metricsserver/pgbouncer_collector.go:119\\ngithub.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1\\n\\t/root/go/pkg/mod/github.com/prometheus/client_golang@v1.20.5/prometheus/registry.go:456\"\r\n}\r\n```\r\n```\r\npgbouncer=# show pools;\r\n    database     |         user          | cl_active | cl_waiting | cl_active_cancel_req | cl_waiting_cancel_req | sv_active | sv_active_cancel | sv_being_canceled | sv_idle | sv_used | sv_tested | sv_login | maxwait | maxwait_us |  pool_mode  | load_balance_hosts\r\n-----------------+-----------------------+-----------+------------+----------------------+-----------------------+-----------+------------------+-------------------+---------+---------+-----------+----------+---------+------------+-------------+--------------------\r\n test_cluster    | test_user             |        19 |          0 |                    0 |                     0 |         0 |                0 |                 0 |       3 |       5 |         0 |        0 |       0 |          0 | transaction |\r\n pgbouncer       | pgbouncer             |         1 |          0 |                    0 |                     0 |         0 |                0 |                 0 |       0 |       0 |         0 |        0 |       0 |          0 | statement   |\r\n postgres        | cnpg_pooler_pgbouncer |         0 |          0 |                    0 |                     0 |         0 |                0 |                 0 |       2 |       2 |         0 |        0 |       0 |          0 | transaction |\r\n(3 rows)\r\n```\r\nGiven the fact that I am using more or less standard setup, I propose to ignore the null values here instead of error.",
        "messages": "Hi, everyone! After testing the PgBouncer 1.24 and CNPG 1.25 with [this fix](https://github.com/cloudnative-pg/cloudnative-pg/pull/6630) applied I see the following errors in the pooler logs:\r\n```\r\n{\r\n  \"level\": \"error\",\r\n  \"ts\": \"2025-02-10T13:01:21.430417823Z\",\r\n  \"msg\": \"Error while executing SHOW POOLS\",\r\n  \"logger\": \"pgbouncer-manager\",\r\n  \"error\": \"sql: Scan error on column index 16, name \\\"load_balance_hosts\\\": converting NULL to int is unsupported\",\r\n  \"stacktrace\": \"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\t/root/go/pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/pgbouncer/metricsserver.(*Exporter).collectShowPools\\n\\tpkg/management/pgbouncer/metricsserver/pools.go:301\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/pgbouncer/metricsserver.(*Exporter).collectPgBouncerMetrics\\n\\tpkg/management/pgbouncer/metricsserver/pgbouncer_collector.go:143\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/pgbouncer/metricsserver.(*Exporter).Collect\\n\\tpkg/management/pgbouncer/metricsserver/pgbouncer_collector.go:119\\ngithub.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1\\n\\t/root/go/pkg/mod/github.com/prometheus/client_golang@v1.20.5/prometheus/registry.go:456\"\r\n}\r\n```\r\n```\r\npgbouncer=# show pools;\r\n    database     |         user          | cl_active | cl_waiting | cl_active_cancel_req | cl_waiting_cancel_req | sv_active | sv_active_cancel | sv_being_canceled | sv_idle | sv_used | sv_tested | sv_login | maxwait | maxwait_us |  pool_mode  | load_balance_hosts\r\n-----------------+-----------------------+-----------+------------+----------------------+-----------------------+-----------+------------------+-------------------+---------+---------+-----------+----------+---------+------------+-------------+--------------------\r\n test_cluster    | test_user             |        19 |          0 |                    0 |                     0 |         0 |                0 |                 0 |       3 |       5 |         0 |        0 |       0 |          0 | transaction |\r\n pgbouncer       | pgbouncer             |         1 |          0 |                    0 |                     0 |         0 |                0 |                 0 |       0 |       0 |         0 |        0 |       0 |          0 | statement   |\r\n postgres        | cnpg_pooler_pgbouncer |         0 |          0 |                    0 |                     0 |         0 |                0 |                 0 |       2 |       2 |         0 |        0 |       0 |          0 | transaction |\r\n(3 rows)\r\n```\r\nGiven the fact that I am using more or less standard setup, I propose to ignore the null values here instead of error.Hello @usernamedt, thanks for the contribution, I will take a look ASAP."
    },
    {
        "title": "refactor: move the multi-container logging to the logging package",
        "id": 2842975589,
        "state": "open",
        "first": "refactors the code that iterates containers when logging instance pods\r\nfor the `kubectl cnpg report` plugin, moving it upstream and adding tests",
        "messages": "refactors the code that iterates containers when logging instance pods\r\nfor the `kubectl cnpg report` plugin, moving it upstream and adding tests"
    },
    {
        "title": "fix: ensure that `override.conf` is always included",
        "id": 2842779554,
        "state": "open",
        "first": "Closes #5747",
        "messages": "Closes #5747/test depth=push\n---\n/test depth=push"
    },
    {
        "title": " Fixes #6750: PVC naming is unusual",
        "id": 2841789752,
        "state": "open",
        "first": "With this PR we make the naming of PVC's configurable through the configmap.\nWith a Configmap as follows:\n```\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: cnpg-controller-manager-config\n  namespace: cnpg-system\ndata:\n  DATA_VOLUME_SUFFIX: '-data2'\n  WAL_ARCHIVE_VOLUME_SUFFIX: \u2018-wal2\u2019\n```\nthe PVC's will be named {pod-name}-data2 and {pod-name}-wal2 accordingly.\nWithout these values set in teh configmap the operator stays with the original naming:\n(e.a. {pod-name} for data and {pod-name}-wal for WAL).\nAs I went trhough the code I noticed that I needed to change a lot of hardcoded -wal settings, which is fixed in the first commit.",
        "messages": "With this PR we make the naming of PVC's configurable through the configmap.\nWith a Configmap as follows:\n```\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: cnpg-controller-manager-config\n  namespace: cnpg-system\ndata:\n  DATA_VOLUME_SUFFIX: '-data2'\n  WAL_ARCHIVE_VOLUME_SUFFIX: \u2018-wal2\u2019\n```\nthe PVC's will be named {pod-name}-data2 and {pod-name}-wal2 accordingly.\nWithout these values set in teh configmap the operator stays with the original naming:\n(e.a. {pod-name} for data and {pod-name}-wal for WAL).\nAs I went trhough the code I noticed that I needed to change a lot of hardcoded -wal settings, which is fixed in the first commit.If @gbartolini or someone else of the dev team could at least approve the direction, we can go proceed with what we currently have and resolve this customer issue internally ahead of a new release. We could start now.\n---\nBy the way, I don;t think we need to backport this one.\r\nI implemented it as a feature, and I would therefore expect this to be part of the next release, not being backported.\n---\nNice...\n---\nLooking good @sebasmannem ! LGTM!\n---\n@sebasmannem Thanks for taking the time to write a patch for CloudNativePG.\r\nHere are a couple of points to consider:\r\n- The naming convention is not unusual. We have followed a clear and consistent approach, applied to both WAL and tablespaces. Specifically, there is a storage section for the main volume and a `walStorage` section for WALs.\r\n- I personally struggle to see the benefits of this patch beyond your specific and, so far, isolated issue, especially given the potential disruption it could cause to existing deployments (we need an E2E test for this). The PVC name should not be a concern when its identity is guaranteed within the namespace.\r\n- This PR introduces additional complexity requiring ongoing maintenance without a clear justification.\r\nUnless proven otherwise, the current behaviour of CloudNativePG\u2019s custom controller is perfectly valid, as the PVC is correctly identified by the triplet: type, namespace, and name.\r\nHowever, I would like to hear the opinion of other maintainers too."
    },
    {
        "title": "feat: move the container build process to bake",
        "id": 2838598137,
        "state": "open",
        "first": "Closes #6804",
        "messages": "Closes #6804"
    },
    {
        "title": "[Bug]: Sart building operator images using bake",
        "id": 2838213799,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWe're currently using plain docker build and Dockerfile to build the container images for the operator, we should start using bake which will attach the attestation with sboms and everything in one step without adding more actions or process in the middle.\nWe should also add the signature of these images just like we do for postgres containers\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWe're currently using plain docker build and Dockerfile to build the container images for the operator, we should start using bake which will attach the attestation with sboms and everything in one step without adding more actions or process in the middle.\nWe should also add the signature of these images just like we do for postgres containers\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "fix(deps): update github.com/cloudnative-pg/machinery digest to d15e1d1 (release-1.24)",
        "id": 2838211453,
        "state": "open",
        "first": "This PR contains the following updates:\n| Package | Type | Update | Change |\n|---|---|---|---|\n| [github.com/cloudnative-pg/machinery](https://redirect.github.com/cloudnative-pg/machinery) | require | digest | `95c37fe` -> `d15e1d1` |\n---\n### Configuration\n\ud83d\udcc5 **Schedule**: Branch creation - At any time (no schedule defined), Automerge - At any time (no schedule defined).\n\ud83d\udea6 **Automerge**: Disabled by config. Please merge this manually once you are satisfied.\n\u267b **Rebasing**: Never, or you tick the rebase/retry checkbox.\n\ud83d\udd15 **Ignore**: Close this PR and you won't be reminded about this update again.\n---\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n---\nThis PR was generated by [Mend Renovate](https://mend.io/renovate/). View the [repository job log](https://developer.mend.io/github/cloudnative-pg/cloudnative-pg).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzOS4xNDUuMCIsInVwZGF0ZWRJblZlciI6IjM5LjE0NS4wIiwidGFyZ2V0QnJhbmNoIjoicmVsZWFzZS0xLjI0IiwibGFiZWxzIjpbImF1dG9tYXRlZCIsImRvIG5vdCBiYWNrcG9ydCIsIm5vLWlzc3VlIl19-->",
        "messages": "This PR contains the following updates:\n| Package | Type | Update | Change |\n|---|---|---|---|\n| [github.com/cloudnative-pg/machinery](https://redirect.github.com/cloudnative-pg/machinery) | require | digest | `95c37fe` -> `d15e1d1` |\n---\n### Configuration\n\ud83d\udcc5 **Schedule**: Branch creation - At any time (no schedule defined), Automerge - At any time (no schedule defined).\n\ud83d\udea6 **Automerge**: Disabled by config. Please merge this manually once you are satisfied.\n\u267b **Rebasing**: Never, or you tick the rebase/retry checkbox.\n\ud83d\udd15 **Ignore**: Close this PR and you won't be reminded about this update again.\n---\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n---\nThis PR was generated by [Mend Renovate](https://mend.io/renovate/). View the [repository job log](https://developer.mend.io/github/cloudnative-pg/cloudnative-pg).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzOS4xNDUuMCIsInVwZGF0ZWRJblZlciI6IjM5LjE0NS4wIiwidGFyZ2V0QnJhbmNoIjoicmVsZWFzZS0xLjI0IiwibGFiZWxzIjpbImF1dG9tYXRlZCIsImRvIG5vdCBiYWNrcG9ydCIsIm5vLWlzc3VlIl19-->"
    },
    {
        "title": "chore: stop building UBI8 operator images",
        "id": 2838205136,
        "state": "open",
        "first": "We were building UBI8 and UBI9, but only one is required and needed for any\r\ncase, this one aims to stop building UBI8 images and from now on, we provide\r\nonly UBI9 images.\r\nCloses #6801",
        "messages": "We were building UBI8 and UBI9, but only one is required and needed for any\r\ncase, this one aims to stop building UBI8 images and from now on, we provide\r\nonly UBI9 images.\r\nCloses #6801/test d=push tl=4"
    },
    {
        "title": "[Bug]: Stop building UBI8 images",
        "id": 2838160778,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nThese images are really old and no one should be using them, UBI9 is out there and there's no problems on using it, this will reduce the amount of images we build\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nThese images are really old and no one should be using them, UBI9 is out there and there's no problems on using it, this will reduce the amount of images we build\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "feat: allow using '-r' service for Pooler",
        "id": 2837835524,
        "state": "open",
        "first": "",
        "messages": "I have this running on a staging cluster with no issues on application workers so far.\r\n```sh\r\npgbouncer@pooler-r-548bb64446-qzm8d:/$ cat /controller/configs/pgbouncer.ini\r\n[databases]\r\n* = host=timescaledb-r\r\n```"
    },
    {
        "title": "fix(deps): update github.com/cloudnative-pg/barman-cloud digest to 3c69be7 (release-1.22)",
        "id": 2837793238,
        "state": "open",
        "first": "This PR contains the following updates:\n| Package | Type | Update | Change |\n|---|---|---|---|\n| [github.com/cloudnative-pg/barman-cloud](https://redirect.github.com/cloudnative-pg/barman-cloud) | require | digest | `c147262` -> `3c69be7` |\n---\n### Configuration\n\ud83d\udcc5 **Schedule**: Branch creation - At any time (no schedule defined), Automerge - At any time (no schedule defined).\n\ud83d\udea6 **Automerge**: Disabled by config. Please merge this manually once you are satisfied.\n\u267b **Rebasing**: Never, or you tick the rebase/retry checkbox.\n\ud83d\udd15 **Ignore**: Close this PR and you won't be reminded about this update again.\n---\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n---\nThis PR was generated by [Mend Renovate](https://mend.io/renovate/). View the [repository job log](https://developer.mend.io/github/cloudnative-pg/cloudnative-pg).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzOS4xNDUuMCIsInVwZGF0ZWRJblZlciI6IjM5LjE0NS4wIiwidGFyZ2V0QnJhbmNoIjoicmVsZWFzZS0xLjIyIiwibGFiZWxzIjpbImF1dG9tYXRlZCIsImRvIG5vdCBiYWNrcG9ydCIsIm5vLWlzc3VlIl19-->",
        "messages": "This PR contains the following updates:\n| Package | Type | Update | Change |\n|---|---|---|---|\n| [github.com/cloudnative-pg/barman-cloud](https://redirect.github.com/cloudnative-pg/barman-cloud) | require | digest | `c147262` -> `3c69be7` |\n---\n### Configuration\n\ud83d\udcc5 **Schedule**: Branch creation - At any time (no schedule defined), Automerge - At any time (no schedule defined).\n\ud83d\udea6 **Automerge**: Disabled by config. Please merge this manually once you are satisfied.\n\u267b **Rebasing**: Never, or you tick the rebase/retry checkbox.\n\ud83d\udd15 **Ignore**: Close this PR and you won't be reminded about this update again.\n---\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n---\nThis PR was generated by [Mend Renovate](https://mend.io/renovate/). View the [repository job log](https://developer.mend.io/github/cloudnative-pg/cloudnative-pg).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzOS4xNDUuMCIsInVwZGF0ZWRJblZlciI6IjM5LjE0NS4wIiwidGFyZ2V0QnJhbmNoIjoicmVsZWFzZS0xLjIyIiwibGFiZWxzIjpbImF1dG9tYXRlZCIsImRvIG5vdCBiYWNrcG9ydCIsIm5vLWlzc3VlIl19-->"
    },
    {
        "title": "fix(deps): update github.com/cloudnative-pg/barman-cloud digest to 3c69be7 (main)",
        "id": 2837792909,
        "state": "open",
        "first": "This PR contains the following updates:\n| Package | Type | Update | Change |\n|---|---|---|---|\n| [github.com/cloudnative-pg/barman-cloud](https://redirect.github.com/cloudnative-pg/barman-cloud) | require | digest | `c147262` -> `3c69be7` |\n---\n### Configuration\n\ud83d\udcc5 **Schedule**: Branch creation - At any time (no schedule defined), Automerge - At any time (no schedule defined).\n\ud83d\udea6 **Automerge**: Disabled by config. Please merge this manually once you are satisfied.\n\u267b **Rebasing**: Never, or you tick the rebase/retry checkbox.\n\ud83d\udd15 **Ignore**: Close this PR and you won't be reminded about this update again.\n---\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n---\nThis PR was generated by [Mend Renovate](https://mend.io/renovate/). View the [repository job log](https://developer.mend.io/github/cloudnative-pg/cloudnative-pg).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzOS4xNDUuMCIsInVwZGF0ZWRJblZlciI6IjM5LjE0NS4wIiwidGFyZ2V0QnJhbmNoIjoibWFpbiIsImxhYmVscyI6WyJhdXRvbWF0ZWQiLCJkbyBub3QgYmFja3BvcnQiLCJuby1pc3N1ZSJdfQ==-->",
        "messages": "This PR contains the following updates:\n| Package | Type | Update | Change |\n|---|---|---|---|\n| [github.com/cloudnative-pg/barman-cloud](https://redirect.github.com/cloudnative-pg/barman-cloud) | require | digest | `c147262` -> `3c69be7` |\n---\n### Configuration\n\ud83d\udcc5 **Schedule**: Branch creation - At any time (no schedule defined), Automerge - At any time (no schedule defined).\n\ud83d\udea6 **Automerge**: Disabled by config. Please merge this manually once you are satisfied.\n\u267b **Rebasing**: Never, or you tick the rebase/retry checkbox.\n\ud83d\udd15 **Ignore**: Close this PR and you won't be reminded about this update again.\n---\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n---\nThis PR was generated by [Mend Renovate](https://mend.io/renovate/). View the [repository job log](https://developer.mend.io/github/cloudnative-pg/cloudnative-pg).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzOS4xNDUuMCIsInVwZGF0ZWRJblZlciI6IjM5LjE0NS4wIiwidGFyZ2V0QnJhbmNoIjoibWFpbiIsImxhYmVscyI6WyJhdXRvbWF0ZWQiLCJkbyBub3QgYmFja3BvcnQiLCJuby1pc3N1ZSJdfQ==-->"
    },
    {
        "title": "chore: Adding pg_catalog in queries of tablespaces folder",
        "id": 2837529925,
        "state": "open",
        "first": "Linked to #6622 \r\n@gbartolini Don't know yet how to create a PR from my branch directly in your dev/pg_catalog in a clean way. :/",
        "messages": "Linked to #6622 \r\n@gbartolini Don't know yet how to create a PR from my branch directly in your dev/pg_catalog in a clean way. :/"
    },
    {
        "title": "fix: reject if no size is used in shared_buffers (2nd PR)",
        "id": 2837504674,
        "state": "open",
        "first": "New PR after mistake. Fix #6657 \r\nFollowing this closed one : https://github.com/cloudnative-pg/cloudnative-pg/pull/6675.\r\n(So sorry, for this second PR ...)",
        "messages": "New PR after mistake. Fix #6657 \r\nFollowing this closed one : https://github.com/cloudnative-pg/cloudnative-pg/pull/6675.\r\n(So sorry, for this second PR ...)Trying to get all tests OK.\r\nAs we reject when no unit is used, I added `MB` in the tests where values haven't got unit.\r\nDoes it seems correct to you, the way I change those tests ?"
    },
    {
        "title": "fix(deps): update github.com/cloudnative-pg/machinery digest to b713958 (main)",
        "id": 2837441989,
        "state": "open",
        "first": "This PR contains the following updates:\n| Package | Type | Update | Change |\n|---|---|---|---|\n| [github.com/cloudnative-pg/machinery](https://redirect.github.com/cloudnative-pg/machinery) | require | digest | `95c37fe` -> `b713958` |\n---\n### Configuration\n\ud83d\udcc5 **Schedule**: Branch creation - At any time (no schedule defined), Automerge - At any time (no schedule defined).\n\ud83d\udea6 **Automerge**: Disabled by config. Please merge this manually once you are satisfied.\n\u267b **Rebasing**: Never, or you tick the rebase/retry checkbox.\n\ud83d\udd15 **Ignore**: Close this PR and you won't be reminded about this update again.\n---\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n---\nThis PR was generated by [Mend Renovate](https://mend.io/renovate/). View the [repository job log](https://developer.mend.io/github/cloudnative-pg/cloudnative-pg).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzOS4xNDUuMCIsInVwZGF0ZWRJblZlciI6IjM5LjE0NS4wIiwidGFyZ2V0QnJhbmNoIjoibWFpbiIsImxhYmVscyI6WyJhdXRvbWF0ZWQiLCJkbyBub3QgYmFja3BvcnQiLCJuby1pc3N1ZSJdfQ==-->",
        "messages": "This PR contains the following updates:\n| Package | Type | Update | Change |\n|---|---|---|---|\n| [github.com/cloudnative-pg/machinery](https://redirect.github.com/cloudnative-pg/machinery) | require | digest | `95c37fe` -> `b713958` |\n---\n### Configuration\n\ud83d\udcc5 **Schedule**: Branch creation - At any time (no schedule defined), Automerge - At any time (no schedule defined).\n\ud83d\udea6 **Automerge**: Disabled by config. Please merge this manually once you are satisfied.\n\u267b **Rebasing**: Never, or you tick the rebase/retry checkbox.\n\ud83d\udd15 **Ignore**: Close this PR and you won't be reminded about this update again.\n---\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n---\nThis PR was generated by [Mend Renovate](https://mend.io/renovate/). View the [repository job log](https://developer.mend.io/github/cloudnative-pg/cloudnative-pg).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzOS4xNDUuMCIsInVwZGF0ZWRJblZlciI6IjM5LjE0NS4wIiwidGFyZ2V0QnJhbmNoIjoibWFpbiIsImxhYmVscyI6WyJhdXRvbWF0ZWQiLCJkbyBub3QgYmFja3BvcnQiLCJuby1pc3N1ZSJdfQ==-->"
    },
    {
        "title": "feat: 5299 check for nodes cordoned by karpenter",
        "id": 2836988232,
        "state": "open",
        "first": "This is my first attempt at go / operator code, so forgive me if I'm way off here.\r\nThis is an (I think unfinished) attempt at #5299 I say unfinished because `node.Spec.Unschedulable` is used a lot, and I'm unsure on where cnpg wants to differentiate between `node.Spec.Unschedulable` and having a `karpenter.sh/disruption` taint. Guidance would be appreciated here. \r\nI would also like appreciate any advice on how to run the end to end tests. I can neither run them locally via kind nor in the devcontainer via any of the options.",
        "messages": "This is my first attempt at go / operator code, so forgive me if I'm way off here.\r\nThis is an (I think unfinished) attempt at #5299 I say unfinished because `node.Spec.Unschedulable` is used a lot, and I'm unsure on where cnpg wants to differentiate between `node.Spec.Unschedulable` and having a `karpenter.sh/disruption` taint. Guidance would be appreciated here. \r\nI would also like appreciate any advice on how to run the end to end tests. I can neither run them locally via kind nor in the devcontainer via any of the options."
    },
    {
        "title": "fix: broken url in Image Catalog",
        "id": 2835275272,
        "state": "no reaction",
        "first": "",
        "messages": ""
    },
    {
        "title": "[Feature]: Support setting a default storage class",
        "id": 2834587459,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThis would avoid potential accidental misconfiguration, where the default storage class for a k8s cluster might not be appropriate for cnpg clusters to use.\n### Describe the solution you'd like\nAn additional configuration option to the operator which sets the default.\n### Describe alternatives you've considered\nManual audits (which isn't really an alternative).\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThis would avoid potential accidental misconfiguration, where the default storage class for a k8s cluster might not be appropriate for cnpg clusters to use.\n### Describe the solution you'd like\nAn additional configuration option to the operator which sets the default.\n### Describe alternatives you've considered\nManual audits (which isn't really an alternative).\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct@aragilar have you consider that this should be controlled using https://kyverno.io/ ? this sounds more like a policy to be applied than an \"option\", since we rely on the Kubernetes default storage class\n---\nOh cool, thanks, I didn't know about kyverno, that would do what I want to do."
    },
    {
        "title": "Plugin: psql with impersonation",
        "id": 2832454840,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIn this big world, we don't always gave permissions directly to users, for security and so on. Instead, permissions were given to specific groups/users where impersonation is required to perform certain tasks, e.g.: command execution in pods. The plugin feature of `psql` should support impersonation, and maybe other options from `kubectl` flags\n### Describe the solution you'd like\nSupport impersonation in `psql` from the plugin, e.g.: `kubectl cnpg psql some-db --as foo:user:foo`\n### Describe alternatives you've considered\n`kubectl exec` still works\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIn this big world, we don't always gave permissions directly to users, for security and so on. Instead, permissions were given to specific groups/users where impersonation is required to perform certain tasks, e.g.: command execution in pods. The plugin feature of `psql` should support impersonation, and maybe other options from `kubectl` flags\n### Describe the solution you'd like\nSupport impersonation in `psql` from the plugin, e.g.: `kubectl cnpg psql some-db --as foo:user:foo`\n### Describe alternatives you've considered\n`kubectl exec` still works\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductHello,\nCan you make a more clear example? because I don't get what you mean by this, and probably there's a confusion how the command in the plugin works.\n* It will use `kubectl exec` under the hood, meaning that the user will requires that permission anyway, in that case, there's no security involved here\n* To access psql it will be using the local socket, meaning that the user with `kubectl exec` permission will have access to this socket meaning it can login as any user, in theory\n* To have an external password from other user the secret with the password of that user should be accessible by the the plugin, ergo, it will be accessible by the user, meaning that we're not solving any security issue.\nDid I get a bit more clear the idea your exposing ?\nRegards,\n---\nI understand that it uses `kubectl exec`, but using plugin to run `psql` command seems better(?) to do querying instead of `kubectl exec`.\nThe impersonation is so that I could act as admin while using my own user. An example is, when I perform `kubectl cnpg status` without impersonating admin role, I cannot see the whole thing, e.g.: replication role or cnpg version. But, when I use `kubectl cnpg status --as foo:user:foo-admin` I get all the results because the `foo-admin` has the privilege.\n---\nLooks like it was mentioned here https://github.com/cloudnative-pg/cloudnative-pg/pull/6257#issuecomment-2516966471\n---\n@saifulmuhajir so to be clear, you're talking about the impersonation provided by Kubernetes and not PostgreSQL right?\n---\nYes. I forgot to say that from the beginning, impersonation provided by Kubernetes"
    },
    {
        "title": "[Bug]: User database owner password is reset during rolling update (initdb)",
        "id": 2830758985,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\narnold@aov.de\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nHelm\n### What happened?\nIs it expected that the password of the user `appuser` is reset to the value from the secret `appuser-initial-credentials` during a rolling update to a newer postgres minor version?\nIf so, is there any way to only define an **initial** password and to not reset it during updates?\n### Cluster resource\n```shell\nkubectl get cluster -n postgres db-1  -o yaml | yq '.spec.bootstrap':\n{\n  \"initdb\": {\n    \"database\": \"appdb\",\n    \"encoding\": \"UTF8\",\n    \"localeCType\": \"C\",\n    \"localeCollate\": \"C\",\n    \"owner\": \"appuser\",\n    \"postInitSQL\": [\n      \"ALTER USER appuser WITH SUPERUSER\"\n    ],\n    \"secret\": {\n      \"name\": \"appuser-initial-credentials\"\n    }\n  }\n}\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\narnold@aov.de\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nHelm\n### What happened?\nIs it expected that the password of the user `appuser` is reset to the value from the secret `appuser-initial-credentials` during a rolling update to a newer postgres minor version?\nIf so, is there any way to only define an **initial** password and to not reset it during updates?\n### Cluster resource\n```shell\nkubectl get cluster -n postgres db-1  -o yaml | yq '.spec.bootstrap':\n{\n  \"initdb\": {\n    \"database\": \"appdb\",\n    \"encoding\": \"UTF8\",\n    \"localeCType\": \"C\",\n    \"localeCollate\": \"C\",\n    \"owner\": \"appuser\",\n    \"postInitSQL\": [\n      \"ALTER USER appuser WITH SUPERUSER\"\n    ],\n    \"secret\": {\n      \"name\": \"appuser-initial-credentials\"\n    }\n  }\n}\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductCan you give the YAML file ?\n---\n@Nils98Ar you can try doing something like this to initialize an empty pgcluster without the app db.\nThen you can create create the app users via a secret for example and grant the specific privileges you want\n```\n  bootstrap:\n    initdb:\n      localeCollate: 'en_US.utf8'\n      localeCType: 'en_US.utf8'\n      database: postgres\n      owner: postgres\n      secret:\n        name: postgres-superuser-secret\n      dataChecksums: true\n      encoding: 'UTF8' #default\n      postInitTemplateSQLRefs:\n        secretRefs:\n        - name: secret-init-roles\n          key: create-initroles.sql\n  enableSuperuserAccess: true\n  superuserSecret:\n    name: postgres-superuser-secret\n```\n---\n@pchovelon\n> Can you give the YAML file ?\nDo you need more than the bootstrap part?\n```\nkubectl get cluster -n postgres db-1  -o yaml | yq '.spec.bootstrap':\n{\n  \"initdb\": {\n    \"database\": \"appdb\",\n    \"encoding\": \"UTF8\",\n    \"localeCType\": \"C\",\n    \"localeCollate\": \"C\",\n    \"owner\": \"appuser\",\n    \"postInitSQL\": [\n      \"ALTER USER appuser WITH SUPERUSER\"\n    ],\n    \"secret\": {\n      \"name\": \"appuser-initial-credentials\"\n    }\n  }\n}\n```\n@mkris98 Okay `postInitTemplateSQLRefs` could work. Or simply `postInitApplicationSQL`?\nBut if the user password is reset to the value from `initdb.secret` during upgrades, isn't the `postInitTemplateSQLRefs` and `initdb.postInitApplicationSQL` applied during updates again as well? Or only during the initial deployment?\n---\n@Nils98Ar You can use postInitTemplateSQL f.e.\nIf you want to provision the application database with the user and you want to change the password for the user, then you can patch the password in the secret and trigger a reload of the configuration in the cluster.\nRegarding postinitsql / postinitemplatesqls.\nThe sql commands will only execute once right after the creation of the pgcluster.\n---\n@mkris98\nSo `.spec.bootstrap.initdb.secret` is not usable for initial passwords as the password is always reset/reconfigured during rolling updates? The users of the DBs do not have access to Kubernetes and change the passwords via psql / pgAdmin in our case. So editing the secret is not an option for them.\nIf so we should probably switch to the postinitsql / postinitemplatesql approach. And the description of \"secret\" in the API reference should propably be improved (\"initial credential\")? https://cloudnative-pg.io/documentation/1.25/cloudnative-pg.v1/#postgresql-cnpg-io-v1-BootstrapInitDB"
    },
    {
        "title": "[Bug]: If 3 volume snapshots fail in a row, the instance manager stops functioning (connection pool exhausted)",
        "id": 2830541794,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njeffm@gmail.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhen 3 scheduled volume snapshots fail in a row. The super user connection pool becomes exhausted in the instance manger. This causes all endpoints except `/healthz` to fail. This causes a near total failure of CNPG.\n**Impact:**\n- All scheduled backups start to fail (due to endpoints hanging)\n- You run out of WAL space\n- Metrics are no longer collected\n- Hibernate fails\n- `cnpg status` hangs\n- All HTTP endpoints except /healthz hang\n- Instance manager can no longer be updated in-place\nDepending on when you catch it, the real reason for the volume snapshot failure will be buried by `context cancelled: deadline exceeded` (because only the first 2-3 attempts can start a backup, the other ones hang getting the cluster status).\nThis PR has the beginnings of a fix, but there's more to be done:\nhttps://github.com/jmealo/cloudnative-pg/pull/1\n# Big shout outs to the Gophers in the Go Slack:\n## @farhaven and @nbraun\n### Cluster resource\n```shell\nNot relevant\n```\n### Relevant log output\n```shell\npostgres=# SELECT pid, age(clock_timestamp(), query_start), usename, query \nFROM pg_stat_activity \nWHERE query != '<IDLE>' AND query NOT ILIKE '%pg_stat_activity%' \nORDER BY query_start desc;\n pid  |       age       | usename  |                      query                       \n------+-----------------+----------+--------------------------------------------------\n   26 |                 |          | \n   27 |                 |          | \n   28 |                 |          | \n  204 |                 |          | \n 3886 | 00:03:55.430776 | postgres | SELECT pg_backup_start(label => $1, fast => $2);\n 2518 | 01:04:35.711006 | postgres | SELECT pg_backup_start(label => $1, fast => $2);\n 1112 | 02:06:57.669102 | postgres | SELECT pg_backup_start(label => $1, fast => $2);\n(7 rows)\npostgres=# \n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njeffm@gmail.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhen 3 scheduled volume snapshots fail in a row. The super user connection pool becomes exhausted in the instance manger. This causes all endpoints except `/healthz` to fail. This causes a near total failure of CNPG.\n**Impact:**\n- All scheduled backups start to fail (due to endpoints hanging)\n- You run out of WAL space\n- Metrics are no longer collected\n- Hibernate fails\n- `cnpg status` hangs\n- All HTTP endpoints except /healthz hang\n- Instance manager can no longer be updated in-place\nDepending on when you catch it, the real reason for the volume snapshot failure will be buried by `context cancelled: deadline exceeded` (because only the first 2-3 attempts can start a backup, the other ones hang getting the cluster status).\nThis PR has the beginnings of a fix, but there's more to be done:\nhttps://github.com/jmealo/cloudnative-pg/pull/1\n# Big shout outs to the Gophers in the Go Slack:\n## @farhaven and @nbraun\n### Cluster resource\n```shell\nNot relevant\n```\n### Relevant log output\n```shell\npostgres=# SELECT pid, age(clock_timestamp(), query_start), usename, query \nFROM pg_stat_activity \nWHERE query != '<IDLE>' AND query NOT ILIKE '%pg_stat_activity%' \nORDER BY query_start desc;\n pid  |       age       | usename  |                      query                       \n------+-----------------+----------+--------------------------------------------------\n   26 |                 |          | \n   27 |                 |          | \n   28 |                 |          | \n  204 |                 |          | \n 3886 | 00:03:55.430776 | postgres | SELECT pg_backup_start(label => $1, fast => $2);\n 2518 | 01:04:35.711006 | postgres | SELECT pg_backup_start(label => $1, fast => $2);\n 1112 | 02:06:57.669102 | postgres | SELECT pg_backup_start(label => $1, fast => $2);\n(7 rows)\npostgres=# \n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductIs there a reason that the connection is held until the backup completes (never being released if it fails)? The Postgres docs make it sound like this is unnecessary.\n---\nThank you for reporting this issue; this is important and needs to be fixed.\n> Is there a reason that the connection is held until the backup completes (never being released if it fails)? The Postgres docs make it sound like this is unnecessary.\nConcurrent backups are aborted if the controlling connection is terminated."
    },
    {
        "title": "[Docs]: Add development documentation to use `client-gen` tool",
        "id": 2830250336,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThe PR https://github.com/cloudnative-pg/cloudnative-pg/pull/6695 was merged and now we should add the documentation explaining how people can use `client-gen` to generate the clients tarting from the CRDs of CNPG.\nThis will help user to extend the ability to use our CRDs\n### Describe what additions need to be done to the documentation\nWe should create a section in the documentation to help explain this or at least in the source code in the contribution section\n### Describe what pages need to change in the documentation, if any\n_No response_\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThe PR https://github.com/cloudnative-pg/cloudnative-pg/pull/6695 was merged and now we should add the documentation explaining how people can use `client-gen` to generate the clients tarting from the CRDs of CNPG.\nThis will help user to extend the ability to use our CRDs\n### Describe what additions need to be done to the documentation\nWe should create a section in the documentation to help explain this or at least in the source code in the contribution section\n### Describe what pages need to change in the documentation, if any\n_No response_\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductHello @solidDoWant \nYou may want to take a look into this issue\nRegards!\n---\nThanks! Couple of questions:\n* Would a page just need to be added [here](https://github.com/cloudnative-pg/cloudnative-pg/tree/main/docs/src)?\n* As an alternative to docs on how to generate a client, would the project be interested in maintaining the (100% generated) client? This is what I've seen most other projects do."
    },
    {
        "title": "Backport failure for pull request 6695",
        "id": 2829963315,
        "state": "open",
        "first": "### Context\nAutomatically backport failure for pull request 6695\nPull request: https://github.com/cloudnative-pg/cloudnative-pg/pull/6695\nCommit: https://github.com/cloudnative-pg/cloudnative-pg/commit/a725d1f2cc57b09bd3df4c838341ab29b3f63c9b\nWorkflow: https://github.com/cloudnative-pg/cloudnative-pg/actions/runs/13134665520\nTo solve the ticket, open the workflow link above, and for each failed release branch check the following:\n1. Whether the commit should be `cherry-pick`(ed) to this release branch, otherwise skip this release branch\n2. If yes, `cherry-pick` the commit manually and push it to the release branch. You may need to resolve the\nconflicts and issue `cherry-pick --continue` again. Also, a dependent commit missing may be causing the\nfailure, so if that's the case you may need to `cherry-pick` the dependent commit first.",
        "messages": "### Context\nAutomatically backport failure for pull request 6695\nPull request: https://github.com/cloudnative-pg/cloudnative-pg/pull/6695\nCommit: https://github.com/cloudnative-pg/cloudnative-pg/commit/a725d1f2cc57b09bd3df4c838341ab29b3f63c9b\nWorkflow: https://github.com/cloudnative-pg/cloudnative-pg/actions/runs/13134665520\nTo solve the ticket, open the workflow link above, and for each failed release branch check the following:\n1. Whether the commit should be `cherry-pick`(ed) to this release branch, otherwise skip this release branch\n2. If yes, `cherry-pick` the commit manually and push it to the release branch. You may need to resolve the\nconflicts and issue `cherry-pick --continue` again. Also, a dependent commit missing may be causing the\nfailure, so if that's the case you may need to `cherry-pick` the dependent commit first."
    },
    {
        "title": "fix: cluster name length not validated",
        "id": 2829875935,
        "state": "open",
        "first": "Fix proposal to issue #6753 \r\nHere is a simple fix for the issue #6753.\r\nI lowered the limit to 40 characters. So we have now : \r\n`<cluster name>-<instance number>-snapshot-recovery`\r\nwhich is in numbers : \r\n<40>-<4>-<17> = 40 + 1 + 4 +1 + 17 = 63\r\nSo the instance number will be on 4 digits (from 1 to 9999).\r\nI do not know (yet :) ) if there will be some consequences to limit it to 9999.\r\nFurthermore, it might be a good idea to have a doc page saying that cluster name length must be <= 40.",
        "messages": "Fix proposal to issue #6753 \r\nHere is a simple fix for the issue #6753.\r\nI lowered the limit to 40 characters. So we have now : \r\n`<cluster name>-<instance number>-snapshot-recovery`\r\nwhich is in numbers : \r\n<40>-<4>-<17> = 40 + 1 + 4 +1 + 17 = 63\r\nSo the instance number will be on 4 digits (from 1 to 9999).\r\nI do not know (yet :) ) if there will be some consequences to limit it to 9999.\r\nFurthermore, it might be a good idea to have a doc page saying that cluster name length must be <= 40.Hi @pchovelon \r\nOk, why we need a PR to limit something that is already limit? the problem is people using long names all the time not reading the documentation, 50 characters was previously used because was considered a safe number, because you can read here https://kubernetes.io/docs/concepts/overview/working-with-objects/names/ that you have a limit by the DNS, now, someone decided to use a longer name and we need to lower a limit, probably there's a k8s official function to validate an object name? and even if there's one, the problem is that the value will defer in some environments, imagine this, what if someone has a small cluster name and namespace name, they will be able to use a longer than 40 characters name, so now we avoid people that have that long names?\r\nI think that this should be explained in the documentation rather than adding a hard limit in a webhook\n---\n> Ok, why we need a PR to limit something that is already limit? \r\nAs @solidDoWant pointed, recovery did't work ^^ and by modifying the limit, the label or hostname will be <= to 63 characters when recovering and then will work. As you said in the issue, the solution might be to change the suffix used.\r\n> the problem is people using long names all the time not reading the documentation, 50 characters was previously used because was considered a safe number, because you can read here https://kubernetes.io/docs/concepts/overview/working-with-objects/names/ that you have a limit by the DNS, now, someone decided to use a longer name \r\n> and we need to lower a limit, \r\nYep, because the suffix added in the `name`.\r\n>probably there's a k8s official function to validate an object name? and even if there's one, the problem is that the value will defer in some environments, imagine this, what if someone has a small cluster name and namespace name, they will be able to use a longer than 40 characters name, so now we avoid people that have that long names?\r\nIf I'm not mistaken, namespace is not involve in the issue :thinking:. \r\n> I think that this should be explained in the documentation rather than adding a hard limit in a webhook\r\n:+1:"
    },
    {
        "title": "[Bug]: Cluster name length not validated, causing hidden failure upon restore from backup",
        "id": 2829382280,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nHelm\n### What happened?\nWhen restoring (creating a new cluster) from a volume snapshot backup, cluster will be in \"waiting for primary\" state forever if the cluster name is more than 44 characters long. When creating a new cluster from a volume snapshot backup, the controller will create a `<cluster name>-<instance number>-snapshot-recovery` job. Several fields, such as label values and hostname cannot be more than 63 characters long. Due to the `-snapshot-recovery` suffix, this makes the maximum cluster name length 44 characters.\nThe operator should validate this and either reject a new cluster under these conditions, or put it in an \"error\" state.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  creationTimestamp: \"2025-02-04T07:40:47Z\"\n  generation: 1\n  name: vaultwarden-test-backup-2025-02-04t07-40-36z\n  namespace: default\n  resourceVersion: \"16946\"\n  uid: 8ba77da8-c1dc-481d-bfb3-a337f8b2dac7\nspec:\n  affinity:\n    podAntiAffinityType: preferred\n  bootstrap:\n    recovery:\n      backup:\n        name: vaultwarden-clonedg2zgb\n      database: vaultwarden\n      owner: vaultwarden\n  certificates:\n    clientCASecret: vaultwarden-test-backup-2025-02-04t07-40-36z-client-ca\n    replicationTLSSecret: vaultwarden-test-backup-2025-02-04t07-40-36z-streaming-replica-user\n    serverCASecret: vaultwarden-test-backup-2025-02-04t07-40-36z-serving-cert\n    serverTLSSecret: vaultwarden-test-backup-2025-02-04t07-40-36z-serving-cert\n  enablePDB: true\n  enableSuperuserAccess: false\n  failoverDelay: 0\n  imageName: ghcr.io/cloudnative-pg/postgresql:17.0\n  instances: 1\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n    - key: queries\n      name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: false\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: \"on\"\n      archive_timeout: 5min\n      dynamic_shared_memory_type: posix\n      full_page_writes: \"on\"\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: \"0\"\n      log_rotation_size: \"0\"\n      log_truncate_on_rotation: \"false\"\n      logging_collector: \"on\"\n      max_parallel_workers: \"32\"\n      max_replication_slots: \"32\"\n      max_worker_processes: \"32\"\n      shared_memory_type: mmap\n      shared_preload_libraries: \"\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      wal_keep_size: 512MB\n      wal_level: logical\n      wal_log_hints: \"on\"\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n    pg_hba:\n    - hostssl all all all cert\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: restart\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  resources: {}\n  smartShutdownTimeout: 180\n  startDelay: 3600\n  stopDelay: 1800\n  storage:\n    resizeInUseVolumes: true\n    size: 5Gi\n    storageClass: openebs-zfs\n  switchoverDelay: 3600\nstatus:\n  availableArchitectures:\n  - goArch: amd64\n    hash: 575b8d5080a718a1b1c8e6febcb6ccfde6cf546aa1a253acd7336226494ba784\n  - goArch: arm64\n    hash: bab50cc05e920db8bd118118323ef8003201dd3ba0642bbdee87cfdde1672e3e\n  certificates:\n    clientCASecret: vaultwarden-test-backup-2025-02-04t07-40-36z-client-ca\n    expirations:\n      vaultwarden-test-backup-2025-02-04t07-40-36z-client-ca: 2025-02-04 08:40:45\n        +0000 UTC\n      vaultwarden-test-backup-2025-02-04t07-40-36z-serving-cert: 2025-02-04 08:40:44\n        +0000 UTC\n      vaultwarden-test-backup-2025-02-04t07-40-36z-streaming-replica-user: 2025-02-04\n        08:40:46 +0000 UTC\n    replicationTLSSecret: vaultwarden-test-backup-2025-02-04t07-40-36z-streaming-replica-user\n    serverAltDNSNames:\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-rw\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-rw.default\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-rw.default.svc\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-rw.default.svc.cluster.local\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-r\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-r.default\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-r.default.svc\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-r.default.svc.cluster.local\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-ro\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-ro.default\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-ro.default.svc\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-ro.default.svc.cluster.local\n    serverCASecret: vaultwarden-test-backup-2025-02-04t07-40-36z-serving-cert\n    serverTLSSecret: vaultwarden-test-backup-2025-02-04t07-40-36z-serving-cert\n  cloudNativePGCommitHash: 3f96930d\n  cloudNativePGOperatorHash: 575b8d5080a718a1b1c8e6febcb6ccfde6cf546aa1a253acd7336226494ba784\n  conditions:\n  - lastTransitionTime: \"2025-02-04T07:40:47Z\"\n    message: Cluster Is Not Ready\n    reason: ClusterIsNotReady\n    status: \"False\"\n    type: Ready\n  configMapResourceVersion:\n    metrics:\n      cnpg-default-monitoring: \"631\"\n  danglingPVC:\n  - vaultwarden-test-backup-2025-02-04t07-40-36z-1\n  image: ghcr.io/cloudnative-pg/postgresql:17.0\n  instanceNames:\n  - vaultwarden-test-backup-2025-02-04t07-40-36z-1\n  instances: 1\n  latestGeneratedNode: 1\n  managedRolesStatus: {}\n  phase: Setting up primary\n  phaseReason: Creating primary instance vaultwarden-test-backup-2025-02-04t07-40-36z-1\n  poolerIntegrations:\n    pgBouncerIntegration: {}\n  pvcCount: 1\n  readService: vaultwarden-test-backup-2025-02-04t07-40-36z-r\n  secretsResourceVersion:\n    applicationSecretVersion: \"16907\"\n    clientCaSecretVersion: \"16825\"\n    replicationSecretVersion: \"16899\"\n    serverCaSecretVersion: \"16795\"\n    serverSecretVersion: \"16795\"\n  switchReplicaClusterStatus: {}\n  targetPrimary: vaultwarden-test-backup-2025-02-04t07-40-36z-1\n  targetPrimaryTimestamp: \"2025-02-04T07:40:47.516901Z\"\n  topology:\n    successfullyExtracted: true\n  writeService: vaultwarden-test-backup-2025-02-04t07-40-36z-rw\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.375557304Z\",\"logger\":\"cluster-resource\",\"msg\":\"default\",\"version\":\"v1\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.39061577Z\",\"logger\":\"cluster-resource\",\"msg\":\"validate create\",\"version\":\"v1\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.434786509Z\",\"msg\":\"no orphan PVCs found, skipping the restored cluster reconciliation\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.435096865Z\",\"msg\":\"CA certificate is expiring or is already expired\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\",\"secret\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-serving-cert\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.435863491Z\",\"msg\":\"CA certificate is expiring or is already expired\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\",\"secret\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-client-ca\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.445071104Z\",\"msg\":\"creating service\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\",\"serviceName\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-r\",\"updateStrategy\":\"patch\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.461081085Z\",\"msg\":\"creating service\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\",\"serviceName\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-ro\",\"updateStrategy\":\"patch\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.476702642Z\",\"msg\":\"creating service\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\",\"serviceName\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-rw\",\"updateStrategy\":\"patch\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.765028259Z\",\"msg\":\"Creating new Job\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-1-snapshot-recovery\",\"primary\":true}\n{\"level\":\"error\",\"ts\":\"2025-02-04T07:12:55.772143935Z\",\"msg\":\"Unable to create job\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\",\"job\":{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-1-snapshot-recovery\"},\"error\":\"Job.batch \\\"vaultwarden-test-backup-2025-02-04t07-12-39z-1-snapshot-recovery\\\" is invalid: [spec.template.labels: Invalid value: \\\"vaultwarden-test-backup-2025-02-04t07-12-39z-1-snapshot-recovery\\\": must be no more than 63 characters, spec.template.spec.hostname: Invalid value: \\\"vaultwarden-test-backup-2025-02-04t07-12-39z-1-snapshot-recovery\\\": must be no more than 63 characters]\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/controller.(*ClusterReconciler).createPrimaryInstance\\n\\tinternal/controller/cluster_create.go:1178\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/controller.(*ClusterReconciler).reconcilePods\\n\\tinternal/controller/cluster_controller.go:861\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/controller.(*ClusterReconciler).reconcileResources\\n\\tinternal/controller/cluster_controller.go:701\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/controller.(*ClusterReconciler).reconcile\\n\\tinternal/controller/cluster_controller.go:482\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/controller.(*ClusterReconciler).Reconcile\\n\\tinternal/controller/cluster_controller.go:188\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Reconcile\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:116\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:303\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:263\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:224\"}\n{\"level\":\"error\",\"ts\":\"2025-02-04T07:12:55.772613155Z\",\"msg\":\"Reconciler error\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\",\"error\":\"Job.batch \\\"vaultwarden-test-backup-2025-02-04t07-12-39z-1-snapshot-recovery\\\" is invalid: [spec.template.labels: Invalid value: \\\"vaultwarden-test-backup-2025-02-04t07-12-39z-1-snapshot-recovery\\\": must be no more than 63 characters, spec.template.spec.hostname: Invalid value: \\\"vaultwarden-test-backup-2025-02-04t07-12-39z-1-snapshot-recovery\\\": must be no more than 63 characters]\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:316\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:263\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:224\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.794783336Z\",\"msg\":\"CA certificate is expiring or is already expired\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"f3e93cab-067d-4271-93ea-352ea09c9897\",\"secret\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-serving-cert\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.795473025Z\",\"msg\":\"CA certificate is expiring or is already expired\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"f3e93cab-067d-4271-93ea-352ea09c9897\",\"secret\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-client-ca\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.868894962Z\",\"msg\":\"Selected PVC is not ready yet, waiting for 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"f3e93cab-067d-4271-93ea-352ea09c9897\",\"pvc\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-1\",\"status\":\"initializing\",\"instance\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-1\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.889551489Z\",\"msg\":\"CA certificate is expiring or is already expired\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"fc92e79a-2c6a-4c40-be1a-6c2c4835153e\",\"secret\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-serving-cert\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.890095552Z\",\"msg\":\"CA certificate is expiring or is already expired\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"fc92e79a-2c6a-4c40-be1a-6c2c4835153e\",\"secret\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-client-ca\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.966608509Z\",\"msg\":\"Selected PVC is not ready yet, waiting for 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"fc92e79a-2c6a-4c40-be1a-6c2c4835153e\",\"pvc\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-1\",\"status\":\"initializing\",\"instance\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-1\"}\n(The last three lines are repeated. The cert warning can be ignored - I have my cert duration very low, and they logged lines are unrelated.)\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nHelm\n### What happened?\nWhen restoring (creating a new cluster) from a volume snapshot backup, cluster will be in \"waiting for primary\" state forever if the cluster name is more than 44 characters long. When creating a new cluster from a volume snapshot backup, the controller will create a `<cluster name>-<instance number>-snapshot-recovery` job. Several fields, such as label values and hostname cannot be more than 63 characters long. Due to the `-snapshot-recovery` suffix, this makes the maximum cluster name length 44 characters.\nThe operator should validate this and either reject a new cluster under these conditions, or put it in an \"error\" state.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  creationTimestamp: \"2025-02-04T07:40:47Z\"\n  generation: 1\n  name: vaultwarden-test-backup-2025-02-04t07-40-36z\n  namespace: default\n  resourceVersion: \"16946\"\n  uid: 8ba77da8-c1dc-481d-bfb3-a337f8b2dac7\nspec:\n  affinity:\n    podAntiAffinityType: preferred\n  bootstrap:\n    recovery:\n      backup:\n        name: vaultwarden-clonedg2zgb\n      database: vaultwarden\n      owner: vaultwarden\n  certificates:\n    clientCASecret: vaultwarden-test-backup-2025-02-04t07-40-36z-client-ca\n    replicationTLSSecret: vaultwarden-test-backup-2025-02-04t07-40-36z-streaming-replica-user\n    serverCASecret: vaultwarden-test-backup-2025-02-04t07-40-36z-serving-cert\n    serverTLSSecret: vaultwarden-test-backup-2025-02-04t07-40-36z-serving-cert\n  enablePDB: true\n  enableSuperuserAccess: false\n  failoverDelay: 0\n  imageName: ghcr.io/cloudnative-pg/postgresql:17.0\n  instances: 1\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n    - key: queries\n      name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: false\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: \"on\"\n      archive_timeout: 5min\n      dynamic_shared_memory_type: posix\n      full_page_writes: \"on\"\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: \"0\"\n      log_rotation_size: \"0\"\n      log_truncate_on_rotation: \"false\"\n      logging_collector: \"on\"\n      max_parallel_workers: \"32\"\n      max_replication_slots: \"32\"\n      max_worker_processes: \"32\"\n      shared_memory_type: mmap\n      shared_preload_libraries: \"\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      wal_keep_size: 512MB\n      wal_level: logical\n      wal_log_hints: \"on\"\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n    pg_hba:\n    - hostssl all all all cert\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: restart\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  resources: {}\n  smartShutdownTimeout: 180\n  startDelay: 3600\n  stopDelay: 1800\n  storage:\n    resizeInUseVolumes: true\n    size: 5Gi\n    storageClass: openebs-zfs\n  switchoverDelay: 3600\nstatus:\n  availableArchitectures:\n  - goArch: amd64\n    hash: 575b8d5080a718a1b1c8e6febcb6ccfde6cf546aa1a253acd7336226494ba784\n  - goArch: arm64\n    hash: bab50cc05e920db8bd118118323ef8003201dd3ba0642bbdee87cfdde1672e3e\n  certificates:\n    clientCASecret: vaultwarden-test-backup-2025-02-04t07-40-36z-client-ca\n    expirations:\n      vaultwarden-test-backup-2025-02-04t07-40-36z-client-ca: 2025-02-04 08:40:45\n        +0000 UTC\n      vaultwarden-test-backup-2025-02-04t07-40-36z-serving-cert: 2025-02-04 08:40:44\n        +0000 UTC\n      vaultwarden-test-backup-2025-02-04t07-40-36z-streaming-replica-user: 2025-02-04\n        08:40:46 +0000 UTC\n    replicationTLSSecret: vaultwarden-test-backup-2025-02-04t07-40-36z-streaming-replica-user\n    serverAltDNSNames:\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-rw\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-rw.default\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-rw.default.svc\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-rw.default.svc.cluster.local\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-r\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-r.default\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-r.default.svc\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-r.default.svc.cluster.local\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-ro\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-ro.default\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-ro.default.svc\n    - vaultwarden-test-backup-2025-02-04t07-40-36z-ro.default.svc.cluster.local\n    serverCASecret: vaultwarden-test-backup-2025-02-04t07-40-36z-serving-cert\n    serverTLSSecret: vaultwarden-test-backup-2025-02-04t07-40-36z-serving-cert\n  cloudNativePGCommitHash: 3f96930d\n  cloudNativePGOperatorHash: 575b8d5080a718a1b1c8e6febcb6ccfde6cf546aa1a253acd7336226494ba784\n  conditions:\n  - lastTransitionTime: \"2025-02-04T07:40:47Z\"\n    message: Cluster Is Not Ready\n    reason: ClusterIsNotReady\n    status: \"False\"\n    type: Ready\n  configMapResourceVersion:\n    metrics:\n      cnpg-default-monitoring: \"631\"\n  danglingPVC:\n  - vaultwarden-test-backup-2025-02-04t07-40-36z-1\n  image: ghcr.io/cloudnative-pg/postgresql:17.0\n  instanceNames:\n  - vaultwarden-test-backup-2025-02-04t07-40-36z-1\n  instances: 1\n  latestGeneratedNode: 1\n  managedRolesStatus: {}\n  phase: Setting up primary\n  phaseReason: Creating primary instance vaultwarden-test-backup-2025-02-04t07-40-36z-1\n  poolerIntegrations:\n    pgBouncerIntegration: {}\n  pvcCount: 1\n  readService: vaultwarden-test-backup-2025-02-04t07-40-36z-r\n  secretsResourceVersion:\n    applicationSecretVersion: \"16907\"\n    clientCaSecretVersion: \"16825\"\n    replicationSecretVersion: \"16899\"\n    serverCaSecretVersion: \"16795\"\n    serverSecretVersion: \"16795\"\n  switchReplicaClusterStatus: {}\n  targetPrimary: vaultwarden-test-backup-2025-02-04t07-40-36z-1\n  targetPrimaryTimestamp: \"2025-02-04T07:40:47.516901Z\"\n  topology:\n    successfullyExtracted: true\n  writeService: vaultwarden-test-backup-2025-02-04t07-40-36z-rw\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.375557304Z\",\"logger\":\"cluster-resource\",\"msg\":\"default\",\"version\":\"v1\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.39061577Z\",\"logger\":\"cluster-resource\",\"msg\":\"validate create\",\"version\":\"v1\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.434786509Z\",\"msg\":\"no orphan PVCs found, skipping the restored cluster reconciliation\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.435096865Z\",\"msg\":\"CA certificate is expiring or is already expired\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\",\"secret\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-serving-cert\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.435863491Z\",\"msg\":\"CA certificate is expiring or is already expired\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\",\"secret\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-client-ca\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.445071104Z\",\"msg\":\"creating service\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\",\"serviceName\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-r\",\"updateStrategy\":\"patch\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.461081085Z\",\"msg\":\"creating service\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\",\"serviceName\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-ro\",\"updateStrategy\":\"patch\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.476702642Z\",\"msg\":\"creating service\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\",\"serviceName\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-rw\",\"updateStrategy\":\"patch\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.765028259Z\",\"msg\":\"Creating new Job\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-1-snapshot-recovery\",\"primary\":true}\n{\"level\":\"error\",\"ts\":\"2025-02-04T07:12:55.772143935Z\",\"msg\":\"Unable to create job\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\",\"job\":{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-1-snapshot-recovery\"},\"error\":\"Job.batch \\\"vaultwarden-test-backup-2025-02-04t07-12-39z-1-snapshot-recovery\\\" is invalid: [spec.template.labels: Invalid value: \\\"vaultwarden-test-backup-2025-02-04t07-12-39z-1-snapshot-recovery\\\": must be no more than 63 characters, spec.template.spec.hostname: Invalid value: \\\"vaultwarden-test-backup-2025-02-04t07-12-39z-1-snapshot-recovery\\\": must be no more than 63 characters]\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/controller.(*ClusterReconciler).createPrimaryInstance\\n\\tinternal/controller/cluster_create.go:1178\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/controller.(*ClusterReconciler).reconcilePods\\n\\tinternal/controller/cluster_controller.go:861\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/controller.(*ClusterReconciler).reconcileResources\\n\\tinternal/controller/cluster_controller.go:701\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/controller.(*ClusterReconciler).reconcile\\n\\tinternal/controller/cluster_controller.go:482\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/controller.(*ClusterReconciler).Reconcile\\n\\tinternal/controller/cluster_controller.go:188\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Reconcile\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:116\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:303\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:263\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:224\"}\n{\"level\":\"error\",\"ts\":\"2025-02-04T07:12:55.772613155Z\",\"msg\":\"Reconciler error\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"49aab3c5-b87d-4312-98ee-bf24949142a1\",\"error\":\"Job.batch \\\"vaultwarden-test-backup-2025-02-04t07-12-39z-1-snapshot-recovery\\\" is invalid: [spec.template.labels: Invalid value: \\\"vaultwarden-test-backup-2025-02-04t07-12-39z-1-snapshot-recovery\\\": must be no more than 63 characters, spec.template.spec.hostname: Invalid value: \\\"vaultwarden-test-backup-2025-02-04t07-12-39z-1-snapshot-recovery\\\": must be no more than 63 characters]\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:316\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:263\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:224\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.794783336Z\",\"msg\":\"CA certificate is expiring or is already expired\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"f3e93cab-067d-4271-93ea-352ea09c9897\",\"secret\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-serving-cert\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.795473025Z\",\"msg\":\"CA certificate is expiring or is already expired\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"f3e93cab-067d-4271-93ea-352ea09c9897\",\"secret\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-client-ca\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.868894962Z\",\"msg\":\"Selected PVC is not ready yet, waiting for 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"f3e93cab-067d-4271-93ea-352ea09c9897\",\"pvc\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-1\",\"status\":\"initializing\",\"instance\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-1\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.889551489Z\",\"msg\":\"CA certificate is expiring or is already expired\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"fc92e79a-2c6a-4c40-be1a-6c2c4835153e\",\"secret\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-serving-cert\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.890095552Z\",\"msg\":\"CA certificate is expiring or is already expired\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"fc92e79a-2c6a-4c40-be1a-6c2c4835153e\",\"secret\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-client-ca\"}\n{\"level\":\"info\",\"ts\":\"2025-02-04T07:12:55.966608509Z\",\"msg\":\"Selected PVC is not ready yet, waiting for 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"vaultwarden-test-backup-2025-02-04t07-12-39z\",\"reconcileID\":\"fc92e79a-2c6a-4c40-be1a-6c2c4835153e\",\"pvc\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-1\",\"status\":\"initializing\",\"instance\":\"vaultwarden-test-backup-2025-02-04t07-12-39z-1\"}\n(The last three lines are repeated. The cert warning can be ignored - I have my cert duration very low, and they logged lines are unrelated.)\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductKubernetes doc talking about the limit of 63 characters : \nhttps://kubernetes.io/docs/reference/kubectl/generated/kubectl_label/\n---\nI'd like to work on it \ud83d\udc4d\n---\nHi!\nI think we should manage this at cluster level and probably not add that `-snapshot-recovery` suffix or something like that, what do you think @leonardoce ?"
    },
    {
        "title": "[Bug]:no barmanObjectStore section defined on the target cluster",
        "id": 2827954995,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nThe walStorage was filled due to backups were failing(PHASE walArchivingFailing) which cause the Primary to crash.  I disable the backup by removing backup section in the cluster definition and increase the walStorage size then restart the Primary pod. When the standby promote to become Primary and all pods are up and running. I re-enable the backup by adding back the backup to cluster definition. It's now showing no \"barmanObjectStore section defined on the target cluster \"\nWhat should I do in this situation?\n### Cluster resource\n```shell\nkubectl describe cluster sidm-db-cluster -n db-operator\nName:         sidm-db-cluster\nNamespace:    db-operator\nLabels:       app.kubernetes.io/managed-by=Helm\nAnnotations:  kubectl.kubernetes.io/restartedAt: 2025-02-03T16:31:06+01:00\n              meta.helm.sh/release-name: db-operator\n              meta.helm.sh/release-namespace: default\nAPI Version:  postgresql.cnpg.io/v1\nKind:         Cluster\nMetadata:\n  Creation Timestamp:  2024-03-08T12:05:27Z\n  Generation:          10\n  Managed Fields:\n    API Version:  postgresql.cnpg.io/v1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:metadata:\n        f:annotations:\n          .:\n          f:meta.helm.sh/release-name:\n          f:meta.helm.sh/release-namespace:\n        f:labels:\n          .:\n          f:app.kubernetes.io/managed-by:\n      f:spec:\n        .:\n        f:affinity:\n          .:\n          f:enablePodAntiAffinity:\n        f:bootstrap:\n          .:\n          f:initdb:\n            .:\n            f:dataChecksums:\n            f:database:\n            f:encoding:\n            f:localeCollate:\n            f:owner:\n            f:postInitApplicationSQL:\n            f:postInitSQL:\n            f:secret:\n              .:\n              f:name:\n        f:description:\n        f:enableSuperuserAccess:\n        f:failoverDelay:\n        f:imageName:\n        f:imagePullSecrets:\n        f:instances:\n        f:logLevel:\n        f:maxSyncReplicas:\n        f:minSyncReplicas:\n        f:nodeMaintenanceWindow:\n          .:\n          f:inProgress:\n          f:reusePVC:\n        f:postgresGID:\n        f:postgresUID:\n        f:postgresql:\n          .:\n          f:parameters:\n            .:\n            f:auto_explain.log_min_duration:\n            f:pg_stat_statements.max:\n            f:pg_stat_statements.track:\n            f:shared_buffers:\n          f:pg_hba:\n        f:primaryUpdateMethod:\n        f:primaryUpdateStrategy:\n        f:replicationSlots:\n          .:\n          f:highAvailability:\n            .:\n            f:enabled:\n            f:slotPrefix:\n          f:updateInterval:\n        f:resources:\n          .:\n          f:limits:\n            .:\n            f:cpu:\n            f:memory:\n          f:requests:\n            .:\n            f:cpu:\n            f:memory:\n        f:smartShutdownTimeout:\n        f:startDelay:\n        f:stopDelay:\n        f:storage:\n          .:\n          f:resizeInUseVolumes:\n          f:size:\n          f:storageClass:\n        f:superuserSecret:\n          .:\n          f:name:\n        f:switchoverDelay:\n        f:walStorage:\n          .:\n          f:resizeInUseVolumes:\n          f:storageClass:\n    Manager:      helm\n    Operation:    Update\n    Time:         2024-04-08T08:27:27Z\n    API Version:  postgresql.cnpg.io/v1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:spec:\n        f:postgresql:\n          f:parameters:\n            f:full_page_writes:\n            f:wal_level:\n            f:wal_log_hints:\n        f:replicationSlots:\n          f:synchronizeReplicas:\n            .:\n            f:enabled:\n    Manager:      manager\n    Operation:    Update\n    Time:         2024-10-29T14:26:59Z\n    API Version:  postgresql.cnpg.io/v1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:spec:\n        f:backup:\n          .:\n          f:barmanObjectStore:\n            .:\n            f:data:\n              .:\n              f:compression:\n              f:encryption:\n              f:immediateCheckpoint:\n              f:jobs:\n            f:destinationPath:\n            f:endpointURL:\n            f:s3Credentials:\n              .:\n              f:accessKeyId:\n                .:\n                f:key:\n                f:name:\n              f:region:\n                .:\n                f:key:\n                f:name:\n              f:secretAccessKey:\n                .:\n                f:key:\n                f:name:\n            f:wal:\n              .:\n              f:compression:\n              f:encryption:\n              f:maxParallel:\n          f:retentionPolicy:\n          f:target:\n        f:walStorage:\n          f:size:\n    Manager:      kubectl-edit\n    Operation:    Update\n    Time:         2025-02-03T10:58:20Z\n    API Version:  postgresql.cnpg.io/v1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:metadata:\n        f:annotations:\n          f:kubectl.kubernetes.io/restartedAt:\n    Manager:      kubectl-cnpg\n    Operation:    Update\n    Time:         2025-02-03T15:31:06Z\n    API Version:  postgresql.cnpg.io/v1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:status:\n        .:\n        f:availableArchitectures:\n        f:certificates:\n          .:\n          f:clientCASecret:\n          f:expirations:\n            .:\n            f:sidm-db-cluster-ca:\n            f:sidm-db-cluster-replication:\n            f:sidm-db-cluster-server:\n          f:replicationTLSSecret:\n          f:serverAltDNSNames:\n          f:serverCASecret:\n          f:serverTLSSecret:\n        f:cloudNativePGCommitHash:\n        f:cloudNativePGOperatorHash:\n        f:conditions:\n        f:configMapResourceVersion:\n          .:\n          f:metrics:\n            .:\n            f:cnpg-default-monitoring:\n        f:currentPrimary:\n        f:currentPrimaryTimestamp:\n        f:firstRecoverabilityPoint:\n        f:firstRecoverabilityPointByMethod:\n          .:\n          f:barmanObjectStore:\n        f:healthyPVC:\n        f:image:\n        f:instanceNames:\n        f:instances:\n        f:instancesReportedState:\n          .:\n          f:sidm-db-cluster-1:\n            .:\n            f:isPrimary:\n            f:timeLineID:\n          f:sidm-db-cluster-2:\n            .:\n            f:isPrimary:\n            f:timeLineID:\n          f:sidm-db-cluster-3:\n            .:\n            f:isPrimary:\n            f:timeLineID:\n        f:instancesStatus:\n          .:\n          f:healthy:\n        f:lastFailedBackup:\n        f:lastSuccessfulBackup:\n        f:lastSuccessfulBackupByMethod:\n          .:\n          f:barmanObjectStore:\n        f:latestGeneratedNode:\n        f:managedRolesStatus:\n        f:phase:\n        f:poolerIntegrations:\n          .:\n          f:pgBouncerIntegration:\n            .:\n            f:secrets:\n        f:pvcCount:\n        f:readService:\n        f:readyInstances:\n        f:secretsResourceVersion:\n          .:\n          f:applicationSecretVersion:\n          f:clientCaSecretVersion:\n          f:replicationSecretVersion:\n          f:serverCaSecretVersion:\n          f:serverSecretVersion:\n          f:superuserSecretVersion:\n        f:switchReplicaClusterStatus:\n        f:targetPrimary:\n        f:targetPrimaryTimestamp:\n        f:timelineID:\n        f:topology:\n          .:\n          f:instances:\n            .:\n            f:sidm-db-cluster-1:\n            f:sidm-db-cluster-2:\n            f:sidm-db-cluster-3:\n          f:nodesUsed:\n          f:successfullyExtracted:\n        f:writeService:\n    Manager:         manager\n    Operation:       Update\n    Subresource:     status\n    Time:            2025-02-03T15:35:02Z\n  Resource Version:  297384169\n  UID:               27f6b5dd-2864-4aad-afd0-d601c19faea8\nSpec:\n  Affinity:\n    Enable Pod Anti Affinity:  true\n    Pod Anti Affinity Type:    preferred\n  Backup:\n    Barman Object Store:\n      Data:\n        Compression:     gzip\n        Encryption:      AES256\n        Jobs:            2\n      Destination Path:  s3://sidm-se-prod/se-backups\n      Endpoint URL:      https://s3.se.teliacompany.net\n      s3Credentials:\n        Access Key Id:\n          Key:   ACCESS_KEY_ID\n          Name:  backup-creds\n        Region:\n          Key:   REGION_KEY\n          Name:  backup-creds\n        Secret Access Key:\n          Key:   ACCESS_SECRET_KEY\n          Name:  backup-creds\n      Wal:\n        Compression:   gzip\n        Encryption:    AES256\n        Max Parallel:  2\n    Retention Policy:  30d\n    Target:            prefer-standby\n  Bootstrap:\n    Initdb:\n      Data Checksums:  true\n      Database:        sidm-db-se\n      Encoding:        UTF8\n      Locale C Type:   C\n      Locale Collate:  C\n      Owner:           app\n      Secret:\n        Name:               cluster-sidm-app-user\n  Description:              SIDM DB cluster\n  Enable PDB:               true\n  Enable Superuser Access:  true\n  Failover Delay:           0\n  Image Name:               pcoc-remotes-virtual.jfrog.teliacompany.io/cloudnative-pg/postgresql:14.5\n  Image Pull Secrets:\n    Name:             private-registry-creds\n  Instances:          3\n  Log Level:          info\n  Max Sync Replicas:  1\n  Min Sync Replicas:  1\n  Monitoring:\n    Custom Queries Config Map:\n      Key:                    queries\n      Name:                   cnpg-default-monitoring\n    Disable Default Queries:  false\n    Enable Pod Monitor:       false\n  Node Maintenance Window:\n    In Progress:  false\n    Reuse PVC:    false\n  Postgres GID:   26\n  Postgres UID:   26\n  Postgresql:\n    Parameters:\n      archive_mode:                   on\n      archive_timeout:                5min\n      auto_explain.log_min_duration:  10s\n      dynamic_shared_memory_type:     posix\n      full_page_writes:               on\n      log_destination:                csvlog\n      log_directory:                  /controller/log\n      log_filename:                   postgres\n      log_rotation_age:               0\n      log_rotation_size:              0\n      log_truncate_on_rotation:       false\n      logging_collector:              on\n      max_parallel_workers:           32\n      max_replication_slots:          32\n      max_worker_processes:           32\n      pg_stat_statements.max:         10000\n      pg_stat_statements.track:       all\n      shared_buffers:                 256MB\n      shared_memory_type:             mmap\n      shared_preload_libraries:       \n      ssl_max_protocol_version:       TLSv1.3\n      ssl_min_protocol_version:       TLSv1.3\n      wal_keep_size:                  512MB\n      wal_level:                      logical\n      wal_log_hints:                  on\n      wal_receiver_timeout:           5s\n      wal_sender_timeout:             5s\n    pg_hba:\n      host all all all md5\n      local all all trust\n      host all all 127.0.0.1/32 md5\n      host all all ::1/128 md5\n      host all all 10.244.0.0/16 md5\n      host replication postgres 0.0.0.0/0 md5\n    Sync Replica Election Constraint:\n      Enabled:              false\n  Primary Update Method:    restart\n  Primary Update Strategy:  unsupervised\n  Replication Slots:\n    High Availability:\n      Enabled:      true\n      Slot Prefix:  _cnpg_\n    Synchronize Replicas:\n      Enabled:        true\n    Update Interval:  30\n  Resources:\n    Limits:\n      Cpu:     3\n      Memory:  2Gi\n    Requests:\n      Cpu:                 500m\n      Memory:              512Mi\n  Smart Shutdown Timeout:  180\n  Start Delay:             300\n  Stop Delay:              300\n  Storage:\n    Resize In Use Volumes:  true\n    Size:                   50Gi\n    Storage Class:          sidm-sc\n  Superuser Secret:\n    Name:            cluster-sidm-superuser\n  Switchover Delay:  3600\n  Wal Storage:\n    Resize In Use Volumes:  true\n    Size:                   20Gi\n    Storage Class:          sidm-sc\nStatus:\n  Available Architectures:\n    Go Arch:  amd64\n    Hash:     58242fc95faa81d6bba0ba19ea959b82d9b49dc5f8e6c755f3c663665ff0ce1d\n    Go Arch:  arm64\n    Hash:     8e5ef490a05bbd5700047edda5e2440acd14bafa256dbfcc7d4a5107573df75b\n  Certificates:\n    Client CA Secret:  sidm-db-cluster-ca\n    Expirations:\n      Sidm - Db - Cluster - Ca:           2025-05-04 14:43:03 +0000 UTC\n      Sidm - Db - Cluster - Replication:  2025-05-04 14:43:03 +0000 UTC\n      Sidm - Db - Cluster - Server:       2025-04-06 14:59:38 +0000 UTC\n    Replication TLS Secret:               sidm-db-cluster-replication\n    Server Alt DNS Names:\n      sidm-db-cluster-rw\n      sidm-db-cluster-rw.db-operator\n      sidm-db-cluster-rw.db-operator.svc\n      sidm-db-cluster-rw.db-operator.svc.cluster.local\n      sidm-db-cluster-r\n      sidm-db-cluster-r.db-operator\n      sidm-db-cluster-r.db-operator.svc\n      sidm-db-cluster-r.db-operator.svc.cluster.local\n      sidm-db-cluster-ro\n      sidm-db-cluster-ro.db-operator\n      sidm-db-cluster-ro.db-operator.svc\n      sidm-db-cluster-ro.db-operator.svc.cluster.local\n    Server CA Secret:             sidm-db-cluster-ca\n    Server TLS Secret:            sidm-db-cluster-server\n  Cloud Native PG Commit Hash:    bad5a251\n  Cloud Native PG Operator Hash:  58242fc95faa81d6bba0ba19ea959b82d9b49dc5f8e6c755f3c663665ff0ce1d\n  Conditions:\n    Last Transition Time:  2025-02-03T15:34:59Z\n    Message:               Cluster is Ready\n    Reason:                ClusterIsReady\n    Status:                True\n    Type:                  Ready\n    Last Transition Time:  2025-02-03T10:57:25Z\n    Message:               Continuous archiving is working\n    Reason:                ContinuousArchivingSuccess\n    Status:                True\n    Type:                  ContinuousArchiving\n    Last Transition Time:  2025-02-02T00:01:30Z\n    Message:               Backup was successful\n    Reason:                LastBackupSucceeded\n    Status:                True\n    Type:                  LastBackupSucceeded\n  Config Map Resource Version:\n    Metrics:\n      Cnpg - Default - Monitoring:  153392574\n  Current Primary:                  sidm-db-cluster-2\n  Current Primary Timestamp:        2025-02-03T07:57:57.908955Z\n  First Recoverability Point:       2025-01-03T00:01:20Z\n  First Recoverability Point By Method:\n    Barman Object Store:  2025-01-03T00:01:20Z\n  Healthy PVC:\n    sidm-db-cluster-1\n    sidm-db-cluster-1-wal\n    sidm-db-cluster-2\n    sidm-db-cluster-2-wal\n    sidm-db-cluster-3\n    sidm-db-cluster-3-wal\n  Image:  pcoc-remotes-virtual.jfrog.teliacompany.io/cloudnative-pg/postgresql:14.5\n  Instance Names:\n    sidm-db-cluster-1\n    sidm-db-cluster-2\n    sidm-db-cluster-3\n  Instances:  3\n  Instances Reported State:\n    sidm-db-cluster-1:\n      Is Primary:    false\n      Time Line ID:  2\n    sidm-db-cluster-2:\n      Is Primary:    true\n      Time Line ID:  2\n    sidm-db-cluster-3:\n      Is Primary:    false\n      Time Line ID:  2\n  Instances Status:\n    Healthy:\n      sidm-db-cluster-1\n      sidm-db-cluster-2\n      sidm-db-cluster-3\n  Last Failed Backup:      2024-04-05T09:58:45Z\n  Last Successful Backup:  2025-02-02T00:01:28Z\n  Last Successful Backup By Method:\n    Barman Object Store:  2025-02-02T00:01:28Z\n  Latest Generated Node:  3\n  Managed Roles Status:\n  Phase:  Cluster in healthy state\n  Pooler Integrations:\n    Pg Bouncer Integration:\n      Secrets:\n        sidm-db-cluster-pooler\n  Pvc Count:        6\n  Read Service:     sidm-db-cluster-r\n  Ready Instances:  3\n  Secrets Resource Version:\n    Application Secret Version:  10352712\n    Client Ca Secret Version:    297355748\n    Replication Secret Version:  297355749\n    Server Ca Secret Version:    297355748\n    Server Secret Version:       273071370\n    Superuser Secret Version:    10352714\n  Switch Replica Cluster Status:\n  Target Primary:            sidm-db-cluster-2\n  Target Primary Timestamp:  2025-02-03T07:57:57.002210Z\n  Timeline ID:               2\n  Topology:\n    Instances:\n      sidm-db-cluster-1:\n      sidm-db-cluster-2:\n      sidm-db-cluster-3:\n    Nodes Used:              3\n    Successfully Extracted:  true\n  Write Service:             sidm-db-cluster-rw\nEvents:\n  Type    Reason             Age                From            Message\n  ----    ------             ----               ----            -------\n  Normal  UpgradingInstance  41m (x3 over 18d)  cloudnative-pg  Upgrading instance sidm-db-cluster-3\n  Normal  UpgradingInstance  40m (x2 over 18d)  cloudnative-pg  Upgrading instance sidm-db-cluster-1\nkubectl get backups -n db-operator\nNAME                         AGE     CLUSTER           METHOD              PHASE       ERROR\nbackup-sidm-20250104000000   30d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250105000000   29d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250106000000   28d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250107000000   27d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250108000000   26d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250109000000   25d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250110000000   24d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250111000000   23d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250112000000   22d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250113000000   21d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250114000000   20d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250115000000   19d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250116000000   18d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250117000000   17d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250118000000   16d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250119000000   15d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250120000000   14d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250121000000   13d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250122000000   12d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250123000000   11d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250124000000   10d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250125000000   9d      sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250126000000   8d      sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250127000000   7d16h   sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250128000000   6d16h   sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250129000000   5d16h   sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250130000000   4d16h   sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250131000000   3d16h   sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250201000000   2d16h   sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250202000000   40h     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250203000000   16h     sidm-db-cluster   barmanObjectStore   failed      no barmanObjectStore section defined on the target cluster\nkubectl describe backups backup-sidm-20250203000000 -n db-operator\nName:         backup-sidm-20250203000000\nNamespace:    db-operator\nLabels:       cnpg.io/cluster=sidm-db-cluster\n              cnpg.io/immediateBackup=false\n              cnpg.io/scheduled-backup=backup-sidm\nAnnotations:  cnpg.io/operatorVersion: 1.25.0\nAPI Version:  postgresql.cnpg.io/v1\nKind:         Backup\nMetadata:\n  Creation Timestamp:  2025-02-03T00:00:00Z\n  Generation:          1\n  Managed Fields:\n    API Version:  postgresql.cnpg.io/v1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:metadata:\n        f:annotations:\n          .:\n          f:cnpg.io/operatorVersion:\n        f:labels:\n          .:\n          f:cnpg.io/cluster:\n          f:cnpg.io/immediateBackup:\n          f:cnpg.io/scheduled-backup:\n        f:ownerReferences:\n          .:\n          k:{\"uid\":\"27f6b5dd-2864-4aad-afd0-d601c19faea8\"}:\n      f:spec:\n        .:\n        f:cluster:\n          .:\n          f:name:\n        f:method:\n    Manager:      manager\n    Operation:    Update\n    Time:         2025-02-03T00:00:00Z\n    API Version:  postgresql.cnpg.io/v1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:status:\n        .:\n        f:destinationPath:\n        f:encryption:\n        f:endpointURL:\n        f:error:\n        f:instanceID:\n          .:\n          f:ContainerID:\n          f:podName:\n        f:method:\n        f:phase:\n        f:s3Credentials:\n          .:\n          f:accessKeyId:\n            .:\n            f:key:\n            f:name:\n          f:region:\n            .:\n            f:key:\n            f:name:\n          f:secretAccessKey:\n            .:\n            f:key:\n            f:name:\n        f:serverName:\n    Manager:      manager\n    Operation:    Update\n    Subresource:  status\n    Time:         2025-02-03T10:21:17Z\n  Owner References:\n    API Version:     postgresql.cnpg.io/v1\n    Controller:      true\n    Kind:            Cluster\n    Name:            sidm-db-cluster\n    UID:             27f6b5dd-2864-4aad-afd0-d601c19faea8\n  Resource Version:  297195044\n  UID:               1789b591-da76-4ad2-be42-ecc06de813d2\nSpec:\n  Cluster:\n    Name:  sidm-db-cluster\n  Method:  barmanObjectStore\nStatus:\n  Destination Path:  s3://sidm-se-prod/se-backups\n  Encryption:        AES256\n  Endpoint URL:      https://s3.se.teliacompany.net\n  Error:             no barmanObjectStore section defined on the target cluster\n  Instance ID:\n    Container ID:  containerd://1183af71a6ecbeb38faa1b9f9abe6476cf10a19720a629a83c9f2476ab71bd3a\n    Pod Name:      sidm-db-cluster-2\n  Method:          barmanObjectStore\n  Phase:           failed\n  s3Credentials:\n    Access Key Id:\n      Key:   ACCESS_KEY_ID\n      Name:  backup-creds\n    Region:\n      Key:   REGION_KEY\n      Name:  backup-creds\n    Secret Access Key:\n      Key:      ACCESS_SECRET_KEY\n      Name:     backup-creds\n  Server Name:  sidm-db-cluster\nEvents:         <none>\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nThe walStorage was filled due to backups were failing(PHASE walArchivingFailing) which cause the Primary to crash.  I disable the backup by removing backup section in the cluster definition and increase the walStorage size then restart the Primary pod. When the standby promote to become Primary and all pods are up and running. I re-enable the backup by adding back the backup to cluster definition. It's now showing no \"barmanObjectStore section defined on the target cluster \"\nWhat should I do in this situation?\n### Cluster resource\n```shell\nkubectl describe cluster sidm-db-cluster -n db-operator\nName:         sidm-db-cluster\nNamespace:    db-operator\nLabels:       app.kubernetes.io/managed-by=Helm\nAnnotations:  kubectl.kubernetes.io/restartedAt: 2025-02-03T16:31:06+01:00\n              meta.helm.sh/release-name: db-operator\n              meta.helm.sh/release-namespace: default\nAPI Version:  postgresql.cnpg.io/v1\nKind:         Cluster\nMetadata:\n  Creation Timestamp:  2024-03-08T12:05:27Z\n  Generation:          10\n  Managed Fields:\n    API Version:  postgresql.cnpg.io/v1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:metadata:\n        f:annotations:\n          .:\n          f:meta.helm.sh/release-name:\n          f:meta.helm.sh/release-namespace:\n        f:labels:\n          .:\n          f:app.kubernetes.io/managed-by:\n      f:spec:\n        .:\n        f:affinity:\n          .:\n          f:enablePodAntiAffinity:\n        f:bootstrap:\n          .:\n          f:initdb:\n            .:\n            f:dataChecksums:\n            f:database:\n            f:encoding:\n            f:localeCollate:\n            f:owner:\n            f:postInitApplicationSQL:\n            f:postInitSQL:\n            f:secret:\n              .:\n              f:name:\n        f:description:\n        f:enableSuperuserAccess:\n        f:failoverDelay:\n        f:imageName:\n        f:imagePullSecrets:\n        f:instances:\n        f:logLevel:\n        f:maxSyncReplicas:\n        f:minSyncReplicas:\n        f:nodeMaintenanceWindow:\n          .:\n          f:inProgress:\n          f:reusePVC:\n        f:postgresGID:\n        f:postgresUID:\n        f:postgresql:\n          .:\n          f:parameters:\n            .:\n            f:auto_explain.log_min_duration:\n            f:pg_stat_statements.max:\n            f:pg_stat_statements.track:\n            f:shared_buffers:\n          f:pg_hba:\n        f:primaryUpdateMethod:\n        f:primaryUpdateStrategy:\n        f:replicationSlots:\n          .:\n          f:highAvailability:\n            .:\n            f:enabled:\n            f:slotPrefix:\n          f:updateInterval:\n        f:resources:\n          .:\n          f:limits:\n            .:\n            f:cpu:\n            f:memory:\n          f:requests:\n            .:\n            f:cpu:\n            f:memory:\n        f:smartShutdownTimeout:\n        f:startDelay:\n        f:stopDelay:\n        f:storage:\n          .:\n          f:resizeInUseVolumes:\n          f:size:\n          f:storageClass:\n        f:superuserSecret:\n          .:\n          f:name:\n        f:switchoverDelay:\n        f:walStorage:\n          .:\n          f:resizeInUseVolumes:\n          f:storageClass:\n    Manager:      helm\n    Operation:    Update\n    Time:         2024-04-08T08:27:27Z\n    API Version:  postgresql.cnpg.io/v1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:spec:\n        f:postgresql:\n          f:parameters:\n            f:full_page_writes:\n            f:wal_level:\n            f:wal_log_hints:\n        f:replicationSlots:\n          f:synchronizeReplicas:\n            .:\n            f:enabled:\n    Manager:      manager\n    Operation:    Update\n    Time:         2024-10-29T14:26:59Z\n    API Version:  postgresql.cnpg.io/v1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:spec:\n        f:backup:\n          .:\n          f:barmanObjectStore:\n            .:\n            f:data:\n              .:\n              f:compression:\n              f:encryption:\n              f:immediateCheckpoint:\n              f:jobs:\n            f:destinationPath:\n            f:endpointURL:\n            f:s3Credentials:\n              .:\n              f:accessKeyId:\n                .:\n                f:key:\n                f:name:\n              f:region:\n                .:\n                f:key:\n                f:name:\n              f:secretAccessKey:\n                .:\n                f:key:\n                f:name:\n            f:wal:\n              .:\n              f:compression:\n              f:encryption:\n              f:maxParallel:\n          f:retentionPolicy:\n          f:target:\n        f:walStorage:\n          f:size:\n    Manager:      kubectl-edit\n    Operation:    Update\n    Time:         2025-02-03T10:58:20Z\n    API Version:  postgresql.cnpg.io/v1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:metadata:\n        f:annotations:\n          f:kubectl.kubernetes.io/restartedAt:\n    Manager:      kubectl-cnpg\n    Operation:    Update\n    Time:         2025-02-03T15:31:06Z\n    API Version:  postgresql.cnpg.io/v1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:status:\n        .:\n        f:availableArchitectures:\n        f:certificates:\n          .:\n          f:clientCASecret:\n          f:expirations:\n            .:\n            f:sidm-db-cluster-ca:\n            f:sidm-db-cluster-replication:\n            f:sidm-db-cluster-server:\n          f:replicationTLSSecret:\n          f:serverAltDNSNames:\n          f:serverCASecret:\n          f:serverTLSSecret:\n        f:cloudNativePGCommitHash:\n        f:cloudNativePGOperatorHash:\n        f:conditions:\n        f:configMapResourceVersion:\n          .:\n          f:metrics:\n            .:\n            f:cnpg-default-monitoring:\n        f:currentPrimary:\n        f:currentPrimaryTimestamp:\n        f:firstRecoverabilityPoint:\n        f:firstRecoverabilityPointByMethod:\n          .:\n          f:barmanObjectStore:\n        f:healthyPVC:\n        f:image:\n        f:instanceNames:\n        f:instances:\n        f:instancesReportedState:\n          .:\n          f:sidm-db-cluster-1:\n            .:\n            f:isPrimary:\n            f:timeLineID:\n          f:sidm-db-cluster-2:\n            .:\n            f:isPrimary:\n            f:timeLineID:\n          f:sidm-db-cluster-3:\n            .:\n            f:isPrimary:\n            f:timeLineID:\n        f:instancesStatus:\n          .:\n          f:healthy:\n        f:lastFailedBackup:\n        f:lastSuccessfulBackup:\n        f:lastSuccessfulBackupByMethod:\n          .:\n          f:barmanObjectStore:\n        f:latestGeneratedNode:\n        f:managedRolesStatus:\n        f:phase:\n        f:poolerIntegrations:\n          .:\n          f:pgBouncerIntegration:\n            .:\n            f:secrets:\n        f:pvcCount:\n        f:readService:\n        f:readyInstances:\n        f:secretsResourceVersion:\n          .:\n          f:applicationSecretVersion:\n          f:clientCaSecretVersion:\n          f:replicationSecretVersion:\n          f:serverCaSecretVersion:\n          f:serverSecretVersion:\n          f:superuserSecretVersion:\n        f:switchReplicaClusterStatus:\n        f:targetPrimary:\n        f:targetPrimaryTimestamp:\n        f:timelineID:\n        f:topology:\n          .:\n          f:instances:\n            .:\n            f:sidm-db-cluster-1:\n            f:sidm-db-cluster-2:\n            f:sidm-db-cluster-3:\n          f:nodesUsed:\n          f:successfullyExtracted:\n        f:writeService:\n    Manager:         manager\n    Operation:       Update\n    Subresource:     status\n    Time:            2025-02-03T15:35:02Z\n  Resource Version:  297384169\n  UID:               27f6b5dd-2864-4aad-afd0-d601c19faea8\nSpec:\n  Affinity:\n    Enable Pod Anti Affinity:  true\n    Pod Anti Affinity Type:    preferred\n  Backup:\n    Barman Object Store:\n      Data:\n        Compression:     gzip\n        Encryption:      AES256\n        Jobs:            2\n      Destination Path:  s3://sidm-se-prod/se-backups\n      Endpoint URL:      https://s3.se.teliacompany.net\n      s3Credentials:\n        Access Key Id:\n          Key:   ACCESS_KEY_ID\n          Name:  backup-creds\n        Region:\n          Key:   REGION_KEY\n          Name:  backup-creds\n        Secret Access Key:\n          Key:   ACCESS_SECRET_KEY\n          Name:  backup-creds\n      Wal:\n        Compression:   gzip\n        Encryption:    AES256\n        Max Parallel:  2\n    Retention Policy:  30d\n    Target:            prefer-standby\n  Bootstrap:\n    Initdb:\n      Data Checksums:  true\n      Database:        sidm-db-se\n      Encoding:        UTF8\n      Locale C Type:   C\n      Locale Collate:  C\n      Owner:           app\n      Secret:\n        Name:               cluster-sidm-app-user\n  Description:              SIDM DB cluster\n  Enable PDB:               true\n  Enable Superuser Access:  true\n  Failover Delay:           0\n  Image Name:               pcoc-remotes-virtual.jfrog.teliacompany.io/cloudnative-pg/postgresql:14.5\n  Image Pull Secrets:\n    Name:             private-registry-creds\n  Instances:          3\n  Log Level:          info\n  Max Sync Replicas:  1\n  Min Sync Replicas:  1\n  Monitoring:\n    Custom Queries Config Map:\n      Key:                    queries\n      Name:                   cnpg-default-monitoring\n    Disable Default Queries:  false\n    Enable Pod Monitor:       false\n  Node Maintenance Window:\n    In Progress:  false\n    Reuse PVC:    false\n  Postgres GID:   26\n  Postgres UID:   26\n  Postgresql:\n    Parameters:\n      archive_mode:                   on\n      archive_timeout:                5min\n      auto_explain.log_min_duration:  10s\n      dynamic_shared_memory_type:     posix\n      full_page_writes:               on\n      log_destination:                csvlog\n      log_directory:                  /controller/log\n      log_filename:                   postgres\n      log_rotation_age:               0\n      log_rotation_size:              0\n      log_truncate_on_rotation:       false\n      logging_collector:              on\n      max_parallel_workers:           32\n      max_replication_slots:          32\n      max_worker_processes:           32\n      pg_stat_statements.max:         10000\n      pg_stat_statements.track:       all\n      shared_buffers:                 256MB\n      shared_memory_type:             mmap\n      shared_preload_libraries:       \n      ssl_max_protocol_version:       TLSv1.3\n      ssl_min_protocol_version:       TLSv1.3\n      wal_keep_size:                  512MB\n      wal_level:                      logical\n      wal_log_hints:                  on\n      wal_receiver_timeout:           5s\n      wal_sender_timeout:             5s\n    pg_hba:\n      host all all all md5\n      local all all trust\n      host all all 127.0.0.1/32 md5\n      host all all ::1/128 md5\n      host all all 10.244.0.0/16 md5\n      host replication postgres 0.0.0.0/0 md5\n    Sync Replica Election Constraint:\n      Enabled:              false\n  Primary Update Method:    restart\n  Primary Update Strategy:  unsupervised\n  Replication Slots:\n    High Availability:\n      Enabled:      true\n      Slot Prefix:  _cnpg_\n    Synchronize Replicas:\n      Enabled:        true\n    Update Interval:  30\n  Resources:\n    Limits:\n      Cpu:     3\n      Memory:  2Gi\n    Requests:\n      Cpu:                 500m\n      Memory:              512Mi\n  Smart Shutdown Timeout:  180\n  Start Delay:             300\n  Stop Delay:              300\n  Storage:\n    Resize In Use Volumes:  true\n    Size:                   50Gi\n    Storage Class:          sidm-sc\n  Superuser Secret:\n    Name:            cluster-sidm-superuser\n  Switchover Delay:  3600\n  Wal Storage:\n    Resize In Use Volumes:  true\n    Size:                   20Gi\n    Storage Class:          sidm-sc\nStatus:\n  Available Architectures:\n    Go Arch:  amd64\n    Hash:     58242fc95faa81d6bba0ba19ea959b82d9b49dc5f8e6c755f3c663665ff0ce1d\n    Go Arch:  arm64\n    Hash:     8e5ef490a05bbd5700047edda5e2440acd14bafa256dbfcc7d4a5107573df75b\n  Certificates:\n    Client CA Secret:  sidm-db-cluster-ca\n    Expirations:\n      Sidm - Db - Cluster - Ca:           2025-05-04 14:43:03 +0000 UTC\n      Sidm - Db - Cluster - Replication:  2025-05-04 14:43:03 +0000 UTC\n      Sidm - Db - Cluster - Server:       2025-04-06 14:59:38 +0000 UTC\n    Replication TLS Secret:               sidm-db-cluster-replication\n    Server Alt DNS Names:\n      sidm-db-cluster-rw\n      sidm-db-cluster-rw.db-operator\n      sidm-db-cluster-rw.db-operator.svc\n      sidm-db-cluster-rw.db-operator.svc.cluster.local\n      sidm-db-cluster-r\n      sidm-db-cluster-r.db-operator\n      sidm-db-cluster-r.db-operator.svc\n      sidm-db-cluster-r.db-operator.svc.cluster.local\n      sidm-db-cluster-ro\n      sidm-db-cluster-ro.db-operator\n      sidm-db-cluster-ro.db-operator.svc\n      sidm-db-cluster-ro.db-operator.svc.cluster.local\n    Server CA Secret:             sidm-db-cluster-ca\n    Server TLS Secret:            sidm-db-cluster-server\n  Cloud Native PG Commit Hash:    bad5a251\n  Cloud Native PG Operator Hash:  58242fc95faa81d6bba0ba19ea959b82d9b49dc5f8e6c755f3c663665ff0ce1d\n  Conditions:\n    Last Transition Time:  2025-02-03T15:34:59Z\n    Message:               Cluster is Ready\n    Reason:                ClusterIsReady\n    Status:                True\n    Type:                  Ready\n    Last Transition Time:  2025-02-03T10:57:25Z\n    Message:               Continuous archiving is working\n    Reason:                ContinuousArchivingSuccess\n    Status:                True\n    Type:                  ContinuousArchiving\n    Last Transition Time:  2025-02-02T00:01:30Z\n    Message:               Backup was successful\n    Reason:                LastBackupSucceeded\n    Status:                True\n    Type:                  LastBackupSucceeded\n  Config Map Resource Version:\n    Metrics:\n      Cnpg - Default - Monitoring:  153392574\n  Current Primary:                  sidm-db-cluster-2\n  Current Primary Timestamp:        2025-02-03T07:57:57.908955Z\n  First Recoverability Point:       2025-01-03T00:01:20Z\n  First Recoverability Point By Method:\n    Barman Object Store:  2025-01-03T00:01:20Z\n  Healthy PVC:\n    sidm-db-cluster-1\n    sidm-db-cluster-1-wal\n    sidm-db-cluster-2\n    sidm-db-cluster-2-wal\n    sidm-db-cluster-3\n    sidm-db-cluster-3-wal\n  Image:  pcoc-remotes-virtual.jfrog.teliacompany.io/cloudnative-pg/postgresql:14.5\n  Instance Names:\n    sidm-db-cluster-1\n    sidm-db-cluster-2\n    sidm-db-cluster-3\n  Instances:  3\n  Instances Reported State:\n    sidm-db-cluster-1:\n      Is Primary:    false\n      Time Line ID:  2\n    sidm-db-cluster-2:\n      Is Primary:    true\n      Time Line ID:  2\n    sidm-db-cluster-3:\n      Is Primary:    false\n      Time Line ID:  2\n  Instances Status:\n    Healthy:\n      sidm-db-cluster-1\n      sidm-db-cluster-2\n      sidm-db-cluster-3\n  Last Failed Backup:      2024-04-05T09:58:45Z\n  Last Successful Backup:  2025-02-02T00:01:28Z\n  Last Successful Backup By Method:\n    Barman Object Store:  2025-02-02T00:01:28Z\n  Latest Generated Node:  3\n  Managed Roles Status:\n  Phase:  Cluster in healthy state\n  Pooler Integrations:\n    Pg Bouncer Integration:\n      Secrets:\n        sidm-db-cluster-pooler\n  Pvc Count:        6\n  Read Service:     sidm-db-cluster-r\n  Ready Instances:  3\n  Secrets Resource Version:\n    Application Secret Version:  10352712\n    Client Ca Secret Version:    297355748\n    Replication Secret Version:  297355749\n    Server Ca Secret Version:    297355748\n    Server Secret Version:       273071370\n    Superuser Secret Version:    10352714\n  Switch Replica Cluster Status:\n  Target Primary:            sidm-db-cluster-2\n  Target Primary Timestamp:  2025-02-03T07:57:57.002210Z\n  Timeline ID:               2\n  Topology:\n    Instances:\n      sidm-db-cluster-1:\n      sidm-db-cluster-2:\n      sidm-db-cluster-3:\n    Nodes Used:              3\n    Successfully Extracted:  true\n  Write Service:             sidm-db-cluster-rw\nEvents:\n  Type    Reason             Age                From            Message\n  ----    ------             ----               ----            -------\n  Normal  UpgradingInstance  41m (x3 over 18d)  cloudnative-pg  Upgrading instance sidm-db-cluster-3\n  Normal  UpgradingInstance  40m (x2 over 18d)  cloudnative-pg  Upgrading instance sidm-db-cluster-1\nkubectl get backups -n db-operator\nNAME                         AGE     CLUSTER           METHOD              PHASE       ERROR\nbackup-sidm-20250104000000   30d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250105000000   29d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250106000000   28d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250107000000   27d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250108000000   26d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250109000000   25d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250110000000   24d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250111000000   23d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250112000000   22d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250113000000   21d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250114000000   20d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250115000000   19d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250116000000   18d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250117000000   17d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250118000000   16d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250119000000   15d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250120000000   14d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250121000000   13d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250122000000   12d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250123000000   11d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250124000000   10d     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250125000000   9d      sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250126000000   8d      sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250127000000   7d16h   sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250128000000   6d16h   sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250129000000   5d16h   sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250130000000   4d16h   sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250131000000   3d16h   sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250201000000   2d16h   sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250202000000   40h     sidm-db-cluster   barmanObjectStore   completed   \nbackup-sidm-20250203000000   16h     sidm-db-cluster   barmanObjectStore   failed      no barmanObjectStore section defined on the target cluster\nkubectl describe backups backup-sidm-20250203000000 -n db-operator\nName:         backup-sidm-20250203000000\nNamespace:    db-operator\nLabels:       cnpg.io/cluster=sidm-db-cluster\n              cnpg.io/immediateBackup=false\n              cnpg.io/scheduled-backup=backup-sidm\nAnnotations:  cnpg.io/operatorVersion: 1.25.0\nAPI Version:  postgresql.cnpg.io/v1\nKind:         Backup\nMetadata:\n  Creation Timestamp:  2025-02-03T00:00:00Z\n  Generation:          1\n  Managed Fields:\n    API Version:  postgresql.cnpg.io/v1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:metadata:\n        f:annotations:\n          .:\n          f:cnpg.io/operatorVersion:\n        f:labels:\n          .:\n          f:cnpg.io/cluster:\n          f:cnpg.io/immediateBackup:\n          f:cnpg.io/scheduled-backup:\n        f:ownerReferences:\n          .:\n          k:{\"uid\":\"27f6b5dd-2864-4aad-afd0-d601c19faea8\"}:\n      f:spec:\n        .:\n        f:cluster:\n          .:\n          f:name:\n        f:method:\n    Manager:      manager\n    Operation:    Update\n    Time:         2025-02-03T00:00:00Z\n    API Version:  postgresql.cnpg.io/v1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:status:\n        .:\n        f:destinationPath:\n        f:encryption:\n        f:endpointURL:\n        f:error:\n        f:instanceID:\n          .:\n          f:ContainerID:\n          f:podName:\n        f:method:\n        f:phase:\n        f:s3Credentials:\n          .:\n          f:accessKeyId:\n            .:\n            f:key:\n            f:name:\n          f:region:\n            .:\n            f:key:\n            f:name:\n          f:secretAccessKey:\n            .:\n            f:key:\n            f:name:\n        f:serverName:\n    Manager:      manager\n    Operation:    Update\n    Subresource:  status\n    Time:         2025-02-03T10:21:17Z\n  Owner References:\n    API Version:     postgresql.cnpg.io/v1\n    Controller:      true\n    Kind:            Cluster\n    Name:            sidm-db-cluster\n    UID:             27f6b5dd-2864-4aad-afd0-d601c19faea8\n  Resource Version:  297195044\n  UID:               1789b591-da76-4ad2-be42-ecc06de813d2\nSpec:\n  Cluster:\n    Name:  sidm-db-cluster\n  Method:  barmanObjectStore\nStatus:\n  Destination Path:  s3://sidm-se-prod/se-backups\n  Encryption:        AES256\n  Endpoint URL:      https://s3.se.teliacompany.net\n  Error:             no barmanObjectStore section defined on the target cluster\n  Instance ID:\n    Container ID:  containerd://1183af71a6ecbeb38faa1b9f9abe6476cf10a19720a629a83c9f2476ab71bd3a\n    Pod Name:      sidm-db-cluster-2\n  Method:          barmanObjectStore\n  Phase:           failed\n  s3Credentials:\n    Access Key Id:\n      Key:   ACCESS_KEY_ID\n      Name:  backup-creds\n    Region:\n      Key:   REGION_KEY\n      Name:  backup-creds\n    Secret Access Key:\n      Key:      ACCESS_SECRET_KEY\n      Name:     backup-creds\n  Server Name:  sidm-db-cluster\nEvents:         <none>\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: PVC naming is unusual",
        "id": 2826990160,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nsebas@dbyond.nl\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nOLM\n### What happened?\nIf we create a CNPG cluster, we get 2 PVC's for every pod.\nThe WAL pvc is named [podname]-wal, which is consistent with other common deployment types (such as StatefulSet's).\nThe data pvc however is named same as the pod, which is unusual.\nThis in combination with a vsphere-csi bug makes our vCenter been flooded with API calls bringing us to a DoS.\nAlthough this is a vsphere-csi issue primarily, I think we can do better (more in line with common deployment patterns) by naming the pvc to [podname]-data.\nI am offering to submit a PR. We could add enable/disable with a config flag, and could even use conversions to keep the old pattern if unset in cluster CR.\n**note** I pre-discussed this in slack channel...\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nsebas@dbyond.nl\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nOLM\n### What happened?\nIf we create a CNPG cluster, we get 2 PVC's for every pod.\nThe WAL pvc is named [podname]-wal, which is consistent with other common deployment types (such as StatefulSet's).\nThe data pvc however is named same as the pod, which is unusual.\nThis in combination with a vsphere-csi bug makes our vCenter been flooded with API calls bringing us to a DoS.\nAlthough this is a vsphere-csi issue primarily, I think we can do better (more in line with common deployment patterns) by naming the pvc to [podname]-data.\nI am offering to submit a PR. We could add enable/disable with a config flag, and could even use conversions to keep the old pattern if unset in cluster CR.\n**note** I pre-discussed this in slack channel...\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductI will work on a PR"
    },
    {
        "title": "docs: clarify the bootstrap section",
        "id": 2826707167,
        "state": "no reaction",
        "first": "",
        "messages": ""
    },
    {
        "title": "Draft: Fixes to increase reliability and reduce pain during backup connection pool exhaustion #6599",
        "id": 2825924761,
        "state": "open",
        "first": "- Fixes the \"not ready\" pod issue caused by 3 consecutive failing volume snapshots #6599 #6761 \r\n- Adds a `/restart` endpoint to restart the instance manager if it's in a bad state w/o restarting the pod (likely unneeded now)\r\n- Adds additional logging and tracing to troubleshoot HTTP issues.\r\n- Improves error logging throughout (no more `context cancelled`)",
        "messages": "- Fixes the \"not ready\" pod issue caused by 3 consecutive failing volume snapshots #6599 #6761 \r\n- Adds a `/restart` endpoint to restart the instance manager if it's in a bad state w/o restarting the pod (likely unneeded now)\r\n- Adds additional logging and tracing to troubleshoot HTTP issues.\r\n- Improves error logging throughout (no more `context cancelled`)"
    },
    {
        "title": "[Bug]: Unable to deploy clusters with webhook failers, cluster election leaders.",
        "id": 2825736371,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nmax@self-hosted.io\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nYAML manifest\n### What happened?\nBootstrap a new k3s cluster ( 1 control plane node, 2 workers ) 4vcpu/4gb ram, workers have 8gb ram. The ability to deploy psql clusters is hit and miss. \nSystem has longhorn installed with a dedicated storage class:\n```\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: psql\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"false\"\nprovisioner: driver.longhorn.io\nallowVolumeExpansion: true\nreclaimPolicy: \"Delete\"\nvolumeBindingMode: Immediate\nparameters:\n  numberOfReplicas: \"1\"\n  staleReplicaTimeout: \"30\"\n  fromBackup: \"\"\n  fsType: \"ext4\"\n  dataLocality: \"disabled\"\n  unmapMarkSnapChainRemoved: \"ignored\"\n  disableRevisionCounter: \"true\"\n  dataEngine: \"v1\"\n```\nand the cluster config\n```\nserver_config_yaml: |\n      debug: true\n      write-kubeconfig-mode: \"0644\"\n      flannel-backend: \"wireguard-native\"\n      secrets-encryption: \"true\"\n```\nInstall of the CNPG operator once the cluster is bootstrapped in an ansible task:\n```\n- name: Install CNPG Operator\n  kubernetes.core.k8s:\n    state: present\n    src: https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.25/releases/cnpg-1.25.0.yaml\n  delegate_to: \"{{ hostvars[groups['server'][0]]['ansible_host'] | default(groups['server'][0]) }}\"\n```\nPerform an install of the following manifest:\n```\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: jira-postgres\n  namespace: jira\nspec:\n  instances: 3\n  # Parameters and pg_hba configuration will be append\n  # to the default ones to make the cluster work\n  postgresql:\n    parameters:\n      max_worker_processes: \"100\"\n      max_connections: \"1000\"\n      shared_buffers: \"256MB\"\n    pg_hba:\n      - host all all all scram-sha-256\n  bootstrap:\n    initdb:\n      database: jira\n      owner: jira\n  # Example of rolling update strategy:\n  # - unsupervised: automated update of the primary once all\n  #                 replicas have been upgraded (default)\n  # - supervised: requires manual supervision to perform\n  #               the switchover of the primary\n  primaryUpdateStrategy: unsupervised\n  # Require 1Gi of space per instance using default storage class\n  storage:\n    storageClass: \"psql\"\n    size: 5Gi\n```\nOn the deployment, the following error is returned when `kubectl apply -f` is performed:\n```\n'Failed to create object: b''{\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Internal error occurred: failed calling webhook \\\\\"mcluster.cnpg.io\\\\\": failed to call webhook: Post \\\\\"[https://cnpg-webhook-service.cnpg-system.svc:443/mutate-postgresql-cnpg-io-v1-cluster?timeout=10s\\\\](https://cnpg-webhook-service.cnpg-system.svc/mutate-postgresql-cnpg-io-v1-cluster?timeout=10s\\\\)\": no endpoints available for service \\\\\"cnpg-webhook-service\\\\\"\",\"reason\":\"InternalError\",\"details\":{\"causes\":[{\"message\":\"failed calling webhook \\\\\"mcluster.cnpg.io\\\\\": failed to call webhook: Post \\\\\"[https://cnpg-webhook-service.cnpg-system.svc:443/mutate-postgresql-cnpg-io-v1-cluster?timeout=10s\\\\](https://cnpg-webhook-service.cnpg-system.svc/mutate-postgresql-cnpg-io-v1-cluster?timeout=10s\\\\)\": no endpoints available for service \\\\\"cnpg-webhook-service\\\\\"\"}]},\"code\":500}\\n\n```\nWaiting a period of time, manually applying creates a cluster:\n```\n[root@k3s-dev01-control-plane-0 ~]# kubectl apply -f d.yml\ncluster.postgresql.cnpg.io/jira-postgres created\n```\nOutput of manual deploy, the cluster becomes healthy.\n```\n[root@k3s-dev01-control-plane-0 ~]# kubectl -n jira get cluster,po\nNAME                                       AGE   INSTANCES   READY   STATUS                                       PRIMARY\ncluster.postgresql.cnpg.io/jira-postgres   50s   1                   Waiting for the instances to become active   jira-postgres-1\nNAME                               READY   STATUS      RESTARTS   AGE\npod/jira-postgres-1                0/1     Running     0          14s\npod/jira-postgres-1-initdb-xzzx7   0/1     Completed   0          50s\n```\n### Cluster resource\n```shell\nMachinea are all 4vCPU, and run on U.2/enterprise NVMe local disk. \nNAME                        CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   \nk3s-dev01-control-plane-0   83m          4%       1320Mi          74%         \nk3s-dev01-worker-0          82m          2%       670Mi           8%          \nk3s-dev01-worker-1          40m          1%       696Mi           9%\n```\n### Relevant log output\n```shell\n[root@k3s-dev01-control-plane-0 ~]# kubectl -n cnpg-system logs -l app=cloudnative-pg\nDEBU[0000] Asset dir /var/lib/rancher/k3s/data/def189ea4d6d3f91781467cdee8a14c124d826f1b3b370103bd7a6e866f1144f \nDEBU[0000] Running /var/lib/rancher/k3s/data/def189ea4d6d3f91781467cdee8a14c124d826f1b3b370103bd7a6e866f1144f/bin/kubectl [kubectl -n cnpg-system logs -l app=cloudnative-pg] \nNo resources found in cnpg-system namespace.\n[root@k3s-dev01-control-plane-0 ~]# kubectl -n cnpg-system logs -l app.kubernetes.io/name=cloudnative-pg\nDEBU[0000] Asset dir /var/lib/rancher/k3s/data/def189ea4d6d3f91781467cdee8a14c124d826f1b3b370103bd7a6e866f1144f \nDEBU[0000] Running /var/lib/rancher/k3s/data/def189ea4d6d3f91781467cdee8a14c124d826f1b3b370103bd7a6e866f1144f/bin/kubectl [kubectl -n cnpg-system logs -l app.kubernetes.io/name=cloudnative-pg] \n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.298741429Z\",\"msg\":\"Starting EventSource\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"source\":\"kind source: *v1.ServiceAccount\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.298742952Z\",\"msg\":\"Starting EventSource\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"source\":\"kind source: *v1.Role\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.298744625Z\",\"msg\":\"Starting EventSource\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"source\":\"kind source: *v1.RoleBinding\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.298745868Z\",\"msg\":\"Starting EventSource\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"source\":\"kind source: *v1.Secret\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.29874738Z\",\"msg\":\"Starting Controller\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.399619982Z\",\"msg\":\"Starting workers\",\"controller\":\"plugin\",\"controllerGroup\":\"\",\"controllerKind\":\"Service\",\"worker count\":10}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.399622208Z\",\"msg\":\"Starting workers\",\"controller\":\"backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Backup\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.402932418Z\",\"msg\":\"Starting workers\",\"controller\":\"scheduled-backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"ScheduledBackup\",\"worker count\":10}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.413148848Z\",\"msg\":\"Starting workers\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"worker count\":10}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.413178894Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":10}\n[root@k3s-dev01-control-plane-0 ~]# kubectl -n cnpg-system logs -l app.kubernetes.io/name=cloudnative-pg -f\nDEBU[0000] Asset dir /var/lib/rancher/k3s/data/def189ea4d6d3f91781467cdee8a14c124d826f1b3b370103bd7a6e866f1144f \nDEBU[0000] Running /var/lib/rancher/k3s/data/def189ea4d6d3f91781467cdee8a14c124d826f1b3b370103bd7a6e866f1144f/bin/kubectl [kubectl -n cnpg-system logs -l app.kubernetes.io/name=cloudnative-pg -f] \n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.298741429Z\",\"msg\":\"Starting EventSource\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"source\":\"kind source: *v1.ServiceAccount\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.298742952Z\",\"msg\":\"Starting EventSource\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"source\":\"kind source: *v1.Role\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.298744625Z\",\"msg\":\"Starting EventSource\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"source\":\"kind source: *v1.RoleBinding\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.298745868Z\",\"msg\":\"Starting EventSource\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"source\":\"kind source: *v1.Secret\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.29874738Z\",\"msg\":\"Starting Controller\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.399619982Z\",\"msg\":\"Starting workers\",\"controller\":\"plugin\",\"controllerGroup\":\"\",\"controllerKind\":\"Service\",\"worker count\":10}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.399622208Z\",\"msg\":\"Starting workers\",\"controller\":\"backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Backup\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.402932418Z\",\"msg\":\"Starting workers\",\"controller\":\"scheduled-backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"ScheduledBackup\",\"worker count\":10}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.413148848Z\",\"msg\":\"Starting workers\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"worker count\":10}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.413178894Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":10}\n{\"level\":\"error\",\"ts\":\"2025-02-02T10:21:16.846463133Z\",\"msg\":\"Failed to update lock optimistically: Put \\\"https://10.43.0.1:443/apis/coordination.k8s.io/v1/namespaces/cnpg-system/leases/db9c8771.cnpg.io?timeout=5s\\\": net/http: request canceled (Client.Timeout exceeded while awaiting headers), falling back to slow path\",\"stacktrace\":\"k8s.io/client-go/tools/leaderelection.(*LeaderElector).tryAcquireOrRenew\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:429\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).renew.func1.1\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:285\\nk8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/loop.go:53\\nk8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/loop.go:54\\nk8s.io/apimachinery/pkg/util/wait.PollUntilContextTimeout\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/poll.go:48\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).renew.func1\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:283\\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/backoff.go:226\\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/backoff.go:227\\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/backoff.go:204\\nk8s.io/apimachinery/pkg/util/wait.Until\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/backoff.go:161\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).renew\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:282\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).Run\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:221\\nsigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).Start.func3\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.3/pkg/manager/internal.go:449\"}\n{\"level\":\"error\",\"ts\":\"2025-02-02T10:21:21.846320855Z\",\"msg\":\"error retrieving resource lock cnpg-system/db9c8771.cnpg.io: Get \\\"https://10.43.0.1:443/apis/coordination.k8s.io/v1/namespaces/cnpg-system/leases/db9c8771.cnpg.io?timeout=5s\\\": context deadline exceeded\",\"stacktrace\":\"k8s.io/client-go/tools/leaderelection.(*LeaderElector).tryAcquireOrRenew\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:436\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).renew.func1.1\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:285\\nk8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/loop.go:53\\nk8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/loop.go:54\\nk8s.io/apimachinery/pkg/util/wait.PollUntilContextTimeout\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/poll.go:48\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).renew.func1\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:283\\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/backoff.go:226\\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/backoff.go:227\\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/backoff.go:204\\nk8s.io/apimachinery/pkg/util/wait.Until\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/backoff.go:161\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).renew\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:282\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).Run\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:221\\nsigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).Start.func3\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.3/pkg/manager/internal.go:449\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:21:21.846390968Z\",\"msg\":\"failed to renew lease cnpg-system/db9c8771.cnpg.io: context deadline exceeded\"}\n{\"level\":\"error\",\"ts\":\"2025-02-02T10:21:26.847513102Z\",\"msg\":\"Failed to release lock: Put \\\"https://10.43.0.1:443/apis/coordination.k8s.io/v1/namespaces/cnpg-system/leases/db9c8771.cnpg.io?timeout=5s\\\": context deadline exceeded\",\"stacktrace\":\"k8s.io/client-go/tools/leaderelection.(*LeaderElector).release\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:322\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).renew\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:303\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).Run\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:221\\nsigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).Start.func3\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.3/pkg/manager/internal.go:449\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:21:26.847589448Z\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:21:26.847599668Z\",\"msg\":\"Stopping and waiting for leader election runnables\"}\n{\"level\":\"error\",\"ts\":\"2025-02-02T10:21:26.847579579Z\",\"logger\":\"setup\",\"msg\":\"problem running manager\",\"error\":\"leader election lost\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/controller.RunController\\n\\tinternal/cmd/manager/controller/controller.go:302\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/controller.NewCmd.func1\\n\\tinternal/cmd/manager/controller/cmd.go:43\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.4/x64/src/runtime/proc.go:272\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:21:26.847607343Z\",\"msg\":\"Stopping and waiting for caches\"}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nmax@self-hosted.io\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nYAML manifest\n### What happened?\nBootstrap a new k3s cluster ( 1 control plane node, 2 workers ) 4vcpu/4gb ram, workers have 8gb ram. The ability to deploy psql clusters is hit and miss. \nSystem has longhorn installed with a dedicated storage class:\n```\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: psql\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"false\"\nprovisioner: driver.longhorn.io\nallowVolumeExpansion: true\nreclaimPolicy: \"Delete\"\nvolumeBindingMode: Immediate\nparameters:\n  numberOfReplicas: \"1\"\n  staleReplicaTimeout: \"30\"\n  fromBackup: \"\"\n  fsType: \"ext4\"\n  dataLocality: \"disabled\"\n  unmapMarkSnapChainRemoved: \"ignored\"\n  disableRevisionCounter: \"true\"\n  dataEngine: \"v1\"\n```\nand the cluster config\n```\nserver_config_yaml: |\n      debug: true\n      write-kubeconfig-mode: \"0644\"\n      flannel-backend: \"wireguard-native\"\n      secrets-encryption: \"true\"\n```\nInstall of the CNPG operator once the cluster is bootstrapped in an ansible task:\n```\n- name: Install CNPG Operator\n  kubernetes.core.k8s:\n    state: present\n    src: https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.25/releases/cnpg-1.25.0.yaml\n  delegate_to: \"{{ hostvars[groups['server'][0]]['ansible_host'] | default(groups['server'][0]) }}\"\n```\nPerform an install of the following manifest:\n```\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: jira-postgres\n  namespace: jira\nspec:\n  instances: 3\n  # Parameters and pg_hba configuration will be append\n  # to the default ones to make the cluster work\n  postgresql:\n    parameters:\n      max_worker_processes: \"100\"\n      max_connections: \"1000\"\n      shared_buffers: \"256MB\"\n    pg_hba:\n      - host all all all scram-sha-256\n  bootstrap:\n    initdb:\n      database: jira\n      owner: jira\n  # Example of rolling update strategy:\n  # - unsupervised: automated update of the primary once all\n  #                 replicas have been upgraded (default)\n  # - supervised: requires manual supervision to perform\n  #               the switchover of the primary\n  primaryUpdateStrategy: unsupervised\n  # Require 1Gi of space per instance using default storage class\n  storage:\n    storageClass: \"psql\"\n    size: 5Gi\n```\nOn the deployment, the following error is returned when `kubectl apply -f` is performed:\n```\n'Failed to create object: b''{\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Internal error occurred: failed calling webhook \\\\\"mcluster.cnpg.io\\\\\": failed to call webhook: Post \\\\\"[https://cnpg-webhook-service.cnpg-system.svc:443/mutate-postgresql-cnpg-io-v1-cluster?timeout=10s\\\\](https://cnpg-webhook-service.cnpg-system.svc/mutate-postgresql-cnpg-io-v1-cluster?timeout=10s\\\\)\": no endpoints available for service \\\\\"cnpg-webhook-service\\\\\"\",\"reason\":\"InternalError\",\"details\":{\"causes\":[{\"message\":\"failed calling webhook \\\\\"mcluster.cnpg.io\\\\\": failed to call webhook: Post \\\\\"[https://cnpg-webhook-service.cnpg-system.svc:443/mutate-postgresql-cnpg-io-v1-cluster?timeout=10s\\\\](https://cnpg-webhook-service.cnpg-system.svc/mutate-postgresql-cnpg-io-v1-cluster?timeout=10s\\\\)\": no endpoints available for service \\\\\"cnpg-webhook-service\\\\\"\"}]},\"code\":500}\\n\n```\nWaiting a period of time, manually applying creates a cluster:\n```\n[root@k3s-dev01-control-plane-0 ~]# kubectl apply -f d.yml\ncluster.postgresql.cnpg.io/jira-postgres created\n```\nOutput of manual deploy, the cluster becomes healthy.\n```\n[root@k3s-dev01-control-plane-0 ~]# kubectl -n jira get cluster,po\nNAME                                       AGE   INSTANCES   READY   STATUS                                       PRIMARY\ncluster.postgresql.cnpg.io/jira-postgres   50s   1                   Waiting for the instances to become active   jira-postgres-1\nNAME                               READY   STATUS      RESTARTS   AGE\npod/jira-postgres-1                0/1     Running     0          14s\npod/jira-postgres-1-initdb-xzzx7   0/1     Completed   0          50s\n```\n### Cluster resource\n```shell\nMachinea are all 4vCPU, and run on U.2/enterprise NVMe local disk. \nNAME                        CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   \nk3s-dev01-control-plane-0   83m          4%       1320Mi          74%         \nk3s-dev01-worker-0          82m          2%       670Mi           8%          \nk3s-dev01-worker-1          40m          1%       696Mi           9%\n```\n### Relevant log output\n```shell\n[root@k3s-dev01-control-plane-0 ~]# kubectl -n cnpg-system logs -l app=cloudnative-pg\nDEBU[0000] Asset dir /var/lib/rancher/k3s/data/def189ea4d6d3f91781467cdee8a14c124d826f1b3b370103bd7a6e866f1144f \nDEBU[0000] Running /var/lib/rancher/k3s/data/def189ea4d6d3f91781467cdee8a14c124d826f1b3b370103bd7a6e866f1144f/bin/kubectl [kubectl -n cnpg-system logs -l app=cloudnative-pg] \nNo resources found in cnpg-system namespace.\n[root@k3s-dev01-control-plane-0 ~]# kubectl -n cnpg-system logs -l app.kubernetes.io/name=cloudnative-pg\nDEBU[0000] Asset dir /var/lib/rancher/k3s/data/def189ea4d6d3f91781467cdee8a14c124d826f1b3b370103bd7a6e866f1144f \nDEBU[0000] Running /var/lib/rancher/k3s/data/def189ea4d6d3f91781467cdee8a14c124d826f1b3b370103bd7a6e866f1144f/bin/kubectl [kubectl -n cnpg-system logs -l app.kubernetes.io/name=cloudnative-pg] \n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.298741429Z\",\"msg\":\"Starting EventSource\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"source\":\"kind source: *v1.ServiceAccount\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.298742952Z\",\"msg\":\"Starting EventSource\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"source\":\"kind source: *v1.Role\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.298744625Z\",\"msg\":\"Starting EventSource\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"source\":\"kind source: *v1.RoleBinding\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.298745868Z\",\"msg\":\"Starting EventSource\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"source\":\"kind source: *v1.Secret\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.29874738Z\",\"msg\":\"Starting Controller\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.399619982Z\",\"msg\":\"Starting workers\",\"controller\":\"plugin\",\"controllerGroup\":\"\",\"controllerKind\":\"Service\",\"worker count\":10}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.399622208Z\",\"msg\":\"Starting workers\",\"controller\":\"backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Backup\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.402932418Z\",\"msg\":\"Starting workers\",\"controller\":\"scheduled-backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"ScheduledBackup\",\"worker count\":10}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.413148848Z\",\"msg\":\"Starting workers\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"worker count\":10}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.413178894Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":10}\n[root@k3s-dev01-control-plane-0 ~]# kubectl -n cnpg-system logs -l app.kubernetes.io/name=cloudnative-pg -f\nDEBU[0000] Asset dir /var/lib/rancher/k3s/data/def189ea4d6d3f91781467cdee8a14c124d826f1b3b370103bd7a6e866f1144f \nDEBU[0000] Running /var/lib/rancher/k3s/data/def189ea4d6d3f91781467cdee8a14c124d826f1b3b370103bd7a6e866f1144f/bin/kubectl [kubectl -n cnpg-system logs -l app.kubernetes.io/name=cloudnative-pg -f] \n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.298741429Z\",\"msg\":\"Starting EventSource\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"source\":\"kind source: *v1.ServiceAccount\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.298742952Z\",\"msg\":\"Starting EventSource\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"source\":\"kind source: *v1.Role\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.298744625Z\",\"msg\":\"Starting EventSource\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"source\":\"kind source: *v1.RoleBinding\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.298745868Z\",\"msg\":\"Starting EventSource\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"source\":\"kind source: *v1.Secret\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.29874738Z\",\"msg\":\"Starting Controller\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.399619982Z\",\"msg\":\"Starting workers\",\"controller\":\"plugin\",\"controllerGroup\":\"\",\"controllerKind\":\"Service\",\"worker count\":10}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.399622208Z\",\"msg\":\"Starting workers\",\"controller\":\"backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Backup\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.402932418Z\",\"msg\":\"Starting workers\",\"controller\":\"scheduled-backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"ScheduledBackup\",\"worker count\":10}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.413148848Z\",\"msg\":\"Starting workers\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"worker count\":10}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:14:40.413178894Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":10}\n{\"level\":\"error\",\"ts\":\"2025-02-02T10:21:16.846463133Z\",\"msg\":\"Failed to update lock optimistically: Put \\\"https://10.43.0.1:443/apis/coordination.k8s.io/v1/namespaces/cnpg-system/leases/db9c8771.cnpg.io?timeout=5s\\\": net/http: request canceled (Client.Timeout exceeded while awaiting headers), falling back to slow path\",\"stacktrace\":\"k8s.io/client-go/tools/leaderelection.(*LeaderElector).tryAcquireOrRenew\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:429\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).renew.func1.1\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:285\\nk8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/loop.go:53\\nk8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/loop.go:54\\nk8s.io/apimachinery/pkg/util/wait.PollUntilContextTimeout\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/poll.go:48\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).renew.func1\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:283\\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/backoff.go:226\\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/backoff.go:227\\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/backoff.go:204\\nk8s.io/apimachinery/pkg/util/wait.Until\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/backoff.go:161\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).renew\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:282\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).Run\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:221\\nsigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).Start.func3\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.3/pkg/manager/internal.go:449\"}\n{\"level\":\"error\",\"ts\":\"2025-02-02T10:21:21.846320855Z\",\"msg\":\"error retrieving resource lock cnpg-system/db9c8771.cnpg.io: Get \\\"https://10.43.0.1:443/apis/coordination.k8s.io/v1/namespaces/cnpg-system/leases/db9c8771.cnpg.io?timeout=5s\\\": context deadline exceeded\",\"stacktrace\":\"k8s.io/client-go/tools/leaderelection.(*LeaderElector).tryAcquireOrRenew\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:436\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).renew.func1.1\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:285\\nk8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/loop.go:53\\nk8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/loop.go:54\\nk8s.io/apimachinery/pkg/util/wait.PollUntilContextTimeout\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/poll.go:48\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).renew.func1\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:283\\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/backoff.go:226\\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/backoff.go:227\\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/backoff.go:204\\nk8s.io/apimachinery/pkg/util/wait.Until\\n\\tpkg/mod/k8s.io/apimachinery@v0.32.0/pkg/util/wait/backoff.go:161\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).renew\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:282\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).Run\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:221\\nsigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).Start.func3\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.3/pkg/manager/internal.go:449\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:21:21.846390968Z\",\"msg\":\"failed to renew lease cnpg-system/db9c8771.cnpg.io: context deadline exceeded\"}\n{\"level\":\"error\",\"ts\":\"2025-02-02T10:21:26.847513102Z\",\"msg\":\"Failed to release lock: Put \\\"https://10.43.0.1:443/apis/coordination.k8s.io/v1/namespaces/cnpg-system/leases/db9c8771.cnpg.io?timeout=5s\\\": context deadline exceeded\",\"stacktrace\":\"k8s.io/client-go/tools/leaderelection.(*LeaderElector).release\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:322\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).renew\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:303\\nk8s.io/client-go/tools/leaderelection.(*LeaderElector).Run\\n\\tpkg/mod/k8s.io/client-go@v0.32.0/tools/leaderelection/leaderelection.go:221\\nsigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).Start.func3\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.3/pkg/manager/internal.go:449\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:21:26.847589448Z\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:21:26.847599668Z\",\"msg\":\"Stopping and waiting for leader election runnables\"}\n{\"level\":\"error\",\"ts\":\"2025-02-02T10:21:26.847579579Z\",\"logger\":\"setup\",\"msg\":\"problem running manager\",\"error\":\"leader election lost\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/controller.RunController\\n\\tinternal/cmd/manager/controller/controller.go:302\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/controller.NewCmd.func1\\n\\tinternal/cmd/manager/controller/cmd.go:43\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.4/x64/src/runtime/proc.go:272\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:21:26.847607343Z\",\"msg\":\"Stopping and waiting for caches\"}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductLeaving it for a while, the pod goes into a crash loop. \n```\n\"level\":\"error\",\"ts\":\"2025-02-02T10:57:03.116359724Z\",\"logger\":\"setup\",\"msg\":\"unable to read ConfigMap\",\"namespace\":\"cnpg-system\",\"name\":\"cnpg-controller-manager-config\",\"error\":\"failed to get API group resources: unable to retrieve the complete list of server APIs: v1: Get \\\"https://10.43.0.1:443/api/v1\\\": dial tcp 10.43.0.1:443: connect: no route to host\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/controller.loadConfiguration\\n\\tinternal/cmd/manager/controller/controller.go:323\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/controller.RunController\\n\\tinternal/cmd/manager/controller/controller.go:175\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/controller.NewCmd.func1\\n\\tinternal/cmd/manager/controller/cmd.go:43\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.4/x64/src/runtime/proc.go:272\"}\n```\nDeleting this pod, it seems to come back. \n```\nroot@k3s-dev01-control-plane-0 ~]# kubectl -n cnpg-system delete po/cnpg-controller-manager-7db9c889df-pcrdr\npod \"cnpg-controller-manager-7db9c889df-pcrdr\" deleted\n[root@k3s-dev01-control-plane-0 ~]# kubectl -n cnpg-system logs -l app.kubernetes.io/name=cloudnative-pg -f\nDEBU[0000] Asset dir /var/lib/rancher/k3s/data/def189ea4d6d3f91781467cdee8a14c124d826f1b3b370103bd7a6e866f1144f \nDEBU[0000] Running /var/lib/rancher/k3s/data/def189ea4d6d3f91781467cdee8a14c124d826f1b3b370103bd7a6e866f1144f/bin/kubectl [kubectl -n cnpg-system logs -l app.kubernetes.io/name=cloudnative-pg -f] \n{\"level\":\"info\",\"ts\":\"2025-02-02T10:58:46.138868264Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Pooler\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:58:46.138870317Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Node\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:58:46.138872482Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.ImageCatalog\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:58:46.138875537Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.ClusterImageCatalog\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:58:46.138877672Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:58:46.433403549Z\",\"msg\":\"Starting workers\",\"controller\":\"backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Backup\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:58:46.433567702Z\",\"msg\":\"Starting workers\",\"controller\":\"scheduled-backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"ScheduledBackup\",\"worker count\":10}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:58:46.434794262Z\",\"msg\":\"Starting workers\",\"controller\":\"plugin\",\"controllerGroup\":\"\",\"controllerKind\":\"Service\",\"worker count\":10}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:58:46.43480955Z\",\"msg\":\"Starting workers\",\"controller\":\"pooler\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Pooler\",\"worker count\":10}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:58:46.437064641Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":10}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:58:46.53351808Z\",\"msg\":\"Creating new Pod to reattach a PVC\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"jira-postgres\",\"namespace\":\"jira\"},\"namespace\":\"jira\",\"name\":\"jira-postgres\",\"reconcileID\":\"bad6d395-338b-467a-9298-137ebce1b778\",\"pod\":\"jira-postgres-1\",\"pvc\":\"jira-postgres-1\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:58:46.631735843Z\",\"msg\":\"Cluster is not healthy\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"jira-postgres\",\"namespace\":\"jira\"},\"namespace\":\"jira\",\"name\":\"jira-postgres\",\"reconcileID\":\"24552973-66d5-4ce0-aa5b-191a249b3d9f\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:58:55.388636052Z\",\"msg\":\"Setting replica label\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"jira-postgres\",\"namespace\":\"jira\"},\"namespace\":\"jira\",\"name\":\"jira-postgres\",\"reconcileID\":\"587740bd-3fbb-42a8-b800-196d82b255a4\",\"pod\":\"jira-postgres-1\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:58:55.402956675Z\",\"msg\":\"Creating new Pod to reattach a PVC\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"jira-postgres\",\"namespace\":\"jira\"},\"namespace\":\"jira\",\"name\":\"jira-postgres\",\"reconcileID\":\"587740bd-3fbb-42a8-b800-196d82b255a4\",\"pod\":\"jira-postgres-3\",\"pvc\":\"jira-postgres-3\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:59:08.436003163Z\",\"msg\":\"Setting replica label\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"jira-postgres\",\"namespace\":\"jira\"},\"namespace\":\"jira\",\"name\":\"jira-postgres\",\"reconcileID\":\"ed794315-022f-4654-998c-8b1d2dd9b3f5\",\"pod\":\"jira-postgres-3\"}\n{\"level\":\"info\",\"ts\":\"2025-02-02T10:59:16.161171225Z\",\"msg\":\"Cluster is healthy\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"jira-postgres\",\"namespace\":\"jira\"},\"namespace\":\"jira\",\"name\":\"jira-postgres\",\"reconcileID\":\"20879841-b62c-43c5-a4e3-53e01b8cb317\"}\n```"
    },
    {
        "title": "chore: simplify configparser",
        "id": 2823669029,
        "state": "open",
        "first": "",
        "messages": "/test d=push tl=4"
    },
    {
        "title": "[Feature]: bootstrap container in the DB pod should do init/join tasks instead of a separate job for ReadWriteOnce StorageClass",
        "id": 2820949886,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI only have storage with the RedWriteOnce policy, meaning the PVC can be mounted to multiple pods/jobs ONLY if they reside on the same k8s node. My init/join jobs are created on separate nodes, creating unsolvable problems.\nI believe relying on ReadWriteMany is not a correct approach for redistributing the PG cluster.\n### Describe the solution you'd like\nThe bootstrap container in the DB pod will always be located with the DB instance, it is an ideal candidate to perform init/join tasks instead of separate job. Jobs should not mount volumes since they can run on another node.\n### Describe alternatives you've considered\nTrying to play with `nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution` so the job would execute on the same node as the container to correctly bootstrap/init. Or you must have a ReadWriteMany storageClass...\n### Additional context\nWorkaround idea: You can create an init container in the DB POD that would not mount any volumes untill the bootsrtap job is completed. In this case even the volume is attached to a Job on one node, the ReadWriteOnce volume will be detached and can be re-attached by the CSI to another node.\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI only have storage with the RedWriteOnce policy, meaning the PVC can be mounted to multiple pods/jobs ONLY if they reside on the same k8s node. My init/join jobs are created on separate nodes, creating unsolvable problems.\nI believe relying on ReadWriteMany is not a correct approach for redistributing the PG cluster.\n### Describe the solution you'd like\nThe bootstrap container in the DB pod will always be located with the DB instance, it is an ideal candidate to perform init/join tasks instead of separate job. Jobs should not mount volumes since they can run on another node.\n### Describe alternatives you've considered\nTrying to play with `nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution` so the job would execute on the same node as the container to correctly bootstrap/init. Or you must have a ReadWriteMany storageClass...\n### Additional context\nWorkaround idea: You can create an init container in the DB POD that would not mount any volumes untill the bootsrtap job is completed. In this case even the volume is attached to a Job on one node, the ReadWriteOnce volume will be detached and can be re-attached by the CSI to another node.\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "feat: add ttlSecondsAfterFinished flag for pgbench command and automatic clean after 1 day",
        "id": 2820881847,
        "state": "open",
        "first": "Fix #4374 \r\nJobs are staying indefinitely and are never cleaned. This PR :\r\n- adds the new flag --ttl that can be used to specify a duration in secondes ;\r\n- sets it by default to one day.",
        "messages": "Fix #4374 \r\nJobs are staying indefinitely and are never cleaned. This PR :\r\n- adds the new flag --ttl that can be used to specify a duration in secondes ;\r\n- sets it by default to one day."
    },
    {
        "title": "[Bug]: Error: secret \"cluster-example-full-app\" not found",
        "id": 2813243905,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nramakrishnan.A@nokia.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nhi , \nwhen i try to run pgbench for cluster cluster-example-full i am getting following error \n**Error: secret \"cluster-example-full-app\" not found****\n$ oc get cluster -n cnpg-system\nNAME                   AGE   INSTANCES   READY   STATUS                                       PRIMARY\ncluster-example-full   40d   3           2       Waiting for the instances to become active   cluster-example-full-2\ncnpg-cluster           40d   3           2       Waiting for the instances to become active   cnpg-cluster-2\n$  kubectl cnpg pgbench cluster-example-full -n cnpg-system\njob/cluster-example-full-pgbench-241277 created\n$ oc get pods -n cnpg-system\nNAME                                        READY   STATUS                            RESTARTS         AGE\ncluster-example-full-1                      1/1     Running                           0                8d\ncluster-example-full-2                      1/1     Running                           0                8d\ncluster-example-full-3                      0/1     Pending                           0                3d3h\n**cluster-example-full-pgbench-241277-qm846   0/1     Init:CreateContainerConfigError   0                9s**\ncnpg-cluster-1                              0/1     Running                           0                3d7h\ncnpg-cluster-2                              1/1     Running                           3 (3d7h ago)     40d\ncnpg-cluster-3                              1/1     Running                           0                12d\ncnpg-controller-manager-6c44f84668-6qcjp    1/1     Running                           29 (3d21h ago)   40d\n$ oc get events -n cnpg-system\n12s         Normal    Pulled             pod/cluster-example-full-pgbench-241277-qm846   Container image \"nmapps.tre.nsn-rdnet.net/cal-shared-product/postgresql:16.1\" already present on machine\n**12s         Warning   Failed             pod/cluster-example-full-pgbench-241277-qm846   Error: secret \"cluster-example-full-app\" not found**\n41s         Normal    SuccessfulCreate   job/cluster-example-full-pgbench-241277         Created pod: cluster-example-full-pgbench-241277-qtuscode: 500\n$ oc get secrets -n cnpg-system\nNAME                                     TYPE                                  DATA   AGE\nbackup-creds                             Opaque                                2      40d\nbuilder-dockercfg-qmgvn                  kubernetes.io/dockercfg               1      43d\nbuilder-token-m99kn                      kubernetes.io/service-account-token   4      43d\ncluster-example-app-user                 kubernetes.io/basic-auth              2      40d\ncluster-example-full-ca                  Opaque                                2      40d\ncluster-example-full-dockercfg-hljp2     kubernetes.io/dockercfg               1      40d\ncluster-example-full-replication         kubernetes.io/tls                     2      40d\ncluster-example-full-server              kubernetes.io/tls                     2      40d\ncluster-example-full-token-7kmqr         kubernetes.io/service-account-token   4      40d\ncluster-example-superuser                kubernetes.io/basic-auth              2      40d\ncnpg-ca-secret                           Opaque                                2      40d\ncnpg-cluster-app                         kubernetes.io/basic-auth              9      40d\ncnpg-cluster-ca                          Opaque                                2      40d\ncnpg-cluster-dockercfg-lj464             kubernetes.io/dockercfg               1      40d\ncnpg-cluster-pooler                      kubernetes.io/tls                     2      7d5h\ncnpg-cluster-replication                 kubernetes.io/tls                     2      40d\ncnpg-cluster-server                      kubernetes.io/tls                     2      40d\ncnpg-cluster-superuser                   kubernetes.io/basic-auth              9      40d\ncnpg-cluster-token-4bcg9                 kubernetes.io/service-account-token   4      40d\ncnpg-manager-dockercfg-lpwmq             kubernetes.io/dockercfg               1      43d\ncnpg-manager-token-qptqj                 kubernetes.io/service-account-token   4      43d\ncnpg-webhook-cert                        kubernetes.io/tls                     2      40d\nLooks we need to have secrete name  as  \"cluster-example-full-app\" \n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nramakrishnan.A@nokia.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nhi , \nwhen i try to run pgbench for cluster cluster-example-full i am getting following error \n**Error: secret \"cluster-example-full-app\" not found****\n$ oc get cluster -n cnpg-system\nNAME                   AGE   INSTANCES   READY   STATUS                                       PRIMARY\ncluster-example-full   40d   3           2       Waiting for the instances to become active   cluster-example-full-2\ncnpg-cluster           40d   3           2       Waiting for the instances to become active   cnpg-cluster-2\n$  kubectl cnpg pgbench cluster-example-full -n cnpg-system\njob/cluster-example-full-pgbench-241277 created\n$ oc get pods -n cnpg-system\nNAME                                        READY   STATUS                            RESTARTS         AGE\ncluster-example-full-1                      1/1     Running                           0                8d\ncluster-example-full-2                      1/1     Running                           0                8d\ncluster-example-full-3                      0/1     Pending                           0                3d3h\n**cluster-example-full-pgbench-241277-qm846   0/1     Init:CreateContainerConfigError   0                9s**\ncnpg-cluster-1                              0/1     Running                           0                3d7h\ncnpg-cluster-2                              1/1     Running                           3 (3d7h ago)     40d\ncnpg-cluster-3                              1/1     Running                           0                12d\ncnpg-controller-manager-6c44f84668-6qcjp    1/1     Running                           29 (3d21h ago)   40d\n$ oc get events -n cnpg-system\n12s         Normal    Pulled             pod/cluster-example-full-pgbench-241277-qm846   Container image \"nmapps.tre.nsn-rdnet.net/cal-shared-product/postgresql:16.1\" already present on machine\n**12s         Warning   Failed             pod/cluster-example-full-pgbench-241277-qm846   Error: secret \"cluster-example-full-app\" not found**\n41s         Normal    SuccessfulCreate   job/cluster-example-full-pgbench-241277         Created pod: cluster-example-full-pgbench-241277-qtuscode: 500\n$ oc get secrets -n cnpg-system\nNAME                                     TYPE                                  DATA   AGE\nbackup-creds                             Opaque                                2      40d\nbuilder-dockercfg-qmgvn                  kubernetes.io/dockercfg               1      43d\nbuilder-token-m99kn                      kubernetes.io/service-account-token   4      43d\ncluster-example-app-user                 kubernetes.io/basic-auth              2      40d\ncluster-example-full-ca                  Opaque                                2      40d\ncluster-example-full-dockercfg-hljp2     kubernetes.io/dockercfg               1      40d\ncluster-example-full-replication         kubernetes.io/tls                     2      40d\ncluster-example-full-server              kubernetes.io/tls                     2      40d\ncluster-example-full-token-7kmqr         kubernetes.io/service-account-token   4      40d\ncluster-example-superuser                kubernetes.io/basic-auth              2      40d\ncnpg-ca-secret                           Opaque                                2      40d\ncnpg-cluster-app                         kubernetes.io/basic-auth              9      40d\ncnpg-cluster-ca                          Opaque                                2      40d\ncnpg-cluster-dockercfg-lj464             kubernetes.io/dockercfg               1      40d\ncnpg-cluster-pooler                      kubernetes.io/tls                     2      7d5h\ncnpg-cluster-replication                 kubernetes.io/tls                     2      40d\ncnpg-cluster-server                      kubernetes.io/tls                     2      40d\ncnpg-cluster-superuser                   kubernetes.io/basic-auth              9      40d\ncnpg-cluster-token-4bcg9                 kubernetes.io/service-account-token   4      40d\ncnpg-manager-dockercfg-lpwmq             kubernetes.io/dockercfg               1      43d\ncnpg-manager-token-qptqj                 kubernetes.io/service-account-token   4      43d\ncnpg-webhook-cert                        kubernetes.io/tls                     2      40d\nLooks we need to have secrete name  as  \"cluster-example-full-app\" \n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductHi\nI create a secrete \"cluster-example-full-app\" as name as expected, then pgbench works for the cluster \"cluster-example-full\" \nStill got below error.  but looks pgbench test run was successful \noc get jobs job/cluster-example-full-pgbench-573495 -n cnpg-system\nerror: there is no need to specify a resource type as a separate argument when passing arguments in resource/name form (e.g. 'oc get resource/<resource_name>' instead of 'oc get resource resource/<resource_name>'\n---\nCould you share the YAML of `cluster-example-full`? One scenario where the `-app` secret is intentionally not created is for Replica Clusters, it's documented on https://cloudnative-pg.io/documentation/1.25/replica_cluster/.\nFrom [#example-of-standalone-replica-cluster-using-pg_basebackup](https://cloudnative-pg.io/documentation/1.25/replica_cluster/#example-of-standalone-replica-cluster-using-pg_basebackup):\n> You should also consider copying over the application user secret from the original cluster and keep it synchronized with the source.\n---\nyaml file attached [cnpg-cluster-example-full.txt](https://github.com/user-attachments/files/18613553/cnpg-cluster-example-full.txt)\n---\nYou have passed your own custom application user secret:\n```\n  bootstrap:\n    initdb:\n      database: app\n      owner: app\n      secret:\n        name: cluster-example-app-user\n```\nso the `<cluster-name>-app` secret won't be created.\nSee https://cloudnative-pg.io/documentation/1.25/cloudnative-pg.v1/#postgresql-cnpg-io-v1-BootstrapInitDB:\n> secret: Name of the secret containing the initial credentials for the owner of the user database. If empty a new secret will be created from scratch\n---\nI have a similar issue, where i have set my own custom application user secrets, but the operator says it cannot find the \"username\" key in the secret, but it is actually there.\n```{\"level\":\"error\",\"ts\":\"2025-02-06T10:24:28.607438401+01:00\",\"msg\":\"Reconciler error\",\"logger\":\"instance-manager\",\"logging_pod\":\"autpgc16logging01-staging-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"autpgc16logging01-staging\",\"namespace\":\"logging\"},\"namespace\":\"logging\",\"name\":\"autpgc16logging01-staging\",\"reconcileID\":\"a9b4027c-c642-4a71-b144-6d3a7e8252c4\",\"error\":\"while updating database owner password: username key doesn't exist inside the secret\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:316\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:263\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:224\"}```\n```  bootstrap:\n    initdb:\n      localeCollate: 'en_US.utf8'\n      localeCType: 'en_US.utf8'\n      database: fluentbit\n      owner: fluentbit\n      secret:\n        name: loggingdb-secret\n      dataChecksums: true\n      encoding: 'UTF8' #default\n```\n```\nk describe secret loggingdb-secret -n logging\nName:         loggingdb-secret\nNamespace:    logging\nLabels:       <none>\nAnnotations:  <none>\nType:  Opaque\nData\n====\npassword:    64 bytes\nuser:        9 bytes\nusername:    10 bytes\nPGPASSWORD:  64 bytes\n```\n---\nThe username key did not exist in the custom superusersecret, so actually here the error message was wrong"
    },
    {
        "title": "[Bug]: fio performance test fails with \"exec /opt/runall.sh: exec format error\" on ARM64",
        "id": 2812645822,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nst.koelle@gmail.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nYAML manifest\n### What happened?\nRunning the fio performance test from the manual fails on ARM64:\nCommand:\n`kubectl cnpg fio fio-job-hv   -n ns    --storageClass hcloud-volumes   --pvcSize 10Gi`\nStatus of the pod:\n`Back-off restarting failed container fio in pod `\nLog of the pod:\n```\nkubectl logs -f -n ns fio-job-hv-776dc858-wq2rx\nexec /opt/runall.sh: exec format error\n```\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nst.koelle@gmail.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nYAML manifest\n### What happened?\nRunning the fio performance test from the manual fails on ARM64:\nCommand:\n`kubectl cnpg fio fio-job-hv   -n ns    --storageClass hcloud-volumes   --pvcSize 10Gi`\nStatus of the pod:\n`Back-off restarting failed container fio in pod `\nLog of the pod:\n```\nkubectl logs -f -n ns fio-job-hv-776dc858-wq2rx\nexec /opt/runall.sh: exec format error\n```\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductHi @stephankoelle \nSadly, the image we use for the fio test it's not multiarch and probably will not be multiarch anytime soon, if you can find a solution with a multiarch for fio will be great, but is out of the scope of the project to provide those images.\nRegards,"
    },
    {
        "title": "fix(instance-controller): error on absence of user secrets",
        "id": 2811790255,
        "state": "open",
        "first": "Fix for https://github.com/cloudnative-pg/cloudnative-pg/issues/6069 that makes the existence of user-related secrets required to proceed",
        "messages": "Fix for https://github.com/cloudnative-pg/cloudnative-pg/issues/6069 that makes the existence of user-related secrets required to proceed"
    },
    {
        "title": "feat: declarative support for PostgreSQL major in-place upgrades",
        "id": 2811116468,
        "state": "open",
        "first": "This PR introduces support for upgrading PostgreSQL clusters to a higher major version in CloudNativePG, enabling a fully declarative approach to major version upgrades.\r\nUsers can specify a new image with a higher major version using any supported method. The operator tracks the last known image from the previous version in `.status.majorVersionUpgradeFromImage`, which also acts as a signal that a major version upgrade is in progress.\r\nThe upgrade process begins by hibernating the cluster: all pods and jobs are deleted to ensure nothing is accessing the data directory. A dedicated upgrade job is then created to perform the upgrade on the primary instance. This job uses an `initContainer` to copy the old PostgreSQL installation and runs `pg_upgrade --link` to efficiently migrate the data. Once the upgrade is complete, all non-primary instances are removed, and new standbys are cloned to restore the desired number of instances.\r\nCloses #4682",
        "messages": "This PR introduces support for upgrading PostgreSQL clusters to a higher major version in CloudNativePG, enabling a fully declarative approach to major version upgrades.\r\nUsers can specify a new image with a higher major version using any supported method. The operator tracks the last known image from the previous version in `.status.majorVersionUpgradeFromImage`, which also acts as a signal that a major version upgrade is in progress.\r\nThe upgrade process begins by hibernating the cluster: all pods and jobs are deleted to ensure nothing is accessing the data directory. A dedicated upgrade job is then created to perform the upgrade on the primary instance. This job uses an `initContainer` to copy the old PostgreSQL installation and runs `pg_upgrade --link` to efficiently migrate the data. Once the upgrade is complete, all non-primary instances are removed, and new standbys are cloned to restore the desired number of instances.\r\nCloses #4682/test\n---\nJust to be sure I understand this correctly: This is an offline migration as in: The cluster gets taken down completely, the primary upgraded in-place, started up again and new replicas are cloned from that, right?\n---\n> Just to be sure I understand this correctly: This is an offline migration as in: The cluster gets taken down completely, the primary upgraded in-place, started up again and new replicas are cloned from that, right?\r\nYes exactly. However, it will be speedy, as no data will be copied around."
    },
    {
        "title": "[Bug]: inconsistent default calculation for shared_buffers GUCs",
        "id": 2809522777,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nneel.patel@enterprisedb.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWebhook validation is failing when user provide `request.limits` along with shared_buffers GUCs ( provide value without unit )\nValidation is  failing with `Memory request is lower than PostgreSQL `shared_buffers`\nIf user provide `shared_buffers` value as \"256\", it convert to 256 MB and which will fail against the `request.memory` validation.\n**Expected result:**\nIf user do not provide any unit, it should not convert to MB as default.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: test-shared-buffers\n  namespace: default\nspec:\n  bootstrap:\n    initdb:\n      database: app\n      encoding: UTF8\n      localeCType: C\n      localeCollate: C\n      owner: app\n  imageName: postgres:16.6\n  instances: 1\n  postgresql:\n    parameters:\n      shared_buffers: \"540\"\n  resources:\n    limits:\n      cpu: 500m\n      memory: 512Mi\n    requests:\n      cpu: 500m\n      memory: 512Mi\n  storage:\n    resizeInUseVolumes: true\n    size: 1Gi\n```\n### Relevant log output\n```shell\n\u276f k apply -f test-shared_buffers.yaml\nThe Cluster \"test-shared-buffers\" is invalid: spec.resources.requests.memory: Invalid value: \"512Mi\": Memory request is lower than PostgreSQL `shared_buffers` value\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nneel.patel@enterprisedb.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWebhook validation is failing when user provide `request.limits` along with shared_buffers GUCs ( provide value without unit )\nValidation is  failing with `Memory request is lower than PostgreSQL `shared_buffers`\nIf user provide `shared_buffers` value as \"256\", it convert to 256 MB and which will fail against the `request.memory` validation.\n**Expected result:**\nIf user do not provide any unit, it should not convert to MB as default.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: test-shared-buffers\n  namespace: default\nspec:\n  bootstrap:\n    initdb:\n      database: app\n      encoding: UTF8\n      localeCType: C\n      localeCollate: C\n      owner: app\n  imageName: postgres:16.6\n  instances: 1\n  postgresql:\n    parameters:\n      shared_buffers: \"540\"\n  resources:\n    limits:\n      cpu: 500m\n      memory: 512Mi\n    requests:\n      cpu: 500m\n      memory: 512Mi\n  storage:\n    resizeInUseVolumes: true\n    size: 1Gi\n```\n### Relevant log output\n```shell\n\u276f k apply -f test-shared_buffers.yaml\nThe Cluster \"test-shared-buffers\" is invalid: spec.resources.requests.memory: Invalid value: \"512Mi\": Memory request is lower than PostgreSQL `shared_buffers` value\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductWhat do you think of following what's done in PostgreSQL ? i.e without units, it is taken as blocks of a size of 8kB.\n[https://www.postgresql.org/docs/current/runtime-config-resource.html  ](https://www.postgresql.org/docs/current/runtime-config-resource.html)\n---\nJust push a PR. It's a proposal of what I said.\n#6675"
    },
    {
        "title": "[Bug]: Cluster import failing due to cnpg_pooler_pgbouncer Role",
        "id": 2809056504,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24 (latest patch)\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\n**Issue Description**\nI am creating a cnpg cluster using another cnpg cluster in bootstap > initdb > import.   \nImport is failing on error: could not execute query: ERROR:  role \\\"cnpg_pooler_pgbouncer\\\" does not exist.\nI know cnpg_pooler_pgbouncer Roles is not to be imported but if pooler is deployed in target cluster, import process should not fail \n**Steps to reproduce**\n1. Decide a namespace\n`MY_NS=\"cnpg-import-test\"`\n`echo $MY_NS`\n2. Create Namespace\n`kubectl create ns $MY_NS`\n3. Create Admin Secret. This secret will be used by both source as well as target cluster. \n`kubectl apply -f createAdminSecret.yaml -n $MY_NS`\n4. Create Source Cluster and Pooler\n`kubectl apply -f createSourceClusternPooler.yaml -n $MY_NS`\n5. Check source cluster and pooler\n`kubectl get all -n $MY_NS`\n7. Create target pooler and cluster using source cluster. `kubectl apply -f createTargetClusternPooler.yaml -n $MY_NS`\nImport process will start and failed after some time.  Complete error is attached.\n**createAdminSecret.yaml**\n```\napiVersion: v1\ndata:\n  password: c2VjcmV0 # secret\n  username: cG9zdGdyZXM=  # postgres\nkind: Secret\nmetadata:\n  name: postgres-admin-user-secret\ntype: kubernetes.io/basic-auth\n```\n**createSourceClusternPooler.yaml**\n```\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: postgres-cluster-source\nspec:\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.3-1\n  instances: 1\n  bootstrap:\n    initdb:\n      database: app\n      owner: app\n      secret:\n        name: cnpg-cluster-app-secret\n  enableSuperuserAccess: true\n  superuserSecret:\n    name: postgres-admin-user-secret\n  storage:\n    #storageClass: managed-nfs-storage\n    size: 100Gi\n---   \napiVersion: postgresql.cnpg.io/v1\nkind: Pooler\nmetadata:\n  name: postgres-cluster-source-pooler-rw\nspec:\n  cluster:\n    name: postgres-cluster-source\n  type: rw\n  instances: 1\n  template:\n    metadata:\n      labels:\n        app: pooler\n    spec:\n      containers:\n        - name: pgbouncer\n          image: ghcr.io/cloudnative-pg/pgbouncer:1.22.1-14\n  pgbouncer:\n    poolMode: session\n    parameters:\n      max_client_conn: \"1000\"\n      default_pool_size: \"10\"\n  serviceTemplate:\n    spec:\n      type: LoadBalancer\n```\n**createTargetClusternPooler.yaml**\n```\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: postgres-cluster-target\nspec:\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.3-1\n  instances: 1\n  bootstrap:\n    initdb:\n      import:\n        type: monolith\n        databases:\n          - \"*\"\n        roles:\n          - \"*\"\n        source:\n          externalCluster: source-cnpg-cluster\n  externalClusters:\n    - name: source-cnpg-cluster\n      connectionParameters:\n        host: postgres-cluster-source-rw\n        user: postgres\n        #sslmode: verify-full\n        dbname: postgres\n      password: \n        name: postgres-admin-user-secret\n        key: password\n  enableSuperuserAccess: true\n  superuserSecret:\n    name: postgres-admin-user-secret\n  storage:\n    #storageClass: managed-nfs-storage\n    size: 100Gi\n---   \napiVersion: postgresql.cnpg.io/v1\nkind: Pooler\nmetadata:\n  name: postgres-cluster-target-pooler-rw\nspec:\n  cluster:\n    name: postgres-cluster-target\n  type: rw\n  instances: 1\n  template:\n    metadata:\n      labels:\n        app: pooler\n    spec:\n      containers:\n        - name: pgbouncer\n          image: ghcr.io/cloudnative-pg/pgbouncer:1.22.1-14\n  pgbouncer:\n    poolMode: session\n    parameters:\n      max_client_conn: \"1000\"\n      default_pool_size: \"10\"\n  serviceTemplate:\n    spec:\n      type: LoadBalancer\n```\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.339515384Z\",\"logger\":\"pg_dump\",\"msg\":\"pg_dump: saving encoding = UTF8\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.339613287Z\",\"logger\":\"pg_dump\",\"msg\":\"pg_dump: saving standard_conforming_strings = on\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.339863926Z\",\"logger\":\"pg_dump\",\"msg\":\"pg_dump: saving search_path = \",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.339936108Z\",\"logger\":\"pg_dump\",\"msg\":\"pg_dump: saving database definition\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.438904125Z\",\"msg\":\"executing database importing section\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"databaseName\":\"app\",\"section\":\"pre-data\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.476891589Z\",\"msg\":\"Running pg_restore\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"cmd\":\"pg_restore\",\"options\":[\"-U\",\"postgres\",\"-d\",\"host=/controller/run port=5432 user=postgres sslmode=disable application_name=cnpg-instance-manager dbname=app\",\"--section\",\"pre-data\",\"/var/lib/postgresql/data/pgdata/dumps/app.dump\"]}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.830024178Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"record\":{\"log_time\":\"2025-01-24 10:06:10.829 UTC\",\"user_name\":\"postgres\",\"database_name\":\"app\",\"process_id\":\"44\",\"connection_from\":\"[local]\",\"session_id\":\"67936612.2c\",\"session_line_num\":\"1\",\"command_tag\":\"GRANT\",\"session_start_time\":\"2025-01-24 10:06:10 UTC\",\"virtual_transaction_id\":\"3/34\",\"transaction_id\":\"733\",\"error_severity\":\"ERROR\",\"sql_state_code\":\"42704\",\"message\":\"role \\\"cnpg_pooler_pgbouncer\\\" does not exist\",\"query\":\"REVOKE ALL ON FUNCTION public.user_search(uname text) FROM PUBLIC;\\nGRANT ALL ON FUNCTION public.user_search(uname text) TO cnpg_pooler_pgbouncer;\\n\\n\\n\",\"application_name\":\"cnpg-instance-manager\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.830119294Z\",\"logger\":\"pg_restore\",\"msg\":\"pg_restore: error: could not execute query: ERROR:  role \\\"cnpg_pooler_pgbouncer\\\" does not exist\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.830125666Z\",\"logger\":\"pg_restore\",\"msg\":\"Command was: REVOKE ALL ON FUNCTION public.user_search(uname text) FROM PUBLIC;\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.830141268Z\",\"logger\":\"pg_restore\",\"msg\":\"GRANT ALL ON FUNCTION public.user_search(uname text) TO cnpg_pooler_pgbouncer;\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.830153995Z\",\"logger\":\"pg_restore\",\"msg\":\"pg_restore: warning: errors ignored on restore: 1\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.876217807Z\",\"logger\":\"pg_ctl\",\"msg\":\"pg_ctl: server is running (PID: 29)\\n/usr/lib/postgresql/16/bin/postgres \\\"-D\\\" \\\"/var/lib/postgresql/data/pgdata\\\" \\\"-c\\\" \\\"port=5432\\\" \\\"-c\\\" \\\"unix_socket_directories=/controller/run\\\" \\\"-c\\\" \\\"listen_addresses=127.0.0.1\\\"\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.876262185Z\",\"msg\":\"Shutting down instance\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"mode\":\"fast\",\"timeout\":null}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.878328336Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"record\":{\"log_time\":\"2025-01-24 10:06:10.878 UTC\",\"process_id\":\"29\",\"session_id\":\"679365f3.1d\",\"session_line_num\":\"6\",\"session_start_time\":\"2025-01-24 10:05:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"received fast shutdown request\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.934900871Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"record\":{\"log_time\":\"2025-01-24 10:06:10.934 UTC\",\"process_id\":\"29\",\"session_id\":\"679365f3.1d\",\"session_line_num\":\"7\",\"session_start_time\":\"2025-01-24 10:05:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"aborting any active transactions\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.957644275Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"record\":{\"log_time\":\"2025-01-24 10:06:10.957 UTC\",\"process_id\":\"29\",\"session_id\":\"679365f3.1d\",\"session_line_num\":\"8\",\"session_start_time\":\"2025-01-24 10:05:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"background worker \\\"logical replication launcher\\\" (PID 36) exited with exit code 1\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:11.065954155Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"record\":{\"log_time\":\"2025-01-24 10:06:11.065 UTC\",\"process_id\":\"31\",\"session_id\":\"679365f3.1f\",\"session_line_num\":\"1\",\"session_start_time\":\"2025-01-24 10:05:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"shutting down\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:11.11958448Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"record\":{\"log_time\":\"2025-01-24 10:06:11.119 UTC\",\"process_id\":\"31\",\"session_id\":\"679365f3.1f\",\"session_line_num\":\"2\",\"session_start_time\":\"2025-01-24 10:05:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"checkpoint starting: shutdown immediate\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:14.778962416Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"record\":{\"log_time\":\"2025-01-24 10:06:14.778 UTC\",\"process_id\":\"31\",\"session_id\":\"679365f3.1f\",\"session_line_num\":\"3\",\"session_start_time\":\"2025-01-24 10:05:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"checkpoint complete: wrote 931 buffers (5.7%); 0 WAL file(s) added, 0 removed, 0 recycled; write=3.492 s, sync=0.001 s, total=3.713 s; sync files=0, longest=0.000 s, average=0.000 s; distance=7388 kB, estimate=7388 kB; lsn=0/1C1CEB0, redo lsn=0/1C1CEB0\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:16.499183381Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"record\":{\"log_time\":\"2025-01-24 10:06:16.498 UTC\",\"process_id\":\"29\",\"session_id\":\"679365f3.1d\",\"session_line_num\":\"9\",\"session_start_time\":\"2025-01-24 10:05:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:16.530322796Z\",\"logger\":\"pg_ctl\",\"msg\":\"waiting for server to shut down......... done\",\"pipe\":\"stdout\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:16.530356328Z\",\"logger\":\"pg_ctl\",\"msg\":\"server stopped\",\"pipe\":\"stdout\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:16.530606775Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"error\",\"ts\":\"2025-01-24T10:06:16.530646703Z\",\"msg\":\"Error while bootstrapping data directory\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"error\":\"while executing logical import: error while executing pg_restore, section:pre-data, exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.initSubCommand\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:160\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.NewCmd.func2\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:109\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\nError: while executing logical import: error while executing pg_restore, section:pre-data, exit status 1\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24 (latest patch)\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\n**Issue Description**\nI am creating a cnpg cluster using another cnpg cluster in bootstap > initdb > import.   \nImport is failing on error: could not execute query: ERROR:  role \\\"cnpg_pooler_pgbouncer\\\" does not exist.\nI know cnpg_pooler_pgbouncer Roles is not to be imported but if pooler is deployed in target cluster, import process should not fail \n**Steps to reproduce**\n1. Decide a namespace\n`MY_NS=\"cnpg-import-test\"`\n`echo $MY_NS`\n2. Create Namespace\n`kubectl create ns $MY_NS`\n3. Create Admin Secret. This secret will be used by both source as well as target cluster. \n`kubectl apply -f createAdminSecret.yaml -n $MY_NS`\n4. Create Source Cluster and Pooler\n`kubectl apply -f createSourceClusternPooler.yaml -n $MY_NS`\n5. Check source cluster and pooler\n`kubectl get all -n $MY_NS`\n7. Create target pooler and cluster using source cluster. `kubectl apply -f createTargetClusternPooler.yaml -n $MY_NS`\nImport process will start and failed after some time.  Complete error is attached.\n**createAdminSecret.yaml**\n```\napiVersion: v1\ndata:\n  password: c2VjcmV0 # secret\n  username: cG9zdGdyZXM=  # postgres\nkind: Secret\nmetadata:\n  name: postgres-admin-user-secret\ntype: kubernetes.io/basic-auth\n```\n**createSourceClusternPooler.yaml**\n```\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: postgres-cluster-source\nspec:\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.3-1\n  instances: 1\n  bootstrap:\n    initdb:\n      database: app\n      owner: app\n      secret:\n        name: cnpg-cluster-app-secret\n  enableSuperuserAccess: true\n  superuserSecret:\n    name: postgres-admin-user-secret\n  storage:\n    #storageClass: managed-nfs-storage\n    size: 100Gi\n---   \napiVersion: postgresql.cnpg.io/v1\nkind: Pooler\nmetadata:\n  name: postgres-cluster-source-pooler-rw\nspec:\n  cluster:\n    name: postgres-cluster-source\n  type: rw\n  instances: 1\n  template:\n    metadata:\n      labels:\n        app: pooler\n    spec:\n      containers:\n        - name: pgbouncer\n          image: ghcr.io/cloudnative-pg/pgbouncer:1.22.1-14\n  pgbouncer:\n    poolMode: session\n    parameters:\n      max_client_conn: \"1000\"\n      default_pool_size: \"10\"\n  serviceTemplate:\n    spec:\n      type: LoadBalancer\n```\n**createTargetClusternPooler.yaml**\n```\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: postgres-cluster-target\nspec:\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.3-1\n  instances: 1\n  bootstrap:\n    initdb:\n      import:\n        type: monolith\n        databases:\n          - \"*\"\n        roles:\n          - \"*\"\n        source:\n          externalCluster: source-cnpg-cluster\n  externalClusters:\n    - name: source-cnpg-cluster\n      connectionParameters:\n        host: postgres-cluster-source-rw\n        user: postgres\n        #sslmode: verify-full\n        dbname: postgres\n      password: \n        name: postgres-admin-user-secret\n        key: password\n  enableSuperuserAccess: true\n  superuserSecret:\n    name: postgres-admin-user-secret\n  storage:\n    #storageClass: managed-nfs-storage\n    size: 100Gi\n---   \napiVersion: postgresql.cnpg.io/v1\nkind: Pooler\nmetadata:\n  name: postgres-cluster-target-pooler-rw\nspec:\n  cluster:\n    name: postgres-cluster-target\n  type: rw\n  instances: 1\n  template:\n    metadata:\n      labels:\n        app: pooler\n    spec:\n      containers:\n        - name: pgbouncer\n          image: ghcr.io/cloudnative-pg/pgbouncer:1.22.1-14\n  pgbouncer:\n    poolMode: session\n    parameters:\n      max_client_conn: \"1000\"\n      default_pool_size: \"10\"\n  serviceTemplate:\n    spec:\n      type: LoadBalancer\n```\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.339515384Z\",\"logger\":\"pg_dump\",\"msg\":\"pg_dump: saving encoding = UTF8\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.339613287Z\",\"logger\":\"pg_dump\",\"msg\":\"pg_dump: saving standard_conforming_strings = on\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.339863926Z\",\"logger\":\"pg_dump\",\"msg\":\"pg_dump: saving search_path = \",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.339936108Z\",\"logger\":\"pg_dump\",\"msg\":\"pg_dump: saving database definition\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.438904125Z\",\"msg\":\"executing database importing section\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"databaseName\":\"app\",\"section\":\"pre-data\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.476891589Z\",\"msg\":\"Running pg_restore\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"cmd\":\"pg_restore\",\"options\":[\"-U\",\"postgres\",\"-d\",\"host=/controller/run port=5432 user=postgres sslmode=disable application_name=cnpg-instance-manager dbname=app\",\"--section\",\"pre-data\",\"/var/lib/postgresql/data/pgdata/dumps/app.dump\"]}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.830024178Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"record\":{\"log_time\":\"2025-01-24 10:06:10.829 UTC\",\"user_name\":\"postgres\",\"database_name\":\"app\",\"process_id\":\"44\",\"connection_from\":\"[local]\",\"session_id\":\"67936612.2c\",\"session_line_num\":\"1\",\"command_tag\":\"GRANT\",\"session_start_time\":\"2025-01-24 10:06:10 UTC\",\"virtual_transaction_id\":\"3/34\",\"transaction_id\":\"733\",\"error_severity\":\"ERROR\",\"sql_state_code\":\"42704\",\"message\":\"role \\\"cnpg_pooler_pgbouncer\\\" does not exist\",\"query\":\"REVOKE ALL ON FUNCTION public.user_search(uname text) FROM PUBLIC;\\nGRANT ALL ON FUNCTION public.user_search(uname text) TO cnpg_pooler_pgbouncer;\\n\\n\\n\",\"application_name\":\"cnpg-instance-manager\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.830119294Z\",\"logger\":\"pg_restore\",\"msg\":\"pg_restore: error: could not execute query: ERROR:  role \\\"cnpg_pooler_pgbouncer\\\" does not exist\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.830125666Z\",\"logger\":\"pg_restore\",\"msg\":\"Command was: REVOKE ALL ON FUNCTION public.user_search(uname text) FROM PUBLIC;\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.830141268Z\",\"logger\":\"pg_restore\",\"msg\":\"GRANT ALL ON FUNCTION public.user_search(uname text) TO cnpg_pooler_pgbouncer;\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.830153995Z\",\"logger\":\"pg_restore\",\"msg\":\"pg_restore: warning: errors ignored on restore: 1\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.876217807Z\",\"logger\":\"pg_ctl\",\"msg\":\"pg_ctl: server is running (PID: 29)\\n/usr/lib/postgresql/16/bin/postgres \\\"-D\\\" \\\"/var/lib/postgresql/data/pgdata\\\" \\\"-c\\\" \\\"port=5432\\\" \\\"-c\\\" \\\"unix_socket_directories=/controller/run\\\" \\\"-c\\\" \\\"listen_addresses=127.0.0.1\\\"\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.876262185Z\",\"msg\":\"Shutting down instance\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"mode\":\"fast\",\"timeout\":null}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.878328336Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"record\":{\"log_time\":\"2025-01-24 10:06:10.878 UTC\",\"process_id\":\"29\",\"session_id\":\"679365f3.1d\",\"session_line_num\":\"6\",\"session_start_time\":\"2025-01-24 10:05:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"received fast shutdown request\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.934900871Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"record\":{\"log_time\":\"2025-01-24 10:06:10.934 UTC\",\"process_id\":\"29\",\"session_id\":\"679365f3.1d\",\"session_line_num\":\"7\",\"session_start_time\":\"2025-01-24 10:05:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"aborting any active transactions\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:10.957644275Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"record\":{\"log_time\":\"2025-01-24 10:06:10.957 UTC\",\"process_id\":\"29\",\"session_id\":\"679365f3.1d\",\"session_line_num\":\"8\",\"session_start_time\":\"2025-01-24 10:05:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"background worker \\\"logical replication launcher\\\" (PID 36) exited with exit code 1\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:11.065954155Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"record\":{\"log_time\":\"2025-01-24 10:06:11.065 UTC\",\"process_id\":\"31\",\"session_id\":\"679365f3.1f\",\"session_line_num\":\"1\",\"session_start_time\":\"2025-01-24 10:05:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"shutting down\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:11.11958448Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"record\":{\"log_time\":\"2025-01-24 10:06:11.119 UTC\",\"process_id\":\"31\",\"session_id\":\"679365f3.1f\",\"session_line_num\":\"2\",\"session_start_time\":\"2025-01-24 10:05:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"checkpoint starting: shutdown immediate\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:14.778962416Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"record\":{\"log_time\":\"2025-01-24 10:06:14.778 UTC\",\"process_id\":\"31\",\"session_id\":\"679365f3.1f\",\"session_line_num\":\"3\",\"session_start_time\":\"2025-01-24 10:05:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"checkpoint complete: wrote 931 buffers (5.7%); 0 WAL file(s) added, 0 removed, 0 recycled; write=3.492 s, sync=0.001 s, total=3.713 s; sync files=0, longest=0.000 s, average=0.000 s; distance=7388 kB, estimate=7388 kB; lsn=0/1C1CEB0, redo lsn=0/1C1CEB0\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:16.499183381Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"record\":{\"log_time\":\"2025-01-24 10:06:16.498 UTC\",\"process_id\":\"29\",\"session_id\":\"679365f3.1d\",\"session_line_num\":\"9\",\"session_start_time\":\"2025-01-24 10:05:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:16.530322796Z\",\"logger\":\"pg_ctl\",\"msg\":\"waiting for server to shut down......... done\",\"pipe\":\"stdout\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:16.530356328Z\",\"logger\":\"pg_ctl\",\"msg\":\"server stopped\",\"pipe\":\"stdout\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"info\",\"ts\":\"2025-01-24T10:06:16.530606775Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"postgres-cluster-target-1-import\"}\n{\"level\":\"error\",\"ts\":\"2025-01-24T10:06:16.530646703Z\",\"msg\":\"Error while bootstrapping data directory\",\"logging_pod\":\"postgres-cluster-target-1-import\",\"error\":\"while executing logical import: error while executing pg_restore, section:pre-data, exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.initSubCommand\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:160\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.NewCmd.func2\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:109\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\nError: while executing logical import: error while executing pg_restore, section:pre-data, exit status 1\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: make hibernated clusters visible",
        "id": 2808951905,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nafter hibernating a cluster with `kubectl cnpg hibernate on xxx` there is \n- no way to know that there is a hibernated cluster except for searching for orphaned pvc's\n- no way to delete the cluster except for `kubectl cnpg hibernate off xxx` and then delete it\n### Describe the solution you'd like\ndo not delete the cluster resource\nmake `hibernated` a status of the cluster\n### Describe alternatives you've considered\nmaking notes of clusters I hibernated\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nafter hibernating a cluster with `kubectl cnpg hibernate on xxx` there is \n- no way to know that there is a hibernated cluster except for searching for orphaned pvc's\n- no way to delete the cluster except for `kubectl cnpg hibernate off xxx` and then delete it\n### Describe the solution you'd like\ndo not delete the cluster resource\nmake `hibernated` a status of the cluster\n### Describe alternatives you've considered\nmaking notes of clusters I hibernated\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct+1"
    },
    {
        "title": "[Docs]: Missing URL in the official docs",
        "id": 2808425041,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThe official docs at https://cloudnative-pg.io/documentation/1.16/quickstart/#part-4-monitor-clusters-with-prometheus-and-grafana mentions about the URL https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/main/docs/src/samples/monitoring/grafana-configmap.yaml\n to install grafana dashboards for cloudnative-pg \nBut the above url is 404 \nPlease fix the URL on the docs. There is no `grafana-configmap.yaml` file in the current github repository.\n### Describe what additions need to be done to the documentation\n_No response_\n### Describe what pages need to change in the documentation, if any\n_No response_\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nN/A\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThe official docs at https://cloudnative-pg.io/documentation/1.16/quickstart/#part-4-monitor-clusters-with-prometheus-and-grafana mentions about the URL https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/main/docs/src/samples/monitoring/grafana-configmap.yaml\n to install grafana dashboards for cloudnative-pg \nBut the above url is 404 \nPlease fix the URL on the docs. There is no `grafana-configmap.yaml` file in the current github repository.\n### Describe what additions need to be done to the documentation\n_No response_\n### Describe what pages need to change in the documentation, if any\n_No response_\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nN/A\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductThe documentation should redirect to [cloudnative-pg/grafana-dashboards](https://github.com/cloudnative-pg/grafana-dashboards) git repository."
    },
    {
        "title": "[Bug]: No <cluster>-app secret created when configuring custom owner with secret for custom password",
        "id": 2806956693,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nploef.ploef@hotmail.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nHi, I just found out that the -app secret is not created if you set the secret field in the initdb part. Checking the code I see that there is indeed a check for the existence of that secret to decide if that secret should be created.\nUnfortunately that secret also contains the info other apps need to connect to the cluster (like host and port) and that is lost now. The other apps use a reference to the -app secret for that.\nWould it be possible to keep the -app secret in this situation?\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nploef.ploef@hotmail.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nHi, I just found out that the -app secret is not created if you set the secret field in the initdb part. Checking the code I see that there is indeed a check for the existence of that secret to decide if that secret should be created.\nUnfortunately that secret also contains the info other apps need to connect to the cluster (like host and port) and that is lost now. The other apps use a reference to the -app secret for that.\nWould it be possible to keep the -app secret in this situation?\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Provide GRANT at Database CRD",
        "id": 2803959761,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nHello,\nwe have a readonly role in our cluster which holds connect and some other privileges to databases in the pgcluster. Currently there is no way to specify privileges or run postCreateDatabase tasks for new databases created with the new Database CR.\n[Slack Message](https://cloudnativepg.slack.com/archives/C03AT6KSKDL/p1733905769579189?thread_ts=1733813473.440219&cid=C03AT6KSKDL)\n### Describe the solution you'd like\nProvide a way to grant privileges to roles in the database CR.\nf.e call it postInitDatabaseSqlRefs and offer an opportunity to specify a configmap/secret with sql statements which will be executed after the database was created with the postgresql user. \n### Describe alternatives you've considered\nthere are currently none\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nHello,\nwe have a readonly role in our cluster which holds connect and some other privileges to databases in the pgcluster. Currently there is no way to specify privileges or run postCreateDatabase tasks for new databases created with the new Database CR.\n[Slack Message](https://cloudnativepg.slack.com/archives/C03AT6KSKDL/p1733905769579189?thread_ts=1733813473.440219&cid=C03AT6KSKDL)\n### Describe the solution you'd like\nProvide a way to grant privileges to roles in the database CR.\nf.e call it postInitDatabaseSqlRefs and offer an opportunity to specify a configmap/secret with sql statements which will be executed after the database was created with the postgresql user. \n### Describe alternatives you've considered\nthere are currently none\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: Hetzner S3 Object Storage: Backup/Restore Procedure - Error Barman cloud restore exception",
        "id": 2803657937,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\npsavva@gmail.com\n### Version\n1.24 (latest patch)\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nAfter completing a successfull back using `kubectl cnpg backup`, I can see a successful backup to Hetzner S3 Object Storage.\n```\nmaster1:~# kubectl get backup -l cnpg.io/cluster=racefeed-db -n data\nNAME                         AGE   CLUSTER       METHOD              PHASE       ERROR\nracefeed-db-20250122060028   83m   racefeed-db   barmanObjectStore   completed   \n```\nUpon Trying to restore the backup, I see the following logs:\nIn `racefeed-dev-db-1-full-recovery-5d6m5`\n```\n{\n  \"level\": \"info\",\n  \"ts\": \"2025-01-22T07:40:38.575441992Z\",\n  \"logger\": \"barman-cloud-restore\",\n  \"msg\": \"2025-01-22 07:40:38,573 [21] ERROR: Barman cloud restore exception: 10445368 read, but total bytes expected is 1994999143.\",\n  \"pipe\": \"stderr\",\n  \"logging_pod\": \"racefeed-dev-db-1-full-recovery\"\n}\n{\n  \"level\": \"error\",\n  \"ts\": \"2025-01-22T07:40:38.784650841Z\",\n  \"msg\": \"Can't restore backup\",\n  \"logging_pod\": \"racefeed-dev-db-1-full-recovery\",\n  \"error\": \"General error (exit code 4)\",\n  \"stacktrace\": \"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.InitInfo.restoreDataDir\\n\\tpkg/management/postgres/restore.go:461\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.InitInfo.Restore\\n\\tpkg/management/postgres/restore.go:297\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/restore.go:75\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.(*restoreRunnable).Start\\n\\tinternal/cmd/manager/instance/restore/restore.go:59\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.3/pkg/manager/runnable_group.go:226\"\n}\n{\n  \"level\": \"error\",\n  \"ts\": \"2025-01-22T07:40:38.785313804Z\",\n  \"msg\": \"Error while restoring a backup\",\n  \"logging_pod\": \"racefeed-dev-db-1-full-recovery\",\n  \"error\": \"General error (exit code 4)\",\n  \"stacktrace\": \"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/restore.go:76\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.(*restoreRunnable).Start\\n\\tinternal/cmd/manager/instance/restore/restore.go:59\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.3/pkg/manager/runnable_group.go:226\"\n}\n```\nIt seems that writing the data to the s3 storage is OK:\nIt can be see here:\n```\nmaster1:~# s3cmd la --recursive\n2025-01-22 04:22            0  s3://mybucket/cnpg\n2025-01-22 06:41         1333  s3://mybucket/cnpg/racefeed-db/base/20250122T060029/backup.info\n2025-01-22 06:41   1994999143  s3://mybucket/cnpg/racefeed-db/base/20250122T060029/data.tar.bz2\n2025-01-22 05:03          255  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD0000002C.00000028.backup.bz2\n2025-01-22 04:26          244  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD0000002C.bz2\n2025-01-22 05:03          138  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD0000002D.bz2\n2025-01-22 05:03          150  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD0000002E.bz2\n2025-01-22 05:08          349  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD0000002F.bz2\n2025-01-22 06:41          256  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD00000030.00000028.backup.bz2\n2025-01-22 06:03          239  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD00000030.bz2\n2025-01-22 06:41          140  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD00000031.bz2\n2025-01-22 06:41          153  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD00000032.bz2\n2025-01-22 06:46          345  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD00000033.bz2\n```\nVersion Info:\n```\nmaster1:~# kubectl version\nClient Version: v1.31.3+k3s1\nKustomize Version: v5.4.2\nServer Version: v1.31.3+k3s1\nmaster1:~# kubectl cnpg version\nBuild: {Version:1.24.0 Commit:5fe5bb6b Date:2024-08-22}\nmaster1:~# \n```\n### Cluster resource\n```shell\nExisting Cluster:\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: racefeed-db\n  namespace: data\nspec:\n  description: \"Race Feed PostgreSQL instance\"\n  imageName: docker.io/silicondali/postgresql-pgcron:16.2\n  instances: 1\n  enableSuperuserAccess: true\n  startDelay: 30\n  stopDelay: 100\n  primaryUpdateStrategy: unsupervised\n  postgresql:\n    parameters:\n      pg_stat_statements.max: \"10000\"\n      pg_stat_statements.track: all\n      auto_explain.log_min_duration: \"10s\"\n      cron.database_name: \"racefeed-db\"\n      cron.use_background_workers: \"on\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.2\n      max_connections: \"20\"\n      shared_buffers: \"1024MB\"\n      effective_cache_size: \"3072MB\"\n      maintenance_work_mem: \"256MB\"\n      checkpoint_completion_target: \"0.9\"\n      wal_buffers: \"16MB\"\n      default_statistics_target: \"100\"\n      random_page_cost: \"1.1\"\n      effective_io_concurrency: \"300\"\n      work_mem: \"26214kB\"\n      huge_pages: \"off\"\n      min_wal_size: \"1GB\"\n      max_wal_size: \"4GB\"\n    pg_hba:\n      - host all all 10.244.0.0/16 scram-sha-256\n    shared_preload_libraries:\n      - pg_cron\n  backup:\n    barmanObjectStore:\n      destinationPath: \"s3://mybucket/cnpg/\"\n      endpointURL: \"https://nbg1.your-objectstorage.com\"\n      s3Credentials:\n        accessKeyId:\n          name: aws-creds-hetzer-s3\n          key: AWS_ACCESS_KEY_ID\n        secretAccessKey:\n          name: aws-creds-hetzer-s3\n          key: AWS_SECRET_ACCESS_KEY\n      wal:\n        compression: bzip2\n        maxParallel: 4\n      data:\n        compression: bzip2\n        immediateCheckpoint: true\n        jobs: 4\n    retentionPolicy: \"7d\"\n  monitoring:\n    enablePodMonitor: false\n  bootstrap:\n    initdb:\n      database: racefeed-db\n      owner: racefeed-db-user\n      secret:\n        name: racefeed-db-app\n  superuserSecret:\n    name: racefeed-db-admin\n  storage:\n    storageClass: ceph-block\n    size: 200Gi\n    resizeInUseVolumes: false\n  resources:\n    requests:\n      memory: \"4Gi\"\n      cpu: \"2\"\n    limits:\n      memory: \"6Gi\"\n      cpu: \"4\"\n  affinity:\n    enablePodAntiAffinity: true\n    topologyKey: failure-domain.beta.kubernetes.io/zone\n  imagePullSecrets:\n  - name: regcred\nRestore to new Cluster:\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: racefeed-dev-db\n  namespace: data\n  labels:\n    environment: dev\nspec:\n  description: \"Racefeed PostgreSQL instance\"\n  imageName: docker.io/silicondali/postgresql-pgcron:16.2\n  instances: 1\n  enableSuperuserAccess: true\n  startDelay: 30\n  stopDelay: 100\n  primaryUpdateStrategy: unsupervised\n  postgresql:\n    parameters:\n      pg_stat_statements.max: \"10000\"\n      pg_stat_statements.track: all\n      auto_explain.log_min_duration: \"10s\"\n      cron.database_name: \"racefeed-db\"\n      cron.use_background_workers: \"on\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.2\n      max_connections: \"20\"\n      shared_buffers: \"1024MB\"\n      effective_cache_size: \"3072MB\"\n      maintenance_work_mem: \"256MB\"\n      checkpoint_completion_target: \"0.9\"\n      wal_buffers: \"16MB\"\n      default_statistics_target: \"100\"\n      random_page_cost: \"1.1\"\n      effective_io_concurrency: \"300\"\n      work_mem: \"26214kB\"\n      huge_pages: \"off\"\n      min_wal_size: \"1GB\"\n      max_wal_size: \"4GB\"\n    pg_hba:\n      - host all all 0.0.0.0/0 md5\n    shared_preload_libraries:\n      - pg_cron\n  backup:\n    barmanObjectStore:\n      destinationPath: \"s3://mybucket/cnpg/\"\n      endpointURL: \"https://nbg1.your-objectstorage.com\"\n      s3Credentials:\n        accessKeyId:\n          name: aws-creds-hetzer-s3\n          key: AWS_ACCESS_KEY_ID\n        secretAccessKey:\n          name: aws-creds-hetzer-s3\n          key: AWS_SECRET_ACCESS_KEY\n      wal:\n        compression: bzip2\n        maxParallel: 4\n      data:\n        compression: bzip2\n        jobs: 2\n    retentionPolicy: \"7d\"\n  monitoring:\n    enablePodMonitor: false\n  bootstrap:\n    recovery:\n      source: racefeed-db\n  externalClusters:\n    - name: racefeed-db\n      barmanObjectStore:\n        destinationPath: \"s3://mybucket/cnpg/\"\n        endpointURL: \"https://nbg1.your-objectstorage.com\"\n        s3Credentials:\n          accessKeyId:\n            name: aws-creds-hetzer-s3\n            key: AWS_ACCESS_KEY_ID\n          secretAccessKey:\n            name: aws-creds-hetzer-s3\n            key: AWS_SECRET_ACCESS_KEY\n        wal:\n          compression: bzip2\n        data:\n          compression: bzip2\n  superuserSecret:\n    name: racefeed-db-admin\n  storage:\n    storageClass: ceph-block\n    size: 50Gi\n    resizeInUseVolumes: true\n  resources:\n    requests:\n      memory: \"1024Mi\"\n      cpu: \"500m\"\n    limits:\n      memory: \"1024Mi\"\n      cpu: \"1\"\n  affinity:\n    enablePodAntiAffinity: true\n    topologyKey: topology.kubernetes.io/zone\n  imagePullSecrets:\n    - name: regcred\n```\n### Relevant log output\n```shell\n{\n  \"level\": \"info\",\n  \"ts\": \"2025-01-22T07:40:38.575441992Z\",\n  \"logger\": \"barman-cloud-restore\",\n  \"msg\": \"2025-01-22 07:40:38,573 [21] ERROR: Barman cloud restore exception: 10445368 read, but total bytes expected is 1994999143.\",\n  \"pipe\": \"stderr\",\n  \"logging_pod\": \"racefeed-dev-db-1-full-recovery\"\n}\n{\n  \"level\": \"error\",\n  \"ts\": \"2025-01-22T07:40:38.784650841Z\",\n  \"msg\": \"Can't restore backup\",\n  \"logging_pod\": \"racefeed-dev-db-1-full-recovery\",\n  \"error\": \"General error (exit code 4)\",\n  \"stacktrace\": \"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.InitInfo.restoreDataDir\\n\\tpkg/management/postgres/restore.go:461\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.InitInfo.Restore\\n\\tpkg/management/postgres/restore.go:297\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/restore.go:75\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.(*restoreRunnable).Start\\n\\tinternal/cmd/manager/instance/restore/restore.go:59\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.3/pkg/manager/runnable_group.go:226\"\n}\n{\n  \"level\": \"error\",\n  \"ts\": \"2025-01-22T07:40:38.785313804Z\",\n  \"msg\": \"Error while restoring a backup\",\n  \"logging_pod\": \"racefeed-dev-db-1-full-recovery\",\n  \"error\": \"General error (exit code 4)\",\n  \"stacktrace\": \"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/restore.go:76\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.(*restoreRunnable).Start\\n\\tinternal/cmd/manager/instance/restore/restore.go:59\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.3/pkg/manager/runnable_group.go:226\"\n}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\npsavva@gmail.com\n### Version\n1.24 (latest patch)\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nAfter completing a successfull back using `kubectl cnpg backup`, I can see a successful backup to Hetzner S3 Object Storage.\n```\nmaster1:~# kubectl get backup -l cnpg.io/cluster=racefeed-db -n data\nNAME                         AGE   CLUSTER       METHOD              PHASE       ERROR\nracefeed-db-20250122060028   83m   racefeed-db   barmanObjectStore   completed   \n```\nUpon Trying to restore the backup, I see the following logs:\nIn `racefeed-dev-db-1-full-recovery-5d6m5`\n```\n{\n  \"level\": \"info\",\n  \"ts\": \"2025-01-22T07:40:38.575441992Z\",\n  \"logger\": \"barman-cloud-restore\",\n  \"msg\": \"2025-01-22 07:40:38,573 [21] ERROR: Barman cloud restore exception: 10445368 read, but total bytes expected is 1994999143.\",\n  \"pipe\": \"stderr\",\n  \"logging_pod\": \"racefeed-dev-db-1-full-recovery\"\n}\n{\n  \"level\": \"error\",\n  \"ts\": \"2025-01-22T07:40:38.784650841Z\",\n  \"msg\": \"Can't restore backup\",\n  \"logging_pod\": \"racefeed-dev-db-1-full-recovery\",\n  \"error\": \"General error (exit code 4)\",\n  \"stacktrace\": \"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.InitInfo.restoreDataDir\\n\\tpkg/management/postgres/restore.go:461\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.InitInfo.Restore\\n\\tpkg/management/postgres/restore.go:297\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/restore.go:75\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.(*restoreRunnable).Start\\n\\tinternal/cmd/manager/instance/restore/restore.go:59\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.3/pkg/manager/runnable_group.go:226\"\n}\n{\n  \"level\": \"error\",\n  \"ts\": \"2025-01-22T07:40:38.785313804Z\",\n  \"msg\": \"Error while restoring a backup\",\n  \"logging_pod\": \"racefeed-dev-db-1-full-recovery\",\n  \"error\": \"General error (exit code 4)\",\n  \"stacktrace\": \"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/restore.go:76\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.(*restoreRunnable).Start\\n\\tinternal/cmd/manager/instance/restore/restore.go:59\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.3/pkg/manager/runnable_group.go:226\"\n}\n```\nIt seems that writing the data to the s3 storage is OK:\nIt can be see here:\n```\nmaster1:~# s3cmd la --recursive\n2025-01-22 04:22            0  s3://mybucket/cnpg\n2025-01-22 06:41         1333  s3://mybucket/cnpg/racefeed-db/base/20250122T060029/backup.info\n2025-01-22 06:41   1994999143  s3://mybucket/cnpg/racefeed-db/base/20250122T060029/data.tar.bz2\n2025-01-22 05:03          255  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD0000002C.00000028.backup.bz2\n2025-01-22 04:26          244  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD0000002C.bz2\n2025-01-22 05:03          138  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD0000002D.bz2\n2025-01-22 05:03          150  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD0000002E.bz2\n2025-01-22 05:08          349  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD0000002F.bz2\n2025-01-22 06:41          256  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD00000030.00000028.backup.bz2\n2025-01-22 06:03          239  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD00000030.bz2\n2025-01-22 06:41          140  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD00000031.bz2\n2025-01-22 06:41          153  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD00000032.bz2\n2025-01-22 06:46          345  s3://mybucket/cnpg/racefeed-db/wals/00000010000007CD/00000010000007CD00000033.bz2\n```\nVersion Info:\n```\nmaster1:~# kubectl version\nClient Version: v1.31.3+k3s1\nKustomize Version: v5.4.2\nServer Version: v1.31.3+k3s1\nmaster1:~# kubectl cnpg version\nBuild: {Version:1.24.0 Commit:5fe5bb6b Date:2024-08-22}\nmaster1:~# \n```\n### Cluster resource\n```shell\nExisting Cluster:\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: racefeed-db\n  namespace: data\nspec:\n  description: \"Race Feed PostgreSQL instance\"\n  imageName: docker.io/silicondali/postgresql-pgcron:16.2\n  instances: 1\n  enableSuperuserAccess: true\n  startDelay: 30\n  stopDelay: 100\n  primaryUpdateStrategy: unsupervised\n  postgresql:\n    parameters:\n      pg_stat_statements.max: \"10000\"\n      pg_stat_statements.track: all\n      auto_explain.log_min_duration: \"10s\"\n      cron.database_name: \"racefeed-db\"\n      cron.use_background_workers: \"on\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.2\n      max_connections: \"20\"\n      shared_buffers: \"1024MB\"\n      effective_cache_size: \"3072MB\"\n      maintenance_work_mem: \"256MB\"\n      checkpoint_completion_target: \"0.9\"\n      wal_buffers: \"16MB\"\n      default_statistics_target: \"100\"\n      random_page_cost: \"1.1\"\n      effective_io_concurrency: \"300\"\n      work_mem: \"26214kB\"\n      huge_pages: \"off\"\n      min_wal_size: \"1GB\"\n      max_wal_size: \"4GB\"\n    pg_hba:\n      - host all all 10.244.0.0/16 scram-sha-256\n    shared_preload_libraries:\n      - pg_cron\n  backup:\n    barmanObjectStore:\n      destinationPath: \"s3://mybucket/cnpg/\"\n      endpointURL: \"https://nbg1.your-objectstorage.com\"\n      s3Credentials:\n        accessKeyId:\n          name: aws-creds-hetzer-s3\n          key: AWS_ACCESS_KEY_ID\n        secretAccessKey:\n          name: aws-creds-hetzer-s3\n          key: AWS_SECRET_ACCESS_KEY\n      wal:\n        compression: bzip2\n        maxParallel: 4\n      data:\n        compression: bzip2\n        immediateCheckpoint: true\n        jobs: 4\n    retentionPolicy: \"7d\"\n  monitoring:\n    enablePodMonitor: false\n  bootstrap:\n    initdb:\n      database: racefeed-db\n      owner: racefeed-db-user\n      secret:\n        name: racefeed-db-app\n  superuserSecret:\n    name: racefeed-db-admin\n  storage:\n    storageClass: ceph-block\n    size: 200Gi\n    resizeInUseVolumes: false\n  resources:\n    requests:\n      memory: \"4Gi\"\n      cpu: \"2\"\n    limits:\n      memory: \"6Gi\"\n      cpu: \"4\"\n  affinity:\n    enablePodAntiAffinity: true\n    topologyKey: failure-domain.beta.kubernetes.io/zone\n  imagePullSecrets:\n  - name: regcred\nRestore to new Cluster:\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: racefeed-dev-db\n  namespace: data\n  labels:\n    environment: dev\nspec:\n  description: \"Racefeed PostgreSQL instance\"\n  imageName: docker.io/silicondali/postgresql-pgcron:16.2\n  instances: 1\n  enableSuperuserAccess: true\n  startDelay: 30\n  stopDelay: 100\n  primaryUpdateStrategy: unsupervised\n  postgresql:\n    parameters:\n      pg_stat_statements.max: \"10000\"\n      pg_stat_statements.track: all\n      auto_explain.log_min_duration: \"10s\"\n      cron.database_name: \"racefeed-db\"\n      cron.use_background_workers: \"on\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.2\n      max_connections: \"20\"\n      shared_buffers: \"1024MB\"\n      effective_cache_size: \"3072MB\"\n      maintenance_work_mem: \"256MB\"\n      checkpoint_completion_target: \"0.9\"\n      wal_buffers: \"16MB\"\n      default_statistics_target: \"100\"\n      random_page_cost: \"1.1\"\n      effective_io_concurrency: \"300\"\n      work_mem: \"26214kB\"\n      huge_pages: \"off\"\n      min_wal_size: \"1GB\"\n      max_wal_size: \"4GB\"\n    pg_hba:\n      - host all all 0.0.0.0/0 md5\n    shared_preload_libraries:\n      - pg_cron\n  backup:\n    barmanObjectStore:\n      destinationPath: \"s3://mybucket/cnpg/\"\n      endpointURL: \"https://nbg1.your-objectstorage.com\"\n      s3Credentials:\n        accessKeyId:\n          name: aws-creds-hetzer-s3\n          key: AWS_ACCESS_KEY_ID\n        secretAccessKey:\n          name: aws-creds-hetzer-s3\n          key: AWS_SECRET_ACCESS_KEY\n      wal:\n        compression: bzip2\n        maxParallel: 4\n      data:\n        compression: bzip2\n        jobs: 2\n    retentionPolicy: \"7d\"\n  monitoring:\n    enablePodMonitor: false\n  bootstrap:\n    recovery:\n      source: racefeed-db\n  externalClusters:\n    - name: racefeed-db\n      barmanObjectStore:\n        destinationPath: \"s3://mybucket/cnpg/\"\n        endpointURL: \"https://nbg1.your-objectstorage.com\"\n        s3Credentials:\n          accessKeyId:\n            name: aws-creds-hetzer-s3\n            key: AWS_ACCESS_KEY_ID\n          secretAccessKey:\n            name: aws-creds-hetzer-s3\n            key: AWS_SECRET_ACCESS_KEY\n        wal:\n          compression: bzip2\n        data:\n          compression: bzip2\n  superuserSecret:\n    name: racefeed-db-admin\n  storage:\n    storageClass: ceph-block\n    size: 50Gi\n    resizeInUseVolumes: true\n  resources:\n    requests:\n      memory: \"1024Mi\"\n      cpu: \"500m\"\n    limits:\n      memory: \"1024Mi\"\n      cpu: \"1\"\n  affinity:\n    enablePodAntiAffinity: true\n    topologyKey: topology.kubernetes.io/zone\n  imagePullSecrets:\n    - name: regcred\n```\n### Relevant log output\n```shell\n{\n  \"level\": \"info\",\n  \"ts\": \"2025-01-22T07:40:38.575441992Z\",\n  \"logger\": \"barman-cloud-restore\",\n  \"msg\": \"2025-01-22 07:40:38,573 [21] ERROR: Barman cloud restore exception: 10445368 read, but total bytes expected is 1994999143.\",\n  \"pipe\": \"stderr\",\n  \"logging_pod\": \"racefeed-dev-db-1-full-recovery\"\n}\n{\n  \"level\": \"error\",\n  \"ts\": \"2025-01-22T07:40:38.784650841Z\",\n  \"msg\": \"Can't restore backup\",\n  \"logging_pod\": \"racefeed-dev-db-1-full-recovery\",\n  \"error\": \"General error (exit code 4)\",\n  \"stacktrace\": \"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.InitInfo.restoreDataDir\\n\\tpkg/management/postgres/restore.go:461\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.InitInfo.Restore\\n\\tpkg/management/postgres/restore.go:297\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/restore.go:75\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.(*restoreRunnable).Start\\n\\tinternal/cmd/manager/instance/restore/restore.go:59\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.3/pkg/manager/runnable_group.go:226\"\n}\n{\n  \"level\": \"error\",\n  \"ts\": \"2025-01-22T07:40:38.785313804Z\",\n  \"msg\": \"Error while restoring a backup\",\n  \"logging_pod\": \"racefeed-dev-db-1-full-recovery\",\n  \"error\": \"General error (exit code 4)\",\n  \"stacktrace\": \"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/restore.go:76\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.(*restoreRunnable).Start\\n\\tinternal/cmd/manager/instance/restore/restore.go:59\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.3/pkg/manager/runnable_group.go:226\"\n}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductIt looks like there might be a compatibility issue between the Hetzner S3 implementation and the version of Barman Cloud or `boto3` installed in the container you\u2019re using.\nTo investigate, please check the versions of Barman and `boto3` in the container by inspecting the `/requirements.txt` file. You can do this with the following command:\n```bash\nkubectl exec -it \"<pod_name>\" -- cat /requirements.txt\n```\nIf the versions are outdated, I recommend trying a more recent container image. If the issue persists, it would also be worth investigating whether Hetzner has documented any known limitations or compatibility issues with their S3 implementation.\n---\nThank you for your reply @mnencia \nThe versions are:\n```\nbarman[azure,cloud,google,snappy]==3.10.0 \\\n    --hash=sha256:c40b42481a16b960e19d803bbe1f7d549749a5089d99d49cf77ec129297074a0 \\\n    --hash=sha256:dd9850ebe6a8a3be1c4157aa32d73349542e23b59881f207a4ba12667e7e062f\n    # via -r requirements.in\nboto3==1.34.101 \\\n    --hash=sha256:1d854b5880e185db546b4c759fcb664bf3326275064d2b44229cc217e8be9d7e \\\n    --hash=sha256:79b93f3370ea96ce838042bc2eac0c996aee204b01e7e6452eb77abcbe697d6a\nbotocore==1.34.101 \\\n    --hash=sha256:01f3802d25558dd7945d83884bf6885e2f84e1ff27f90b5f09614966fe18c18f \\\n    --hash=sha256:f145e8b4b8fc9968f5eb695bdc2fcc8e675df7fbc3c56102dc1f5471be6baf35\n    # via\n    #   boto3\n    #   s3transfer\n```\nI will do a restoration on a more recent version based on 16.6.\n---\nI can confirm that updating to the latest image doesn't resolve the issue.\n```\nbarman[azure,cloud,google,snappy]==3.12.1 \\\n    --hash=sha256:258ef7100717f66032402e0abe03c977089c50fc47143df5708e92aa1d772937 \\\n    --hash=sha256:9dd7be219b6f74954b80cdc28f9a72f2acb923e7da65edd0f41cdc82fd32e169\n    # via -r requirements.in\nboto3==1.35.99 \\\n    --hash=sha256:83e560faaec38a956dfb3d62e05e1703ee50432b45b788c09e25107c5058bd71 \\\n    --hash=sha256:e0abd794a7a591d90558e92e29a9f8837d25ece8e3c120e530526fe27eba5fca\n    # via\n    #   -r requirements.in\n    #   barman\nbotocore==1.35.99 \\\n    --hash=sha256:1eab44e969c39c5f3d9a3104a0836c24715579a455f12b3979a31d7cde51b3c3 \\\n    --hash=sha256:b22d27b6b617fc2d7342090d6129000af2efd20174215948c0d7ae2da0fab445\n    # via\n    #   boto3\n    #   s3transfer\n```\nand logs trying to restore:\n```\n{\n  \"level\": \"info\",\n  \"ts\": \"2025-01-22T12:12:28.728883587Z\",\n  \"msg\": \"Starting barman-cloud-restore\",\n  \"logging_pod\": \"racefeed-dev-db-1-full-recovery\",\n  \"options\": [\n    \"--endpoint-url\",\n    \"https://nbg1.your-objectstorage.com\",\n    \"s3://mybucket/cnpg/\",\n    \"racefeed-db\",\n    \"20250122T060029\",\n    \"--cloud-provider\",\n    \"aws-s3\",\n    \"/var/lib/postgresql/data/pgdata\"\n  ]\n}\n{\n  \"level\": \"info\",\n  \"ts\": \"2025-01-22T12:16:06.706095994Z\",\n  \"logger\": \"barman-cloud-restore\",\n  \"msg\": \"2025-01-22 12:16:06,704 [20] ERROR: Barman cloud restore exception: 10177047 read, but total bytes expected is 1994999143.\",\n  \"pipe\": \"stderr\",\n  \"logging_pod\": \"racefeed-dev-db-1-full-recovery\"\n}\n{\n  \"level\": \"error\",\n  \"ts\": \"2025-01-22T12:16:06.835335752Z\",\n  \"msg\": \"Can't restore backup\",\n  \"logging_pod\": \"racefeed-dev-db-1-full-recovery\",\n  \"error\": \"General error (exit code 4)\",\n  \"stacktrace\": \"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.InitInfo.restoreDataDir\\n\\tpkg/management/postgres/restore.go:461\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.InitInfo.Restore\\n\\tpkg/management/postgres/restore.go:297\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/restore.go:75\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.(*restoreRunnable).Start\\n\\tinternal/cmd/manager/instance/restore/restore.go:59\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.3/pkg/manager/runnable_group.go:226\"\n}\n{\n  \"level\": \"error\",\n  \"ts\": \"2025-01-22T12:16:06.836424032Z\",\n  \"msg\": \"Error while restoring a backup\",\n  \"logging_pod\": \"racefeed-dev-db-1-full-recovery\",\n  \"error\": \"General error (exit code 4)\",\n  \"stacktrace\": \"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/restore.go:76\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.(*restoreRunnable).Start\\n\\tinternal/cmd/manager/instance/restore/restore.go:59\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.3/pkg/manager/runnable_group.go:226\"\n}\n```\nI will check with Hetzner in parallel, and ask if there is any known limitations.\nupdate: Limitations as documented by Hetzner \nhttps://docs.hetzner.com/storage/object-storage/supported-actions/\n---\nTrying to understand where the problem might come from, i'm thinking Barman...\nin the s3 provider implementation, perhaps downloading in chunks would avoid timeouts occuring, which I think might be happening here due to the file size.\nSomething similar to this would make downloading of larger files more resilient.\nhttps://github.com/EnterpriseDB/barman/blob/master/barman/cloud_providers/aws_s3.py\nUpdated logic:\n```\ndef download_file(self, key, dest_path, decompress):\n    \"\"\"\n    Download a file from S3\n    :param str key: The S3 key to download\n    :param str dest_path: Where to put the destination file\n    :param str|None decompress: Compression scheme to use for decompression\n    \"\"\"\n    object_size = self.s3.meta.client.head_object(\n        Bucket=self.bucket_name, Key=key\n    )[\"ContentLength\"]\n    chunk_size = 1024 * 1024 * 10  # 10 MB chunks\n    chunk_start = 0\n    chunk_end = chunk_start + chunk_size - 1\n    with open(dest_path, \"wb\") as dest_file:\n        while chunk_start <= object_size:\n            if body := self.s3.meta.client.get_object(\n                Bucket=self.bucket_name, Key=key, Range=f\"bytes={chunk_start}-{chunk_end}\"\n            )[\"Body\"]:\n                chunk = body.read()\n                dest_file.write(chunk)\n                chunk_start += chunk_size\n                chunk_end += chunk_size\n```\n---\nI'm investigating a similar issue right now. While doing so I found the following issue from barman.\nhttps://github.com/EnterpriseDB/barman/issues/1057\nReverting back to an older version of the CNPG image with boto3 before 1.36.0 resolved the issue for me.\n---\n@Joker9944  The version I originally faced this issue is with boto3 v1.34.101\nSeems the issue you're facing is different.\nThe issue i'm seeing is with downloading, not uploading.\n---\nThank you for providing detailed context about the error with Barman and Hetzner S3 Object Storage. Given that this appears to be a compatibility issue with Barman's implementation, I recommend opening a ticket directly in the [Barman project repository](https://github.com/EnterpriseDB/barman) to discuss a potential fix with their team.\nThe Barman developers might be better equipped to identify the root cause and work towards a solution. Opening the discussion there would also help others facing a similar issue."
    },
    {
        "title": "[Docs]: Document how to delete a database cluster (including all the optional bits)",
        "id": 2803064749,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nPlease document what resources are associated with each database cluster and the best way for us to delete them. I know that by default we don't want to delete backups of a cluster when the cluster is deleted, but please also document where to find them so we can clean them up manually if needed.\n### Describe what additions need to be done to the documentation\n_No response_\n### Describe what pages need to change in the documentation, if any\n_No response_\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nPlease document what resources are associated with each database cluster and the best way for us to delete them. I know that by default we don't want to delete backups of a cluster when the cluster is deleted, but please also document where to find them so we can clean them up manually if needed.\n### Describe what additions need to be done to the documentation\n_No response_\n### Describe what pages need to change in the documentation, if any\n_No response_\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductBased on my own experiments, I believe the following works. Note, I am only covering the case where the operator was installed using Helm, a DB cluster using manifests, and both were installed into their own namespaces.\n* To delete a DB cluster, delete all the services in its namespace and then delete the namespace.\n* To delete an operator, uninstall the Helm release, delete its namespace and then delete all custom resources with a group name of \"postgresql.cnpg.io\"."
    },
    {
        "title": "[Feature]: Support for Regex on Watched Namespaces",
        "id": 2802478405,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nHello,\nI would like to request an enhancement for the `WATCH_NAMESPACE` variable in CloudNativePG. Currently, this variable is used to specify the namespace(s) the operator should watch. However, it appears that the value must be explicitly defined as specific namespaces.\nIt would be incredibly useful to support regex to define watched namespaces. It allows more dynamic and flexible namespace selection. \nFor instance, instead of listing all matching namespaces explicitly, one could define a regex pattern to match namespaces that adhere to a certain naming convention (e.g., `prod-.*`, `staging-.*`).\n### Describe the solution you'd like\nI envision two potential approaches for implementing this feature:\n-  Introduce a new variable such as `WATCH_NAMESPACE_REGEX`, to explicitly define regex-based namespace patterns. This keeps the current behavior of `WATCH_NAMESPACE` intact and adds flexibility for users who need regex support. However, we shall decide if the two variables can be defined at the same time. In my opinion, either `WATCH_NAMESPACE_REGEX` or `WATCH_NAMESPACE` should be defined but not both at the same time.\n- Add a \"companion variable\" to `WATCH_NAMESPACE` (e.g., `WATCH_NAMESPACE_REGEX_MODE`) that, when set to true, treats the value of WATCH_NAMESPACE as a regex pattern. In my opinion, this avoids having two variables very close to each other that can be confusing.\n### Describe alternatives you've considered\nOf course, we do have considered to manually list each namespace in the `WATCH_NAMESPACE` variable. While this approach works on smaller clusters, it quickly becomes unmanageable when dealing with a large number of namespaces. Manually updating the configuration for hundreds of namespaces is not only time-consuming but also prone to human error, especially when namespaces are frequently added or removed.\n### Additional context\nThank you for considering this feature request! Please let me know if further clarification or details are needed.\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nHello,\nI would like to request an enhancement for the `WATCH_NAMESPACE` variable in CloudNativePG. Currently, this variable is used to specify the namespace(s) the operator should watch. However, it appears that the value must be explicitly defined as specific namespaces.\nIt would be incredibly useful to support regex to define watched namespaces. It allows more dynamic and flexible namespace selection. \nFor instance, instead of listing all matching namespaces explicitly, one could define a regex pattern to match namespaces that adhere to a certain naming convention (e.g., `prod-.*`, `staging-.*`).\n### Describe the solution you'd like\nI envision two potential approaches for implementing this feature:\n-  Introduce a new variable such as `WATCH_NAMESPACE_REGEX`, to explicitly define regex-based namespace patterns. This keeps the current behavior of `WATCH_NAMESPACE` intact and adds flexibility for users who need regex support. However, we shall decide if the two variables can be defined at the same time. In my opinion, either `WATCH_NAMESPACE_REGEX` or `WATCH_NAMESPACE` should be defined but not both at the same time.\n- Add a \"companion variable\" to `WATCH_NAMESPACE` (e.g., `WATCH_NAMESPACE_REGEX_MODE`) that, when set to true, treats the value of WATCH_NAMESPACE as a regex pattern. In my opinion, this avoids having two variables very close to each other that can be confusing.\n### Describe alternatives you've considered\nOf course, we do have considered to manually list each namespace in the `WATCH_NAMESPACE` variable. While this approach works on smaller clusters, it quickly becomes unmanageable when dealing with a large number of namespaces. Manually updating the configuration for hundreds of namespaces is not only time-consuming but also prone to human error, especially when namespaces are frequently added or removed.\n### Additional context\nThank you for considering this feature request! Please let me know if further clarification or details are needed.\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Make WebhookServiceName Configurable in Operator Configuration",
        "id": 2802013572,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI would like to request a new feature in CloudNativePG that makes the Operator Webhook service name configurable as a parameter in the operator configuration.\nCurrently, the Webhook service name is hardcoded, which limits flexibility in specific deployment scenarios. For instance, in our setup, we maintain one cluster per product with separate prod, staging and review namespaces. We aim to deploy two Helm releases of the CloudNativePG operator, each configured to monitor a distinct group of namespaces. However, the fixed Webhook service name prevents us from deploying both operator instances in the `cnpg-system` namespace, as naming conflicts arise.\n### Describe the solution you'd like\nIntroduce a new parameter in the operator configuration (e.g., webhookServiceName) to allow users to specify the name of the Webhook service. This parameter should also be configurable in the Helm chart, enabling users to customize the Webhook service name as needed.\nAdditionally, when the operator is configured to monitor only a subset of namespaces, those namespaces should be specified as a namespaceSelector in the Webhook configuration. The webhooks should target the appropriate namespaces, reducing the risk of conflicts.\nBased on my understanding, the webhooks currently apply only to namespaced-scoped resources. To further enhance this behavior, cluster-wide webhooks should be disabled when the operator is configured to operate within a limited scope. This would provide a more conflict-free deployment experience, particularly in multi-operator release setups.\n### Describe alternatives you've considered\nOne alternative we explored was deploying multiple namespaces such as `cnpg-[product name]-prod`, `cnpg-[product name]-staging`, and so on, with separate CNPG operator instances in each namespace. While this approach is technically feasible, it feels like a workaround rather than a clean solution. We believe this introduces unnecessary complexity and deviates from standard practices. Instead, we would prefer to contribute towards implementing a more straightforward and robust solution that aligns with the operator's design principles.\n### Additional context\nThank you for considering this request! Please let me know if you need further clarification or if there\u2019s anything I can do to help.\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI would like to request a new feature in CloudNativePG that makes the Operator Webhook service name configurable as a parameter in the operator configuration.\nCurrently, the Webhook service name is hardcoded, which limits flexibility in specific deployment scenarios. For instance, in our setup, we maintain one cluster per product with separate prod, staging and review namespaces. We aim to deploy two Helm releases of the CloudNativePG operator, each configured to monitor a distinct group of namespaces. However, the fixed Webhook service name prevents us from deploying both operator instances in the `cnpg-system` namespace, as naming conflicts arise.\n### Describe the solution you'd like\nIntroduce a new parameter in the operator configuration (e.g., webhookServiceName) to allow users to specify the name of the Webhook service. This parameter should also be configurable in the Helm chart, enabling users to customize the Webhook service name as needed.\nAdditionally, when the operator is configured to monitor only a subset of namespaces, those namespaces should be specified as a namespaceSelector in the Webhook configuration. The webhooks should target the appropriate namespaces, reducing the risk of conflicts.\nBased on my understanding, the webhooks currently apply only to namespaced-scoped resources. To further enhance this behavior, cluster-wide webhooks should be disabled when the operator is configured to operate within a limited scope. This would provide a more conflict-free deployment experience, particularly in multi-operator release setups.\n### Describe alternatives you've considered\nOne alternative we explored was deploying multiple namespaces such as `cnpg-[product name]-prod`, `cnpg-[product name]-staging`, and so on, with separate CNPG operator instances in each namespace. While this approach is technically feasible, it feels like a workaround rather than a clean solution. We believe this introduces unnecessary complexity and deviates from standard practices. Instead, we would prefer to contribute towards implementing a more straightforward and robust solution that aligns with the operator's design principles.\n### Additional context\nThank you for considering this request! Please let me know if you need further clarification or if there\u2019s anything I can do to help.\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Sign operator container images",
        "id": 2801892660,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWe should implement cosign to sign the operator images that we produce, we already sign the packages using GPG but now we should do the same with the operator container images.\n### Describe the solution you'd like\nUsing cosing just like we do here https://github.com/cloudnative-pg/postgres-containers/pull/137\n### Describe alternatives you've considered\nNone\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWe should implement cosign to sign the operator images that we produce, we already sign the packages using GPG but now we should do the same with the operator container images.\n### Describe the solution you'd like\nUsing cosing just like we do here https://github.com/cloudnative-pg/postgres-containers/pull/137\n### Describe alternatives you've considered\nNone\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "fix(cnpg-plugin): iterate containers while executing `--logs` flag",
        "id": 2800656055,
        "state": "open",
        "first": "This patch makes sure the `kubectl cnpg report <cluster> --logs`\r\ncan get logs from instance pod with more than one container. The \r\nlogs  for all the containers will be collected from the command.\r\nCloses: #6632",
        "messages": "This patch makes sure the `kubectl cnpg report <cluster> --logs`\r\ncan get logs from instance pod with more than one container. The \r\nlogs  for all the containers will be collected from the command.\r\nCloses: #6632I have made substantial changes: moved the container-iterating logic down to the core `pkg/utils/logs` to make it more reusable.\r\nAnd did some cleanup there while I was at it.\r\n@litaocdl @armru  please take a look\n---\nAs requested by @armru I've moved my commits to a separate PR: https://github.com/cloudnative-pg/cloudnative-pg/pull/6809"
    },
    {
        "title": "[Bug]: kubectl-cnpg report plugin failed to retrieve log if sidecar exists ",
        "id": 2798877154,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nkubectl cnpg report cluster xxx --logs right now did not enforce the container name, in multiple container env, retrieve log will fail. \n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nkubectl cnpg report cluster xxx --logs right now did not enforce the container name, in multiple container env, retrieve log will fail. \n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductWe could either retrieve logs from `postgres` container only, or go through all the containers in a the pod and get logs for each."
    },
    {
        "title": "[Docs]: Google prioritizes documentation for outdated versions",
        "id": 2798632374,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nSearching for e.g. https://www.google.com/search?q=cnpg+enablePodMonitor shows https://cloudnative-pg.io/documentation/1.16/monitoring/ as its first result. \nCould you please either add a warning at the top of the page mentioning users are viewing an outdated page or guide Google to the newest page version? \nMaybe https://webmasters.stackexchange.com/a/109118 is related. \n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nSearching for e.g. https://www.google.com/search?q=cnpg+enablePodMonitor shows https://cloudnative-pg.io/documentation/1.16/monitoring/ as its first result. \nCould you please either add a warning at the top of the page mentioning users are viewing an outdated page or guide Google to the newest page version? \nMaybe https://webmasters.stackexchange.com/a/109118 is related. \n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductThe current documentation is far from optimal. We'd appreciate help from the community to rebuild it. Any contributions are welcome.\n---\n> The current documentation is far from optimal.\nActually, it's not bad at all.\n---\nI meant, the current technology for the documentation. :)"
    },
    {
        "title": "feat: use P384 instead of P256 elliptic algorithm",
        "id": 2797588870,
        "state": "open",
        "first": "Even if the P256 elliptic algorithm is approved, using a P384 makes it\r\nmore secure and increase the security during the creation of root CA \r\nand certificates for the internal authentication.\r\nCloses #6597",
        "messages": "Even if the P256 elliptic algorithm is approved, using a P384 makes it\r\nmore secure and increase the security during the creation of root CA \r\nand certificates for the internal authentication.\r\nCloses #6597/test depth=push"
    },
    {
        "title": "feat: startup and readiness probes for replicas",
        "id": 2796939419,
        "state": "open",
        "first": "Extend the startup and readiness probes configured through the `.spec.probes.startup` and `.spec.probes.readiness` sections by adding two additional parameters:\r\n- `type`: Defines the criteria for considering the probe successful. Accepted values include:\r\n    - `pg_isready`: This setting marks the probe as successful when the `pg_isready` command exits with a status of `0`. This is the default for both primary instances and replicas.\r\n    - `query`: This setting marks the probe as successful when a basic query is executed on the `postgres` database locally.\r\n    - `streaming`: This setting marks the probe as successful when the replica starts streaming from its source and meets the specified lag requirements (details below).\r\n- `lag`: Specifies the maximum acceptable replication lag, measured in bytes (expressed using Kubernetes quantities). This parameter is applicable only when `type` is set to `streaming`. If the `lag` parameter is not specified, the replica is considered successfully started/ready as soon as it begins streaming.\r\nCloses: #6621",
        "messages": "Extend the startup and readiness probes configured through the `.spec.probes.startup` and `.spec.probes.readiness` sections by adding two additional parameters:\r\n- `type`: Defines the criteria for considering the probe successful. Accepted values include:\r\n    - `pg_isready`: This setting marks the probe as successful when the `pg_isready` command exits with a status of `0`. This is the default for both primary instances and replicas.\r\n    - `query`: This setting marks the probe as successful when a basic query is executed on the `postgres` database locally.\r\n    - `streaming`: This setting marks the probe as successful when the replica starts streaming from its source and meets the specified lag requirements (details below).\r\n- `lag`: Specifies the maximum acceptable replication lag, measured in bytes (expressed using Kubernetes quantities). This parameter is applicable only when `type` is set to `streaming`. If the `lag` parameter is not specified, the replica is considered successfully started/ready as soon as it begins streaming.\r\nCloses: #6621IIUC, when replicas re-join they need to catch-up WAL - which has two effects:\r\n1. clients can connect as soon as the readiness probe succeeds and during catch-up their queries may get results that lag more than usual.\r\n2. the impetus for this PR: with `preferred` DataDurability (new feature in 1.25) all writes on the primary will completely hang during catch-up. the primary needs to write WAL synchronously and the latest WAL won't get acknowledged by the replica until it's caught up, leading to the hang on the primary.\r\n+1 using a startup probe seems like a good idea\r\nif someone is choosing `preferred` dataDurability then i don't think they would ever want the primary to hang. as a user, my hope is that replicas (re)joining a cluster should be as seamless as possible. a primary database hanging even briefly can be impactful on a large workload. I\u2019m not sure we\u2019d ever want a replica to be considered ready as soon as it starts streaming, when dataDurability is `preferred`. i suspect that users who choose `preferred` are specifically doing it because they want high availability.\r\nmy initial thought is that this should default to a small number of bytes and might not need to be configurable at all, unless as a setting for debugging or troubleshooting.\r\nthis would result in a slightly longer delay for replicas becoming ready even with dataDurability=`required` however it also means we eliminate the period on startup of higher-than-usual lag, which doesn't seem like a bad idea to me.\r\n*note: what i've written above is based on my understanding but i have not had time to test or verify it, so there might be mistakes. i haven't looked yet, but it could also be interesting to check how patroni approaches this.*\n---\n> IIUC, when replicas re-join they need to catch-up WAL - which has two effects:\r\n> \r\n> 1. clients can connect as soon as the readiness probe succeeds and during catch-up their queries may get results that lag more than usual.\r\n> 2. the impetus for this PR: with `preferred` DataDurability (new feature in 1.25) all writes on the primary will completely hang during catch-up. the primary needs to write WAL synchronously and the latest WAL won't get acknowledged by the replica until it's caught up, leading to the hang on the primary.\r\nThat's correct.\n---\ne2e: https://github.com/EnterpriseDB/cloudnative-pg/actions/runs/12933848954\n---\nIs this PR in patroni solving a similar problem?\r\nhttps://github.com/patroni/patroni/pull/1786\n---\ninteresting - from this recent pgcon talk it sounds like actually patroni might even still have the pause\r\nhttps://youtu.be/CWrFPPG5USA?feature=shared&t=1190\r\ni think in the talk he's focused on a 3rd node rejoining `synchronous_standby_names` with only 2 nodes required (so it doesn't cause the hang) - so i don't know for sure if there's any \"special case\" treatment for a synchronous cluster with only 2 nodes.\n---\n> interesting - from this recent pgcon talk it sounds like actually patroni might even still have the pause [...]\r\nYes. I think there is no solution this problem but only compromises.\r\nHaving the cluster paused reduces the time needed for the replica to get in sync.\r\nIf the cluster is not paused the replica will need more time.\r\nIf may not even be able to get in sync again - depending on the workload of the primary.\n---\nfor the parameter, do you have thoughts on whether `maximum_lag` might be a better name than `lag`? or stick with the PR as it is?\n---\nE2e: https://github.com/EnterpriseDB/cloudnative-pg/actions/runs/12945712758\r\nUnfortunately, I need to run these E2e tests in the EDB Organization to spare some OSS workers.\n---\n> for the parameter, do you have thoughts on whether `maximum_lag` might be a better name than `lag`? or stick with the PR as it is?\r\nI like it. It is definitely clearer than just `lag`. What about `maximumLag`?\r\nWe're using camelCase unless when referring to some external tool/parameter, like `pg_isready`.\n---\nE2e tests are green!!!\n---\nE2e: https://github.com/EnterpriseDB/cloudnative-pg/actions/runs/12953656542\n---\n/test depth=push"
    },
    {
        "title": "chore: add `pg_catalog` schema where needed",
        "id": 2796935638,
        "state": "open",
        "first": "Add the `pg_catalog` schema to fully qualify the usage of system functions and views provided by PostgreSQL.",
        "messages": "Add the `pg_catalog` schema to fully qualify the usage of system functions and views provided by PostgreSQL./test\n---\nI see other files in which pg_catalog can be added, for example  :\r\n- internal/management/controller/tablespaces/infrastructure/postgres.go\r\n- internal/management/controller/tablespaces/infrastructure/postgres_test.go\r\n- internal/management/controller/tablespaces/controller_test.go\n---\n> * internal/management/controller/tablespaces/infrastructure/postgres.go\r\nDefinitely. Wanna give it a shot?\n---\nWe could also add pg_catalog here:\r\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/main/tests/e2e/update_user_test.go#L167\r\n`query := \"SELECT rolpassword IS NULL FROM pg_authid WHERE rolname='postgres'\"`\r\nwith\r\n`query := \"SELECT rolpassword IS NULL FROM [pg.catalog.pg](http://pg.catalog.pg/)_authid WHERE rolname='postgres'\"`\r\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/ebf36a59e54b5634b56ee6058d1b7551eafde083/tests/e2e/failover_test.go#L108\r\n`query = fmt.Sprintf(\"SELECT pg_terminate_backend(pid) FROM pg_stat_replication \"+`\r\nwith\r\n`query = fmt.Sprintf(\"SELECT [pg.catalog.pg](http://pg.catalog.pg/)_terminate_backend(pid) FROM [pg.catalog.pg](http://pg.catalog.pg/)_stat_replication \"+`\r\nWe could apply it also when `pg_stat_wal_receiver` occurs inside e2e.\n---\n> > * internal/management/controller/tablespaces/infrastructure/postgres.go\r\n> \r\n> Definitely. Wanna give it a shot?\r\nYep, WIP."
    },
    {
        "title": "[Feature]: Separate configurable endpoint for startup probe",
        "id": 2796881561,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe current implementation of the startup probe uses the same endpoint as the liveness probe, which is `/healthz`. This endpoint relies on the [`pg_isready` command](https://www.postgresql.org/docs/current/app-pg-isready.html). The liveness probe reports success when `pg_isready` returns either 0 (indicating that the system is ready for connections) or 1 (indicating that the system is starting up, such as during crash recovery).\nThe readiness probe begins after this point. However, in some cases, such as with replicas, this implementation may pose a limitation.\n### Describe the solution you'd like\nWe would like to differentiate the startup probe to define multiple waiting stages before the readiness probe kicks in:\n- `pg_isready`: pg_isready returns 0 (connection is possible)\n- `streaming`: in case of a streaming connected replica, the WAL receiver is up and `lag` is within a desired bound\n### Describe alternatives you've considered\nThe issue arose from a request by one of our customers at EDB to \"tune\" the instances participating in the synchronous replication quorum, with a focus on prioritising data durability. Following the shutdown of their only synchronous replica, the replica was automatically added to the `synchronous_standby_names` list as \"ready\" when it restarted. This caused write operations on the primary to halt until the replica became fully synchronous.\nOur initial approach was to address this issue at the level of synchronous replication, pushing the problem to PostgreSQL's handling of replica readiness. However, after extensive discussions and scenario analysis between @leonardoce and me, we realised that modifying the readiness probe wasn't the optimal solution (better, wasn't the right place to solve it). Instead, the better approach would have been to focus on the startup probe, which operates exclusively during the pod's startup phase.\n### Additional context\nN/A\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe current implementation of the startup probe uses the same endpoint as the liveness probe, which is `/healthz`. This endpoint relies on the [`pg_isready` command](https://www.postgresql.org/docs/current/app-pg-isready.html). The liveness probe reports success when `pg_isready` returns either 0 (indicating that the system is ready for connections) or 1 (indicating that the system is starting up, such as during crash recovery).\nThe readiness probe begins after this point. However, in some cases, such as with replicas, this implementation may pose a limitation.\n### Describe the solution you'd like\nWe would like to differentiate the startup probe to define multiple waiting stages before the readiness probe kicks in:\n- `pg_isready`: pg_isready returns 0 (connection is possible)\n- `streaming`: in case of a streaming connected replica, the WAL receiver is up and `lag` is within a desired bound\n### Describe alternatives you've considered\nThe issue arose from a request by one of our customers at EDB to \"tune\" the instances participating in the synchronous replication quorum, with a focus on prioritising data durability. Following the shutdown of their only synchronous replica, the replica was automatically added to the `synchronous_standby_names` list as \"ready\" when it restarted. This caused write operations on the primary to halt until the replica became fully synchronous.\nOur initial approach was to address this issue at the level of synchronous replication, pushing the problem to PostgreSQL's handling of replica readiness. However, after extensive discussions and scenario analysis between @leonardoce and me, we realised that modifying the readiness probe wasn't the optimal solution (better, wasn't the right place to solve it). Instead, the better approach would have been to focus on the startup probe, which operates exclusively during the pod's startup phase.\n### Additional context\nN/A\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: configure CA with ConfigMap",
        "id": 2795703546,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nOur company is providing the CA certificate in a `ConfigMap` resource.\nCurrently only secrets can be used to configure CA for the cluster configuration (`spec.postgresql.certificates`) and for barman configuration (`spec.barmanObjectStore.endpointCA`). \nAlso, the key name used by our company is not `ca.crt`\nTherefore it would be necessary to be able to specify a different key name than the default `ca.crt` for the cluster CA.\n### Describe the solution you'd like\nAccept ConfigMap in addition to secrets for CA certificates.\nBoth are key-value stores and CA certificates does not need to be secrets.\n### Describe alternatives you've considered\nWe currently manage the CA bundle secret ourself\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nOur company is providing the CA certificate in a `ConfigMap` resource.\nCurrently only secrets can be used to configure CA for the cluster configuration (`spec.postgresql.certificates`) and for barman configuration (`spec.barmanObjectStore.endpointCA`). \nAlso, the key name used by our company is not `ca.crt`\nTherefore it would be necessary to be able to specify a different key name than the default `ca.crt` for the cluster CA.\n### Describe the solution you'd like\nAccept ConfigMap in addition to secrets for CA certificates.\nBoth are key-value stores and CA certificates does not need to be secrets.\n### Describe alternatives you've considered\nWe currently manage the CA bundle secret ourself\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: finalizers inclusion",
        "id": 2793116347,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThere must be option to include finalizers in cluster template.\nThis feature can help to prevent accidentally deletion of underlying objects like pvc,secrets etc.\nAlso finalizer condition to save backup of cluster before deletion.\n### Describe the solution you'd like\nCluster template have the option to include finalizers for underlying objects and also conditions like backing up cluster before deletion.\n### Describe alternatives you've considered\nI coun't find any alternate. Rightnow handling using rbac policies for different users.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThere must be option to include finalizers in cluster template.\nThis feature can help to prevent accidentally deletion of underlying objects like pvc,secrets etc.\nAlso finalizer condition to save backup of cluster before deletion.\n### Describe the solution you'd like\nCluster template have the option to include finalizers for underlying objects and also conditions like backing up cluster before deletion.\n### Describe alternatives you've considered\nI coun't find any alternate. Rightnow handling using rbac policies for different users.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Add ability to debug and resolve out-of-space conditions ",
        "id": 2792967890,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI would like to add the ability to debug why an out-of-disk situation occurs and resolve it. \n### Describe the solution you'd like\n- By default, instead of exiting immediately, postgres starts in single-user mode and dumps stats about orphaned base files, replication slots, wal files, etc so we have information to debug the conditions that caused the out-of-space situation.\n- `kubectl cnpg recovery` command that automates starting up a pod in single user mode to debug the situation (I'm working on a temporary bash version of that here: https://gist.github.com/jmealo/ef191db24ee94edf3d71b3b1d63ad755)\nI'm not sure if we can do both as one may conflict with the other :) It's possible that we only need to record the debug information once, and then we can go back into the crash loop.\n### Describe alternatives you've considered\nYou can manually create a pod that runs on the same node and mount the volumes manually. You can use a different image, or the same, but you need to change the entry point.\n**We could add a `kubectl cnpg` command to automate this process?**\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: production-pg-etl-1-debug\n  namespace: db\nspec:\n  nodeName: <same-node-as-original-pod>  # Force it to the same node\n  containers:\n  - name: debug\n    image: ubuntu:latest\n    command: [\"sleep\", \"infinity\"]\n    resources:\n      requests:\n        memory: \"100Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"200Mi\"\n        cpu: \"200m\"\n    volumeMounts:\n    - name: pgdata\n      mountPath: /var/lib/postgresql/data\n    - name: pg-wal\n      mountPath: /var/lib/postgresql/wal\n  volumes:\n  - name: pgdata\n    persistentVolumeClaim:\n      claimName: production-pg-etl-1\n  - name: pg-wal\n    persistentVolumeClaim:\n      claimName: production-pg-etl-1-wal\n```\nYou can then `exec` to get a shell in the container to do post-mortem.\n```\n\u276f kubectl apply -f debug-pod.yaml\npod/production-pg-etl-1-debug created\n\u276f kubectl exec -n db production-pg-etl-1-debug -it -- /bin/bash\nroot@production-pg-etl-1-debug:/# df -h\nFilesystem      Size  Used Avail Use% Mounted on\noverlay         248G   24G  225G  10% /\ntmpfs            64M     0   64M   0% /dev\n/dev/root       248G   24G  225G  10% /etc/hosts\nshm              64M     0   64M   0% /dev/shm\n/dev/sdc       1007G  529G  479G  53% /var/lib/postgresql/data\n/dev/sdb        383G  4.1G  379G   2% /var/lib/postgresql/wal\ntmpfs           200M   12K  200M   1% /run/secrets/kubernetes.io/serviceaccount\ntmpfs            16G     0   16G   0% /proc/acpi\ntmpfs            16G     0   16G   0% /proc/scsi\ntmpfs            16G     0   16G   0% /sys/firmware\n```\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI would like to add the ability to debug why an out-of-disk situation occurs and resolve it. \n### Describe the solution you'd like\n- By default, instead of exiting immediately, postgres starts in single-user mode and dumps stats about orphaned base files, replication slots, wal files, etc so we have information to debug the conditions that caused the out-of-space situation.\n- `kubectl cnpg recovery` command that automates starting up a pod in single user mode to debug the situation (I'm working on a temporary bash version of that here: https://gist.github.com/jmealo/ef191db24ee94edf3d71b3b1d63ad755)\nI'm not sure if we can do both as one may conflict with the other :) It's possible that we only need to record the debug information once, and then we can go back into the crash loop.\n### Describe alternatives you've considered\nYou can manually create a pod that runs on the same node and mount the volumes manually. You can use a different image, or the same, but you need to change the entry point.\n**We could add a `kubectl cnpg` command to automate this process?**\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: production-pg-etl-1-debug\n  namespace: db\nspec:\n  nodeName: <same-node-as-original-pod>  # Force it to the same node\n  containers:\n  - name: debug\n    image: ubuntu:latest\n    command: [\"sleep\", \"infinity\"]\n    resources:\n      requests:\n        memory: \"100Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"200Mi\"\n        cpu: \"200m\"\n    volumeMounts:\n    - name: pgdata\n      mountPath: /var/lib/postgresql/data\n    - name: pg-wal\n      mountPath: /var/lib/postgresql/wal\n  volumes:\n  - name: pgdata\n    persistentVolumeClaim:\n      claimName: production-pg-etl-1\n  - name: pg-wal\n    persistentVolumeClaim:\n      claimName: production-pg-etl-1-wal\n```\nYou can then `exec` to get a shell in the container to do post-mortem.\n```\n\u276f kubectl apply -f debug-pod.yaml\npod/production-pg-etl-1-debug created\n\u276f kubectl exec -n db production-pg-etl-1-debug -it -- /bin/bash\nroot@production-pg-etl-1-debug:/# df -h\nFilesystem      Size  Used Avail Use% Mounted on\noverlay         248G   24G  225G  10% /\ntmpfs            64M     0   64M   0% /dev\n/dev/root       248G   24G  225G  10% /etc/hosts\nshm              64M     0   64M   0% /dev/shm\n/dev/sdc       1007G  529G  479G  53% /var/lib/postgresql/data\n/dev/sdb        383G  4.1G  379G   2% /var/lib/postgresql/wal\ntmpfs           200M   12K  200M   1% /run/secrets/kubernetes.io/serviceaccount\ntmpfs            16G     0   16G   0% /proc/acpi\ntmpfs            16G     0   16G   0% /proc/scsi\ntmpfs            16G     0   16G   0% /sys/firmware\n```\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductI'm still unclear about your plans once you are in the pod. Could you please list some of the actions you intend to perform? This would help us better understand the broader goals of this command. Thank you.\n---\n@gbartolini: To be able to resolve the out of disk space scenario (drop replication slots to non-existent instances, or failed backups) ... that's why dropping into single user mode seems like the most important consideration. Some folks might want to get a shell as well."
    },
    {
        "title": "feat: add security context for pods and containers",
        "id": 2792594757,
        "state": "open",
        "first": "Adds the ability to define SecurityContext for pods and containers, solves #2821",
        "messages": "Adds the ability to define SecurityContext for pods and containers, solves #2821/test\n---\n@sxd could you please review and merge this one?\n---\nThank you so much for this PR! I need this feature to be allowed to deploy CloudNative-PG at my org.\n---\nVery nice!  Can we get this added?\n---\n@x0ddf hi!\r\nI haven't had time to look into this with full time but the first things that comes to my eyes is, where is the documentation and explanation for the usage of this and also all the risks that can come from a bad configuration of a security context? Something like that should explain what kind of things we can expect if something is badly configured, for example, PostgreSQL not starting because the security context define a different user from the one on the image, or that was defined previously ergo you'll see permissions issue.\r\nAlso, you're touching the pgbouncer one, that should be handled using the PodTemplate not using the options from the cluster.\r\nOn the proposal we allow to change the Security Context even on running pods, should we allow that?\r\nJust to finish a quick review, please, this is lacking some test and E2E tests.\r\nRegards,\n---\n@sxd  hi!\r\n> \r\n> I haven't had time to look into this with full time but the first things that comes to my eyes is, where is the documentation and explanation for the usage of this and also all the risks that can come from a bad configuration of a security context? Something like that should explain what kind of things we can expect if something is badly configured, for example, PostgreSQL not starting because the security context define a different user from the one on the image, or that was defined previously ergo you'll see permissions issue.\r\n I do agree with the statement that misconfigured SecurityContext(SC) is a risk. Still, the lack of opportunity to configure it is a risk also and could be a blocker to adopting it in some environments.  For example, in the OpenShift/OKD environments, [UID/GID could be randomized](https://www.redhat.com/en/blog/a-guide-to-openshift-and-uids), and now you have to do tricks with AdmissionControllers/Kyverno and `Cluster` manifest simultaneously(just to set SC for the cnpg pods)! Also, at the moment, `spec.postgres[UID, GID]` is not propagated to the SC level, which is a big problem for me.\r\n> \r\n> Also, you're touching the pgbouncer one, that should be handled using the PodTemplate not using the options from the cluster.\r\n> \r\nIf you look at my change around the pgbouncer, you'll see that I'm not doing anything new (the seccomp profile was consumed from the `Cluster` spec before my changes).\r\nAfter a deeper code evaluation, you'll find these two helpers used to create the deployment for pooler/pgbouncer:\r\n- [WithSecurityContext](https://github.com/cloudnative-pg/cloudnative-pg/blob/main/pkg/podspec/builder.go#L85)\r\n- [WithContainerSecurityContext](https://github.com/cloudnative-pg/cloudnative-pg/blob/main/pkg/podspec/builder.go#L268)\r\nIt's already working as you want it to: if SC is defined in the `PodTemplate`, it will be utilized; otherwise, it will use the cnpg defaults."
    },
    {
        "title": "fix: ensure cluster dependant resources reconcile after cluster rehydratation",
        "id": 2789541779,
        "state": "open",
        "first": "# Release notes\r\nAfter the cluster rehydration, the `Database`, `Publication`, and `Subscription` CRDs now properly reconcile instead of being stuck at `cluster resource has been deleted, skipping reconciliation.`\r\nCloses #6550",
        "messages": "# Release notes\r\nAfter the cluster rehydration, the `Database`, `Publication`, and `Subscription` CRDs now properly reconcile instead of being stuck at `cluster resource has been deleted, skipping reconciliation.`\r\nCloses #6550/test depth=push"
    },
    {
        "title": "[Feature]: Add memory medium support for pooler's emptyDir",
        "id": 2789485036,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nSometime we could not rely on node fs, especially when root fs is full.\nAdding memory medium support for scratch-data could prevent poolers' pod eviction due a node disk pressure.\n```\nibo-partner-config-db-pooler-ro-65f8d447b5-qm449      0/1     Evicted                  0          75m   <none>           b2b-stage-worker4011   <none>\n--\nThe node had condition: [DiskPressure].\nThe node was low on resource: ephemeral-storage. Threshold quantity: 3153042683, available: 3078888Ki. Container pgbouncer was using 32Ki, request is 0, has larger consumption of ephemeral-storage.\n```\n### Describe the solution you'd like\nPoC example\npkg/specs/pgbouncer/deployments.go\n```\nvar medium string\nif useMemoryMedium {\n    medium = \"Memory\"\n}\nWithVolume(&corev1.Volume{\n    Name: \"scratch-data\",\n    VolumeSource: corev1.VolumeSource{\n        EmptyDir: &corev1.EmptyDirVolumeSource{\n            Medium: corev1.StorageMedium(medium),\n        },\n    },\n}).\n```\n### Describe alternatives you've considered\nUse small PVC for scratch-data volume\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nSometime we could not rely on node fs, especially when root fs is full.\nAdding memory medium support for scratch-data could prevent poolers' pod eviction due a node disk pressure.\n```\nibo-partner-config-db-pooler-ro-65f8d447b5-qm449      0/1     Evicted                  0          75m   <none>           b2b-stage-worker4011   <none>\n--\nThe node had condition: [DiskPressure].\nThe node was low on resource: ephemeral-storage. Threshold quantity: 3153042683, available: 3078888Ki. Container pgbouncer was using 32Ki, request is 0, has larger consumption of ephemeral-storage.\n```\n### Describe the solution you'd like\nPoC example\npkg/specs/pgbouncer/deployments.go\n```\nvar medium string\nif useMemoryMedium {\n    medium = \"Memory\"\n}\nWithVolume(&corev1.Volume{\n    Name: \"scratch-data\",\n    VolumeSource: corev1.VolumeSource{\n        EmptyDir: &corev1.EmptyDirVolumeSource{\n            Medium: corev1.StorageMedium(medium),\n        },\n    },\n}).\n```\n### Describe alternatives you've considered\nUse small PVC for scratch-data volume\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: panic when using v1.Cluster in own operator",
        "id": 2789369573,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nsebastian@platform9.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nHelm\n### What happened?\nWe're building an in-house operator on top of CNPG and observing a panic in our own operator inside https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.19.4/pkg/controller/controllerutil#CreateOrUpdate when reconciling on a `Cluster`\n```\nan unexported field was encountered, nested like this: *v1.Cluster -> v1.Cluster -> v1.ClusterSpec -> *v1.ReplicationSlotsConfiguration -> v1.ReplicationSlotsConfiguration -> *v1.SynchronizeReplicasConfiguration -> v1.SynchronizeReplicasConfiguration -> v1.synchronizeReplicasCache -> bool\n```\nThis is due to https://github.com/cloudnative-pg/cloudnative-pg/pull/3710/files#diff-23ef73aeeb33654fa19d5f71b6e2c231660b6ec9023ad82dd736918ce6950617R1004 introducing an unexported field without specifying an an Equality function, see https://github.com/cloudnative-pg/cloudnative-pg/pull/3710/files#diff-23ef73aeeb33654fa19d5f71b6e2c231660b6ec9023ad82dd736918ce6950617R1004 \nIt should be reproducable by calling `reflect.DeepEqual` on two `v1.Cluster`s with a trivially populated `SynchronizeReplicasConfiguration`. \n## Stacktrace\n```\nk8s.io/apimachinery/pkg/util/runtime.logPanic({0x1f4a378, 0x4000a58150}, {0x18f4280, 0x40009182b8})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/pkg/util/runtime/runtime.go:107 +0x98\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Reconcile.func1()\n        /go/pkg/mod/sigs.k8s.io/controller-runtime@v0.19.1/pkg/internal/controller/controller.go:105 +0xf0\npanic({0x18f4280?, 0x40009182b8?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x1bde9a0?, 0x4000378808?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x40009182a0?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x1ab3440?, 0x4000378808?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x4000918288?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x1bd7e20?, 0x4000378910?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x4000918270?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x196c2a0?, 0x4000378a08?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x4000918258?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x19c36a0?, 0x4000918108?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x4000918240?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x199e3a0?, 0x4000918118?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x4000918228?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x19c35e0?, 0x4000809260?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x4000918210?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x1a15e40?, 0x4000809280?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x40009181f8?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x17cf8c0?, 0x4000809298?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x40009181e0?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x17cf8c0?, 0x4000809298?, 0x23164?}, {0x17cf8c0?, 0x40008091d8?, 0x2?}, 0x4000113a48, 0x1, 0x8)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:257 +0x1290\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x1a15e40?, 0x4000809280?, 0x1?}, {0x1a15e40?, 0x40008091c0?, 0x4000112b08?}, 0x4000113a48, 0x1, 0x7)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:206 +0x1310\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x19c35e0?, 0x4000809260?, 0x8?}, {0x19c35e0?, 0x40008091a0?, 0xffff5bf398d8?}, 0x4000113a48, 0x1, 0x6)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:206 +0x1310\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x199e3a0?, 0x4000918118?, 0x17cf4c0?}, {0x199e3a0?, 0x400012dc60?, 0x4000112f08?}, 0x4000113a48, 0x1, 0x5)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:203 +0xed0\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x19c36a0?, 0x4000918108?, 0x17cf4c0?}, {0x19c36a0?, 0x400012dc50?, 0x4000113118?}, 0x4000113a48, 0x1, 0x4)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:206 +0x1310\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x196c2a0?, 0x4000378a08?, 0x4000113300?}, {0x196c2a0?, 0x40005c9208?, 0x40005c9008?}, 0x4000113a48, 0x1, 0x3)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:203 +0xed0\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x1bd7e20?, 0x4000378910?, 0x0?}, {0x1bd7e20?, 0x40005c9110?, 0x0?}, 0x4000113a48, 0x1, 0x2)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:206 +0x1310\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x1ab3440?, 0x4000378808?, 0x7f98c?}, {0x1ab3440?, 0x40005c9008?, 0xffff5bef9fd0?}, 0x4000113a48, 0x1, 0x1)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:206 +0x1310\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x1bde9a0?, 0x4000378808?, 0x17cf4c0?}, {0x1bde9a0?, 0x40005c9008?, 0x4000113a18?}, 0x4000113a48, 0x1, 0x0)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:203 +0xed0\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepEqual(0x40000245d0, {0x1bde9a0?, 0x4000378808?}, {0x1bde9a0?, 0x40005c9008?}, 0x1)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:289 +0x210\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.DeepEqual(...)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:273\nsigs.k8s.io/controller-runtime/pkg/controller/controllerutil.CreateOrUpdate({0x1f4a378, 0x4000a58150}, {0x1f5b440, 0x40002d71f0}, {0x1f70a80, 0x40005c9008}, 0x1d11110)\n        /go/pkg/mod/sigs.k8s.io/controller-runtime@v0.19.1/pkg/controller/controllerutil/controllerutil.go:312 +0x204\nextensions.platform9.horse/extensions/internal/controller.(*ServicePostgresReconciler).Reconcile(0x400000db90, {0x1f4a378, 0x4000a58150}, {{{0x400016af00?, 0x0?}, {0x400016aee8?, 0x5?}}})\n        /workspace/internal/controller/servicepostgres_controller.go:103 +0x2cc\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Reconcile(0x4000a580c0?, {0x1f4a378?, 0x4000a58150?}, {{{0x400016af00?, 0x0?}, {0x400016aee8?, 0x0?}}})\n        /go/pkg/mod/sigs.k8s.io/controller-runtime@v0.19.1/pkg/internal/controller/controller.go:116 +0x9c\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).reconcileHandler(0x1f60e20, {0x1f4a3b0, 0x40006b8690}, {{{0x400016af00, 0x17}, {0x400016aee8, 0x16}}})\n        /go/pkg/mod/sigs.k8s.io/controller-runtime@v0.19.1/pkg/internal/controller/controller.go:303 +0x2b0\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).processNextWorkItem(0x1f60e20, {0x1f4a3b0, 0x40006b8690})\n        /go/pkg/mod/sigs.k8s.io/controller-runtime@v0.19.1/pkg/internal/controller/controller.go:263 +0x18c\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Start.func2.2()\n        /go/pkg/mod/sigs.k8s.io/controller-runtime@v0.19.1/pkg/internal/controller/controller.go:224 +0x80\ncreated by sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Start.func2 in goroutine 117\n        /go/pkg/mod/sigs.k8s.io/controller-runtime@v0.19.1/pkg/internal/controller/controller.go:220 +0x3c8\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nsebastian@platform9.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nHelm\n### What happened?\nWe're building an in-house operator on top of CNPG and observing a panic in our own operator inside https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.19.4/pkg/controller/controllerutil#CreateOrUpdate when reconciling on a `Cluster`\n```\nan unexported field was encountered, nested like this: *v1.Cluster -> v1.Cluster -> v1.ClusterSpec -> *v1.ReplicationSlotsConfiguration -> v1.ReplicationSlotsConfiguration -> *v1.SynchronizeReplicasConfiguration -> v1.SynchronizeReplicasConfiguration -> v1.synchronizeReplicasCache -> bool\n```\nThis is due to https://github.com/cloudnative-pg/cloudnative-pg/pull/3710/files#diff-23ef73aeeb33654fa19d5f71b6e2c231660b6ec9023ad82dd736918ce6950617R1004 introducing an unexported field without specifying an an Equality function, see https://github.com/cloudnative-pg/cloudnative-pg/pull/3710/files#diff-23ef73aeeb33654fa19d5f71b6e2c231660b6ec9023ad82dd736918ce6950617R1004 \nIt should be reproducable by calling `reflect.DeepEqual` on two `v1.Cluster`s with a trivially populated `SynchronizeReplicasConfiguration`. \n## Stacktrace\n```\nk8s.io/apimachinery/pkg/util/runtime.logPanic({0x1f4a378, 0x4000a58150}, {0x18f4280, 0x40009182b8})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/pkg/util/runtime/runtime.go:107 +0x98\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Reconcile.func1()\n        /go/pkg/mod/sigs.k8s.io/controller-runtime@v0.19.1/pkg/internal/controller/controller.go:105 +0xf0\npanic({0x18f4280?, 0x40009182b8?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x1bde9a0?, 0x4000378808?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x40009182a0?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x1ab3440?, 0x4000378808?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x4000918288?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x1bd7e20?, 0x4000378910?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x4000918270?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x196c2a0?, 0x4000378a08?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x4000918258?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x19c36a0?, 0x4000918108?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x4000918240?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x199e3a0?, 0x4000918118?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x4000918228?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x19c35e0?, 0x4000809260?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x4000918210?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x1a15e40?, 0x4000809280?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x40009181f8?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.makeUsefulPanic({0x17cf8c0?, 0x4000809298?, 0x400072d340?})\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:96 +0x134\npanic({0x18f4280?, 0x40009181e0?})\n        /usr/local/go/src/runtime/panic.go:785 +0x124\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x17cf8c0?, 0x4000809298?, 0x23164?}, {0x17cf8c0?, 0x40008091d8?, 0x2?}, 0x4000113a48, 0x1, 0x8)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:257 +0x1290\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x1a15e40?, 0x4000809280?, 0x1?}, {0x1a15e40?, 0x40008091c0?, 0x4000112b08?}, 0x4000113a48, 0x1, 0x7)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:206 +0x1310\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x19c35e0?, 0x4000809260?, 0x8?}, {0x19c35e0?, 0x40008091a0?, 0xffff5bf398d8?}, 0x4000113a48, 0x1, 0x6)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:206 +0x1310\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x199e3a0?, 0x4000918118?, 0x17cf4c0?}, {0x199e3a0?, 0x400012dc60?, 0x4000112f08?}, 0x4000113a48, 0x1, 0x5)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:203 +0xed0\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x19c36a0?, 0x4000918108?, 0x17cf4c0?}, {0x19c36a0?, 0x400012dc50?, 0x4000113118?}, 0x4000113a48, 0x1, 0x4)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:206 +0x1310\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x196c2a0?, 0x4000378a08?, 0x4000113300?}, {0x196c2a0?, 0x40005c9208?, 0x40005c9008?}, 0x4000113a48, 0x1, 0x3)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:203 +0xed0\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x1bd7e20?, 0x4000378910?, 0x0?}, {0x1bd7e20?, 0x40005c9110?, 0x0?}, 0x4000113a48, 0x1, 0x2)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:206 +0x1310\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x1ab3440?, 0x4000378808?, 0x7f98c?}, {0x1ab3440?, 0x40005c9008?, 0xffff5bef9fd0?}, 0x4000113a48, 0x1, 0x1)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:206 +0x1310\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepValueEqual(0x40000245d0, {0x1bde9a0?, 0x4000378808?, 0x17cf4c0?}, {0x1bde9a0?, 0x40005c9008?, 0x4000113a18?}, 0x4000113a48, 0x1, 0x0)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:203 +0xed0\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.deepEqual(0x40000245d0, {0x1bde9a0?, 0x4000378808?}, {0x1bde9a0?, 0x40005c9008?}, 0x1)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:289 +0x210\nk8s.io/apimachinery/third_party/forked/golang/reflect.Equalities.DeepEqual(...)\n        /go/pkg/mod/k8s.io/apimachinery@v0.31.1/third_party/forked/golang/reflect/deep_equal.go:273\nsigs.k8s.io/controller-runtime/pkg/controller/controllerutil.CreateOrUpdate({0x1f4a378, 0x4000a58150}, {0x1f5b440, 0x40002d71f0}, {0x1f70a80, 0x40005c9008}, 0x1d11110)\n        /go/pkg/mod/sigs.k8s.io/controller-runtime@v0.19.1/pkg/controller/controllerutil/controllerutil.go:312 +0x204\nextensions.platform9.horse/extensions/internal/controller.(*ServicePostgresReconciler).Reconcile(0x400000db90, {0x1f4a378, 0x4000a58150}, {{{0x400016af00?, 0x0?}, {0x400016aee8?, 0x5?}}})\n        /workspace/internal/controller/servicepostgres_controller.go:103 +0x2cc\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Reconcile(0x4000a580c0?, {0x1f4a378?, 0x4000a58150?}, {{{0x400016af00?, 0x0?}, {0x400016aee8?, 0x0?}}})\n        /go/pkg/mod/sigs.k8s.io/controller-runtime@v0.19.1/pkg/internal/controller/controller.go:116 +0x9c\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).reconcileHandler(0x1f60e20, {0x1f4a3b0, 0x40006b8690}, {{{0x400016af00, 0x17}, {0x400016aee8, 0x16}}})\n        /go/pkg/mod/sigs.k8s.io/controller-runtime@v0.19.1/pkg/internal/controller/controller.go:303 +0x2b0\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).processNextWorkItem(0x1f60e20, {0x1f4a3b0, 0x40006b8690})\n        /go/pkg/mod/sigs.k8s.io/controller-runtime@v0.19.1/pkg/internal/controller/controller.go:263 +0x18c\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Start.func2.2()\n        /go/pkg/mod/sigs.k8s.io/controller-runtime@v0.19.1/pkg/internal/controller/controller.go:224 +0x80\ncreated by sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Start.func2 in goroutine 117\n        /go/pkg/mod/sigs.k8s.io/controller-runtime@v0.19.1/pkg/internal/controller/controller.go:220 +0x3c8\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: Streaming replication from existing manually-installed instance",
        "id": 2789094810,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\npavel.kutakov@gmail.com\n### Version\n1.24 (latest patch)\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nI'm trying to create replica cluster based on example in **cluster-example-replica-streaming.yaml** manifest. My installation differs from example in two major aspects:\n- source instance was manually installed in VM (user with REPLICATION role was created)\n- It was not configured to use SSL\nThe problem: when I bootstrap the cluster without \"replica\" stanza - it works perfect (put spec.replica.enabled=false). But when I set this value to true, initial job <clustername>-pgbasebackup-xxxx succeded, but instance is in CrashLoopBackoff state. Logs of the instance shows the following error:\n`{\"level\":\"error\",\"ts\":\"2025-01-15T07:50:28.253632552Z\",\"msg\":\"PostgreSQL process exited with errors\",\"logger\":\"instance-manager\",\"logging_pod\":\"target-db-1\",\"error\":\"exit status 2\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start.func1\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:105\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:113\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/manager/runnable_group.go:226\"}`\nCouple of lines above this I can see one more trouble:\n`{\"level\":\"info\",\"ts\":\"2025-01-15T07:50:28.250498316Z\",\"logger\":\"postgres\",\"msg\":\"postgres: could not access the server configuration file \\\"/var/lib/postgresql/data/pgdata/postgresql.conf\\\": No such file or directory\",\"pipe\":\"stderr\",\"logging_pod\":\"target-db-1\"}`\nI also noticed big difference in -pgbasebackup- job pod. When \"replica\" stanza is turned off, it does something meaningful after finishing with pgbasebackup. When replica is on, the last message in the log will be the following:\n`{\"level\":\"info\",\"ts\":\"2025-01-15T07:49:25.629438306Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"target-db-1-pgbasebackup\",\"filename\":\"override.conf\"}`\nIt looks like other configurations are not finished in the job.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: target-db\nspec:\n  instances: 1\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.2\n  bootstrap:\n    pg_basebackup:\n      source: source-db\n  replica:\n    enabled: true\n    source: source-db\n  storage:\n    size: 1Gi\n  externalClusters:\n  - name: source-db\n    connectionParameters:\n      host: <host IP here>\n      user: repluser\n      sslmode: require\n      dbname: postgres\n    password:\n      name: source-db-repluser\n      key: password\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\npavel.kutakov@gmail.com\n### Version\n1.24 (latest patch)\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nI'm trying to create replica cluster based on example in **cluster-example-replica-streaming.yaml** manifest. My installation differs from example in two major aspects:\n- source instance was manually installed in VM (user with REPLICATION role was created)\n- It was not configured to use SSL\nThe problem: when I bootstrap the cluster without \"replica\" stanza - it works perfect (put spec.replica.enabled=false). But when I set this value to true, initial job <clustername>-pgbasebackup-xxxx succeded, but instance is in CrashLoopBackoff state. Logs of the instance shows the following error:\n`{\"level\":\"error\",\"ts\":\"2025-01-15T07:50:28.253632552Z\",\"msg\":\"PostgreSQL process exited with errors\",\"logger\":\"instance-manager\",\"logging_pod\":\"target-db-1\",\"error\":\"exit status 2\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start.func1\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:105\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:113\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/manager/runnable_group.go:226\"}`\nCouple of lines above this I can see one more trouble:\n`{\"level\":\"info\",\"ts\":\"2025-01-15T07:50:28.250498316Z\",\"logger\":\"postgres\",\"msg\":\"postgres: could not access the server configuration file \\\"/var/lib/postgresql/data/pgdata/postgresql.conf\\\": No such file or directory\",\"pipe\":\"stderr\",\"logging_pod\":\"target-db-1\"}`\nI also noticed big difference in -pgbasebackup- job pod. When \"replica\" stanza is turned off, it does something meaningful after finishing with pgbasebackup. When replica is on, the last message in the log will be the following:\n`{\"level\":\"info\",\"ts\":\"2025-01-15T07:49:25.629438306Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"target-db-1-pgbasebackup\",\"filename\":\"override.conf\"}`\nIt looks like other configurations are not finished in the job.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: target-db\nspec:\n  instances: 1\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.2\n  bootstrap:\n    pg_basebackup:\n      source: source-db\n  replica:\n    enabled: true\n    source: source-db\n  storage:\n    size: 1Gi\n  externalClusters:\n  - name: source-db\n    connectionParameters:\n      host: <host IP here>\n      user: repluser\n      sslmode: require\n      dbname: postgres\n    password:\n      name: source-db-repluser\n      key: password\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "chore: add SBOMs for the operator container images and archive builds",
        "id": 2788758122,
        "state": "open",
        "first": "Closes #4199",
        "messages": "Closes #41990.30.0 is the release sample\r\nhttps://github.com/EnterpriseDB/cloudnative-pg/releases/tag/v0.30.0\n---\nI think we're generating the SBOM only for one architecture and we have at least two architectures there, can you check if that it's the case @litaocdl  ?\n---\n@sxd you are right, syft only generate reports on the architecture it is running, unless extra params is given. let me try.\n---\n@sxd I update the pr with both arm64 and amd64 scaned, syft can only scan one by one, so we have to call the sbom action multiple times for multiple architecture"
    },
    {
        "title": "Add cnpg suffix of cnpg resources",
        "id": 2787772745,
        "state": "open",
        "first": "fix #6308",
        "messages": "fix #6308"
    },
    {
        "title": "[Bug]: Controller Manager: /readyz and /pg/status endpoint hang indefinitely (even inside container/pod)",
        "id": 2787514303,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njeffm@gisual.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nYAML manifest\n### What happened?\n# Symptoms\n- Any operations which require `/readyz` or `/pg/status` to respond fail\n- `/readyz` and `/pg/status` hang after TLS handshake from inside and outside the pod\n- `cnpg status` hangs\n## Hanging endpoints\n- `/readyz` - logs a failure `context cancelled`\n- `/pg/status` - logs a success -- but doesn't return a response\n## Working endpoints\n- `/healthz` works\n- `/pg/controldata` works\n- `/pg/mode/backup` works\n- `/pg/archive/partial` works\n# Troubleshooting steps\n- I have confirmed this via `curl` to`localhost` from inside the pod.\n- These endpoints established a TLS connection and then never respond. \n- Other Controller Manager endpoints respond in a timely fashion\n# Slack thread\nA full diagnostic Slack thread is available on CNPG slack here:\nhttps://cloudnativepg.slack.com/archives/C03AX0J5P29/p1736880191821519\n```\n\u276f kubectl exec -n db -it production-pg-etl-1 -- /bin/bash\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\npostgres@production-pg-etl-1:/$ curl -kv https://localhost:8000/readyz\n*   Trying 127.0.0.1:8000...\n* Connected to localhost (127.0.0.1) port 8000 (#0)\n* ALPN: offers h2,http/1.1\n* TLSv1.3 (OUT), TLS handshake, Client hello (1):\n* TLSv1.3 (IN), TLS handshake, Server hello (2):\n* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\n* TLSv1.3 (IN), TLS handshake, Certificate (11):\n* TLSv1.3 (IN), TLS handshake, CERT verify (15):\n* TLSv1.3 (IN), TLS handshake, Finished (20):\n* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\n* TLSv1.3 (OUT), TLS handshake, Finished (20):\n* SSL connection using TLSv1.3 / TLS_AES_128_GCM_SHA256\n* ALPN: server accepted h2\n* Server certificate:\n*  subject: CN=production-pg-etl-rw\n*  start date: Jan  9 16:54:17 2025 GMT\n*  expire date: Apr  9 16:54:17 2025 GMT\n*  issuer: OU=db; CN=production-pg-etl\n*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.\n* using HTTP/2\n* h2h3 [:method: GET]\n* h2h3 [:path: /readyz]\n* h2h3 [:scheme: https]\n* h2h3 [:authority: localhost:8000]\n* h2h3 [user-agent: curl/7.88.1]\n* h2h3 [accept: */*]\n* Using Stream ID: 1 (easy handle 0x5641e939ece0)\n> GET /readyz HTTP/2\n> Host: localhost:8000\n> user-agent: curl/7.88.1\n> accept: */*\n> \n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n```\nRelevant Slack Thread:\nhttps://cloudnativepg.slack.com/archives/C03AX0J5P29/p1736795638839879\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  annotations:\n    cnpg.io/reconciliationLoop: disabled\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"metadata\":{\"annotations\":{},\"name\":\"production-pg-etl\",\"namespace\":\"db\"},\"spec\":{\"affinity\":{\"nodeSelector\":{\"kubernetes.azure.com/agentpool\":\"pgetl2\"},\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/postgres\",\"operator\":\"Exists\"}]},\"backup\":{\"barmanObjectStore\":{\"azureCredentials\":{\"connectionString\":{\"key\":\"AZURE_STORAGE_CONNECTION_STRING\",\"name\":\"postgres-backup-azure-creds\"}},\"data\":{\"compression\":\"snappy\",\"jobs\":16},\"destinationPath\":\"https://gisualproduction.blob.core.windows.net/postgres-backup\",\"wal\":{\"maxParallel\":16}},\"retentionPolicy\":\"30d\",\"target\":\"prefer-standby\",\"volumeSnapshot\":{\"className\":\"csi-azure-disk\",\"online\":true,\"onlineConfiguration\":{\"immediateCheckpoint\":false,\"waitForArchive\":true},\"snapshotOwnerReference\":\"cluster\"}},\"bootstrap\":{\"initdb\":{\"database\":\"gisual\",\"owner\":\"gisual\",\"secret\":{\"name\":\"postgres-gisual-db-credentials\"}}},\"enablePDB\":true,\"enableSuperuserAccess\":true,\"env\":[{\"name\":\"ENABLE_AZURE_PVC_UPDATES\",\"value\":\"true\"},{\"name\":\"ENABLE_INSTANCE_MANAGER_INPLACE_UPDATE\",\"value\":\"true\"}],\"failoverDelay\":30,\"imageCatalogRef\":{\"apiGroup\":\"postgresql.cnpg.io\",\"kind\":\"ImageCatalog\",\"major\":17,\"name\":\"custom-postgres-images\"},\"imagePullSecrets\":[{\"name\":\"docker-registry-credentials\"}],\"instances\":3,\"monitoring\":{\"enablePodMonitor\":true},\"postgresql\":{\"parameters\":{\"autovacuum_max_workers\":\"2\",\"autovacuum_naptime\":\"20s\",\"autovacuum_vacuum_cost_limit\":\"1000\",\"autovacuum_vacuum_scale_factor\":\"0.1\",\"checkpoint_completion_target\":\"0.9\",\"checkpoint_timeout\":\"15min\",\"cron.database_name\":\"gisual\",\"cron.use_background_workers\":\"on\",\"default_statistics_target\":\"100\",\"default_toast_compression\":\"lz4\",\"effective_cache_size\":\"21366MB\",\"effective_io_concurrency\":\"1000\",\"idle_in_transaction_session_timeout\":\"30min\",\"lock_timeout\":\"1800s\",\"maintenance_work_mem\":\"1823232kB\",\"max_connections\":\"100\",\"max_parallel_maintenance_workers\":\"4\",\"max_parallel_workers\":\"8\",\"max_parallel_workers_per_gather\":\"4\",\"max_wal_size\":\"4GB\",\"max_worker_processes\":\"8\",\"min_wal_size\":\"1GB\",\"random_page_cost\":\"1.1\",\"shared_buffers\":\"7122MB\",\"statement_timeout\":\"7200s\",\"wal_buffers\":\"16MB\",\"wal_compression\":\"lz4\",\"work_mem\":\"9116kB\"},\"shared_preload_libraries\":[\"pg_stat_statements\",\"auto_explain\",\"pg_cron\"]},\"primaryUpdateStrategy\":\"unsupervised\",\"resources\":{\"limits\":{\"cpu\":\"6858m\",\"memory\":\"28488Mi\"},\"requests\":{\"cpu\":\"6858m\",\"memory\":\"28488Mi\"}},\"storage\":{\"size\":\"1024Gi\",\"storageClass\":\"12800-iops-290mb-tp-premium-v2\"},\"superuserSecret\":{\"name\":\"postgres-superuser-secret\"},\"switchoverDelay\":30,\"walStorage\":{\"size\":\"312Gi\",\"storageClass\":\"12800-iops-290mb-tp-premium-v2\"}}}\n    kubectl.kubernetes.io/restartedAt: \"2025-01-11T19:57:48-05:00\"\n  creationTimestamp: \"2025-01-09T16:59:17Z\"\n  generation: 6\n  name: production-pg-etl\n  namespace: db\n  resourceVersion: \"890729746\"\n  uid: 61049947-c1e9-49c3-a4b4-d0e81a463034\nspec:\n  affinity:\n    nodeSelector:\n      kubernetes.azure.com/agentpool: pgetl2\n    podAntiAffinityType: preferred\n    tolerations:\n    - effect: NoSchedule\n      key: node-role.kubernetes.io/postgres\n      operator: Exists\n  backup:\n    barmanObjectStore:\n      azureCredentials:\n        connectionString:\n          key: AZURE_STORAGE_CONNECTION_STRING\n          name: postgres-backup-azure-creds\n      data:\n        compression: snappy\n        jobs: 16\n      destinationPath: https://gisualproduction.blob.core.windows.net/postgres-backup\n      wal:\n        maxParallel: 16\n    retentionPolicy: 30d\n    target: prefer-standby\n    volumeSnapshot:\n      className: csi-azure-disk\n      online: true\n      onlineConfiguration:\n        immediateCheckpoint: false\n        waitForArchive: true\n      snapshotOwnerReference: cluster\n  bootstrap:\n    initdb:\n      database: gisual\n      encoding: UTF8\n      localeCType: C\n      localeCollate: C\n      owner: gisual\n      secret:\n        name: postgres-gisual-db-credentials\n  enablePDB: true\n  enableSuperuserAccess: true\n  env:\n  - name: ENABLE_AZURE_PVC_UPDATES\n    value: \"true\"\n  - name: ENABLE_INSTANCE_MANAGER_INPLACE_UPDATE\n    value: \"true\"\n  failoverDelay: 30\n  imageCatalogRef:\n    apiGroup: postgresql.cnpg.io\n    kind: ImageCatalog\n    major: 17\n    name: custom-postgres-images\n  imagePullSecrets:\n  - name: docker-registry-credentials\n  instances: 3\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n    - key: queries\n      name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: true\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: \"on\"\n      archive_timeout: 5min\n      autovacuum_max_workers: \"2\"\n      autovacuum_naptime: 20s\n      autovacuum_vacuum_cost_limit: \"1000\"\n      autovacuum_vacuum_scale_factor: \"0.1\"\n      checkpoint_completion_target: \"0.9\"\n      checkpoint_timeout: 15min\n      cron.database_name: gisual\n      cron.use_background_workers: \"on\"\n      default_statistics_target: \"100\"\n      default_toast_compression: lz4\n      dynamic_shared_memory_type: posix\n      effective_cache_size: 21366MB\n      effective_io_concurrency: \"1000\"\n      full_page_writes: \"on\"\n      idle_in_transaction_session_timeout: 30min\n      lock_timeout: 1800s\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: \"0\"\n      log_rotation_size: \"0\"\n      log_truncate_on_rotation: \"false\"\n      logging_collector: \"on\"\n      maintenance_work_mem: 1823232kB\n      max_connections: \"100\"\n      max_parallel_maintenance_workers: \"4\"\n      max_parallel_workers: \"8\"\n      max_parallel_workers_per_gather: \"4\"\n      max_replication_slots: \"32\"\n      max_wal_size: 4GB\n      max_worker_processes: \"8\"\n      min_wal_size: 1GB\n      random_page_cost: \"1.1\"\n      shared_buffers: 7122MB\n      shared_memory_type: mmap\n      shared_preload_libraries: \"\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      statement_timeout: 7200s\n      wal_buffers: 16MB\n      wal_compression: lz4\n      wal_keep_size: 512MB\n      wal_level: logical\n      wal_log_hints: \"on\"\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n      work_mem: 9116kB\n    shared_preload_libraries:\n    - pg_stat_statements\n    - auto_explain\n    - pg_cron\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: restart\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  resources:\n    limits:\n      cpu: 6858m\n      memory: 28488Mi\n    requests:\n      cpu: 6858m\n      memory: 28488Mi\n  smartShutdownTimeout: 180\n  startDelay: 3600\n  stopDelay: 1800\n  storage:\n    resizeInUseVolumes: true\n    size: 1024Gi\n    storageClass: 12800-iops-290mb-tp-premium-v2\n  superuserSecret:\n    name: postgres-superuser-secret\n  switchoverDelay: 30\n  walStorage:\n    resizeInUseVolumes: true\n    size: 312Gi\n    storageClass: 12800-iops-290mb-tp-premium-v2\nstatus:\n  availableArchitectures:\n  - goArch: amd64\n    hash: 58242fc95faa81d6bba0ba19ea959b82d9b49dc5f8e6c755f3c663665ff0ce1d\n  - goArch: arm64\n    hash: 8e5ef490a05bbd5700047edda5e2440acd14bafa256dbfcc7d4a5107573df75b\n  certificates:\n    clientCASecret: production-pg-etl-ca\n    expirations:\n      production-pg-etl-ca: 2025-04-09 16:54:17 +0000 UTC\n      production-pg-etl-replication: 2025-04-09 16:54:17 +0000 UTC\n      production-pg-etl-server: 2025-04-09 16:54:17 +0000 UTC\n    replicationTLSSecret: production-pg-etl-replication\n    serverAltDNSNames:\n    - production-pg-etl-rw\n    - production-pg-etl-rw.db\n    - production-pg-etl-rw.db.svc\n    - production-pg-etl-rw.db.svc.cluster.local\n    - production-pg-etl-r\n    - production-pg-etl-r.db\n    - production-pg-etl-r.db.svc\n    - production-pg-etl-r.db.svc.cluster.local\n    - production-pg-etl-ro\n    - production-pg-etl-ro.db\n    - production-pg-etl-ro.db.svc\n    - production-pg-etl-ro.db.svc.cluster.local\n    serverCASecret: production-pg-etl-ca\n    serverTLSSecret: production-pg-etl-server\n  cloudNativePGCommitHash: bad5a251\n  cloudNativePGOperatorHash: 58242fc95faa81d6bba0ba19ea959b82d9b49dc5f8e6c755f3c663665ff0ce1d\n  conditions:\n  - lastTransitionTime: \"2025-01-13T04:32:48Z\"\n    message: Cluster Is Not Ready\n    reason: ClusterIsNotReady\n    status: \"False\"\n    type: Ready\n  - lastTransitionTime: \"2025-01-09T16:59:59Z\"\n    message: Continuous archiving is working\n    reason: ContinuousArchivingSuccess\n    status: \"True\"\n    type: ContinuousArchiving\n  - lastTransitionTime: \"2025-01-13T00:47:27Z\"\n    message: 'while trying to start the backup: while executing http request: Post\n      \"https://10.130.0.239:8000/pg/mode/backup\": context deadline exceeded (Client.Timeout\n      exceeded while awaiting headers)'\n    reason: LastBackupFailed\n    status: \"False\"\n    type: LastBackupSucceeded\n  configMapResourceVersion:\n    metrics:\n      cnpg-default-monitoring: \"824255109\"\n  currentPrimary: production-pg-etl-1\n  currentPrimaryTimestamp: \"2025-01-09T16:59:55.604522Z\"\n  firstRecoverabilityPoint: \"2025-01-09T17:08:15Z\"\n  firstRecoverabilityPointByMethod:\n    barmanObjectStore: \"2025-01-10T00:00:03Z\"\n    volumeSnapshot: \"2025-01-09T17:08:15Z\"\n  healthyPVC:\n  - production-pg-etl-1\n  - production-pg-etl-1-wal\n  - production-pg-etl-4\n  - production-pg-etl-4-wal\n  - production-pg-etl-5\n  - production-pg-etl-5-wal\n  image: docker.gisual.net/custom-postgres17:17.2@sha256:9d93740c5fcfa7b3113815d6118f8a0afe35dcef38775dae2b93f6d7badedc91\n  instanceNames:\n  - production-pg-etl-1\n  - production-pg-etl-4\n  - production-pg-etl-5\n  instances: 3\n  instancesReportedState:\n    production-pg-etl-1:\n      isPrimary: true\n      timeLineID: 1\n    production-pg-etl-4:\n      isPrimary: false\n    production-pg-etl-5:\n      isPrimary: false\n  instancesStatus:\n    healthy:\n    - production-pg-etl-1\n    replicating:\n    - production-pg-etl-4\n    - production-pg-etl-5\n  lastFailedBackup: \"2025-01-11T01:19:12Z\"\n  lastSuccessfulBackup: \"2025-01-13T00:47:13Z\"\n  lastSuccessfulBackupByMethod:\n    barmanObjectStore: \"2025-01-13T00:47:13Z\"\n    volumeSnapshot: \"2025-01-12T14:08:35Z\"\n  latestGeneratedNode: 5\n  managedRolesStatus: {}\n  phase: Not enough disk space\n  phaseReason: Insufficient disk space detected in one or more pods is preventing\n    PostgreSQL from running.Please verify your storage settings. Further information\n    inside .status.instancesReportedState\n  poolerIntegrations:\n    pgBouncerIntegration: {}\n  pvcCount: 6\n  readService: production-pg-etl-r\n  readyInstances: 1\n  secretsResourceVersion:\n    clientCaSecretVersion: \"882787408\"\n    replicationSecretVersion: \"882787410\"\n    serverCaSecretVersion: \"882787408\"\n    serverSecretVersion: \"882787409\"\n    superuserSecretVersion: \"824019503\"\n  switchReplicaClusterStatus: {}\n  targetPrimary: production-pg-etl-1\n  targetPrimaryTimestamp: \"2025-01-09T16:59:17.884407Z\"\n  timelineID: 1\n  topology:\n    instances:\n      production-pg-etl-1: {}\n      production-pg-etl-4: {}\n      production-pg-etl-5: {}\n    nodesUsed: 3\n    successfullyExtracted: true\n  writeService: production-pg-etl-rw\n```\n### Relevant log output\n```json\n{\"level\":\"info\",\"ts\":\"2025-01-13T20:08:15.41926761Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"35929757-0e5a-4aa3-a21e-2dbaeaaf8cf7\",\"podName\":\"production-pg-common-2\",\"error\":\"Get \\\"https://10.130.18.19:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2025-01-13T20:08:45.697331048Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"89849473-cf26-42ed-9b91-d5dfa7957ea3\",\"podName\":\"production-pg-common-2\",\"error\":\"Get \\\"https://10.130.18.19:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2025-01-13T20:09:15.849085431Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"43c3444a-b204-4a3a-9b2f-bbe3b212e2bb\",\"podName\":\"production-pg-common-2\",\"error\":\"Get \\\"https://10.130.18.19:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2025-01-13T20:09:46.021972798Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"7e42adf8-bea4-4eb3-b48c-1efdb88d0e8a\",\"podName\":\"production-pg-common-2\",\"error\":\"Get \\\"https://10.130.18.19:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2025-01-13T20:10:16.518873375Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"1f0f5b0d-e3be-4a31-87ed-e3b202b94f69\",\"podName\":\"production-pg-common-2\",\"error\":\"Get \\\"https://10.130.18.19:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njeffm@gisual.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nYAML manifest\n### What happened?\n# Symptoms\n- Any operations which require `/readyz` or `/pg/status` to respond fail\n- `/readyz` and `/pg/status` hang after TLS handshake from inside and outside the pod\n- `cnpg status` hangs\n## Hanging endpoints\n- `/readyz` - logs a failure `context cancelled`\n- `/pg/status` - logs a success -- but doesn't return a response\n## Working endpoints\n- `/healthz` works\n- `/pg/controldata` works\n- `/pg/mode/backup` works\n- `/pg/archive/partial` works\n# Troubleshooting steps\n- I have confirmed this via `curl` to`localhost` from inside the pod.\n- These endpoints established a TLS connection and then never respond. \n- Other Controller Manager endpoints respond in a timely fashion\n# Slack thread\nA full diagnostic Slack thread is available on CNPG slack here:\nhttps://cloudnativepg.slack.com/archives/C03AX0J5P29/p1736880191821519\n```\n\u276f kubectl exec -n db -it production-pg-etl-1 -- /bin/bash\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\npostgres@production-pg-etl-1:/$ curl -kv https://localhost:8000/readyz\n*   Trying 127.0.0.1:8000...\n* Connected to localhost (127.0.0.1) port 8000 (#0)\n* ALPN: offers h2,http/1.1\n* TLSv1.3 (OUT), TLS handshake, Client hello (1):\n* TLSv1.3 (IN), TLS handshake, Server hello (2):\n* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\n* TLSv1.3 (IN), TLS handshake, Certificate (11):\n* TLSv1.3 (IN), TLS handshake, CERT verify (15):\n* TLSv1.3 (IN), TLS handshake, Finished (20):\n* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\n* TLSv1.3 (OUT), TLS handshake, Finished (20):\n* SSL connection using TLSv1.3 / TLS_AES_128_GCM_SHA256\n* ALPN: server accepted h2\n* Server certificate:\n*  subject: CN=production-pg-etl-rw\n*  start date: Jan  9 16:54:17 2025 GMT\n*  expire date: Apr  9 16:54:17 2025 GMT\n*  issuer: OU=db; CN=production-pg-etl\n*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.\n* using HTTP/2\n* h2h3 [:method: GET]\n* h2h3 [:path: /readyz]\n* h2h3 [:scheme: https]\n* h2h3 [:authority: localhost:8000]\n* h2h3 [user-agent: curl/7.88.1]\n* h2h3 [accept: */*]\n* Using Stream ID: 1 (easy handle 0x5641e939ece0)\n> GET /readyz HTTP/2\n> Host: localhost:8000\n> user-agent: curl/7.88.1\n> accept: */*\n> \n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n```\nRelevant Slack Thread:\nhttps://cloudnativepg.slack.com/archives/C03AX0J5P29/p1736795638839879\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  annotations:\n    cnpg.io/reconciliationLoop: disabled\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"metadata\":{\"annotations\":{},\"name\":\"production-pg-etl\",\"namespace\":\"db\"},\"spec\":{\"affinity\":{\"nodeSelector\":{\"kubernetes.azure.com/agentpool\":\"pgetl2\"},\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/postgres\",\"operator\":\"Exists\"}]},\"backup\":{\"barmanObjectStore\":{\"azureCredentials\":{\"connectionString\":{\"key\":\"AZURE_STORAGE_CONNECTION_STRING\",\"name\":\"postgres-backup-azure-creds\"}},\"data\":{\"compression\":\"snappy\",\"jobs\":16},\"destinationPath\":\"https://gisualproduction.blob.core.windows.net/postgres-backup\",\"wal\":{\"maxParallel\":16}},\"retentionPolicy\":\"30d\",\"target\":\"prefer-standby\",\"volumeSnapshot\":{\"className\":\"csi-azure-disk\",\"online\":true,\"onlineConfiguration\":{\"immediateCheckpoint\":false,\"waitForArchive\":true},\"snapshotOwnerReference\":\"cluster\"}},\"bootstrap\":{\"initdb\":{\"database\":\"gisual\",\"owner\":\"gisual\",\"secret\":{\"name\":\"postgres-gisual-db-credentials\"}}},\"enablePDB\":true,\"enableSuperuserAccess\":true,\"env\":[{\"name\":\"ENABLE_AZURE_PVC_UPDATES\",\"value\":\"true\"},{\"name\":\"ENABLE_INSTANCE_MANAGER_INPLACE_UPDATE\",\"value\":\"true\"}],\"failoverDelay\":30,\"imageCatalogRef\":{\"apiGroup\":\"postgresql.cnpg.io\",\"kind\":\"ImageCatalog\",\"major\":17,\"name\":\"custom-postgres-images\"},\"imagePullSecrets\":[{\"name\":\"docker-registry-credentials\"}],\"instances\":3,\"monitoring\":{\"enablePodMonitor\":true},\"postgresql\":{\"parameters\":{\"autovacuum_max_workers\":\"2\",\"autovacuum_naptime\":\"20s\",\"autovacuum_vacuum_cost_limit\":\"1000\",\"autovacuum_vacuum_scale_factor\":\"0.1\",\"checkpoint_completion_target\":\"0.9\",\"checkpoint_timeout\":\"15min\",\"cron.database_name\":\"gisual\",\"cron.use_background_workers\":\"on\",\"default_statistics_target\":\"100\",\"default_toast_compression\":\"lz4\",\"effective_cache_size\":\"21366MB\",\"effective_io_concurrency\":\"1000\",\"idle_in_transaction_session_timeout\":\"30min\",\"lock_timeout\":\"1800s\",\"maintenance_work_mem\":\"1823232kB\",\"max_connections\":\"100\",\"max_parallel_maintenance_workers\":\"4\",\"max_parallel_workers\":\"8\",\"max_parallel_workers_per_gather\":\"4\",\"max_wal_size\":\"4GB\",\"max_worker_processes\":\"8\",\"min_wal_size\":\"1GB\",\"random_page_cost\":\"1.1\",\"shared_buffers\":\"7122MB\",\"statement_timeout\":\"7200s\",\"wal_buffers\":\"16MB\",\"wal_compression\":\"lz4\",\"work_mem\":\"9116kB\"},\"shared_preload_libraries\":[\"pg_stat_statements\",\"auto_explain\",\"pg_cron\"]},\"primaryUpdateStrategy\":\"unsupervised\",\"resources\":{\"limits\":{\"cpu\":\"6858m\",\"memory\":\"28488Mi\"},\"requests\":{\"cpu\":\"6858m\",\"memory\":\"28488Mi\"}},\"storage\":{\"size\":\"1024Gi\",\"storageClass\":\"12800-iops-290mb-tp-premium-v2\"},\"superuserSecret\":{\"name\":\"postgres-superuser-secret\"},\"switchoverDelay\":30,\"walStorage\":{\"size\":\"312Gi\",\"storageClass\":\"12800-iops-290mb-tp-premium-v2\"}}}\n    kubectl.kubernetes.io/restartedAt: \"2025-01-11T19:57:48-05:00\"\n  creationTimestamp: \"2025-01-09T16:59:17Z\"\n  generation: 6\n  name: production-pg-etl\n  namespace: db\n  resourceVersion: \"890729746\"\n  uid: 61049947-c1e9-49c3-a4b4-d0e81a463034\nspec:\n  affinity:\n    nodeSelector:\n      kubernetes.azure.com/agentpool: pgetl2\n    podAntiAffinityType: preferred\n    tolerations:\n    - effect: NoSchedule\n      key: node-role.kubernetes.io/postgres\n      operator: Exists\n  backup:\n    barmanObjectStore:\n      azureCredentials:\n        connectionString:\n          key: AZURE_STORAGE_CONNECTION_STRING\n          name: postgres-backup-azure-creds\n      data:\n        compression: snappy\n        jobs: 16\n      destinationPath: https://gisualproduction.blob.core.windows.net/postgres-backup\n      wal:\n        maxParallel: 16\n    retentionPolicy: 30d\n    target: prefer-standby\n    volumeSnapshot:\n      className: csi-azure-disk\n      online: true\n      onlineConfiguration:\n        immediateCheckpoint: false\n        waitForArchive: true\n      snapshotOwnerReference: cluster\n  bootstrap:\n    initdb:\n      database: gisual\n      encoding: UTF8\n      localeCType: C\n      localeCollate: C\n      owner: gisual\n      secret:\n        name: postgres-gisual-db-credentials\n  enablePDB: true\n  enableSuperuserAccess: true\n  env:\n  - name: ENABLE_AZURE_PVC_UPDATES\n    value: \"true\"\n  - name: ENABLE_INSTANCE_MANAGER_INPLACE_UPDATE\n    value: \"true\"\n  failoverDelay: 30\n  imageCatalogRef:\n    apiGroup: postgresql.cnpg.io\n    kind: ImageCatalog\n    major: 17\n    name: custom-postgres-images\n  imagePullSecrets:\n  - name: docker-registry-credentials\n  instances: 3\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n    - key: queries\n      name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: true\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: \"on\"\n      archive_timeout: 5min\n      autovacuum_max_workers: \"2\"\n      autovacuum_naptime: 20s\n      autovacuum_vacuum_cost_limit: \"1000\"\n      autovacuum_vacuum_scale_factor: \"0.1\"\n      checkpoint_completion_target: \"0.9\"\n      checkpoint_timeout: 15min\n      cron.database_name: gisual\n      cron.use_background_workers: \"on\"\n      default_statistics_target: \"100\"\n      default_toast_compression: lz4\n      dynamic_shared_memory_type: posix\n      effective_cache_size: 21366MB\n      effective_io_concurrency: \"1000\"\n      full_page_writes: \"on\"\n      idle_in_transaction_session_timeout: 30min\n      lock_timeout: 1800s\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: \"0\"\n      log_rotation_size: \"0\"\n      log_truncate_on_rotation: \"false\"\n      logging_collector: \"on\"\n      maintenance_work_mem: 1823232kB\n      max_connections: \"100\"\n      max_parallel_maintenance_workers: \"4\"\n      max_parallel_workers: \"8\"\n      max_parallel_workers_per_gather: \"4\"\n      max_replication_slots: \"32\"\n      max_wal_size: 4GB\n      max_worker_processes: \"8\"\n      min_wal_size: 1GB\n      random_page_cost: \"1.1\"\n      shared_buffers: 7122MB\n      shared_memory_type: mmap\n      shared_preload_libraries: \"\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      statement_timeout: 7200s\n      wal_buffers: 16MB\n      wal_compression: lz4\n      wal_keep_size: 512MB\n      wal_level: logical\n      wal_log_hints: \"on\"\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n      work_mem: 9116kB\n    shared_preload_libraries:\n    - pg_stat_statements\n    - auto_explain\n    - pg_cron\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: restart\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  resources:\n    limits:\n      cpu: 6858m\n      memory: 28488Mi\n    requests:\n      cpu: 6858m\n      memory: 28488Mi\n  smartShutdownTimeout: 180\n  startDelay: 3600\n  stopDelay: 1800\n  storage:\n    resizeInUseVolumes: true\n    size: 1024Gi\n    storageClass: 12800-iops-290mb-tp-premium-v2\n  superuserSecret:\n    name: postgres-superuser-secret\n  switchoverDelay: 30\n  walStorage:\n    resizeInUseVolumes: true\n    size: 312Gi\n    storageClass: 12800-iops-290mb-tp-premium-v2\nstatus:\n  availableArchitectures:\n  - goArch: amd64\n    hash: 58242fc95faa81d6bba0ba19ea959b82d9b49dc5f8e6c755f3c663665ff0ce1d\n  - goArch: arm64\n    hash: 8e5ef490a05bbd5700047edda5e2440acd14bafa256dbfcc7d4a5107573df75b\n  certificates:\n    clientCASecret: production-pg-etl-ca\n    expirations:\n      production-pg-etl-ca: 2025-04-09 16:54:17 +0000 UTC\n      production-pg-etl-replication: 2025-04-09 16:54:17 +0000 UTC\n      production-pg-etl-server: 2025-04-09 16:54:17 +0000 UTC\n    replicationTLSSecret: production-pg-etl-replication\n    serverAltDNSNames:\n    - production-pg-etl-rw\n    - production-pg-etl-rw.db\n    - production-pg-etl-rw.db.svc\n    - production-pg-etl-rw.db.svc.cluster.local\n    - production-pg-etl-r\n    - production-pg-etl-r.db\n    - production-pg-etl-r.db.svc\n    - production-pg-etl-r.db.svc.cluster.local\n    - production-pg-etl-ro\n    - production-pg-etl-ro.db\n    - production-pg-etl-ro.db.svc\n    - production-pg-etl-ro.db.svc.cluster.local\n    serverCASecret: production-pg-etl-ca\n    serverTLSSecret: production-pg-etl-server\n  cloudNativePGCommitHash: bad5a251\n  cloudNativePGOperatorHash: 58242fc95faa81d6bba0ba19ea959b82d9b49dc5f8e6c755f3c663665ff0ce1d\n  conditions:\n  - lastTransitionTime: \"2025-01-13T04:32:48Z\"\n    message: Cluster Is Not Ready\n    reason: ClusterIsNotReady\n    status: \"False\"\n    type: Ready\n  - lastTransitionTime: \"2025-01-09T16:59:59Z\"\n    message: Continuous archiving is working\n    reason: ContinuousArchivingSuccess\n    status: \"True\"\n    type: ContinuousArchiving\n  - lastTransitionTime: \"2025-01-13T00:47:27Z\"\n    message: 'while trying to start the backup: while executing http request: Post\n      \"https://10.130.0.239:8000/pg/mode/backup\": context deadline exceeded (Client.Timeout\n      exceeded while awaiting headers)'\n    reason: LastBackupFailed\n    status: \"False\"\n    type: LastBackupSucceeded\n  configMapResourceVersion:\n    metrics:\n      cnpg-default-monitoring: \"824255109\"\n  currentPrimary: production-pg-etl-1\n  currentPrimaryTimestamp: \"2025-01-09T16:59:55.604522Z\"\n  firstRecoverabilityPoint: \"2025-01-09T17:08:15Z\"\n  firstRecoverabilityPointByMethod:\n    barmanObjectStore: \"2025-01-10T00:00:03Z\"\n    volumeSnapshot: \"2025-01-09T17:08:15Z\"\n  healthyPVC:\n  - production-pg-etl-1\n  - production-pg-etl-1-wal\n  - production-pg-etl-4\n  - production-pg-etl-4-wal\n  - production-pg-etl-5\n  - production-pg-etl-5-wal\n  image: docker.gisual.net/custom-postgres17:17.2@sha256:9d93740c5fcfa7b3113815d6118f8a0afe35dcef38775dae2b93f6d7badedc91\n  instanceNames:\n  - production-pg-etl-1\n  - production-pg-etl-4\n  - production-pg-etl-5\n  instances: 3\n  instancesReportedState:\n    production-pg-etl-1:\n      isPrimary: true\n      timeLineID: 1\n    production-pg-etl-4:\n      isPrimary: false\n    production-pg-etl-5:\n      isPrimary: false\n  instancesStatus:\n    healthy:\n    - production-pg-etl-1\n    replicating:\n    - production-pg-etl-4\n    - production-pg-etl-5\n  lastFailedBackup: \"2025-01-11T01:19:12Z\"\n  lastSuccessfulBackup: \"2025-01-13T00:47:13Z\"\n  lastSuccessfulBackupByMethod:\n    barmanObjectStore: \"2025-01-13T00:47:13Z\"\n    volumeSnapshot: \"2025-01-12T14:08:35Z\"\n  latestGeneratedNode: 5\n  managedRolesStatus: {}\n  phase: Not enough disk space\n  phaseReason: Insufficient disk space detected in one or more pods is preventing\n    PostgreSQL from running.Please verify your storage settings. Further information\n    inside .status.instancesReportedState\n  poolerIntegrations:\n    pgBouncerIntegration: {}\n  pvcCount: 6\n  readService: production-pg-etl-r\n  readyInstances: 1\n  secretsResourceVersion:\n    clientCaSecretVersion: \"882787408\"\n    replicationSecretVersion: \"882787410\"\n    serverCaSecretVersion: \"882787408\"\n    serverSecretVersion: \"882787409\"\n    superuserSecretVersion: \"824019503\"\n  switchReplicaClusterStatus: {}\n  targetPrimary: production-pg-etl-1\n  targetPrimaryTimestamp: \"2025-01-09T16:59:17.884407Z\"\n  timelineID: 1\n  topology:\n    instances:\n      production-pg-etl-1: {}\n      production-pg-etl-4: {}\n      production-pg-etl-5: {}\n    nodesUsed: 3\n    successfullyExtracted: true\n  writeService: production-pg-etl-rw\n```\n### Relevant log output\n```json\n{\"level\":\"info\",\"ts\":\"2025-01-13T20:08:15.41926761Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"35929757-0e5a-4aa3-a21e-2dbaeaaf8cf7\",\"podName\":\"production-pg-common-2\",\"error\":\"Get \\\"https://10.130.18.19:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2025-01-13T20:08:45.697331048Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"89849473-cf26-42ed-9b91-d5dfa7957ea3\",\"podName\":\"production-pg-common-2\",\"error\":\"Get \\\"https://10.130.18.19:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2025-01-13T20:09:15.849085431Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"43c3444a-b204-4a3a-9b2f-bbe3b212e2bb\",\"podName\":\"production-pg-common-2\",\"error\":\"Get \\\"https://10.130.18.19:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2025-01-13T20:09:46.021972798Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"7e42adf8-bea4-4eb3-b48c-1efdb88d0e8a\",\"podName\":\"production-pg-common-2\",\"error\":\"Get \\\"https://10.130.18.19:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2025-01-13T20:10:16.518873375Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"1f0f5b0d-e3be-4a31-87ed-e3b202b94f69\",\"podName\":\"production-pg-common-2\",\"error\":\"Get \\\"https://10.130.18.19:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct@sxd **This issue is also reported here:**\n- https://github.com/cloudnative-pg/cloudnative-pg/issues/3934#issuecomment-2026888483\n- https://github.com/cloudnative-pg/cloudnative-pg/issues/6362\n---\n@sxn: Here's a compact Slack thread with the troubleshooting steps, I put the shell output/logs into a gist per step.\nhttps://cloudnativepg.slack.com/archives/C03AX0J5P29/p1736880191821519\nCluster and operator reports with `--logs` are available upon request.\nFor my records, these are the filenames for those reports:\n1. `~/Desktop/CNPG-ISSUE-6599/report_cluster_staging-pg-common_2025-01-14T19:27:28Z.zip`\n2. `~/Desktop/CNPG-ISSUE-6599/report_operator_2025-01-14T19:27:47Z.zip`\n---\ndiscussed this issue on community call this morning\nFYI - useful link - the readiness probe for the database pod itself is here\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/45a147bafedf3baa61d059dcf9780c5eadc6e03c/pkg/management/postgres/webserver/remote.go#L138-L148\nAlso cf logLevel setting in ClusterSpec; setting this to trace might be worth a try https://cloudnative-pg.io/documentation/current/cloudnative-pg.v1/#postgresql-cnpg-io-v1-ClusterSpec\n---\nI had `cnpg status` in a `watch -n2` window and noticed it stopped moving.\nSure enough, my pod was not ready (but my workloads connected to the primary were still working fine).\nNew connections to the hostname failed (due to service health).\nRequests to `/pg/status` hang indefinitely (no headers/body), however, they log success.\nRequests to `/readyz` hang indefinitely and log a failure.\n```\n\u276f kubectl logs -n db production-pg-etl-1 | grep \"Liveness probe succeeding\" | wc -l\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\n    1081\n\u276f kubectl logs -n db production-pg-etl-1 | grep \"Liveness probe skipped\" | wc -l\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\n       0\n\u276f kubectl logs -n db production-pg-etl-1 | grep \"Liveness probe failing\" | wc -l\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\n       0\n```\nThere is nothing useful in the logs, but, they're available upon request and are in the Slack thread. \n```{\"level\":\"debug\",\"ts\":\"2025-01-15T20:00:15.034837695Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:00:25.03743881Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:00:35.030961507Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:00:40.037390481Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:00:45.032155883Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:00:55.031575414Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:01:05.032427396Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:01:15.031224161Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:01:25.031104644Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:01:35.03214586Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:01:45.03362983Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:01:55.031942625Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:02:00.033012833Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:02:05.033175589Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:02:15.031364855Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:02:25.03752444Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:02:35.037405417Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:02:45.037410177Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:02:55.037431022Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:03:05.031724732Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:03:14.363372006Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:03:19.367475812Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:03:25.032273992Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:03:35.032155229Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:03:45.031510389Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:03:55.031308819Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:04:05.031160571Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:04:15.03156762Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:04:25.031079722Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:04:35.031743948Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:04:44.359343213Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:04:49.360296822Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:04:55.03170307Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:05:05.031730637Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:05:15.031514507Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:05:25.030947405Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:05:35.032350026Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:05:45.031345749Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:05:50.358926959Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:05:55.359474502Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:06:05.03293449Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:06:15.031642903Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:06:25.031927109Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:06:35.037419883Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:06:45.032211746Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:06:55.031513739Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:07:03.359175111Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:07:08.362102669Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:07:15.031852267Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:07:25.031814236Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:07:35.032144708Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:07:45.03151054Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:07:55.030876349Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:08:04.358726824Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:08:09.359726779Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:08:15.031199619Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:08:25.031329375Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:08:35.031881563Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:08:45.031075708Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:08:55.031926537Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:09:05.031223955Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:09:10.361934179Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:09:15.361226417Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:09:25.031889566Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:09:35.032030938Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:09:45.031785781Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:09:55.031453269Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:10:05.032003432Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:10:15.031316203Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:10:20.031744859Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:10:25.037429751Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:10:35.037369099Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:10:45.037470167Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:10:55.032306905Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:11:05.037379235Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:11:15.032656507Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:11:25.031601351Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:11:30.037413671Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:11:35.033591132Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:11:45.037390932Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:11:55.037410169Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:12:05.037363507Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:12:15.037359681Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:12:25.031688017Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:12:35.031254925Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:12:42.358527519Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:12:47.359674843Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:12:55.032202905Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:13:05.03149535Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:13:15.032235356Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:13:25.031271457Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:13:35.031897863Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:13:45.03151068Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:13:55.031513019Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:14:05.031877979Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:14:10.033360354Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:14:15.034697039Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:14:25.031454734Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:14:35.03192997Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:14:45.032103281Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:14:55.032233076Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:15:05.03203773Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:15:15.031857033Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:15:25.032236244Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:15:34.365461317Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:15:39.364187595Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:15:45.031849532Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:15:55.032111007Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:16:05.031441492Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:16:15.032029484Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:16:25.037408699Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:16:35.037791139Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:16:40.030940946Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:16:45.037409703Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:16:55.03074834Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:17:05.03747707Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:17:15.031836577Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:17:25.031607972Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:17:35.030746868Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:17:45.031372018Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:17:55.031865202Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:18:00.358650219Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:18:05.359246046Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:18:15.031533512Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:18:25.032455567Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:18:35.031671546Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:18:45.032219161Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:18:55.03136534Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:19:05.032545287Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:19:15.032264498Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:19:20.032049628Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:19:25.032614437Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:19:35.032011256Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:19:45.031776402Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:19:55.032059483Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:20:05.037408285Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:20:15.037374294Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:20:25.038419226Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:20:35.037417665Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:20:43.359988926Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:20:48.362248694Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:20:55.032816211Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:21:05.037377903Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:21:15.032242904Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:21:25.031586141Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:21:35.032386501Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:21:45.037548331Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:21:55.037402322Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:22:00.362518762Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:22:05.364852089Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:22:15.031767684Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:22:25.031664968Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:22:35.031531982Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:22:45.031454024Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:22:55.031462986Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:23:05.031961826Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:23:15.031245344Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:23:25.030957812Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:23:30.031342943Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:23:35.031974385Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:23:45.031780784Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:23:55.031058394Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:24:05.031514428Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:24:15.031627295Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:24:25.031960573Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:24:33.359007876Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:24:38.359146557Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:24:45.030760157Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:24:55.031501453Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:25:05.037440348Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:25:15.037406198Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:25:25.037438535Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:25:35.037415477Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:25:45.037367748Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:25:52.358830391Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:25:57.359939056Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:26:05.030982324Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:26:15.031653289Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:26:25.037359504Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:26:35.037632119Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:26:45.037410847Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:26:55.037393454Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:27:05.0323387Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:27:10.359736011Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:27:15.359616828Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:27:25.031381647Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:27:35.031649951Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:27:45.037420586Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:27:55.032273203Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:28:05.031839344Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:28:15.032105921Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:28:25.031012917Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:28:30.031939422Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:28:35.03254334Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:28:45.031971739Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:28:55.031192999Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:29:05.03181484Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:29:15.031516035Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:29:25.03249857Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:29:32.358559388Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:29:37.359184898Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:29:45.031361044Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:29:55.031831826Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:30:05.03249024Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:30:15.032220304Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:30:25.032194361Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:30:35.031669596Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:30:40.031727422Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:30:45.032428225Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:30:55.031819433Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:31:05.031965346Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:31:15.03113282Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:31:25.03125714Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:31:35.031209534Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:31:45.032242839Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:31:52.359141509Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:31:57.359809122Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:32:05.031485806Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:32:15.031689408Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:32:25.031626829Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:32:35.037479643Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:32:45.037406099Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:32:55.037874861Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:33:00.031939679Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:33:05.03786376Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:33:15.031704555Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:33:25.031544639Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:33:35.031879493Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:33:45.037549557Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:33:55.037452844Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:34:05.037379541Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:34:15.03735515Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:34:20.037392319Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:34:25.033155091Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:34:35.031225081Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:34:45.037406585Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:34:55.037404888Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:35:05.037418465Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:35:15.031491138Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:35:25.03736958Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:35:35.031754184Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:35:44.359292617Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:35:49.360869653Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:35:55.031120836Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:36:05.031217307Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:36:15.031251744Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:36:25.03110294Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:36:35.031645286Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:36:45.031323386Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:36:50.358712084Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:36:55.360434624Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:37:05.031117399Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:37:15.031563437Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:37:25.032134767Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:37:35.031415869Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:37:45.031170755Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:37:55.032412739Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:38:00.359885343Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:38:05.360664249Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:38:15.031481227Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:38:25.03361774Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:38:35.031451355Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:38:45.031765647Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:38:55.03103573Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:39:05.032282282Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:39:11.359461272Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:39:16.360018668Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:39:25.037451753Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:39:35.038496768Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:39:45.031468323Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:39:55.032820247Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:40:05.032029752Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:40:15.03318786Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:40:20.037418468Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:40:25.037396828Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:40:35.037486932Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:40:45.03737273Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:40:55.037381879Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:41:05.03752342Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:41:15.031997967Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:41:22.362410725Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n{\"level\":\"debug\",\"ts\":\"2025-01-15T20:41:27.363211977Z\",\"msg\":\"Readiness probe failing\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241219102532-2807bc88310d/pkg/log/log.go:178\",\"logging_pod\":\"production-pg-etl-1\",\"err\":\"context canceled\"}\n```\n---\nIs there anything we can do to avoid forcing a failover to remedy this? Can we send a signal inside the pod to restart the web server?\n---\nOne of the root causes have been identified: #6761 \nI haven't closed this issue because there are outstanding defects identified in the probes and connection/context/concurrency handling that need to be addressed."
    },
    {
        "title": "[Bug]: Switching to P384() when generating the keys",
        "id": 2787072767,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nCurrently in the https://github.com/cloudnative-pg/cloudnative-pg/blob/main/pkg/certs/certs.go#L437 we use elliptic.P256() to generate a key, we should change that function and use elliptic.P384() for the keys to be closter to FIPS 140-3\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nCurrently in the https://github.com/cloudnative-pg/cloudnative-pg/blob/main/pkg/certs/certs.go#L437 we use elliptic.P256() to generate a key, we should change that function and use elliptic.P384() for the keys to be closter to FIPS 140-3\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "fix(cnpgi-plugins): add archive and backup capabilities fields to configuration",
        "id": 2784153886,
        "state": "open",
        "first": "",
        "messages": "/test deph=push\n---\n/test depth=push\n---\n/test depth=push\n---\nLGTM"
    },
    {
        "title": "chore(deps): update module sigs.k8s.io/controller-tools to v0.17.1 (main)",
        "id": 2783621640,
        "state": "open",
        "first": "This PR contains the following updates:\n| Package | Change | Age | Adoption | Passing | Confidence |\n|---|---|---|---|---|---|\n| [sigs.k8s.io/controller-tools](https://redirect.github.com/kubernetes-sigs/controller-tools) | `v0.16.5` -> `v0.17.1` | [![age](https://developer.mend.io/api/mc/badges/age/go/sigs.k8s.io%2fcontroller-tools/v0.17.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/go/sigs.k8s.io%2fcontroller-tools/v0.17.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/go/sigs.k8s.io%2fcontroller-tools/v0.16.5/v0.17.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/go/sigs.k8s.io%2fcontroller-tools/v0.16.5/v0.17.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n---\n### Release Notes\n<details>\n<summary>kubernetes-sigs/controller-tools (sigs.k8s.io/controller-tools)</summary>\n### [`v0.17.1`](https://redirect.github.com/kubernetes-sigs/controller-tools/releases/tag/v0.17.1)\n[Compare Source](https://redirect.github.com/kubernetes-sigs/controller-tools/compare/v0.17.0...v0.17.1)\n#### What's Changed\n-   :bug: pkg/crd: fix alias type parsing for struct type alias by [@&#8203;mtardy](https://redirect.github.com/mtardy) in [https://github.com/kubernetes-sigs/controller-tools/pull/1122](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1122)\n#### Dependencies\n-   :seedling: Bump the all-github-actions group across 1 directory with 3 updates by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1125](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1125)\n-   :seedling: Bump golang.org/x/tools from 0.28.0 to 0.29.0 in the all-go-mod-patch-and-minor group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1124](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1124)\n**Full Changelog**: https://github.com/kubernetes-sigs/controller-tools/compare/v0.17.0...v0.17.1\n### [`v0.17.0`](https://redirect.github.com/kubernetes-sigs/controller-tools/releases/tag/v0.17.0)\n[Compare Source](https://redirect.github.com/kubernetes-sigs/controller-tools/compare/v0.16.5...v0.17.0)\n#### What's Changed\n-   \ud83d\udc1b Fix duplicate default value when generating CRDs with corev1.Protocol by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1035](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1035)\n-   \ud83d\udc1b rbac: fix adding nonResourceURLs including normalisation by [@&#8203;chrischdi](https://redirect.github.com/chrischdi) in [https://github.com/kubernetes-sigs/controller-tools/pull/1044](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1044)\n-   \ud83d\udc1b rbac: fix deduplication of core group and add test coverage by [@&#8203;chrischdi](https://redirect.github.com/chrischdi) in [https://github.com/kubernetes-sigs/controller-tools/pull/1045](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1045)\n-   :bug: Allow CLI binaries to set a version by [@&#8203;josvazg](https://redirect.github.com/josvazg) in [https://github.com/kubernetes-sigs/controller-tools/pull/1049](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1049)\n-   \u2728 Allow customizing generated webhook configuration's name by [@&#8203;davidxia](https://redirect.github.com/davidxia) in [https://github.com/kubernetes-sigs/controller-tools/pull/1002](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1002)\n-   \ud83d\udc1b pkg/crd: fix type casting panic with new default `*types.Alias` with Go 1.23 by [@&#8203;mtardy](https://redirect.github.com/mtardy) in [https://github.com/kubernetes-sigs/controller-tools/pull/1061](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1061)\n-   :sparkles: Add selectablefield marker by [@&#8203;everesio](https://redirect.github.com/everesio) in [https://github.com/kubernetes-sigs/controller-tools/pull/1050](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1050)\n-   \ud83d\udc1b pkg/crd: fix a missed type casting panic with new \\*types.Alias by [@&#8203;mtardy](https://redirect.github.com/mtardy) in [https://github.com/kubernetes-sigs/controller-tools/pull/1079](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1079)\n-   \ud83d\udc1b pkg/crd: support validation on type alias to basic types by [@&#8203;mtardy](https://redirect.github.com/mtardy) in [https://github.com/kubernetes-sigs/controller-tools/pull/1078](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1078)\n-   \ud83d\udc1b Fix item validation for unhashable markers by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1080](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1080)\n-   :bug: Handling Identical Kubebuilder Annotations in Different CRs with \\* Verbs by [@&#8203;OdedViner](https://redirect.github.com/OdedViner) in [https://github.com/kubernetes-sigs/controller-tools/pull/1081](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1081)\n-   :warning: Add support for encoding.TextMarshaler by [@&#8203;twz123](https://redirect.github.com/twz123) in [https://github.com/kubernetes-sigs/controller-tools/pull/1015](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1015)\n-   :sparkles: Handle word boundaries and add ellipsis for `MaxDescLen` by [@&#8203;dhaiducek](https://redirect.github.com/dhaiducek) in [https://github.com/kubernetes-sigs/controller-tools/pull/1006](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1006)\n#### Misc\n-   \ud83c\udfc3 Rename default branch to main by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1038](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1038)\n-   \ud83c\udfc3 Verify PR titles with shell script by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1057](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1057)\n-   \ud83d\udcd6 github: update PR template by [@&#8203;chrischdi](https://redirect.github.com/chrischdi) in [https://github.com/kubernetes-sigs/controller-tools/pull/1060](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1060)\n-   \ud83c\udf31 OWNERS: Promote chrischdi to reviewer by [@&#8203;chrischdi](https://redirect.github.com/chrischdi) in [https://github.com/kubernetes-sigs/controller-tools/pull/1059](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1059)\n-   \ud83c\udf31 pr-verify: use env var for passing the PR title by [@&#8203;chrischdi](https://redirect.github.com/chrischdi) in [https://github.com/kubernetes-sigs/controller-tools/pull/1067](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1067)\n-   \ud83d\udcd6 Fix compatibility table by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1093](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1093)\n#### envtest\n-   \ud83d\udc1b Fix envtest build for v1.32.0 by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1110](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1110)\n-   \u2728 Release envtest v1.32.0 by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1106](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1106)\n-   \u2728 Release envtest v1.32.0 (try 2) by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1111](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1111)\n-   \ud83c\udf31 Promotion of envtest release for Kubernetes v1.32.0 by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1114](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1114)\n#### Dependency bumps\n-   :seedling: Bump tj-actions/changed-files from 44.5.7 to 45.0.0 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1042](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1042)\n-   :seedling: Bump github.com/onsi/gomega from 1.34.1 to 1.34.2 in the all-go-mod-patch-and-minor group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1047](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1047)\n-   :seedling: Bump tj-actions/changed-files from 45.0.0 to 45.0.1 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1048](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1048)\n-   :seedling: Bump peter-evans/create-pull-request from 6.1.0 to 7.0.1 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1052](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1052)\n-   :seedling: Bump the all-go-mod-patch-and-minor group with 4 updates by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1055](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1055)\n-   :seedling: Bump peter-evans/create-pull-request from 7.0.1 to 7.0.2 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1056](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1056)\n-   :seedling: Bump github.com/gobuffalo/flect from 1.0.2 to 1.0.3 in the all-go-mod-patch-and-minor group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1065](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1065)\n-   :seedling: Bump the all-github-actions group with 2 updates by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1066](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1066)\n-   :seedling: Bump actions/checkout from 4.1.7 to 4.2.0 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1070](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1070)\n-   :seedling: Bump golang.org/x/tools from 0.25.0 to 0.26.0 in the all-go-mod-patch-and-minor group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1072](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1072)\n-   :seedling: Bump the all-github-actions group with 2 updates by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1073](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1073)\n-   :seedling: Bump actions/checkout from 4.2.0 to 4.2.1 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1075](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1075)\n-   :seedling: Bump the all-go-mod-patch-and-minor group with 4 updates by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1082](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1082)\n-   :seedling: Bump the all-github-actions group with 2 updates by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1083](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1083)\n-   :seedling: Bump github.com/onsi/gomega from 1.34.2 to 1.35.1 in the all-go-mod-patch-and-minor group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1085](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1085)\n-   :seedling: Bump softprops/action-gh-release from 2.0.8 to 2.0.9 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1086](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1086)\n-   :seedling: Bump golang.org/x/tools from 0.26.0 to 0.27.0 in the all-go-mod-patch-and-minor group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1091](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1091)\n-   :seedling: Bump tj-actions/changed-files from 45.0.3 to 45.0.4 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1092](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1092)\n-   :seedling: Bump softprops/action-gh-release from 2.0.9 to 2.1.0 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1095](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1095)\n-   :seedling: Bump the all-go-mod-patch-and-minor group across 1 directory with 5 updates by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1103](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1103)\n-   :seedling: Bump the all-github-actions group across 1 directory with 3 updates by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1104](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1104)\n-   :seedling: Bump github.com/onsi/gomega from 1.36.1 to 1.36.2 in the all-go-mod-patch-and-minor group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1117](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1117)\n-   :seedling: Bump k8s.io/\\* to v0.32.0 by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1115](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1115)\n-   :seedling: Downgrade gh-release action by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1112](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1112)\n#### New Contributors\n-   [@&#8203;josvazg](https://redirect.github.com/josvazg) made their first contribution in [https://github.com/kubernetes-sigs/controller-tools/pull/1049](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1049)\n-   [@&#8203;davidxia](https://redirect.github.com/davidxia) made their first contribution in [https://github.com/kubernetes-sigs/controller-tools/pull/1002](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1002)\n-   [@&#8203;mtardy](https://redirect.github.com/mtardy) made their first contribution in [https://github.com/kubernetes-sigs/controller-tools/pull/1061](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1061)\n-   [@&#8203;everesio](https://redirect.github.com/everesio) made their first contribution in [https://github.com/kubernetes-sigs/controller-tools/pull/1050](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1050)\n-   [@&#8203;OdedViner](https://redirect.github.com/OdedViner) made their first contribution in [https://github.com/kubernetes-sigs/controller-tools/pull/1081](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1081)\n-   [@&#8203;dhaiducek](https://redirect.github.com/dhaiducek) made their first contribution in [https://github.com/kubernetes-sigs/controller-tools/pull/1006](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1006)\n**Full Changelog**: https://github.com/kubernetes-sigs/controller-tools/compare/v0.16.0...v0.17.0\n</details>\n---\n### Configuration\n\ud83d\udcc5 **Schedule**: Branch creation - At any time (no schedule defined), Automerge - At any time (no schedule defined).\n\ud83d\udea6 **Automerge**: Disabled by config. Please merge this manually once you are satisfied.\n\u267b **Rebasing**: Never, or you tick the rebase/retry checkbox.\n\ud83d\udd15 **Ignore**: Close this PR and you won't be reminded about this update again.\n---\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n---\nThis PR was generated by [Mend Renovate](https://mend.io/renovate/). View the [repository job log](https://developer.mend.io/github/cloudnative-pg/cloudnative-pg).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzOS45Mi4wIiwidXBkYXRlZEluVmVyIjoiMzkuMTI1LjEiLCJ0YXJnZXRCcmFuY2giOiJtYWluIiwibGFiZWxzIjpbImF1dG9tYXRlZCIsImRvIG5vdCBiYWNrcG9ydCIsIm5vLWlzc3VlIl19-->",
        "messages": "This PR contains the following updates:\n| Package | Change | Age | Adoption | Passing | Confidence |\n|---|---|---|---|---|---|\n| [sigs.k8s.io/controller-tools](https://redirect.github.com/kubernetes-sigs/controller-tools) | `v0.16.5` -> `v0.17.1` | [![age](https://developer.mend.io/api/mc/badges/age/go/sigs.k8s.io%2fcontroller-tools/v0.17.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/go/sigs.k8s.io%2fcontroller-tools/v0.17.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/go/sigs.k8s.io%2fcontroller-tools/v0.16.5/v0.17.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/go/sigs.k8s.io%2fcontroller-tools/v0.16.5/v0.17.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n---\n### Release Notes\n<details>\n<summary>kubernetes-sigs/controller-tools (sigs.k8s.io/controller-tools)</summary>\n### [`v0.17.1`](https://redirect.github.com/kubernetes-sigs/controller-tools/releases/tag/v0.17.1)\n[Compare Source](https://redirect.github.com/kubernetes-sigs/controller-tools/compare/v0.17.0...v0.17.1)\n#### What's Changed\n-   :bug: pkg/crd: fix alias type parsing for struct type alias by [@&#8203;mtardy](https://redirect.github.com/mtardy) in [https://github.com/kubernetes-sigs/controller-tools/pull/1122](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1122)\n#### Dependencies\n-   :seedling: Bump the all-github-actions group across 1 directory with 3 updates by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1125](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1125)\n-   :seedling: Bump golang.org/x/tools from 0.28.0 to 0.29.0 in the all-go-mod-patch-and-minor group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1124](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1124)\n**Full Changelog**: https://github.com/kubernetes-sigs/controller-tools/compare/v0.17.0...v0.17.1\n### [`v0.17.0`](https://redirect.github.com/kubernetes-sigs/controller-tools/releases/tag/v0.17.0)\n[Compare Source](https://redirect.github.com/kubernetes-sigs/controller-tools/compare/v0.16.5...v0.17.0)\n#### What's Changed\n-   \ud83d\udc1b Fix duplicate default value when generating CRDs with corev1.Protocol by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1035](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1035)\n-   \ud83d\udc1b rbac: fix adding nonResourceURLs including normalisation by [@&#8203;chrischdi](https://redirect.github.com/chrischdi) in [https://github.com/kubernetes-sigs/controller-tools/pull/1044](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1044)\n-   \ud83d\udc1b rbac: fix deduplication of core group and add test coverage by [@&#8203;chrischdi](https://redirect.github.com/chrischdi) in [https://github.com/kubernetes-sigs/controller-tools/pull/1045](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1045)\n-   :bug: Allow CLI binaries to set a version by [@&#8203;josvazg](https://redirect.github.com/josvazg) in [https://github.com/kubernetes-sigs/controller-tools/pull/1049](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1049)\n-   \u2728 Allow customizing generated webhook configuration's name by [@&#8203;davidxia](https://redirect.github.com/davidxia) in [https://github.com/kubernetes-sigs/controller-tools/pull/1002](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1002)\n-   \ud83d\udc1b pkg/crd: fix type casting panic with new default `*types.Alias` with Go 1.23 by [@&#8203;mtardy](https://redirect.github.com/mtardy) in [https://github.com/kubernetes-sigs/controller-tools/pull/1061](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1061)\n-   :sparkles: Add selectablefield marker by [@&#8203;everesio](https://redirect.github.com/everesio) in [https://github.com/kubernetes-sigs/controller-tools/pull/1050](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1050)\n-   \ud83d\udc1b pkg/crd: fix a missed type casting panic with new \\*types.Alias by [@&#8203;mtardy](https://redirect.github.com/mtardy) in [https://github.com/kubernetes-sigs/controller-tools/pull/1079](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1079)\n-   \ud83d\udc1b pkg/crd: support validation on type alias to basic types by [@&#8203;mtardy](https://redirect.github.com/mtardy) in [https://github.com/kubernetes-sigs/controller-tools/pull/1078](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1078)\n-   \ud83d\udc1b Fix item validation for unhashable markers by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1080](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1080)\n-   :bug: Handling Identical Kubebuilder Annotations in Different CRs with \\* Verbs by [@&#8203;OdedViner](https://redirect.github.com/OdedViner) in [https://github.com/kubernetes-sigs/controller-tools/pull/1081](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1081)\n-   :warning: Add support for encoding.TextMarshaler by [@&#8203;twz123](https://redirect.github.com/twz123) in [https://github.com/kubernetes-sigs/controller-tools/pull/1015](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1015)\n-   :sparkles: Handle word boundaries and add ellipsis for `MaxDescLen` by [@&#8203;dhaiducek](https://redirect.github.com/dhaiducek) in [https://github.com/kubernetes-sigs/controller-tools/pull/1006](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1006)\n#### Misc\n-   \ud83c\udfc3 Rename default branch to main by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1038](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1038)\n-   \ud83c\udfc3 Verify PR titles with shell script by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1057](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1057)\n-   \ud83d\udcd6 github: update PR template by [@&#8203;chrischdi](https://redirect.github.com/chrischdi) in [https://github.com/kubernetes-sigs/controller-tools/pull/1060](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1060)\n-   \ud83c\udf31 OWNERS: Promote chrischdi to reviewer by [@&#8203;chrischdi](https://redirect.github.com/chrischdi) in [https://github.com/kubernetes-sigs/controller-tools/pull/1059](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1059)\n-   \ud83c\udf31 pr-verify: use env var for passing the PR title by [@&#8203;chrischdi](https://redirect.github.com/chrischdi) in [https://github.com/kubernetes-sigs/controller-tools/pull/1067](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1067)\n-   \ud83d\udcd6 Fix compatibility table by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1093](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1093)\n#### envtest\n-   \ud83d\udc1b Fix envtest build for v1.32.0 by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1110](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1110)\n-   \u2728 Release envtest v1.32.0 by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1106](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1106)\n-   \u2728 Release envtest v1.32.0 (try 2) by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1111](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1111)\n-   \ud83c\udf31 Promotion of envtest release for Kubernetes v1.32.0 by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1114](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1114)\n#### Dependency bumps\n-   :seedling: Bump tj-actions/changed-files from 44.5.7 to 45.0.0 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1042](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1042)\n-   :seedling: Bump github.com/onsi/gomega from 1.34.1 to 1.34.2 in the all-go-mod-patch-and-minor group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1047](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1047)\n-   :seedling: Bump tj-actions/changed-files from 45.0.0 to 45.0.1 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1048](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1048)\n-   :seedling: Bump peter-evans/create-pull-request from 6.1.0 to 7.0.1 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1052](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1052)\n-   :seedling: Bump the all-go-mod-patch-and-minor group with 4 updates by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1055](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1055)\n-   :seedling: Bump peter-evans/create-pull-request from 7.0.1 to 7.0.2 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1056](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1056)\n-   :seedling: Bump github.com/gobuffalo/flect from 1.0.2 to 1.0.3 in the all-go-mod-patch-and-minor group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1065](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1065)\n-   :seedling: Bump the all-github-actions group with 2 updates by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1066](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1066)\n-   :seedling: Bump actions/checkout from 4.1.7 to 4.2.0 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1070](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1070)\n-   :seedling: Bump golang.org/x/tools from 0.25.0 to 0.26.0 in the all-go-mod-patch-and-minor group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1072](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1072)\n-   :seedling: Bump the all-github-actions group with 2 updates by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1073](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1073)\n-   :seedling: Bump actions/checkout from 4.2.0 to 4.2.1 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1075](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1075)\n-   :seedling: Bump the all-go-mod-patch-and-minor group with 4 updates by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1082](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1082)\n-   :seedling: Bump the all-github-actions group with 2 updates by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1083](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1083)\n-   :seedling: Bump github.com/onsi/gomega from 1.34.2 to 1.35.1 in the all-go-mod-patch-and-minor group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1085](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1085)\n-   :seedling: Bump softprops/action-gh-release from 2.0.8 to 2.0.9 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1086](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1086)\n-   :seedling: Bump golang.org/x/tools from 0.26.0 to 0.27.0 in the all-go-mod-patch-and-minor group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1091](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1091)\n-   :seedling: Bump tj-actions/changed-files from 45.0.3 to 45.0.4 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1092](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1092)\n-   :seedling: Bump softprops/action-gh-release from 2.0.9 to 2.1.0 in the all-github-actions group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1095](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1095)\n-   :seedling: Bump the all-go-mod-patch-and-minor group across 1 directory with 5 updates by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1103](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1103)\n-   :seedling: Bump the all-github-actions group across 1 directory with 3 updates by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1104](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1104)\n-   :seedling: Bump github.com/onsi/gomega from 1.36.1 to 1.36.2 in the all-go-mod-patch-and-minor group by [@&#8203;dependabot](https://redirect.github.com/dependabot) in [https://github.com/kubernetes-sigs/controller-tools/pull/1117](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1117)\n-   :seedling: Bump k8s.io/\\* to v0.32.0 by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1115](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1115)\n-   :seedling: Downgrade gh-release action by [@&#8203;sbueringer](https://redirect.github.com/sbueringer) in [https://github.com/kubernetes-sigs/controller-tools/pull/1112](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1112)\n#### New Contributors\n-   [@&#8203;josvazg](https://redirect.github.com/josvazg) made their first contribution in [https://github.com/kubernetes-sigs/controller-tools/pull/1049](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1049)\n-   [@&#8203;davidxia](https://redirect.github.com/davidxia) made their first contribution in [https://github.com/kubernetes-sigs/controller-tools/pull/1002](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1002)\n-   [@&#8203;mtardy](https://redirect.github.com/mtardy) made their first contribution in [https://github.com/kubernetes-sigs/controller-tools/pull/1061](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1061)\n-   [@&#8203;everesio](https://redirect.github.com/everesio) made their first contribution in [https://github.com/kubernetes-sigs/controller-tools/pull/1050](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1050)\n-   [@&#8203;OdedViner](https://redirect.github.com/OdedViner) made their first contribution in [https://github.com/kubernetes-sigs/controller-tools/pull/1081](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1081)\n-   [@&#8203;dhaiducek](https://redirect.github.com/dhaiducek) made their first contribution in [https://github.com/kubernetes-sigs/controller-tools/pull/1006](https://redirect.github.com/kubernetes-sigs/controller-tools/pull/1006)\n**Full Changelog**: https://github.com/kubernetes-sigs/controller-tools/compare/v0.16.0...v0.17.0\n</details>\n---\n### Configuration\n\ud83d\udcc5 **Schedule**: Branch creation - At any time (no schedule defined), Automerge - At any time (no schedule defined).\n\ud83d\udea6 **Automerge**: Disabled by config. Please merge this manually once you are satisfied.\n\u267b **Rebasing**: Never, or you tick the rebase/retry checkbox.\n\ud83d\udd15 **Ignore**: Close this PR and you won't be reminded about this update again.\n---\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n---\nThis PR was generated by [Mend Renovate](https://mend.io/renovate/). View the [repository job log](https://developer.mend.io/github/cloudnative-pg/cloudnative-pg).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzOS45Mi4wIiwidXBkYXRlZEluVmVyIjoiMzkuMTI1LjEiLCJ0YXJnZXRCcmFuY2giOiJtYWluIiwibGFiZWxzIjpbImF1dG9tYXRlZCIsImRvIG5vdCBiYWNrcG9ydCIsIm5vLWlzc3VlIl19-->"
    },
    {
        "title": "[Bug]: Remove the fixed usage of MinIO in the E2E tests",
        "id": 2781798413,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nCurrently, every time we trigger the backups, we create a new deployment of MinIO even sometimes when this is not needed using resources and consuming up to 5 minutes for that, we should create it on demand.\nThe proposal is to create in the package minio package the proper functions to request and create all the needed resources in the requested namespace. Also, the package should query if the central minio deployment is already running, if that's the case just return with the already running deployment, if that's not the case, should wait for the deployment to be ready or create it and then wait.\nAll this should include a new set of functions to work with the ObjectStore and check for the content, properties, etc, stuff we already do, also it should be able to work with any object store, for this, we should create a new package in the e2e tests, this will help to remove the current minio client pod too, and improve the checks for other object stores. \n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nCurrently, every time we trigger the backups, we create a new deployment of MinIO even sometimes when this is not needed using resources and consuming up to 5 minutes for that, we should create it on demand.\nThe proposal is to create in the package minio package the proper functions to request and create all the needed resources in the requested namespace. Also, the package should query if the central minio deployment is already running, if that's the case just return with the already running deployment, if that's not the case, should wait for the deployment to be ready or create it and then wait.\nAll this should include a new set of functions to work with the ObjectStore and check for the content, properties, etc, stuff we already do, also it should be able to work with any object store, for this, we should create a new package in the e2e tests, this will help to remove the current minio client pod too, and improve the checks for other object stores. \n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: final replica instance never finishes initializing",
        "id": 2781178124,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\npete.grace@gmail.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nI installed cloudnative-pg via helm chart, deploying chart version 0.23.0.  I created a basic cluster resource and waited for it to finish provisioning.  The final instance of the cluster never comes online, just continuously re-creates, terminates, then re-creates again.  The other two instances seem to have come up fine.  I removed the cluster and pvcs and tried a second time and the same result occurred.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"metadata\":{\"annotations\":{},\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"spec\":{\"instances\":3,\"monitoring\":{\"enablePodMonitor\":true},\"storage\":{\"size\":\"100Gi\"}}}\n  creationTimestamp: \"2025-01-10T17:31:24Z\"\n  generation: 1\n  name: paperless-ngx\n  namespace: paperless-ngx\n  resourceVersion: \"185987761\"\n  uid: 628a18ec-74e7-4834-b943-d8b2015f2cbd\nspec:\n  affinity:\n    podAntiAffinityType: preferred\n  bootstrap:\n    initdb:\n      database: app\n      encoding: UTF8\n      localeCType: C\n      localeCollate: C\n      owner: app\n  enablePDB: true\n  enableSuperuserAccess: false\n  failoverDelay: 0\n  imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n  instances: 3\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n    - key: queries\n      name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: true\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: \"on\"\n      archive_timeout: 5min\n      dynamic_shared_memory_type: posix\n      full_page_writes: \"on\"\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: \"0\"\n      log_rotation_size: \"0\"\n      log_truncate_on_rotation: \"false\"\n      logging_collector: \"on\"\n      max_parallel_workers: \"32\"\n      max_replication_slots: \"32\"\n      max_worker_processes: \"32\"\n      shared_memory_type: mmap\n      shared_preload_libraries: \"\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      wal_keep_size: 512MB\n      wal_level: logical\n      wal_log_hints: \"on\"\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: restart\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  resources: {}\n  smartShutdownTimeout: 180\n  startDelay: 3600\n  stopDelay: 1800\n  storage:\n    resizeInUseVolumes: true\n    size: 100Gi\n  switchoverDelay: 3600\nstatus:\n  availableArchitectures:\n  - goArch: amd64\n    hash: 58242fc95faa81d6bba0ba19ea959b82d9b49dc5f8e6c755f3c663665ff0ce1d\n  - goArch: arm64\n    hash: 8e5ef490a05bbd5700047edda5e2440acd14bafa256dbfcc7d4a5107573df75b\n  certificates:\n    clientCASecret: paperless-ngx-ca\n    expirations:\n      paperless-ngx-ca: 2025-04-10 17:26:24 +0000 UTC\n      paperless-ngx-replication: 2025-04-10 17:26:24 +0000 UTC\n      paperless-ngx-server: 2025-04-10 17:26:24 +0000 UTC\n    replicationTLSSecret: paperless-ngx-replication\n    serverAltDNSNames:\n    - paperless-ngx-rw\n    - paperless-ngx-rw.paperless-ngx\n    - paperless-ngx-rw.paperless-ngx.svc\n    - paperless-ngx-rw.paperless-ngx.svc.cluster.local\n    - paperless-ngx-r\n    - paperless-ngx-r.paperless-ngx\n    - paperless-ngx-r.paperless-ngx.svc\n    - paperless-ngx-r.paperless-ngx.svc.cluster.local\n    - paperless-ngx-ro\n    - paperless-ngx-ro.paperless-ngx\n    - paperless-ngx-ro.paperless-ngx.svc\n    - paperless-ngx-ro.paperless-ngx.svc.cluster.local\n    serverCASecret: paperless-ngx-ca\n    serverTLSSecret: paperless-ngx-server\n  cloudNativePGCommitHash: bad5a251\n  cloudNativePGOperatorHash: 58242fc95faa81d6bba0ba19ea959b82d9b49dc5f8e6c755f3c663665ff0ce1d\n  conditions:\n  - lastTransitionTime: \"2025-01-10T17:31:25Z\"\n    message: Cluster Is Not Ready\n    reason: ClusterIsNotReady\n    status: \"False\"\n    type: Ready\n  - lastTransitionTime: \"2025-01-10T17:54:14Z\"\n    message: Continuous archiving is working\n    reason: ContinuousArchivingSuccess\n    status: \"True\"\n    type: ContinuousArchiving\n  configMapResourceVersion:\n    metrics:\n      cnpg-default-monitoring: \"185778772\"\n  currentPrimary: paperless-ngx-1\n  currentPrimaryTimestamp: \"2025-01-10T17:54:14.264198Z\"\n  healthyPVC:\n  - paperless-ngx-1\n  - paperless-ngx-2\n  - paperless-ngx-3\n  image: ghcr.io/cloudnative-pg/postgresql:17.2\n  instanceNames:\n  - paperless-ngx-1\n  - paperless-ngx-2\n  - paperless-ngx-3\n  instances: 3\n  instancesReportedState:\n    paperless-ngx-1:\n      isPrimary: true\n      timeLineID: 1\n    paperless-ngx-2:\n      isPrimary: false\n      timeLineID: 1\n  instancesStatus:\n    failed:\n    - paperless-ngx-3\n    healthy:\n    - paperless-ngx-1\n    - paperless-ngx-2\n  jobCount: 3\n  latestGeneratedNode: 3\n  managedRolesStatus: {}\n  phase: Waiting for the instances to become active\n  phaseReason: Some instances are not yet active. Please wait.\n  poolerIntegrations:\n    pgBouncerIntegration: {}\n  pvcCount: 3\n  readService: paperless-ngx-r\n  readyInstances: 2\n  secretsResourceVersion:\n    applicationSecretVersion: \"185778742\"\n    clientCaSecretVersion: \"185778738\"\n    replicationSecretVersion: \"185778740\"\n    serverCaSecretVersion: \"185778738\"\n    serverSecretVersion: \"185778739\"\n  switchReplicaClusterStatus: {}\n  targetPrimary: paperless-ngx-1\n  targetPrimaryTimestamp: \"2025-01-10T17:31:25.301870Z\"\n  timelineID: 1\n  topology:\n    instances:\n      paperless-ngx-1: {}\n      paperless-ngx-2: {}\n      paperless-ngx-3: {}\n    nodesUsed: 3\n    successfullyExtracted: true\n  writeService: paperless-ngx-rw\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:45.987104003Z\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"version\":\"1.25.0\",\"build\":{\"Version\":\"1.25.0\",\"Commit\":\"bad5a251\",\"Date\":\"2024-12-23\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:45.987536749Z\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.065481472Z\",\"msg\":\"starting tablespace manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.065543385Z\",\"msg\":\"starting external server manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.065575964Z\",\"msg\":\"starting controller-runtime manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.066152379Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-publication\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Publication\",\"source\":\"kind source: *v1.Publication\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.06622348Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-publication\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Publication\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.06653679Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-database\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Database\",\"source\":\"kind source: *v1.Database\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.066584824Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-database\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Database\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.06666119Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.066684424Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.066787797Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-subscription\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Subscription\",\"source\":\"kind source: *v1.Subscription\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.066830684Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-subscription\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Subscription\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.06665109Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.066897063Z\",\"logger\":\"roles_reconciler\",\"msg\":\"skipping the RoleSynchronizer in replicas\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.066963348Z\",\"msg\":\"Starting webserver\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"address\":\":8000\",\"hasTLS\":true}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.067013399Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.067384967Z\",\"msg\":\"Starting webserver\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"address\":\":9187\",\"hasTLS\":false}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.066838812Z\",\"msg\":\"Starting webserver\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"address\":\"localhost:8010\",\"hasTLS\":false}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.067547401Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.067596171Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.06768998Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.067727404Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.167664225Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.167777917Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.167726224Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-publication\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Publication\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.168121839Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-subscription\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Subscription\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.168199718Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.168228278Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-database\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Database\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.196734272Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"namespace\":\"paperless-ngx\",\"name\":\"paperless-ngx\",\"reconcileID\":\"b8f4c14a-fbd9-4c02-877e-1f2dc8665533\",\"logging_pod\":\"paperless-ngx-3\",\"filename\":\"/controller/certificates/server.crt\",\"secret\":\"paperless-ngx-server\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.201026437Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"namespace\":\"paperless-ngx\",\"name\":\"paperless-ngx\",\"reconcileID\":\"b8f4c14a-fbd9-4c02-877e-1f2dc8665533\",\"logging_pod\":\"paperless-ngx-3\",\"filename\":\"/controller/certificates/server.key\",\"secret\":\"paperless-ngx-server\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.221886591Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"namespace\":\"paperless-ngx\",\"name\":\"paperless-ngx\",\"reconcileID\":\"b8f4c14a-fbd9-4c02-877e-1f2dc8665533\",\"logging_pod\":\"paperless-ngx-3\",\"filename\":\"/controller/certificates/streaming_replica.crt\",\"secret\":\"paperless-ngx-replication\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.225114Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"namespace\":\"paperless-ngx\",\"name\":\"paperless-ngx\",\"reconcileID\":\"b8f4c14a-fbd9-4c02-877e-1f2dc8665533\",\"logging_pod\":\"paperless-ngx-3\",\"filename\":\"/controller/certificates/streaming_replica.key\",\"secret\":\"paperless-ngx-replication\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.237502351Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"namespace\":\"paperless-ngx\",\"name\":\"paperless-ngx\",\"reconcileID\":\"b8f4c14a-fbd9-4c02-877e-1f2dc8665533\",\"logging_pod\":\"paperless-ngx-3\",\"filename\":\"/controller/certificates/client-ca.crt\",\"secret\":\"paperless-ngx-ca\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.2455453Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"namespace\":\"paperless-ngx\",\"name\":\"paperless-ngx\",\"reconcileID\":\"b8f4c14a-fbd9-4c02-877e-1f2dc8665533\",\"logging_pod\":\"paperless-ngx-3\",\"filename\":\"/controller/certificates/server-ca.crt\",\"secret\":\"paperless-ngx-ca\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.248559355Z\",\"msg\":\"Found previous run flag\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"namespace\":\"paperless-ngx\",\"name\":\"paperless-ngx\",\"reconcileID\":\"b8f4c14a-fbd9-4c02-877e-1f2dc8665533\",\"logging_pod\":\"paperless-ngx-3\",\"filename\":\"/var/lib/postgresql/data/pgdata/cnpg_initialized-paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.257291745Z\",\"msg\":\"Extracting pg_controldata information\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"reason\":\"postmaster start up\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.262832513Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1700\\nCatalog version number:               202406281\\nDatabase system identifier:           7458346455658311705\\nDatabase cluster state:               shut down in recovery\\npg_control last modified:             Fri 10 Jan 2025 09:02:02 PM UTC\\nLatest checkpoint location:           0/80000B8\\nLatest checkpoint's REDO location:    0/8000060\\nLatest checkpoint's REDO WAL file:    000000010000000000000008\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:753\\nLatest checkpoint's NextOID:          24578\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        730\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  753\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Fri 10 Jan 2025 06:55:05 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/9000060\\nMin recovery ending loc's timeline:   1\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            9ee2f95bedfe4697fa66d9b663257274a678cc678bceac7aa65dbf4186a23934\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.264809439Z\",\"msg\":\"postmaster started\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"postMasterPID\":25}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.338759778Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"namespace\":\"paperless-ngx\",\"name\":\"paperless-ngx\",\"reconcileID\":\"b8f4c14a-fbd9-4c02-877e-1f2dc8665533\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.360190563Z\",\"logger\":\"postgres\",\"msg\":\"2025-01-10 21:02:46.359 UTC [25] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.360250781Z\",\"logger\":\"postgres\",\"msg\":\"2025-01-10 21:02:46.359 UTC [25] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.361146991Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.360 UTC\",\"process_id\":\"25\",\"session_id\":\"67818af6.19\",\"session_line_num\":\"1\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.361480826Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.360 UTC\",\"process_id\":\"25\",\"session_id\":\"67818af6.19\",\"session_line_num\":\"2\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 17.2 (Debian 17.2-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.361512848Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.361 UTC\",\"process_id\":\"25\",\"session_id\":\"67818af6.19\",\"session_line_num\":\"3\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.361542831Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.361 UTC\",\"process_id\":\"25\",\"session_id\":\"67818af6.19\",\"session_line_num\":\"4\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.361604877Z\",\"logger\":\"postgres\",\"msg\":\"2025-01-10 21:02:46.360 UTC [25] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.361646102Z\",\"logger\":\"postgres\",\"msg\":\"2025-01-10 21:02:46.360 UTC [25] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.370398187Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.370 UTC\",\"process_id\":\"25\",\"session_id\":\"67818af6.19\",\"session_line_num\":\"5\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.381500758Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.381 UTC\",\"process_id\":\"29\",\"session_id\":\"67818af6.1d\",\"session_line_num\":\"1\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was shut down in recovery at 2025-01-10 21:02:02 UTC\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.76358874Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.762 UTC\",\"process_id\":\"29\",\"session_id\":\"67818af6.1d\",\"session_line_num\":\"2\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"entering standby mode\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.774625742Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.774 UTC\",\"process_id\":\"29\",\"session_id\":\"67818af6.1d\",\"session_line_num\":\"3\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"virtual_transaction_id\":\"148/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"redo starts at 0/8000060\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.965826934Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.965 UTC\",\"process_id\":\"29\",\"session_id\":\"67818af6.1d\",\"session_line_num\":\"4\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"virtual_transaction_id\":\"148/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"consistent recovery state reached at 0/9000060\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.965935763Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.965 UTC\",\"process_id\":\"29\",\"session_id\":\"67818af6.1d\",\"session_line_num\":\"5\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"virtual_transaction_id\":\"148/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"invalid record length at 0/9000060: expected at least 24, got 0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.965996861Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.965 UTC\",\"process_id\":\"25\",\"session_id\":\"67818af6.19\",\"session_line_num\":\"6\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is ready to accept read-only connections\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.992991261Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.992 UTC\",\"process_id\":\"60\",\"session_id\":\"67818af6.3c\",\"session_line_num\":\"1\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"started streaming WAL from primary at 0/9000000 on timeline 1\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.771030673Z\",\"msg\":\"Received termination signal\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"signal\":\"interrupt\",\"smartShutdownTimeout\":180}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.771187184Z\",\"msg\":\"Requesting smart shutdown of the PostgreSQL instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.774175928Z\",\"logger\":\"pg_ctl\",\"msg\":\"pg_ctl: server is running (PID: 25)\\n/usr/lib/postgresql/17/bin/postgres \\\"-D\\\" \\\"/var/lib/postgresql/data/pgdata\\\"\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.77421282Z\",\"msg\":\"Shutting down instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"mode\":\"smart\",\"timeout\":180}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.776541558Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:56.776 UTC\",\"process_id\":\"25\",\"session_id\":\"67818af6.19\",\"session_line_num\":\"7\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"received smart shutdown request\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.781061017Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:56.779 UTC\",\"process_id\":\"60\",\"session_id\":\"67818af6.3c\",\"session_line_num\":\"2\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P01\",\"message\":\"terminating walreceiver process due to administrator command\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.784356103Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:56.784 UTC\",\"process_id\":\"27\",\"session_id\":\"67818af6.1b\",\"session_line_num\":\"1\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"shutting down\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.79290641Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:56.792 UTC\",\"process_id\":\"25\",\"session_id\":\"67818af6.19\",\"session_line_num\":\"8\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.796720619Z\",\"msg\":\"postmaster exited\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"postmasterExitStatus\":null,\"postMasterPID\":25}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.79676593Z\",\"msg\":\"Extracting pg_controldata information\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"reason\":\"postmaster has exited\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.799319498Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1700\\nCatalog version number:               202406281\\nDatabase system identifier:           7458346455658311705\\nDatabase cluster state:               shut down in recovery\\npg_control last modified:             Fri 10 Jan 2025 09:02:56 PM UTC\\nLatest checkpoint location:           0/80000B8\\nLatest checkpoint's REDO location:    0/8000060\\nLatest checkpoint's REDO WAL file:    000000010000000000000008\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:753\\nLatest checkpoint's NextOID:          24578\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        730\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  753\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Fri 10 Jan 2025 06:55:05 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/9000060\\nMin recovery ending loc's timeline:   1\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            9ee2f95bedfe4697fa66d9b663257274a678cc678bceac7aa65dbf4186a23934\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.876734359Z\",\"logger\":\"pg_ctl\",\"msg\":\"waiting for server to shut down.... done\",\"pipe\":\"stdout\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.876799228Z\",\"logger\":\"pg_ctl\",\"msg\":\"server stopped\",\"pipe\":\"stdout\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.87702681Z\",\"msg\":\"PostgreSQL instance shut down\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877115642Z\",\"msg\":\"Stopping and waiting for non leader election runnables\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877143381Z\",\"msg\":\"Stopping and waiting for leader election runnables\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877186691Z\",\"logger\":\"Replicator\",\"msg\":\"Terminated slot Replicator loop\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877283793Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877247174Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-database\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Database\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877307729Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877362593Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-publication\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Publication\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877370731Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877392132Z\",\"msg\":\"All workers finished\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-publication\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Publication\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877375651Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-subscription\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Subscription\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877406677Z\",\"msg\":\"All workers finished\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-subscription\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Subscription\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877411729Z\",\"msg\":\"Exited log pipe\",\"logger\":\"instance-manager\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877418873Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877428816Z\",\"msg\":\"All workers finished\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877438976Z\",\"msg\":\"All workers finished\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877444849Z\",\"msg\":\"All workers finished\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877463801Z\",\"msg\":\"Webserver exited\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"address\":\":9187\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877464244Z\",\"msg\":\"Exited log pipe\",\"logger\":\"instance-manager\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877484197Z\",\"msg\":\"All workers finished\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-database\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Database\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877483708Z\",\"msg\":\"Exited log pipe\",\"logger\":\"instance-manager\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877486747Z\",\"msg\":\"Webserver exited\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"address\":\"localhost:8010\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.878235569Z\",\"msg\":\"Webserver exited\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"address\":\":8000\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.878269766Z\",\"msg\":\"Stopping and waiting for caches\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.878630913Z\",\"msg\":\"pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251: watch of *v1.Cluster ended with: an error on the server (\\\"unable to decode an event from the watch stream: context canceled\\\") has prevented the request from succeeding\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.878972516Z\",\"msg\":\"pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251: watch of *v1.Subscription ended with: an error on the server (\\\"unable to decode an event from the watch stream: context canceled\\\") has prevented the request from succeeding\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.879033543Z\",\"msg\":\"Stopping and waiting for webhooks\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.879058724Z\",\"msg\":\"Stopping and waiting for HTTP servers\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.879068165Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.879083492Z\",\"msg\":\"Checking for free disk space for WALs after PostgreSQL finished\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.879232868Z\",\"msg\":\"pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251: watch of *v1.Publication ended with: an error on the server (\\\"unable to decode an event from the watch stream: context canceled\\\") has prevented the request from succeeding\"}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\npete.grace@gmail.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nI installed cloudnative-pg via helm chart, deploying chart version 0.23.0.  I created a basic cluster resource and waited for it to finish provisioning.  The final instance of the cluster never comes online, just continuously re-creates, terminates, then re-creates again.  The other two instances seem to have come up fine.  I removed the cluster and pvcs and tried a second time and the same result occurred.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"metadata\":{\"annotations\":{},\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"spec\":{\"instances\":3,\"monitoring\":{\"enablePodMonitor\":true},\"storage\":{\"size\":\"100Gi\"}}}\n  creationTimestamp: \"2025-01-10T17:31:24Z\"\n  generation: 1\n  name: paperless-ngx\n  namespace: paperless-ngx\n  resourceVersion: \"185987761\"\n  uid: 628a18ec-74e7-4834-b943-d8b2015f2cbd\nspec:\n  affinity:\n    podAntiAffinityType: preferred\n  bootstrap:\n    initdb:\n      database: app\n      encoding: UTF8\n      localeCType: C\n      localeCollate: C\n      owner: app\n  enablePDB: true\n  enableSuperuserAccess: false\n  failoverDelay: 0\n  imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n  instances: 3\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n    - key: queries\n      name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: true\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: \"on\"\n      archive_timeout: 5min\n      dynamic_shared_memory_type: posix\n      full_page_writes: \"on\"\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: \"0\"\n      log_rotation_size: \"0\"\n      log_truncate_on_rotation: \"false\"\n      logging_collector: \"on\"\n      max_parallel_workers: \"32\"\n      max_replication_slots: \"32\"\n      max_worker_processes: \"32\"\n      shared_memory_type: mmap\n      shared_preload_libraries: \"\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      wal_keep_size: 512MB\n      wal_level: logical\n      wal_log_hints: \"on\"\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: restart\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  resources: {}\n  smartShutdownTimeout: 180\n  startDelay: 3600\n  stopDelay: 1800\n  storage:\n    resizeInUseVolumes: true\n    size: 100Gi\n  switchoverDelay: 3600\nstatus:\n  availableArchitectures:\n  - goArch: amd64\n    hash: 58242fc95faa81d6bba0ba19ea959b82d9b49dc5f8e6c755f3c663665ff0ce1d\n  - goArch: arm64\n    hash: 8e5ef490a05bbd5700047edda5e2440acd14bafa256dbfcc7d4a5107573df75b\n  certificates:\n    clientCASecret: paperless-ngx-ca\n    expirations:\n      paperless-ngx-ca: 2025-04-10 17:26:24 +0000 UTC\n      paperless-ngx-replication: 2025-04-10 17:26:24 +0000 UTC\n      paperless-ngx-server: 2025-04-10 17:26:24 +0000 UTC\n    replicationTLSSecret: paperless-ngx-replication\n    serverAltDNSNames:\n    - paperless-ngx-rw\n    - paperless-ngx-rw.paperless-ngx\n    - paperless-ngx-rw.paperless-ngx.svc\n    - paperless-ngx-rw.paperless-ngx.svc.cluster.local\n    - paperless-ngx-r\n    - paperless-ngx-r.paperless-ngx\n    - paperless-ngx-r.paperless-ngx.svc\n    - paperless-ngx-r.paperless-ngx.svc.cluster.local\n    - paperless-ngx-ro\n    - paperless-ngx-ro.paperless-ngx\n    - paperless-ngx-ro.paperless-ngx.svc\n    - paperless-ngx-ro.paperless-ngx.svc.cluster.local\n    serverCASecret: paperless-ngx-ca\n    serverTLSSecret: paperless-ngx-server\n  cloudNativePGCommitHash: bad5a251\n  cloudNativePGOperatorHash: 58242fc95faa81d6bba0ba19ea959b82d9b49dc5f8e6c755f3c663665ff0ce1d\n  conditions:\n  - lastTransitionTime: \"2025-01-10T17:31:25Z\"\n    message: Cluster Is Not Ready\n    reason: ClusterIsNotReady\n    status: \"False\"\n    type: Ready\n  - lastTransitionTime: \"2025-01-10T17:54:14Z\"\n    message: Continuous archiving is working\n    reason: ContinuousArchivingSuccess\n    status: \"True\"\n    type: ContinuousArchiving\n  configMapResourceVersion:\n    metrics:\n      cnpg-default-monitoring: \"185778772\"\n  currentPrimary: paperless-ngx-1\n  currentPrimaryTimestamp: \"2025-01-10T17:54:14.264198Z\"\n  healthyPVC:\n  - paperless-ngx-1\n  - paperless-ngx-2\n  - paperless-ngx-3\n  image: ghcr.io/cloudnative-pg/postgresql:17.2\n  instanceNames:\n  - paperless-ngx-1\n  - paperless-ngx-2\n  - paperless-ngx-3\n  instances: 3\n  instancesReportedState:\n    paperless-ngx-1:\n      isPrimary: true\n      timeLineID: 1\n    paperless-ngx-2:\n      isPrimary: false\n      timeLineID: 1\n  instancesStatus:\n    failed:\n    - paperless-ngx-3\n    healthy:\n    - paperless-ngx-1\n    - paperless-ngx-2\n  jobCount: 3\n  latestGeneratedNode: 3\n  managedRolesStatus: {}\n  phase: Waiting for the instances to become active\n  phaseReason: Some instances are not yet active. Please wait.\n  poolerIntegrations:\n    pgBouncerIntegration: {}\n  pvcCount: 3\n  readService: paperless-ngx-r\n  readyInstances: 2\n  secretsResourceVersion:\n    applicationSecretVersion: \"185778742\"\n    clientCaSecretVersion: \"185778738\"\n    replicationSecretVersion: \"185778740\"\n    serverCaSecretVersion: \"185778738\"\n    serverSecretVersion: \"185778739\"\n  switchReplicaClusterStatus: {}\n  targetPrimary: paperless-ngx-1\n  targetPrimaryTimestamp: \"2025-01-10T17:31:25.301870Z\"\n  timelineID: 1\n  topology:\n    instances:\n      paperless-ngx-1: {}\n      paperless-ngx-2: {}\n      paperless-ngx-3: {}\n    nodesUsed: 3\n    successfullyExtracted: true\n  writeService: paperless-ngx-rw\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:45.987104003Z\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"version\":\"1.25.0\",\"build\":{\"Version\":\"1.25.0\",\"Commit\":\"bad5a251\",\"Date\":\"2024-12-23\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:45.987536749Z\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.065481472Z\",\"msg\":\"starting tablespace manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.065543385Z\",\"msg\":\"starting external server manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.065575964Z\",\"msg\":\"starting controller-runtime manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.066152379Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-publication\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Publication\",\"source\":\"kind source: *v1.Publication\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.06622348Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-publication\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Publication\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.06653679Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-database\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Database\",\"source\":\"kind source: *v1.Database\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.066584824Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-database\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Database\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.06666119Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.066684424Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.066787797Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-subscription\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Subscription\",\"source\":\"kind source: *v1.Subscription\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.066830684Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-subscription\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Subscription\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.06665109Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.066897063Z\",\"logger\":\"roles_reconciler\",\"msg\":\"skipping the RoleSynchronizer in replicas\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.066963348Z\",\"msg\":\"Starting webserver\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"address\":\":8000\",\"hasTLS\":true}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.067013399Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.067384967Z\",\"msg\":\"Starting webserver\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"address\":\":9187\",\"hasTLS\":false}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.066838812Z\",\"msg\":\"Starting webserver\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"address\":\"localhost:8010\",\"hasTLS\":false}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.067547401Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.067596171Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.06768998Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.067727404Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.167664225Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.167777917Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.167726224Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-publication\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Publication\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.168121839Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-subscription\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Subscription\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.168199718Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.168228278Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-database\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Database\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.196734272Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"namespace\":\"paperless-ngx\",\"name\":\"paperless-ngx\",\"reconcileID\":\"b8f4c14a-fbd9-4c02-877e-1f2dc8665533\",\"logging_pod\":\"paperless-ngx-3\",\"filename\":\"/controller/certificates/server.crt\",\"secret\":\"paperless-ngx-server\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.201026437Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"namespace\":\"paperless-ngx\",\"name\":\"paperless-ngx\",\"reconcileID\":\"b8f4c14a-fbd9-4c02-877e-1f2dc8665533\",\"logging_pod\":\"paperless-ngx-3\",\"filename\":\"/controller/certificates/server.key\",\"secret\":\"paperless-ngx-server\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.221886591Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"namespace\":\"paperless-ngx\",\"name\":\"paperless-ngx\",\"reconcileID\":\"b8f4c14a-fbd9-4c02-877e-1f2dc8665533\",\"logging_pod\":\"paperless-ngx-3\",\"filename\":\"/controller/certificates/streaming_replica.crt\",\"secret\":\"paperless-ngx-replication\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.225114Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"namespace\":\"paperless-ngx\",\"name\":\"paperless-ngx\",\"reconcileID\":\"b8f4c14a-fbd9-4c02-877e-1f2dc8665533\",\"logging_pod\":\"paperless-ngx-3\",\"filename\":\"/controller/certificates/streaming_replica.key\",\"secret\":\"paperless-ngx-replication\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.237502351Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"namespace\":\"paperless-ngx\",\"name\":\"paperless-ngx\",\"reconcileID\":\"b8f4c14a-fbd9-4c02-877e-1f2dc8665533\",\"logging_pod\":\"paperless-ngx-3\",\"filename\":\"/controller/certificates/client-ca.crt\",\"secret\":\"paperless-ngx-ca\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.2455453Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"namespace\":\"paperless-ngx\",\"name\":\"paperless-ngx\",\"reconcileID\":\"b8f4c14a-fbd9-4c02-877e-1f2dc8665533\",\"logging_pod\":\"paperless-ngx-3\",\"filename\":\"/controller/certificates/server-ca.crt\",\"secret\":\"paperless-ngx-ca\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.248559355Z\",\"msg\":\"Found previous run flag\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"namespace\":\"paperless-ngx\",\"name\":\"paperless-ngx\",\"reconcileID\":\"b8f4c14a-fbd9-4c02-877e-1f2dc8665533\",\"logging_pod\":\"paperless-ngx-3\",\"filename\":\"/var/lib/postgresql/data/pgdata/cnpg_initialized-paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.257291745Z\",\"msg\":\"Extracting pg_controldata information\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"reason\":\"postmaster start up\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.262832513Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1700\\nCatalog version number:               202406281\\nDatabase system identifier:           7458346455658311705\\nDatabase cluster state:               shut down in recovery\\npg_control last modified:             Fri 10 Jan 2025 09:02:02 PM UTC\\nLatest checkpoint location:           0/80000B8\\nLatest checkpoint's REDO location:    0/8000060\\nLatest checkpoint's REDO WAL file:    000000010000000000000008\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:753\\nLatest checkpoint's NextOID:          24578\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        730\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  753\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Fri 10 Jan 2025 06:55:05 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/9000060\\nMin recovery ending loc's timeline:   1\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            9ee2f95bedfe4697fa66d9b663257274a678cc678bceac7aa65dbf4186a23934\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.264809439Z\",\"msg\":\"postmaster started\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"postMasterPID\":25}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.338759778Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"paperless-ngx\",\"namespace\":\"paperless-ngx\"},\"namespace\":\"paperless-ngx\",\"name\":\"paperless-ngx\",\"reconcileID\":\"b8f4c14a-fbd9-4c02-877e-1f2dc8665533\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.360190563Z\",\"logger\":\"postgres\",\"msg\":\"2025-01-10 21:02:46.359 UTC [25] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.360250781Z\",\"logger\":\"postgres\",\"msg\":\"2025-01-10 21:02:46.359 UTC [25] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.361146991Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.360 UTC\",\"process_id\":\"25\",\"session_id\":\"67818af6.19\",\"session_line_num\":\"1\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.361480826Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.360 UTC\",\"process_id\":\"25\",\"session_id\":\"67818af6.19\",\"session_line_num\":\"2\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 17.2 (Debian 17.2-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.361512848Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.361 UTC\",\"process_id\":\"25\",\"session_id\":\"67818af6.19\",\"session_line_num\":\"3\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.361542831Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.361 UTC\",\"process_id\":\"25\",\"session_id\":\"67818af6.19\",\"session_line_num\":\"4\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.361604877Z\",\"logger\":\"postgres\",\"msg\":\"2025-01-10 21:02:46.360 UTC [25] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.361646102Z\",\"logger\":\"postgres\",\"msg\":\"2025-01-10 21:02:46.360 UTC [25] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.370398187Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.370 UTC\",\"process_id\":\"25\",\"session_id\":\"67818af6.19\",\"session_line_num\":\"5\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.381500758Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.381 UTC\",\"process_id\":\"29\",\"session_id\":\"67818af6.1d\",\"session_line_num\":\"1\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was shut down in recovery at 2025-01-10 21:02:02 UTC\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.76358874Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.762 UTC\",\"process_id\":\"29\",\"session_id\":\"67818af6.1d\",\"session_line_num\":\"2\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"entering standby mode\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.774625742Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.774 UTC\",\"process_id\":\"29\",\"session_id\":\"67818af6.1d\",\"session_line_num\":\"3\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"virtual_transaction_id\":\"148/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"redo starts at 0/8000060\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.965826934Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.965 UTC\",\"process_id\":\"29\",\"session_id\":\"67818af6.1d\",\"session_line_num\":\"4\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"virtual_transaction_id\":\"148/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"consistent recovery state reached at 0/9000060\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.965935763Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.965 UTC\",\"process_id\":\"29\",\"session_id\":\"67818af6.1d\",\"session_line_num\":\"5\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"virtual_transaction_id\":\"148/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"invalid record length at 0/9000060: expected at least 24, got 0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.965996861Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.965 UTC\",\"process_id\":\"25\",\"session_id\":\"67818af6.19\",\"session_line_num\":\"6\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is ready to accept read-only connections\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:46.992991261Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:46.992 UTC\",\"process_id\":\"60\",\"session_id\":\"67818af6.3c\",\"session_line_num\":\"1\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"started streaming WAL from primary at 0/9000000 on timeline 1\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.771030673Z\",\"msg\":\"Received termination signal\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"signal\":\"interrupt\",\"smartShutdownTimeout\":180}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.771187184Z\",\"msg\":\"Requesting smart shutdown of the PostgreSQL instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.774175928Z\",\"logger\":\"pg_ctl\",\"msg\":\"pg_ctl: server is running (PID: 25)\\n/usr/lib/postgresql/17/bin/postgres \\\"-D\\\" \\\"/var/lib/postgresql/data/pgdata\\\"\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.77421282Z\",\"msg\":\"Shutting down instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"mode\":\"smart\",\"timeout\":180}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.776541558Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:56.776 UTC\",\"process_id\":\"25\",\"session_id\":\"67818af6.19\",\"session_line_num\":\"7\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"received smart shutdown request\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.781061017Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:56.779 UTC\",\"process_id\":\"60\",\"session_id\":\"67818af6.3c\",\"session_line_num\":\"2\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P01\",\"message\":\"terminating walreceiver process due to administrator command\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.784356103Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:56.784 UTC\",\"process_id\":\"27\",\"session_id\":\"67818af6.1b\",\"session_line_num\":\"1\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"shutting down\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.79290641Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"paperless-ngx-3\",\"record\":{\"log_time\":\"2025-01-10 21:02:56.792 UTC\",\"process_id\":\"25\",\"session_id\":\"67818af6.19\",\"session_line_num\":\"8\",\"session_start_time\":\"2025-01-10 21:02:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.796720619Z\",\"msg\":\"postmaster exited\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"postmasterExitStatus\":null,\"postMasterPID\":25}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.79676593Z\",\"msg\":\"Extracting pg_controldata information\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"reason\":\"postmaster has exited\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.799319498Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1700\\nCatalog version number:               202406281\\nDatabase system identifier:           7458346455658311705\\nDatabase cluster state:               shut down in recovery\\npg_control last modified:             Fri 10 Jan 2025 09:02:56 PM UTC\\nLatest checkpoint location:           0/80000B8\\nLatest checkpoint's REDO location:    0/8000060\\nLatest checkpoint's REDO WAL file:    000000010000000000000008\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:753\\nLatest checkpoint's NextOID:          24578\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        730\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  753\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Fri 10 Jan 2025 06:55:05 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/9000060\\nMin recovery ending loc's timeline:   1\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            9ee2f95bedfe4697fa66d9b663257274a678cc678bceac7aa65dbf4186a23934\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.876734359Z\",\"logger\":\"pg_ctl\",\"msg\":\"waiting for server to shut down.... done\",\"pipe\":\"stdout\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.876799228Z\",\"logger\":\"pg_ctl\",\"msg\":\"server stopped\",\"pipe\":\"stdout\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.87702681Z\",\"msg\":\"PostgreSQL instance shut down\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877115642Z\",\"msg\":\"Stopping and waiting for non leader election runnables\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877143381Z\",\"msg\":\"Stopping and waiting for leader election runnables\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877186691Z\",\"logger\":\"Replicator\",\"msg\":\"Terminated slot Replicator loop\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877283793Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877247174Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-database\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Database\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877307729Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877362593Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-publication\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Publication\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877370731Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877392132Z\",\"msg\":\"All workers finished\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-publication\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Publication\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877375651Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-subscription\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Subscription\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877406677Z\",\"msg\":\"All workers finished\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-subscription\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Subscription\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877411729Z\",\"msg\":\"Exited log pipe\",\"logger\":\"instance-manager\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877418873Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877428816Z\",\"msg\":\"All workers finished\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877438976Z\",\"msg\":\"All workers finished\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877444849Z\",\"msg\":\"All workers finished\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877463801Z\",\"msg\":\"Webserver exited\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"address\":\":9187\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877464244Z\",\"msg\":\"Exited log pipe\",\"logger\":\"instance-manager\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877484197Z\",\"msg\":\"All workers finished\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"controller\":\"instance-database\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Database\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877483708Z\",\"msg\":\"Exited log pipe\",\"logger\":\"instance-manager\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.877486747Z\",\"msg\":\"Webserver exited\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"address\":\"localhost:8010\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.878235569Z\",\"msg\":\"Webserver exited\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\",\"address\":\":8000\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.878269766Z\",\"msg\":\"Stopping and waiting for caches\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.878630913Z\",\"msg\":\"pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251: watch of *v1.Cluster ended with: an error on the server (\\\"unable to decode an event from the watch stream: context canceled\\\") has prevented the request from succeeding\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.878972516Z\",\"msg\":\"pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251: watch of *v1.Subscription ended with: an error on the server (\\\"unable to decode an event from the watch stream: context canceled\\\") has prevented the request from succeeding\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.879033543Z\",\"msg\":\"Stopping and waiting for webhooks\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.879058724Z\",\"msg\":\"Stopping and waiting for HTTP servers\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.879068165Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.879083492Z\",\"msg\":\"Checking for free disk space for WALs after PostgreSQL finished\",\"logger\":\"instance-manager\",\"logging_pod\":\"paperless-ngx-3\"}\n{\"level\":\"info\",\"ts\":\"2025-01-10T21:02:56.879232868Z\",\"msg\":\"pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251: watch of *v1.Publication ended with: an error on the server (\\\"unable to decode an event from the watch stream: context canceled\\\") has prevented the request from succeeding\"}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct`kubectl cnpg status` output:\n```\nCluster Summary\nName                 paperless-ngx/paperless-ngx\nSystem ID:           7458346455658311705\nPostgreSQL Image:    ghcr.io/cloudnative-pg/postgresql:17.2\nPrimary instance:    paperless-ngx-1\nPrimary start time:  2025-01-10 17:54:14 +0000 UTC (uptime 3h21m10s)\nStatus:              Waiting for the instances to become active Some instances are not yet active. Please wait.\nInstances:           3\nReady instances:     2\nSize:                174M\nCurrent Write LSN:   0/9000060 (Timeline: 1 - WAL File: 000000010000000000000009)\nContinuous Backup status\nNot configured\nStreaming Replication status\nReplication Slots Enabled\nName             Sent LSN   Write LSN  Flush LSN  Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority  Replication Slot\n----             --------   ---------  ---------  ----------  ---------  ---------  ----------  -----      ----------  -------------  ----------------\npaperless-ngx-2  0/9000060  0/9000060  0/9000060  0/9000060   00:00:00   00:00:00   00:00:00    streaming  async       0              active\nInstances status\nName             Current LSN  Replication role  Status  QoS         Manager Version  Node\n----             -----------  ----------------  ------  ---         ---------------  ----\npaperless-ngx-1  0/9000060    Primary           OK      Burstable   1.25.0           k8s-libvirt-f6b583d8-22b6-442f-bc29-f3acad1c8a83\npaperless-ngx-2  0/9000060    Standby (async)   OK      Burstable   1.25.0           k8s-libvirt-760a4d4e-abd1-4a6a-a024-dcd0cf40a604\npaperless-ngx-3  -            -                 -       BadRequest  Burstable        -  k8s-libvirt-2fefd400-20c6-4a2b-9d8e-70e1ad763319\nError(s) extracting status\n--------------------------\nfailed to get status by proxying to the pod, you might lack permissions to get pods/proxy: the server rejected our request for an unknown reason (get pods https:paperless-ngx-3:8000)\n```\n---\nGetting the same error on `v1.24.1`"
    },
    {
        "title": "[Bug]: Imparative Hibernation breaks declarative databases",
        "id": 2779651826,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nThese are the steps to reproduce:\n1. Create a `clusters.postgresql.cnpg.io` resource, let's call it `mycluster`\n2. Create a database `mydb` via `databases.postgresql.cnpg.io` targeting `mycluster`\n3. Hibernate the cluster using the `kubectl` plugin: `kubectl cnpg hibernate on mycluster`\nThe status of `mydb` will then read:\n```\nstatus:\n  applied: false\n  message: cluster resource has been deleted, skipping reconciliation\n```\nThis state will continue after hibernation is turned off again, i.e. `mydb` is no longer reconciled from then on.\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nThese are the steps to reproduce:\n1. Create a `clusters.postgresql.cnpg.io` resource, let's call it `mycluster`\n2. Create a database `mydb` via `databases.postgresql.cnpg.io` targeting `mycluster`\n3. Hibernate the cluster using the `kubectl` plugin: `kubectl cnpg hibernate on mycluster`\nThe status of `mydb` will then read:\n```\nstatus:\n  applied: false\n  message: cluster resource has been deleted, skipping reconciliation\n```\nThis state will continue after hibernation is turned off again, i.e. `mydb` is no longer reconciled from then on.\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "Use targetPod.Name in WithValues() instead of targetPod",
        "id": 2778006441,
        "state": "open",
        "first": "Fix #6400 \r\nI also changed the name of the parameter from `targetPod` to `targetPodName` to be more consistent with other log lines.\r\nResult : \r\n``` json\r\n{  \r\n  \"level\": \"info\",\r\n  \"ts\": \"2025-01-09T14:25:24.394293941Z\",\r\n  \"msg\": \"Waiting for VolumeSnapshot to be provisioned\",\r\n  \"controller\": \"backup\",\r\n  \"controllerGroup\": \"postgresql.cnpg.io\",\r\n  \"controllerKind\": \"Backup\",\r\n  \"Backup\": {\r\n    \"name\": \"snapshot-volume-3\",\r\n    \"namespace\": \"default\"\r\n  },\r\n  \"namespace\": \"default\",\r\n  \"name\": \"snapshot-volume-3\",\r\n  \"reconcileID\": \"b524b993-b153-4344-9a6e-6bcbe08a20e9\",\r\n  \"targetPodName\": \"cluster2-1\",\r\n  \"volumeSnapshotName\": \"snapshot-volume-3\"\r\n}\r\n```",
        "messages": "Fix #6400 \r\nI also changed the name of the parameter from `targetPod` to `targetPodName` to be more consistent with other log lines.\r\nResult : \r\n``` json\r\n{  \r\n  \"level\": \"info\",\r\n  \"ts\": \"2025-01-09T14:25:24.394293941Z\",\r\n  \"msg\": \"Waiting for VolumeSnapshot to be provisioned\",\r\n  \"controller\": \"backup\",\r\n  \"controllerGroup\": \"postgresql.cnpg.io\",\r\n  \"controllerKind\": \"Backup\",\r\n  \"Backup\": {\r\n    \"name\": \"snapshot-volume-3\",\r\n    \"namespace\": \"default\"\r\n  },\r\n  \"namespace\": \"default\",\r\n  \"name\": \"snapshot-volume-3\",\r\n  \"reconcileID\": \"b524b993-b153-4344-9a6e-6bcbe08a20e9\",\r\n  \"targetPodName\": \"cluster2-1\",\r\n  \"volumeSnapshotName\": \"snapshot-volume-3\"\r\n}\r\n```"
    },
    {
        "title": "feat: add support for configuring PostgreSQL extensions via Image Volume",
        "id": 2777631907,
        "state": "open",
        "first": "Add support to allow setting up PostgreSQL extensions via container images, taking advance of the `ImageVolume` VolumeSource of Kubernetes and the new proposed GUC of PostgreSQL `extension_control_path`.\r\nIntroduce a new API field `Extensions` in the `cluster.Spec.PostgresConfiguration` stanza, which takes an array of PostgreSQL extensions which should be added to the Cluster.\r\nFor more information, see:\r\n* https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/4639-oci-volume-source\r\n* https://kubernetes.io/docs/tasks/configure-pod-container/image-volumes/\r\n* https://commitfest.postgresql.org/50/4913/\r\n* https://justatheory.com/2024/11/rfc-extension-packaging-lookup/",
        "messages": "Add support to allow setting up PostgreSQL extensions via container images, taking advance of the `ImageVolume` VolumeSource of Kubernetes and the new proposed GUC of PostgreSQL `extension_control_path`.\r\nIntroduce a new API field `Extensions` in the `cluster.Spec.PostgresConfiguration` stanza, which takes an array of PostgreSQL extensions which should be added to the Cluster.\r\nFor more information, see:\r\n* https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/4639-oci-volume-source\r\n* https://kubernetes.io/docs/tasks/configure-pod-container/image-volumes/\r\n* https://commitfest.postgresql.org/50/4913/\r\n* https://justatheory.com/2024/11/rfc-extension-packaging-lookup/"
    },
    {
        "title": "[Bug]: WAL cleanup inconsistency on replica nodes after cluster role changes",
        "id": 2775899353,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njeffm@gisual.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nYAML manifest\n### What happened?\n**Impact:** \nReplica node entered crash-loop backoff due to WAL volume space exhaustion. WAL files from December persisted on one replica after promotion to primary, while other nodes remained healthy.\n**Steps to reproduce:**\n- [ ] Configure 3-node PostgreSQL cluster (1 primary, 2 replicas)\n- [ ] Perform primary/replica role changes\n- [ ] Monitor WAL retention on all nodes\n- [ ] Observe inconsistent WAL cleanup behavior, particularly on nodes that change roles\n**Expected behavior:**\n- WAL cleanup should occur consistently across all instances\n- Role changes should not affect WAL cleanup mechanisms\n- No out-of-disk space scenarios due to retained WAL after failovers\n**Actual behavior:**\n- Inconsistent WAL cleanup after role changes\n- WAL files persisted from December on one node\n- WAL volume space exhaustion on replica\n- Files eventually self-cleaned during investigation\n**Additional details:**\n- **Cluster and operator reports with logs available upon request.**\n- This cluster has at least two separate write transactions every 60 seconds.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"metadata\":{\"annotations\":{},\"name\":\"staging-pg-common\",\"namespace\":\"db\"},\"spec\":{\"affinity\":{\"nodeSelector\":{\"kubernetes.azure.com/agentpool\":\"pgcommon\",\"kubernetes.io/arch\":\"arm64\"},\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/postgres\",\"operator\":\"Exists\"}]},\"backup\":{\"barmanObjectStore\":{\"azureCredentials\":{\"connectionString\":{\"key\":\"AZURE_STORAGE_CONNECTION_STRING\",\"name\":\"postgres-backup-azure-creds\"}},\"data\":{\"compression\":\"snappy\",\"jobs\":2},\"destinationPath\":\"https://redactedstaging.blob.core.windows.net/postgres-backup\",\"wal\":{\"maxParallel\":2}},\"retentionPolicy\":\"30d\",\"target\":\"prefer-standby\",\"volumeSnapshot\":{\"className\":\"csi-azure-disk\",\"online\":true,\"onlineConfiguration\":{\"immediateCheckpoint\":false,\"waitForArchive\":true},\"snapshotOwnerReference\":\"cluster\"}},\"bootstrap\":{\"initdb\":{\"database\":\"redacted\",\"owner\":\"redacted\",\"secret\":{\"name\":\"postgres-redacted-db-credentials\"}}},\"enablePDB\":true,\"enableSuperuserAccess\":true,\"env\":[{\"name\":\"ENABLE_AZURE_PVC_UPDATES\",\"value\":\"true\"}],\"failoverDelay\":30,\"imageCatalogRef\":{\"apiGroup\":\"postgresql.cnpg.io\",\"kind\":\"ImageCatalog\",\"major\":17,\"name\":\"custom-postgres-images\"},\"imagePullSecrets\":[{\"name\":\"docker-registry-credentials\"}],\"instances\":3,\"monitoring\":{\"enablePodMonitor\":true},\"postgresql\":{\"parameters\":{\"auto_explain.log_analyze\":\"true\",\"auto_explain.log_buffers\":\"true\",\"auto_explain.log_min_duration\":\"0\",\"auto_explain.log_timing\":\"true\",\"auto_explain.log_triggers\":\"true\",\"auto_explain.log_verbose\":\"true\",\"autovacuum_max_workers\":\"2\",\"autovacuum_naptime\":\"20s\",\"autovacuum_vacuum_cost_limit\":\"1000\",\"autovacuum_vacuum_scale_factor\":\"0.1\",\"checkpoint_completion_target\":\"0.9\",\"checkpoint_timeout\":\"15min\",\"cron.database_name\":\"redacted\",\"cron.use_background_workers\":\"on\",\"default_statistics_target\":\"100\",\"effective_cache_size\":\"4191MB\",\"effective_io_concurrency\":\"1000\",\"idle_in_transaction_session_timeout\":\"30min\",\"jit\":\"off\",\"lock_timeout\":\"1800s\",\"maintenance_work_mem\":\"341MB\",\"max_connections\":\"100\",\"max_parallel_maintenance_workers\":\"2\",\"max_parallel_workers\":\"4\",\"max_parallel_workers_per_gather\":\"2\",\"max_wal_size\":\"4GB\",\"max_worker_processes\":\"4\",\"min_wal_size\":\"1GB\",\"pg_stat_statements.max\":\"10000\",\"pg_stat_statements.save\":\"on\",\"pg_stat_statements.track\":\"all\",\"pg_stat_statements.track_planning\":\"off\",\"pg_stat_statements.track_utility\":\"on\",\"random_page_cost\":\"1.1\",\"shared_buffers\":\"1397MB\",\"statement_timeout\":\"7200s\",\"wal_buffers\":\"16MB\",\"wal_compression\":\"lz4\",\"work_mem\":\"4988kB\"},\"shared_preload_libraries\":[\"pg_stat_statements\",\"auto_explain\",\"pg_cron\"]},\"primaryUpdateStrategy\":\"unsupervised\",\"resources\":{\"limits\":{\"cpu\":\"2628m\",\"memory\":\"5458Mi\"},\"requests\":{\"cpu\":\"2628m\",\"memory\":\"5458Mi\"}},\"storage\":{\"size\":\"128Gi\",\"storageClass\":\"6400-iops-212mb-tp-premium-v2\"},\"superuserSecret\":{\"name\":\"postgres-superuser-secret\"},\"switchoverDelay\":30,\"walStorage\":{\"size\":\"32Gi\",\"storageClass\":\"6400-iops-212mb-tp-premium-v2\"}}}\n  creationTimestamp: \"2024-12-10T19:09:51Z\"\n  generation: 16\n  name: staging-pg-common\n  namespace: db\n  resourceVersion: \"468310505\"\n  uid: 4f3797ad-a333-40f8-9ae1-0f21fb1b742c\nspec:\n  affinity:\n    nodeSelector:\n      kubernetes.azure.com/agentpool: pgcommon\n      kubernetes.io/arch: arm64\n    podAntiAffinityType: preferred\n    tolerations:\n    - effect: NoSchedule\n      key: node-role.kubernetes.io/postgres\n      operator: Exists\n  backup:\n    barmanObjectStore:\n      azureCredentials:\n        connectionString:\n          key: AZURE_STORAGE_CONNECTION_STRING\n          name: postgres-backup-azure-creds\n      data:\n        compression: snappy\n        jobs: 2\n      destinationPath: https://redactedstaging.blob.core.windows.net/postgres-backup\n      wal:\n        maxParallel: 2\n    retentionPolicy: 30d\n    target: prefer-standby\n    volumeSnapshot:\n      className: csi-azure-disk\n      online: true\n      onlineConfiguration:\n        immediateCheckpoint: false\n        waitForArchive: true\n      snapshotOwnerReference: cluster\n  bootstrap:\n    initdb:\n      database: redacted\n      encoding: UTF8\n      localeCType: C\n      localeCollate: C\n      owner: redacted\n      secret:\n        name: postgres-redacted-db-credentials\n  enablePDB: true\n  enableSuperuserAccess: true\n  env:\n  - name: ENABLE_AZURE_PVC_UPDATES\n    value: \"true\"\n  failoverDelay: 30\n  imageCatalogRef:\n    apiGroup: postgresql.cnpg.io\n    kind: ImageCatalog\n    major: 17\n    name: custom-postgres-images\n  imagePullSecrets:\n  - name: docker-registry-credentials\n  instances: 3\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n    - key: queries\n      name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: true\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: \"on\"\n      archive_timeout: 5min\n      auto_explain.log_analyze: \"true\"\n      auto_explain.log_buffers: \"true\"\n      auto_explain.log_min_duration: \"0\"\n      auto_explain.log_timing: \"true\"\n      auto_explain.log_triggers: \"true\"\n      auto_explain.log_verbose: \"true\"\n      autovacuum_max_workers: \"2\"\n      autovacuum_naptime: 20s\n      autovacuum_vacuum_cost_limit: \"1000\"\n      autovacuum_vacuum_scale_factor: \"0.1\"\n      checkpoint_completion_target: \"0.9\"\n      checkpoint_timeout: 15min\n      cron.database_name: redacted\n      cron.use_background_workers: \"on\"\n      default_statistics_target: \"100\"\n      dynamic_shared_memory_type: posix\n      effective_cache_size: 4191MB\n      effective_io_concurrency: \"1000\"\n      full_page_writes: \"on\"\n      idle_in_transaction_session_timeout: 30min\n      jit: \"off\"\n      lock_timeout: 1800s\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: \"0\"\n      log_rotation_size: \"0\"\n      log_truncate_on_rotation: \"false\"\n      logging_collector: \"on\"\n      maintenance_work_mem: 341MB\n      max_connections: \"100\"\n      max_parallel_maintenance_workers: \"2\"\n      max_parallel_workers: \"4\"\n      max_parallel_workers_per_gather: \"2\"\n      max_replication_slots: \"32\"\n      max_wal_size: 4GB\n      max_worker_processes: \"4\"\n      min_wal_size: 1GB\n      pg_stat_statements.max: \"10000\"\n      pg_stat_statements.save: \"on\"\n      pg_stat_statements.track: all\n      pg_stat_statements.track_planning: \"off\"\n      pg_stat_statements.track_utility: \"on\"\n      random_page_cost: \"1.1\"\n      shared_buffers: 1397MB\n      shared_memory_type: mmap\n      shared_preload_libraries: \"\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      statement_timeout: 7200s\n      wal_buffers: 16MB\n      wal_compression: lz4\n      wal_keep_size: 512MB\n      wal_level: logical\n      wal_log_hints: \"on\"\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n      work_mem: 4988kB\n    shared_preload_libraries:\n    - pg_stat_statements\n    - auto_explain\n    - pg_cron\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: restart\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  resources:\n    limits:\n      cpu: 2628m\n      memory: 5458Mi\n    requests:\n      cpu: 2628m\n      memory: 5458Mi\n  smartShutdownTimeout: 180\n  startDelay: 3600\n  stopDelay: 1800\n  storage:\n    resizeInUseVolumes: true\n    size: 128Gi\n    storageClass: 6400-iops-212mb-tp-premium-v2\n  superuserSecret:\n    name: postgres-superuser-secret\n  switchoverDelay: 30\n  walStorage:\n    resizeInUseVolumes: true\n    size: 32Gi\n    storageClass: 6400-iops-212mb-tp-premium-v2\nstatus:\n  availableArchitectures:\n  - goArch: amd64\n    hash: 58242fc95faa81d6bba0ba19ea959b82d9b49dc5f8e6c755f3c663665ff0ce1d\n  - goArch: arm64\n    hash: 8e5ef490a05bbd5700047edda5e2440acd14bafa256dbfcc7d4a5107573df75b\n  certificates:\n    clientCASecret: staging-pg-common-ca\n    expirations:\n      staging-pg-common-ca: 2025-03-10 19:04:52 +0000 UTC\n      staging-pg-common-replication: 2025-03-10 19:04:52 +0000 UTC\n      staging-pg-common-server: 2025-03-10 19:04:52 +0000 UTC\n    replicationTLSSecret: staging-pg-common-replication\n    serverAltDNSNames:\n    - staging-pg-common-rw\n    - staging-pg-common-rw.db\n    - staging-pg-common-rw.db.svc\n    - staging-pg-common-rw.db.svc.cluster.local\n    - staging-pg-common-r\n    - staging-pg-common-r.db\n    - staging-pg-common-r.db.svc\n    - staging-pg-common-r.db.svc.cluster.local\n    - staging-pg-common-ro\n    - staging-pg-common-ro.db\n    - staging-pg-common-ro.db.svc\n    - staging-pg-common-ro.db.svc.cluster.local\n    serverCASecret: staging-pg-common-ca\n    serverTLSSecret: staging-pg-common-server\n  cloudNativePGCommitHash: bad5a251\n  cloudNativePGOperatorHash: 58242fc95faa81d6bba0ba19ea959b82d9b49dc5f8e6c755f3c663665ff0ce1d\n  conditions:\n  - lastTransitionTime: \"2025-01-08T16:19:02Z\"\n    message: Cluster is Ready\n    reason: ClusterIsReady\n    status: \"True\"\n    type: Ready\n  - lastTransitionTime: \"2024-12-23T15:21:09Z\"\n    message: Continuous archiving is working\n    reason: ContinuousArchivingSuccess\n    status: \"True\"\n    type: ContinuousArchiving\n  - lastTransitionTime: \"2025-01-08T17:00:00Z\"\n    message: New Backup starting up\n    reason: BackupStarted\n    status: \"False\"\n    type: LastBackupSucceeded\n  configMapResourceVersion:\n    metrics:\n      cnpg-default-monitoring: \"445853326\"\n  currentPrimary: staging-pg-common-3\n  currentPrimaryTimestamp: \"2024-12-23T15:21:05.448655Z\"\n  firstRecoverabilityPoint: \"2024-12-10T20:08:33Z\"\n  firstRecoverabilityPointByMethod:\n    barmanObjectStore: \"2024-12-11T00:00:02Z\"\n    volumeSnapshot: \"2024-12-10T20:08:33Z\"\n  healthyPVC:\n  - staging-pg-common-1\n  - staging-pg-common-1-wal\n  - staging-pg-common-3\n  - staging-pg-common-3-wal\n  - staging-pg-common-4\n  - staging-pg-common-4-wal\n  image: docker.redacted.net/custom-postgres17:17.2@sha256:9d93740c5fcfa7b3113815d6118f8a0afe35dcef38775dae2b93f6d7badedc91\n  instanceNames:\n  - staging-pg-common-1\n  - staging-pg-common-3\n  - staging-pg-common-4\n  instances: 3\n  instancesReportedState:\n    staging-pg-common-1:\n      isPrimary: false\n      timeLineID: 4\n    staging-pg-common-3:\n      isPrimary: true\n      timeLineID: 4\n    staging-pg-common-4:\n      isPrimary: false\n      timeLineID: 4\n  instancesStatus:\n    healthy:\n    - staging-pg-common-1\n    - staging-pg-common-3\n    - staging-pg-common-4\n  lastSuccessfulBackup: \"2025-01-08T16:05:41Z\"\n  lastSuccessfulBackupByMethod:\n    barmanObjectStore: \"2025-01-08T00:00:17Z\"\n    volumeSnapshot: \"2025-01-08T16:05:41Z\"\n  latestGeneratedNode: 4\n  managedRolesStatus: {}\n  phase: Cluster in healthy state\n  poolerIntegrations:\n    pgBouncerIntegration:\n      secrets:\n      - staging-pg-common-pooler\n  pvcCount: 6\n  readService: staging-pg-common-r\n  readyInstances: 3\n  secretsResourceVersion:\n    clientCaSecretVersion: \"445853293\"\n    replicationSecretVersion: \"445853295\"\n    serverCaSecretVersion: \"445853293\"\n    serverSecretVersion: \"445853294\"\n    superuserSecretVersion: \"445853282\"\n  switchReplicaClusterStatus: {}\n  targetPrimary: staging-pg-common-3\n  targetPrimaryTimestamp: \"2024-12-23T15:20:52.825593Z\"\n  timelineID: 4\n  topology:\n    instances:\n      staging-pg-common-1: {}\n      staging-pg-common-3: {}\n      staging-pg-common-4: {}\n    nodesUsed: 3\n    successfullyExtracted: true\n  writeService: staging-pg-common-rw\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.506414715Z\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\",\"version\":\"1.25.0\",\"build\":{\"Version\":\"1.25.0\",\"Commit\":\"bad5a251\",\"Date\":\"2024-12-23\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.506595945Z\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.507873931Z\",\"msg\":\"Detected low-disk space condition, avoid starting the instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.519044943Z\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\",\"version\":\"1.25.0\",\"build\":{\"Version\":\"1.25.0\",\"Commit\":\"bad5a251\",\"Date\":\"2024-12-23\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.519064272Z\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.519933252Z\",\"msg\":\"Detected low-disk space condition, avoid starting the instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.531096897Z\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\",\"version\":\"1.25.0\",\"build\":{\"Version\":\"1.25.0\",\"Commit\":\"bad5a251\",\"Date\":\"2024-12-23\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.531116985Z\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.531974061Z\",\"msg\":\"Detected low-disk space condition, avoid starting the instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.542079719Z\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\",\"version\":\"1.25.0\",\"build\":{\"Version\":\"1.25.0\",\"Commit\":\"bad5a251\",\"Date\":\"2024-12-23\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.54210308Z\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.543052519Z\",\"msg\":\"Detected low-disk space condition, avoid starting the instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.554194298Z\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\",\"version\":\"1.25.0\",\"build\":{\"Version\":\"1.25.0\",\"Commit\":\"bad5a251\",\"Date\":\"2024-12-23\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.554224299Z\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.555176234Z\",\"msg\":\"Detected low-disk space condition, avoid starting the instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njeffm@gisual.com\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nYAML manifest\n### What happened?\n**Impact:** \nReplica node entered crash-loop backoff due to WAL volume space exhaustion. WAL files from December persisted on one replica after promotion to primary, while other nodes remained healthy.\n**Steps to reproduce:**\n- [ ] Configure 3-node PostgreSQL cluster (1 primary, 2 replicas)\n- [ ] Perform primary/replica role changes\n- [ ] Monitor WAL retention on all nodes\n- [ ] Observe inconsistent WAL cleanup behavior, particularly on nodes that change roles\n**Expected behavior:**\n- WAL cleanup should occur consistently across all instances\n- Role changes should not affect WAL cleanup mechanisms\n- No out-of-disk space scenarios due to retained WAL after failovers\n**Actual behavior:**\n- Inconsistent WAL cleanup after role changes\n- WAL files persisted from December on one node\n- WAL volume space exhaustion on replica\n- Files eventually self-cleaned during investigation\n**Additional details:**\n- **Cluster and operator reports with logs available upon request.**\n- This cluster has at least two separate write transactions every 60 seconds.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"metadata\":{\"annotations\":{},\"name\":\"staging-pg-common\",\"namespace\":\"db\"},\"spec\":{\"affinity\":{\"nodeSelector\":{\"kubernetes.azure.com/agentpool\":\"pgcommon\",\"kubernetes.io/arch\":\"arm64\"},\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/postgres\",\"operator\":\"Exists\"}]},\"backup\":{\"barmanObjectStore\":{\"azureCredentials\":{\"connectionString\":{\"key\":\"AZURE_STORAGE_CONNECTION_STRING\",\"name\":\"postgres-backup-azure-creds\"}},\"data\":{\"compression\":\"snappy\",\"jobs\":2},\"destinationPath\":\"https://redactedstaging.blob.core.windows.net/postgres-backup\",\"wal\":{\"maxParallel\":2}},\"retentionPolicy\":\"30d\",\"target\":\"prefer-standby\",\"volumeSnapshot\":{\"className\":\"csi-azure-disk\",\"online\":true,\"onlineConfiguration\":{\"immediateCheckpoint\":false,\"waitForArchive\":true},\"snapshotOwnerReference\":\"cluster\"}},\"bootstrap\":{\"initdb\":{\"database\":\"redacted\",\"owner\":\"redacted\",\"secret\":{\"name\":\"postgres-redacted-db-credentials\"}}},\"enablePDB\":true,\"enableSuperuserAccess\":true,\"env\":[{\"name\":\"ENABLE_AZURE_PVC_UPDATES\",\"value\":\"true\"}],\"failoverDelay\":30,\"imageCatalogRef\":{\"apiGroup\":\"postgresql.cnpg.io\",\"kind\":\"ImageCatalog\",\"major\":17,\"name\":\"custom-postgres-images\"},\"imagePullSecrets\":[{\"name\":\"docker-registry-credentials\"}],\"instances\":3,\"monitoring\":{\"enablePodMonitor\":true},\"postgresql\":{\"parameters\":{\"auto_explain.log_analyze\":\"true\",\"auto_explain.log_buffers\":\"true\",\"auto_explain.log_min_duration\":\"0\",\"auto_explain.log_timing\":\"true\",\"auto_explain.log_triggers\":\"true\",\"auto_explain.log_verbose\":\"true\",\"autovacuum_max_workers\":\"2\",\"autovacuum_naptime\":\"20s\",\"autovacuum_vacuum_cost_limit\":\"1000\",\"autovacuum_vacuum_scale_factor\":\"0.1\",\"checkpoint_completion_target\":\"0.9\",\"checkpoint_timeout\":\"15min\",\"cron.database_name\":\"redacted\",\"cron.use_background_workers\":\"on\",\"default_statistics_target\":\"100\",\"effective_cache_size\":\"4191MB\",\"effective_io_concurrency\":\"1000\",\"idle_in_transaction_session_timeout\":\"30min\",\"jit\":\"off\",\"lock_timeout\":\"1800s\",\"maintenance_work_mem\":\"341MB\",\"max_connections\":\"100\",\"max_parallel_maintenance_workers\":\"2\",\"max_parallel_workers\":\"4\",\"max_parallel_workers_per_gather\":\"2\",\"max_wal_size\":\"4GB\",\"max_worker_processes\":\"4\",\"min_wal_size\":\"1GB\",\"pg_stat_statements.max\":\"10000\",\"pg_stat_statements.save\":\"on\",\"pg_stat_statements.track\":\"all\",\"pg_stat_statements.track_planning\":\"off\",\"pg_stat_statements.track_utility\":\"on\",\"random_page_cost\":\"1.1\",\"shared_buffers\":\"1397MB\",\"statement_timeout\":\"7200s\",\"wal_buffers\":\"16MB\",\"wal_compression\":\"lz4\",\"work_mem\":\"4988kB\"},\"shared_preload_libraries\":[\"pg_stat_statements\",\"auto_explain\",\"pg_cron\"]},\"primaryUpdateStrategy\":\"unsupervised\",\"resources\":{\"limits\":{\"cpu\":\"2628m\",\"memory\":\"5458Mi\"},\"requests\":{\"cpu\":\"2628m\",\"memory\":\"5458Mi\"}},\"storage\":{\"size\":\"128Gi\",\"storageClass\":\"6400-iops-212mb-tp-premium-v2\"},\"superuserSecret\":{\"name\":\"postgres-superuser-secret\"},\"switchoverDelay\":30,\"walStorage\":{\"size\":\"32Gi\",\"storageClass\":\"6400-iops-212mb-tp-premium-v2\"}}}\n  creationTimestamp: \"2024-12-10T19:09:51Z\"\n  generation: 16\n  name: staging-pg-common\n  namespace: db\n  resourceVersion: \"468310505\"\n  uid: 4f3797ad-a333-40f8-9ae1-0f21fb1b742c\nspec:\n  affinity:\n    nodeSelector:\n      kubernetes.azure.com/agentpool: pgcommon\n      kubernetes.io/arch: arm64\n    podAntiAffinityType: preferred\n    tolerations:\n    - effect: NoSchedule\n      key: node-role.kubernetes.io/postgres\n      operator: Exists\n  backup:\n    barmanObjectStore:\n      azureCredentials:\n        connectionString:\n          key: AZURE_STORAGE_CONNECTION_STRING\n          name: postgres-backup-azure-creds\n      data:\n        compression: snappy\n        jobs: 2\n      destinationPath: https://redactedstaging.blob.core.windows.net/postgres-backup\n      wal:\n        maxParallel: 2\n    retentionPolicy: 30d\n    target: prefer-standby\n    volumeSnapshot:\n      className: csi-azure-disk\n      online: true\n      onlineConfiguration:\n        immediateCheckpoint: false\n        waitForArchive: true\n      snapshotOwnerReference: cluster\n  bootstrap:\n    initdb:\n      database: redacted\n      encoding: UTF8\n      localeCType: C\n      localeCollate: C\n      owner: redacted\n      secret:\n        name: postgres-redacted-db-credentials\n  enablePDB: true\n  enableSuperuserAccess: true\n  env:\n  - name: ENABLE_AZURE_PVC_UPDATES\n    value: \"true\"\n  failoverDelay: 30\n  imageCatalogRef:\n    apiGroup: postgresql.cnpg.io\n    kind: ImageCatalog\n    major: 17\n    name: custom-postgres-images\n  imagePullSecrets:\n  - name: docker-registry-credentials\n  instances: 3\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n    - key: queries\n      name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: true\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: \"on\"\n      archive_timeout: 5min\n      auto_explain.log_analyze: \"true\"\n      auto_explain.log_buffers: \"true\"\n      auto_explain.log_min_duration: \"0\"\n      auto_explain.log_timing: \"true\"\n      auto_explain.log_triggers: \"true\"\n      auto_explain.log_verbose: \"true\"\n      autovacuum_max_workers: \"2\"\n      autovacuum_naptime: 20s\n      autovacuum_vacuum_cost_limit: \"1000\"\n      autovacuum_vacuum_scale_factor: \"0.1\"\n      checkpoint_completion_target: \"0.9\"\n      checkpoint_timeout: 15min\n      cron.database_name: redacted\n      cron.use_background_workers: \"on\"\n      default_statistics_target: \"100\"\n      dynamic_shared_memory_type: posix\n      effective_cache_size: 4191MB\n      effective_io_concurrency: \"1000\"\n      full_page_writes: \"on\"\n      idle_in_transaction_session_timeout: 30min\n      jit: \"off\"\n      lock_timeout: 1800s\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: \"0\"\n      log_rotation_size: \"0\"\n      log_truncate_on_rotation: \"false\"\n      logging_collector: \"on\"\n      maintenance_work_mem: 341MB\n      max_connections: \"100\"\n      max_parallel_maintenance_workers: \"2\"\n      max_parallel_workers: \"4\"\n      max_parallel_workers_per_gather: \"2\"\n      max_replication_slots: \"32\"\n      max_wal_size: 4GB\n      max_worker_processes: \"4\"\n      min_wal_size: 1GB\n      pg_stat_statements.max: \"10000\"\n      pg_stat_statements.save: \"on\"\n      pg_stat_statements.track: all\n      pg_stat_statements.track_planning: \"off\"\n      pg_stat_statements.track_utility: \"on\"\n      random_page_cost: \"1.1\"\n      shared_buffers: 1397MB\n      shared_memory_type: mmap\n      shared_preload_libraries: \"\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      statement_timeout: 7200s\n      wal_buffers: 16MB\n      wal_compression: lz4\n      wal_keep_size: 512MB\n      wal_level: logical\n      wal_log_hints: \"on\"\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n      work_mem: 4988kB\n    shared_preload_libraries:\n    - pg_stat_statements\n    - auto_explain\n    - pg_cron\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: restart\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  resources:\n    limits:\n      cpu: 2628m\n      memory: 5458Mi\n    requests:\n      cpu: 2628m\n      memory: 5458Mi\n  smartShutdownTimeout: 180\n  startDelay: 3600\n  stopDelay: 1800\n  storage:\n    resizeInUseVolumes: true\n    size: 128Gi\n    storageClass: 6400-iops-212mb-tp-premium-v2\n  superuserSecret:\n    name: postgres-superuser-secret\n  switchoverDelay: 30\n  walStorage:\n    resizeInUseVolumes: true\n    size: 32Gi\n    storageClass: 6400-iops-212mb-tp-premium-v2\nstatus:\n  availableArchitectures:\n  - goArch: amd64\n    hash: 58242fc95faa81d6bba0ba19ea959b82d9b49dc5f8e6c755f3c663665ff0ce1d\n  - goArch: arm64\n    hash: 8e5ef490a05bbd5700047edda5e2440acd14bafa256dbfcc7d4a5107573df75b\n  certificates:\n    clientCASecret: staging-pg-common-ca\n    expirations:\n      staging-pg-common-ca: 2025-03-10 19:04:52 +0000 UTC\n      staging-pg-common-replication: 2025-03-10 19:04:52 +0000 UTC\n      staging-pg-common-server: 2025-03-10 19:04:52 +0000 UTC\n    replicationTLSSecret: staging-pg-common-replication\n    serverAltDNSNames:\n    - staging-pg-common-rw\n    - staging-pg-common-rw.db\n    - staging-pg-common-rw.db.svc\n    - staging-pg-common-rw.db.svc.cluster.local\n    - staging-pg-common-r\n    - staging-pg-common-r.db\n    - staging-pg-common-r.db.svc\n    - staging-pg-common-r.db.svc.cluster.local\n    - staging-pg-common-ro\n    - staging-pg-common-ro.db\n    - staging-pg-common-ro.db.svc\n    - staging-pg-common-ro.db.svc.cluster.local\n    serverCASecret: staging-pg-common-ca\n    serverTLSSecret: staging-pg-common-server\n  cloudNativePGCommitHash: bad5a251\n  cloudNativePGOperatorHash: 58242fc95faa81d6bba0ba19ea959b82d9b49dc5f8e6c755f3c663665ff0ce1d\n  conditions:\n  - lastTransitionTime: \"2025-01-08T16:19:02Z\"\n    message: Cluster is Ready\n    reason: ClusterIsReady\n    status: \"True\"\n    type: Ready\n  - lastTransitionTime: \"2024-12-23T15:21:09Z\"\n    message: Continuous archiving is working\n    reason: ContinuousArchivingSuccess\n    status: \"True\"\n    type: ContinuousArchiving\n  - lastTransitionTime: \"2025-01-08T17:00:00Z\"\n    message: New Backup starting up\n    reason: BackupStarted\n    status: \"False\"\n    type: LastBackupSucceeded\n  configMapResourceVersion:\n    metrics:\n      cnpg-default-monitoring: \"445853326\"\n  currentPrimary: staging-pg-common-3\n  currentPrimaryTimestamp: \"2024-12-23T15:21:05.448655Z\"\n  firstRecoverabilityPoint: \"2024-12-10T20:08:33Z\"\n  firstRecoverabilityPointByMethod:\n    barmanObjectStore: \"2024-12-11T00:00:02Z\"\n    volumeSnapshot: \"2024-12-10T20:08:33Z\"\n  healthyPVC:\n  - staging-pg-common-1\n  - staging-pg-common-1-wal\n  - staging-pg-common-3\n  - staging-pg-common-3-wal\n  - staging-pg-common-4\n  - staging-pg-common-4-wal\n  image: docker.redacted.net/custom-postgres17:17.2@sha256:9d93740c5fcfa7b3113815d6118f8a0afe35dcef38775dae2b93f6d7badedc91\n  instanceNames:\n  - staging-pg-common-1\n  - staging-pg-common-3\n  - staging-pg-common-4\n  instances: 3\n  instancesReportedState:\n    staging-pg-common-1:\n      isPrimary: false\n      timeLineID: 4\n    staging-pg-common-3:\n      isPrimary: true\n      timeLineID: 4\n    staging-pg-common-4:\n      isPrimary: false\n      timeLineID: 4\n  instancesStatus:\n    healthy:\n    - staging-pg-common-1\n    - staging-pg-common-3\n    - staging-pg-common-4\n  lastSuccessfulBackup: \"2025-01-08T16:05:41Z\"\n  lastSuccessfulBackupByMethod:\n    barmanObjectStore: \"2025-01-08T00:00:17Z\"\n    volumeSnapshot: \"2025-01-08T16:05:41Z\"\n  latestGeneratedNode: 4\n  managedRolesStatus: {}\n  phase: Cluster in healthy state\n  poolerIntegrations:\n    pgBouncerIntegration:\n      secrets:\n      - staging-pg-common-pooler\n  pvcCount: 6\n  readService: staging-pg-common-r\n  readyInstances: 3\n  secretsResourceVersion:\n    clientCaSecretVersion: \"445853293\"\n    replicationSecretVersion: \"445853295\"\n    serverCaSecretVersion: \"445853293\"\n    serverSecretVersion: \"445853294\"\n    superuserSecretVersion: \"445853282\"\n  switchReplicaClusterStatus: {}\n  targetPrimary: staging-pg-common-3\n  targetPrimaryTimestamp: \"2024-12-23T15:20:52.825593Z\"\n  timelineID: 4\n  topology:\n    instances:\n      staging-pg-common-1: {}\n      staging-pg-common-3: {}\n      staging-pg-common-4: {}\n    nodesUsed: 3\n    successfullyExtracted: true\n  writeService: staging-pg-common-rw\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.506414715Z\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\",\"version\":\"1.25.0\",\"build\":{\"Version\":\"1.25.0\",\"Commit\":\"bad5a251\",\"Date\":\"2024-12-23\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.506595945Z\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.507873931Z\",\"msg\":\"Detected low-disk space condition, avoid starting the instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.519044943Z\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\",\"version\":\"1.25.0\",\"build\":{\"Version\":\"1.25.0\",\"Commit\":\"bad5a251\",\"Date\":\"2024-12-23\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.519064272Z\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.519933252Z\",\"msg\":\"Detected low-disk space condition, avoid starting the instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.531096897Z\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\",\"version\":\"1.25.0\",\"build\":{\"Version\":\"1.25.0\",\"Commit\":\"bad5a251\",\"Date\":\"2024-12-23\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.531116985Z\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.531974061Z\",\"msg\":\"Detected low-disk space condition, avoid starting the instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.542079719Z\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\",\"version\":\"1.25.0\",\"build\":{\"Version\":\"1.25.0\",\"Commit\":\"bad5a251\",\"Date\":\"2024-12-23\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.54210308Z\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.543052519Z\",\"msg\":\"Detected low-disk space condition, avoid starting the instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.554194298Z\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\",\"version\":\"1.25.0\",\"build\":{\"Version\":\"1.25.0\",\"Commit\":\"bad5a251\",\"Date\":\"2024-12-23\"}}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.554224299Z\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n{\"level\":\"info\",\"ts\":\"2025-01-07T23:59:16.555176234Z\",\"msg\":\"Detected low-disk space condition, avoid starting the instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"staging-pg-common-1\"}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductI experience similar behaviour:\nKubernetes 1.28.6\nOperator: 1.24.1\nPostgress: ghcr.io/cloudnative-pg/postgresql:15.10\nI have 3 instances. Keycloak is running and hitting the rw service instance.\nI deleted the rw pod instance ~ 11:15. The cluster healed itself, but the WALs of the other instances start increaseing the WALs until the PVC's are full.\nIt took approx 3 hours when it started increasing.\n```\nspec:\n  affinity:\n    podAntiAffinityType: preferred\n  backup:\n    barmanObjectStore:\n      data:\n        compression: gzip\n        jobs: 2\n      destinationPath: s3://testns/\n      endpointURL: http://minio-svc.minio:9000\n      s3Credentials:\n        accessKeyId:\n          key: accessKey\n          name: cnpg-secret-backup-s3\n        secretAccessKey:\n          key: secretKey\n          name: cnpg-secret-backup-s3\n      wal:\n        compression: gzip\n    retentionPolicy: 1d\n    target: prefer-standby\n  bootstrap:\n    initdb:\n      database: app\n      encoding: UTF8\n      localeCType: C\n      localeCollate: C\n      owner: app\n  enablePDB: true\n  enableSuperuserAccess: false\n  failoverDelay: 0\n  imageName: docker.localhost/ghcr.io/cloudnative-pg/postgresql:15.10\n  instances: 3\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n      - key: queries\n        name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: true\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: 'on'\n      archive_timeout: 5min\n      dynamic_shared_memory_type: posix\n      full_page_writes: 'on'\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: '0'\n      log_rotation_size: '0'\n      log_truncate_on_rotation: 'false'\n      logging_collector: 'on'\n      max_parallel_workers: '32'\n      max_replication_slots: '32'\n      max_worker_processes: '32'\n      shared_memory_type: mmap\n      shared_preload_libraries: ''\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      wal_keep_size: 512MB\n      wal_level: logical\n      wal_log_hints: 'on'\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: restart\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  resources: {}\n  smartShutdownTimeout: 180\n  startDelay: 3600\n  stopDelay: 1800\n  storage:\n    resizeInUseVolumes: true\n    size: 2Gi\n    storageClass: standard-retained\n  switchoverDelay: 3600\n  walStorage:\n    resizeInUseVolumes: true\n    size: 2Gi\n```\nCN metrics:\n![Image](https://github.com/user-attachments/assets/26bf5d48-8036-4d95-ae6e-3b57343dac95)\nPVC:\n![Image](https://github.com/user-attachments/assets/4b0ad866-6061-4ba1-b5cc-925d7dcc06dd)\n---\n> Expected behavior:\n>\n> * WAL cleanup should occur consistently across all instances\n> * Role changes should not affect WAL cleanup mechanisms\n> * No out-of-disk space scenarios due to retained WAL after failovers\nI don't think this is quite right; role changed impact the algorithms for deciding when WAL can be cleaned up, and there are many workload related reasons it can be inconsistent.\nCan we collect specific data on replication lag, backup/RPO lag, and logical replication? I think we'd need to at least establish status of backups and all slots, since those directly impact WAL cleanup\n---\n@gpcmol: Can you provide the replication slot status for your primary? Are you on Azure AKS by any chance?\n@ardentperf: The two issues that I'm seeing:\n- replication slots holding onto the WAL for volume snapshots that failed due to networking issues (either TLS handshake, or connection reset) when trying to access the backup endpoint.\n- if a standby is brought up via volume snapshot -- it seems like if the wal volume on the primary was full, that it may remain full on the standbys as well.\nSometimes the entire cluster runs out of wal space, other times it's two instances (the primary and one standby) whereas the other standby is fine.\n---\n@jmealo unforunaltely, I can't hunt down the status again, since we had to restart everything again. But now I know what to do. I'm trying to reproduce the issue again, by deleting the rw instance, put some load and wait. No success still.\nWe're on our own data center's kubernetes (on VM-ware).\n---\nI think this may be explained by #6599.\n---\n@ardentperf @gpcmol I'd be interested in your thoughts/feedback on: #6617\n---\nWe ran into the same issue,\nKubernetes 1.31 (on AKS)\ncloudnative-pg: 1.24.1 (helm chart: 0.22.1)\npostgres:15.1 (a supabase fork)"
    },
    {
        "title": "[Tests]: properly test kubectl plugin",
        "id": 2772684820,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIn the 1.25 we've had a regression in the `promote` plugin that we did not catch. (see #6475  )\nWe don't have good coverage for the plugin.\nIn addition, the times we do test plugin commands, calling them in the E2E tests, they are called via `kubectl cnpg` command to the shell.\nThis has two clear weaknesses:\n- The kubectl plugin needs to be built and installed ahead of running the test\n- The test might invoke a previously-installed version of the plugin, and mask regressions\n### Describe the solution you'd like\n1. Refactor the plugin code so that all the commands can be called internally as library functions\n2. Comprehensive tests for the plugin called as library functions. Unit tests if possible, E2E otherwise\n3. Update any existing code that was calling the plugins via `kubectl`\n### Describe alternatives you've considered\nnone\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIn the 1.25 we've had a regression in the `promote` plugin that we did not catch. (see #6475  )\nWe don't have good coverage for the plugin.\nIn addition, the times we do test plugin commands, calling them in the E2E tests, they are called via `kubectl cnpg` command to the shell.\nThis has two clear weaknesses:\n- The kubectl plugin needs to be built and installed ahead of running the test\n- The test might invoke a previously-installed version of the plugin, and mask regressions\n### Describe the solution you'd like\n1. Refactor the plugin code so that all the commands can be called internally as library functions\n2. Comprehensive tests for the plugin called as library functions. Unit tests if possible, E2E otherwise\n3. Update any existing code that was calling the plugins via `kubectl`\n### Describe alternatives you've considered\nnone\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductHello is there a workaround for this behavior."
    },
    {
        "title": "fix: ensure operator version is reconciled",
        "id": 2766084163,
        "state": "open",
        "first": "In the current code, the `operatorVersion` annotation is set manually when creating resources. This has led to several consistency issues, as it is not always invoked.\r\nThis patch proposes a new approach that integrates the `operatorVersion` reconciliation within the `GetFixedInheritedAnnotations` function. This method offers several advantages:\r\n- The function is already called throughout the code, making it easy to implement the change consistently.\r\n- It simplifies code maintenance since the `operatorVersion` no longer requires a separate handling approach from the developer's perspective.\r\nCloses #6457 \r\n## Note for reviewers\r\nWe can consider to drop the operatorVersion on resources",
        "messages": "In the current code, the `operatorVersion` annotation is set manually when creating resources. This has led to several consistency issues, as it is not always invoked.\r\nThis patch proposes a new approach that integrates the `operatorVersion` reconciliation within the `GetFixedInheritedAnnotations` function. This method offers several advantages:\r\n- The function is already called throughout the code, making it easy to implement the change consistently.\r\n- It simplifies code maintenance since the `operatorVersion` no longer requires a separate handling approach from the developer's perspective.\r\nCloses #6457 \r\n## Note for reviewers\r\nWe can consider to drop the operatorVersion on resources/test limit=local\n---\n/ok-to-merge green tests only one unrelated failure"
    },
    {
        "title": "[Bug]: Create script to update the release-X.Y everywhere",
        "id": 2765767037,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nCurrently after we release a new minor version we need to add a the new branch in many places and when we remove one we have to remove the branch from many places, we need to create an script that has the capability to do this just triggering from the release procedure and do:\n- add the release-X.Y branch string into every place is required (mostly yaml files)\n- remove the release-X.Y branch string from every place where is added (mostly yaml files)\nThe list of files and process to trigger it is in the contribute/release_procedure.md\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.25 (latest patch)\n### What version of Kubernetes are you using?\n1.32\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nCurrently after we release a new minor version we need to add a the new branch in many places and when we remove one we have to remove the branch from many places, we need to create an script that has the capability to do this just triggering from the release procedure and do:\n- add the release-X.Y branch string into every place is required (mostly yaml files)\n- remove the release-X.Y branch string from every place where is added (mostly yaml files)\nThe list of files and process to trigger it is in the contribute/release_procedure.md\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conductso what should be done if it is not done yet i can do that and please explain what would be definition of done?"
    },
    {
        "title": "[Docs]: No egress network policy provided to showcase how to setup cluster requirements on restricted environments",
        "id": 2760139130,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThe [networking section of the documentation](https://cloudnative-pg.io/documentation/current/networking/) explains what do to in the case of restricted environments, which need extra network policy definitions for ingress communication between the operator and the instance. However, neither in the [provided example](https://cloudnative-pg.io/documentation/current/samples/networkpolicy-example.yaml) nor the docs, no section explaining the egress setup is present.\nThis is an issue for less experience users, as showcased by #3738 and #1192.\n### Describe what additions need to be done to the documentation\n- Extend [the given example](https://cloudnative-pg.io/documentation/current/samples/networkpolicy-example.yaml) to showcase how to setup egress policies.\n- Add a dedicated paragraph about egress traffic requirements in the docs.\n### Describe what pages need to change in the documentation, if any\n- [The Networking page](https://cloudnative-pg.io/documentation/current/networking/).\n- [The given example](https://cloudnative-pg.io/documentation/current/samples/networkpolicy-example.yaml).\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThe [networking section of the documentation](https://cloudnative-pg.io/documentation/current/networking/) explains what do to in the case of restricted environments, which need extra network policy definitions for ingress communication between the operator and the instance. However, neither in the [provided example](https://cloudnative-pg.io/documentation/current/samples/networkpolicy-example.yaml) nor the docs, no section explaining the egress setup is present.\nThis is an issue for less experience users, as showcased by #3738 and #1192.\n### Describe what additions need to be done to the documentation\n- Extend [the given example](https://cloudnative-pg.io/documentation/current/samples/networkpolicy-example.yaml) to showcase how to setup egress policies.\n- Add a dedicated paragraph about egress traffic requirements in the docs.\n### Describe what pages need to change in the documentation, if any\n- [The Networking page](https://cloudnative-pg.io/documentation/current/networking/).\n- [The given example](https://cloudnative-pg.io/documentation/current/samples/networkpolicy-example.yaml).\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bugs]: cnpg.io/operatorVersion is not updated coherently",
        "id": 2758144530,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThe current documentation for the `cnpg.io/operatorVersion` annotation describes it as `Version of the operator`.\nThat annotation is present on the resources that are directly created by the operator.\nIt is not clear whether it is the version of the operator that has **created** or **last reconciled** the resources though. It seems that, depending on the resource, it assumes one of the two values. For instance, Pods annotation contains the former while services and secrets contain the latter.\n### Describe what additions need to be done to the documentation\nIt would be useful enhancing the documentation of that annotation clarify its meaning across different resources.\n### Describe what pages need to change in the documentation, if any\nhttps://cloudnative-pg.io/documentation/current/labels_annotations/\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThe current documentation for the `cnpg.io/operatorVersion` annotation describes it as `Version of the operator`.\nThat annotation is present on the resources that are directly created by the operator.\nIt is not clear whether it is the version of the operator that has **created** or **last reconciled** the resources though. It seems that, depending on the resource, it assumes one of the two values. For instance, Pods annotation contains the former while services and secrets contain the latter.\n### Describe what additions need to be done to the documentation\nIt would be useful enhancing the documentation of that annotation clarify its meaning across different resources.\n### Describe what pages need to change in the documentation, if any\nhttps://cloudnative-pg.io/documentation/current/labels_annotations/\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: PostgreSQL CrashLoopBackOff Due to Missing pg_tblspc on Kubernetes Node Restart",
        "id": 2757023355,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nIn our Kubernetes environments, we are encountering an issue where PostgreSQL fails to start after a node reboot, entering a CrashLoopBackOff state. The error indicates that the pg_tblspc directory does not exist.\nPostgreSQL enters a CrashLoopBackOff state, reporting that the pg_tblspc directory does not exist.\nManually creating the missing directory under the PGDATA directory sometimes resolves the issue.\nOccasionally, any newly created empty directory under PGDATA (including pg_tblspc) is immediately deleted automatically.\nExpected Behavior:\n**Additional Information:**\nWe are using the OpenEBS storage plugin on a self-deployed Kubernetes cluster (official release version).\nThis issue occurs intermittently across different environments but consistently post node reboots.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: cnpg-cluster\n    meta.helm.sh/release-namespace: pg-system\n  creationTimestamp: \"2024-12-16T11:11:36Z\"\n  generation: 1\n  labels:\n    app.kubernetes.io/instance: cnpg-cluster\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: cnpg-cluster\n    app.kubernetes.io/version: 1.1.0\n    helm.sh/chart: cnpg-cluster-1.1.7\n    helm.toolkit.fluxcd.io/name: cnpg-cluster\n    helm.toolkit.fluxcd.io/namespace: pg-system\n  name: shop\n  namespace: pg-system\n  resourceVersion: \"9082\"\n  uid: c9d0260d-f0d1-48d9-952f-b1fceaf0b5aa\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: app/postgres\n            operator: Exists\n    podAntiAffinityType: preferred\n    tolerations:\n    - effect: NoSchedule\n      key: app/postgres\n      operator: Exists\n    topologyKey: kubernetes.io/hostname\n  bootstrap:\n    initdb:\n      database: example\n      encoding: UTF8\n      localeCType: C\n      localeCollate: C\n      owner: postgres\n      postInitApplicationSQLRefs:\n        secretRefs:\n        - key: init-user.sql\n          name: main-cluster-init-sql\n      secret:\n        name: appuser-postgres\n  description: app postgresql cluster\n  enablePDB: true\n  enableSuperuserAccess: true\n  env:\n  - name: TZ\n    value: Asia/Shanghai\n  failoverDelay: 120\n  imageName: app-prod.mymycloud.com/app/image/cloudnative-pg/postgresql:15.10-15\n  imagePullSecrets:\n  - name: private-registry\n  instances: 1\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n    - key: queries\n      name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: false\n  nodeMaintenanceWindow:\n    inProgress: false\n    reusePVC: true\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: \"on\"\n      archive_timeout: 5min\n      dynamic_shared_memory_type: posix\n      effective_cache_size: 1536MB\n      effective_io_concurrency: \"200\"\n      full_page_writes: \"on\"\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: \"0\"\n      log_rotation_size: \"0\"\n      log_truncate_on_rotation: \"false\"\n      logging_collector: \"on\"\n      maintenance_work_mem: 128MB\n      max_connections: \"1000\"\n      max_parallel_workers: \"32\"\n      max_replication_slots: \"32\"\n      max_wal_size: 2GB\n      max_worker_processes: \"32\"\n      pg_stat_statements.max: \"10000\"\n      pg_stat_statements.track: all\n      shared_buffers: 512MB\n      shared_memory_type: mmap\n      shared_preload_libraries: \"\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.1\n      wal_buffers: 16MB\n      wal_keep_size: 1024MB\n      wal_level: logical\n      wal_log_hints: \"on\"\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: restart\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  resources:\n    limits:\n      cpu: \"3\"\n      memory: 4Gi\n    requests:\n      cpu: 100m\n      memory: 512Mi\n  smartShutdownTimeout: 180\n  startDelay: 300\n  stopDelay: 300\n  storage:\n    resizeInUseVolumes: true\n    size: 10Gi\n    storageClass: local\n  superuserSecret:\n    name: superuser-postgres\n  switchoverDelay: 3600\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"shop-1\",\"version\":\"1.24.0\",\"build\":{\"Version\":\"1.24.0\",\"Commit\":\"5fe5bb6b\",\"Date\":\"2024-08-22\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"setup\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"setup\",\"msg\":\"starting tablespace manager\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"setup\",\"msg\":\"starting external server manager\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting webserver\",\"logging_pod\":\"shop-1\",\"address\":\"localhost:8010\",\"hasTLS\":false}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting webserver\",\"logging_pod\":\"shop-1\",\"address\":\":9187\",\"hasTLS\":false}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting webserver\",\"logging_pod\":\"shop-1\",\"address\":\":8000\",\"hasTLS\":true}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"http: TLS handshake error from 10.32.0.124:41306: tls: no certificates configured\\n\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"http: TLS handshake error from 10.32.0.124:41318: tls: no certificates configured\\n\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"http: TLS handshake error from 10.32.0.124:41328: tls: no certificates configured\\n\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Cluster status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"shop\",\"namespace\":\"pg-system\"},\"namespace\":\"pg-system\",\"name\":\"shop\",\"reconcileID\":\"6c3496ea-0d98-4a66-96ac-ac7ca2cdd06a\",\"logging_pod\":\"shop-1\",\"currentPrimary\":\"shop-1\",\"targetPrimary\":\"shop-1\",\"isReplicaCluster\":false}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"shop-1\",\"reason\":\"postmaster start up\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1201\\nCatalog version number:               201909212\\nDatabase system identifier:           7425160321072427032\\nDatabase cluster state:               in production\\npg_control last modified:             Tue 12 Nov 2024 05:39:57 PM CST\\nLatest checkpoint location:           23/5507BA40\\nLatest checkpoint's REDO location:    23/5507BA08\\nLatest checkpoint's REDO WAL file:    000000010000002300000055\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:5922229\\nLatest checkpoint's NextOID:          589825\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        479\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  5922229\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Tue 12 Nov 2024 05:39:16 PM CST\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              1000\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat4 argument passing:              by value\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            dbf6d642fcba1bc3e76e8a500dfb0dd9b09f4df72dde4559ba2a7c7d13c80202\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"postmaster started\",\"logging_pod\":\"shop-1\",\"postMasterPID\":24}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"shop-1\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): dial error: dial unix /controller/run/.s.PGSQL.5432: connect: no such file or directory\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.309 CST [24] LOG:  starting PostgreSQL 12.19 (Debian 12.19-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"pipe\":\"stderr\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.309 CST [24] LOG:  listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"pipe\":\"stderr\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.309 CST [24] LOG:  listening on IPv6 address \\\"::\\\", port 5432\",\"pipe\":\"stderr\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.326 CST [24] LOG:  listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"pipe\":\"stderr\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.500 CST [24] LOG:  could not open directory \\\"pg_tblspc\\\": No such file or directory\",\"pipe\":\"stderr\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.501 CST [24] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.501 CST [24] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.502 CST [24] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.502 CST [24] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:50.502 CST\",\"process_id\":\"24\",\"session_id\":\"6733402a.18\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:50.512 CST\",\"process_id\":\"26\",\"session_id\":\"6733402a.1a\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was interrupted; last known up at 2024-11-12 17:39:57 CST\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:50.512 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"27\",\"connection_from\":\"[local]\",\"session_id\":\"6733402a.1b\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"shop\",\"namespace\":\"pg-system\"},\"namespace\":\"pg-system\",\"name\":\"shop\",\"reconcileID\":\"6c3496ea-0d98-4a66-96ac-ac7ca2cdd06a\",\"logging_pod\":\"shop-1\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:50.515 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"28\",\"connection_from\":\"[local]\",\"session_id\":\"6733402a.1c\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:50.529 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"29\",\"connection_from\":\"[local]\",\"session_id\":\"6733402a.1d\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:51+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:51.617 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"31\",\"connection_from\":\"[local]\",\"session_id\":\"6733402b.1f\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:51 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:51+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:51.620 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"32\",\"connection_from\":\"[local]\",\"session_id\":\"6733402b.20\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:51 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:51+08:00\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"shop\",\"namespace\":\"pg-system\"},\"namespace\":\"pg-system\",\"name\":\"shop\",\"reconcileID\":\"756c85cb-d612-4bef-a540-82efb0e9da70\",\"logging_pod\":\"shop-1\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:51+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:51.828 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"33\",\"connection_from\":\"[local]\",\"session_id\":\"6733402b.21\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:51 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:51+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:51.887 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"34\",\"connection_from\":\"[local]\",\"session_id\":\"6733402b.22\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:51 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:51+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:51.901 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"35\",\"connection_from\":\"[local]\",\"session_id\":\"6733402b.23\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:51 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:51+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:51.958 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"36\",\"connection_from\":\"[local]\",\"session_id\":\"6733402b.24\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:51 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.232 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"37\",\"connection_from\":\"[local]\",\"session_id\":\"6733402c.25\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:52 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.721 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"40\",\"connection_from\":\"[local]\",\"session_id\":\"6733402c.28\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:52 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.722 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"41\",\"connection_from\":\"[local]\",\"session_id\":\"6733402c.29\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:52 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.725 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"42\",\"connection_from\":\"[local]\",\"session_id\":\"6733402c.2a\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:52 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"shop\",\"namespace\":\"pg-system\"},\"namespace\":\"pg-system\",\"name\":\"shop\",\"reconcileID\":\"2e811d4e-d72e-4283-8e1c-fc2a80412070\",\"logging_pod\":\"shop-1\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.812 CST\",\"process_id\":\"26\",\"session_id\":\"6733402a.1a\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"58P01\",\"message\":\"could not open directory \\\"pg_tblspc\\\": No such file or directory\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.812 CST\",\"process_id\":\"26\",\"session_id\":\"6733402a.1a\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"58P01\",\"message\":\"could not open directory \\\"pg_tblspc\\\": No such file or directory\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.812 CST\",\"process_id\":\"26\",\"session_id\":\"6733402a.1a\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"58P01\",\"message\":\"could not open directory \\\"pg_replslot\\\": No such file or directory\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.821 CST\",\"process_id\":\"24\",\"session_id\":\"6733402a.18\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 26) exited with exit code 1\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.821 CST\",\"process_id\":\"24\",\"session_id\":\"6733402a.18\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"aborting startup due to startup process failure\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.830 CST\",\"process_id\":\"24\",\"session_id\":\"6733402a.18\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"msg\":\"postmaster exited\",\"logging_pod\":\"shop-1\",\"postmasterExitStatus\":\"exit status 1\",\"postMasterPID\":24}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"shop-1\",\"reason\":\"postmaster has exited\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1201\\nCatalog version number:               201909212\\nDatabase system identifier:           7425160321072427032\\nDatabase cluster state:               in production\\npg_control last modified:             Tue 12 Nov 2024 05:39:57 PM CST\\nLatest checkpoint location:           23/5507BA40\\nLatest checkpoint's REDO location:    23/5507BA08\\nLatest checkpoint's REDO WAL file:    000000010000002300000055\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:5922229\\nLatest checkpoint's NextOID:          589825\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        479\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  5922229\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Tue 12 Nov 2024 05:39:16 PM CST\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              1000\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat4 argument passing:              by value\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            dbf6d642fcba1bc3e76e8a500dfb0dd9b09f4df72dde4559ba2a7c7d13c80202\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:53+08:00\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"shop\",\"namespace\":\"pg-system\"},\"namespace\":\"pg-system\",\"name\":\"shop\",\"reconcileID\":\"57a72a24-aa52-463f-b1bb-ee80ef3ab457\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:54+08:00\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"shop\",\"namespace\":\"pg-system\"},\"namespace\":\"pg-system\",\"name\":\"shop\",\"reconcileID\":\"8bfba071-9159-4b74-8c52-091d1d5bda0d\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"shop-1\",\"err\":\"context canceled\"}\n{\"level\":\"error\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"DB not available\",\"logging_pod\":\"shop-1\",\"error\":\"context canceled\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.configureInstancePermissions\\n\\tinternal/cmd/manager/instance/run/lifecycle/run.go:160\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).runPostgresAndWait.func1.1\\n\\tinternal/cmd/manager/instance/run/lifecycle/run.go:108\"}\n{\"level\":\"error\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Unable to update PostgreSQL roles and permissions\",\"logging_pod\":\"shop-1\",\"error\":\"while verifying super user DB connection: context canceled\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).runPostgresAndWait.func1.1\\n\\tinternal/cmd/manager/instance/run/lifecycle/run.go:109\"}\n{\"level\":\"error\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"PostgreSQL process exited with errors\",\"logging_pod\":\"shop-1\",\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start.func1\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:104\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:112\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/manager/runnable_group.go:226\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Stopping and waiting for leader election runnables\"}\n{\"level\":\"error\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"error received after stop sequence was engaged\",\"error\":\"exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).engageStopProcedure.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/manager/internal.go:499\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Webserver exited\",\"logging_pod\":\"shop-1\",\"address\":\":9187\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Webserver exited\",\"logging_pod\":\"shop-1\",\"address\":\"localhost:8010\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Webserver exited\",\"logging_pod\":\"shop-1\",\"address\":\":8000\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Stopping and waiting for caches\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"pkg/mod/k8s.io/client-go@v0.30.3/tools/cache/reflector.go:232: watch of *v1.Cluster ended with: an error on the server (\\\"unable to decode an event from the watch stream: context canceled\\\") has prevented the request from succeeding\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Stopping and waiting for webhooks\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Stopping and waiting for HTTP servers\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"logger\":\"setup\",\"msg\":\"Checking for free disk space for WALs after PostgreSQL finished\",\"logging_pod\":\"shop-1\"}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nIn our Kubernetes environments, we are encountering an issue where PostgreSQL fails to start after a node reboot, entering a CrashLoopBackOff state. The error indicates that the pg_tblspc directory does not exist.\nPostgreSQL enters a CrashLoopBackOff state, reporting that the pg_tblspc directory does not exist.\nManually creating the missing directory under the PGDATA directory sometimes resolves the issue.\nOccasionally, any newly created empty directory under PGDATA (including pg_tblspc) is immediately deleted automatically.\nExpected Behavior:\n**Additional Information:**\nWe are using the OpenEBS storage plugin on a self-deployed Kubernetes cluster (official release version).\nThis issue occurs intermittently across different environments but consistently post node reboots.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: cnpg-cluster\n    meta.helm.sh/release-namespace: pg-system\n  creationTimestamp: \"2024-12-16T11:11:36Z\"\n  generation: 1\n  labels:\n    app.kubernetes.io/instance: cnpg-cluster\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: cnpg-cluster\n    app.kubernetes.io/version: 1.1.0\n    helm.sh/chart: cnpg-cluster-1.1.7\n    helm.toolkit.fluxcd.io/name: cnpg-cluster\n    helm.toolkit.fluxcd.io/namespace: pg-system\n  name: shop\n  namespace: pg-system\n  resourceVersion: \"9082\"\n  uid: c9d0260d-f0d1-48d9-952f-b1fceaf0b5aa\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: app/postgres\n            operator: Exists\n    podAntiAffinityType: preferred\n    tolerations:\n    - effect: NoSchedule\n      key: app/postgres\n      operator: Exists\n    topologyKey: kubernetes.io/hostname\n  bootstrap:\n    initdb:\n      database: example\n      encoding: UTF8\n      localeCType: C\n      localeCollate: C\n      owner: postgres\n      postInitApplicationSQLRefs:\n        secretRefs:\n        - key: init-user.sql\n          name: main-cluster-init-sql\n      secret:\n        name: appuser-postgres\n  description: app postgresql cluster\n  enablePDB: true\n  enableSuperuserAccess: true\n  env:\n  - name: TZ\n    value: Asia/Shanghai\n  failoverDelay: 120\n  imageName: app-prod.mymycloud.com/app/image/cloudnative-pg/postgresql:15.10-15\n  imagePullSecrets:\n  - name: private-registry\n  instances: 1\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n    - key: queries\n      name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: false\n  nodeMaintenanceWindow:\n    inProgress: false\n    reusePVC: true\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: \"on\"\n      archive_timeout: 5min\n      dynamic_shared_memory_type: posix\n      effective_cache_size: 1536MB\n      effective_io_concurrency: \"200\"\n      full_page_writes: \"on\"\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: \"0\"\n      log_rotation_size: \"0\"\n      log_truncate_on_rotation: \"false\"\n      logging_collector: \"on\"\n      maintenance_work_mem: 128MB\n      max_connections: \"1000\"\n      max_parallel_workers: \"32\"\n      max_replication_slots: \"32\"\n      max_wal_size: 2GB\n      max_worker_processes: \"32\"\n      pg_stat_statements.max: \"10000\"\n      pg_stat_statements.track: all\n      shared_buffers: 512MB\n      shared_memory_type: mmap\n      shared_preload_libraries: \"\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.1\n      wal_buffers: 16MB\n      wal_keep_size: 1024MB\n      wal_level: logical\n      wal_log_hints: \"on\"\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: restart\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  resources:\n    limits:\n      cpu: \"3\"\n      memory: 4Gi\n    requests:\n      cpu: 100m\n      memory: 512Mi\n  smartShutdownTimeout: 180\n  startDelay: 300\n  stopDelay: 300\n  storage:\n    resizeInUseVolumes: true\n    size: 10Gi\n    storageClass: local\n  superuserSecret:\n    name: superuser-postgres\n  switchoverDelay: 3600\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"shop-1\",\"version\":\"1.24.0\",\"build\":{\"Version\":\"1.24.0\",\"Commit\":\"5fe5bb6b\",\"Date\":\"2024-08-22\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"setup\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"setup\",\"msg\":\"starting tablespace manager\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"setup\",\"msg\":\"starting external server manager\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting webserver\",\"logging_pod\":\"shop-1\",\"address\":\"localhost:8010\",\"hasTLS\":false}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting webserver\",\"logging_pod\":\"shop-1\",\"address\":\":9187\",\"hasTLS\":false}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting webserver\",\"logging_pod\":\"shop-1\",\"address\":\":8000\",\"hasTLS\":true}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"http: TLS handshake error from 10.32.0.124:41306: tls: no certificates configured\\n\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"http: TLS handshake error from 10.32.0.124:41318: tls: no certificates configured\\n\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"http: TLS handshake error from 10.32.0.124:41328: tls: no certificates configured\\n\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Cluster status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"shop\",\"namespace\":\"pg-system\"},\"namespace\":\"pg-system\",\"name\":\"shop\",\"reconcileID\":\"6c3496ea-0d98-4a66-96ac-ac7ca2cdd06a\",\"logging_pod\":\"shop-1\",\"currentPrimary\":\"shop-1\",\"targetPrimary\":\"shop-1\",\"isReplicaCluster\":false}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"shop-1\",\"reason\":\"postmaster start up\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1201\\nCatalog version number:               201909212\\nDatabase system identifier:           7425160321072427032\\nDatabase cluster state:               in production\\npg_control last modified:             Tue 12 Nov 2024 05:39:57 PM CST\\nLatest checkpoint location:           23/5507BA40\\nLatest checkpoint's REDO location:    23/5507BA08\\nLatest checkpoint's REDO WAL file:    000000010000002300000055\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:5922229\\nLatest checkpoint's NextOID:          589825\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        479\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  5922229\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Tue 12 Nov 2024 05:39:16 PM CST\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              1000\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat4 argument passing:              by value\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            dbf6d642fcba1bc3e76e8a500dfb0dd9b09f4df72dde4559ba2a7c7d13c80202\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"postmaster started\",\"logging_pod\":\"shop-1\",\"postMasterPID\":24}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"shop-1\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): dial error: dial unix /controller/run/.s.PGSQL.5432: connect: no such file or directory\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.309 CST [24] LOG:  starting PostgreSQL 12.19 (Debian 12.19-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"pipe\":\"stderr\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.309 CST [24] LOG:  listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"pipe\":\"stderr\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.309 CST [24] LOG:  listening on IPv6 address \\\"::\\\", port 5432\",\"pipe\":\"stderr\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.326 CST [24] LOG:  listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"pipe\":\"stderr\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.500 CST [24] LOG:  could not open directory \\\"pg_tblspc\\\": No such file or directory\",\"pipe\":\"stderr\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.501 CST [24] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.501 CST [24] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.502 CST [24] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"2024-11-12 19:46:50.502 CST [24] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:50.502 CST\",\"process_id\":\"24\",\"session_id\":\"6733402a.18\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:50.512 CST\",\"process_id\":\"26\",\"session_id\":\"6733402a.1a\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was interrupted; last known up at 2024-11-12 17:39:57 CST\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:50.512 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"27\",\"connection_from\":\"[local]\",\"session_id\":\"6733402a.1b\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"shop\",\"namespace\":\"pg-system\"},\"namespace\":\"pg-system\",\"name\":\"shop\",\"reconcileID\":\"6c3496ea-0d98-4a66-96ac-ac7ca2cdd06a\",\"logging_pod\":\"shop-1\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:50.515 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"28\",\"connection_from\":\"[local]\",\"session_id\":\"6733402a.1c\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:50+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:50.529 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"29\",\"connection_from\":\"[local]\",\"session_id\":\"6733402a.1d\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:51+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:51.617 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"31\",\"connection_from\":\"[local]\",\"session_id\":\"6733402b.1f\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:51 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:51+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:51.620 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"32\",\"connection_from\":\"[local]\",\"session_id\":\"6733402b.20\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:51 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:51+08:00\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"shop\",\"namespace\":\"pg-system\"},\"namespace\":\"pg-system\",\"name\":\"shop\",\"reconcileID\":\"756c85cb-d612-4bef-a540-82efb0e9da70\",\"logging_pod\":\"shop-1\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:51+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:51.828 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"33\",\"connection_from\":\"[local]\",\"session_id\":\"6733402b.21\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:51 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:51+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:51.887 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"34\",\"connection_from\":\"[local]\",\"session_id\":\"6733402b.22\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:51 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:51+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:51.901 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"35\",\"connection_from\":\"[local]\",\"session_id\":\"6733402b.23\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:51 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:51+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:51.958 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"36\",\"connection_from\":\"[local]\",\"session_id\":\"6733402b.24\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:51 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.232 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"37\",\"connection_from\":\"[local]\",\"session_id\":\"6733402c.25\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:52 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.721 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"40\",\"connection_from\":\"[local]\",\"session_id\":\"6733402c.28\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:52 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.722 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"41\",\"connection_from\":\"[local]\",\"session_id\":\"6733402c.29\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:52 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.725 CST\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"42\",\"connection_from\":\"[local]\",\"session_id\":\"6733402c.2a\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-12 19:46:52 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"shop\",\"namespace\":\"pg-system\"},\"namespace\":\"pg-system\",\"name\":\"shop\",\"reconcileID\":\"2e811d4e-d72e-4283-8e1c-fc2a80412070\",\"logging_pod\":\"shop-1\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.812 CST\",\"process_id\":\"26\",\"session_id\":\"6733402a.1a\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"58P01\",\"message\":\"could not open directory \\\"pg_tblspc\\\": No such file or directory\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.812 CST\",\"process_id\":\"26\",\"session_id\":\"6733402a.1a\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"58P01\",\"message\":\"could not open directory \\\"pg_tblspc\\\": No such file or directory\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.812 CST\",\"process_id\":\"26\",\"session_id\":\"6733402a.1a\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"58P01\",\"message\":\"could not open directory \\\"pg_replslot\\\": No such file or directory\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.821 CST\",\"process_id\":\"24\",\"session_id\":\"6733402a.18\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 26) exited with exit code 1\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.821 CST\",\"process_id\":\"24\",\"session_id\":\"6733402a.18\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"aborting startup due to startup process failure\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"shop-1\",\"record\":{\"log_time\":\"2024-11-12 19:46:52.830 CST\",\"process_id\":\"24\",\"session_id\":\"6733402a.18\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-11-12 19:46:50 CST\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"msg\":\"postmaster exited\",\"logging_pod\":\"shop-1\",\"postmasterExitStatus\":\"exit status 1\",\"postMasterPID\":24}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"shop-1\",\"reason\":\"postmaster has exited\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:52+08:00\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1201\\nCatalog version number:               201909212\\nDatabase system identifier:           7425160321072427032\\nDatabase cluster state:               in production\\npg_control last modified:             Tue 12 Nov 2024 05:39:57 PM CST\\nLatest checkpoint location:           23/5507BA40\\nLatest checkpoint's REDO location:    23/5507BA08\\nLatest checkpoint's REDO WAL file:    000000010000002300000055\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:5922229\\nLatest checkpoint's NextOID:          589825\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        479\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  5922229\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Tue 12 Nov 2024 05:39:16 PM CST\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              1000\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat4 argument passing:              by value\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            dbf6d642fcba1bc3e76e8a500dfb0dd9b09f4df72dde4559ba2a7c7d13c80202\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:53+08:00\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"shop\",\"namespace\":\"pg-system\"},\"namespace\":\"pg-system\",\"name\":\"shop\",\"reconcileID\":\"57a72a24-aa52-463f-b1bb-ee80ef3ab457\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:54+08:00\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"shop\",\"namespace\":\"pg-system\"},\"namespace\":\"pg-system\",\"name\":\"shop\",\"reconcileID\":\"8bfba071-9159-4b74-8c52-091d1d5bda0d\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"shop-1\",\"err\":\"context canceled\"}\n{\"level\":\"error\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"DB not available\",\"logging_pod\":\"shop-1\",\"error\":\"context canceled\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.configureInstancePermissions\\n\\tinternal/cmd/manager/instance/run/lifecycle/run.go:160\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).runPostgresAndWait.func1.1\\n\\tinternal/cmd/manager/instance/run/lifecycle/run.go:108\"}\n{\"level\":\"error\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Unable to update PostgreSQL roles and permissions\",\"logging_pod\":\"shop-1\",\"error\":\"while verifying super user DB connection: context canceled\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).runPostgresAndWait.func1.1\\n\\tinternal/cmd/manager/instance/run/lifecycle/run.go:109\"}\n{\"level\":\"error\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"PostgreSQL process exited with errors\",\"logging_pod\":\"shop-1\",\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start.func1\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:104\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:112\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/manager/runnable_group.go:226\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Stopping and waiting for leader election runnables\"}\n{\"level\":\"error\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"error received after stop sequence was engaged\",\"error\":\"exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).engageStopProcedure.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/manager/internal.go:499\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Webserver exited\",\"logging_pod\":\"shop-1\",\"address\":\":9187\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Webserver exited\",\"logging_pod\":\"shop-1\",\"address\":\"localhost:8010\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Webserver exited\",\"logging_pod\":\"shop-1\",\"address\":\":8000\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"shop-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Stopping and waiting for caches\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"pkg/mod/k8s.io/client-go@v0.30.3/tools/cache/reflector.go:232: watch of *v1.Cluster ended with: an error on the server (\\\"unable to decode an event from the watch stream: context canceled\\\") has prevented the request from succeeding\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Stopping and waiting for webhooks\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Stopping and waiting for HTTP servers\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\n{\"level\":\"info\",\"ts\":\"2024-11-12T19:46:55+08:00\",\"logger\":\"setup\",\"msg\":\"Checking for free disk space for WALs after PostgreSQL finished\",\"logging_pod\":\"shop-1\"}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductCan you elaborate more on why empty directories are removed? The operator doesn't do any of that.\n---\nThere is nothing in the CloudNativePG operator that deletes directories within pgdata. This issue likely originates from external factors. I recommend investigating the storage layer (e.g., OpenEBS) and the Kubernetes node configuration to identify potential causes of directory deletion or inconsistencies.\n---\nMissing files: pg_tblspc, pg_commit_ts, pg_twophase, pg_logical..\nA strange phenomenon is occurring: any empty directory created under the pgdata directory is immediately deleted.\n---\n> Can you elaborate more on why empty directories are removed? The operator doesn't do any of that.\nThe  empty directories like `pg_tblspc` ware lost after reboot the k8s node."
    },
    {
        "title": "chore(plugin-backup): use exit code `2` when encountering kube-api server errors",
        "id": 2756341121,
        "state": "open",
        "first": "Closes #6412",
        "messages": "Closes #6412e2e: https://github.com/EnterpriseDB/cloudnative-pg/actions/runs/12469651183\n---\n/test limit=local"
    },
    {
        "title": "[Bug]: Automated the creation of the release-* labels",
        "id": 2756114116,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nDuring the release process we should automated the creation of the release-X.Y labels and assign the new label to the opened PRs that are open and contains the other release-X.Y labels\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nDuring the release process we should automated the creation of the release-X.Y labels and assign the new label to the opened PRs that are open and contains the other release-X.Y labels\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "fix: slow configuration update when decreasing max_connections",
        "id": 2755970456,
        "state": "open",
        "first": "In case of a postgresql configuration update, in case of a decrease of a hot standby sensible parameter with an unsupervised method, the instance manager won't trigger a reconciliation loop of the operator after the primary server has been restarted.\r\nThis patch is doing that by patching the status with a known phase and reason.\r\nFix: #6409",
        "messages": "In case of a postgresql configuration update, in case of a decrease of a hot standby sensible parameter with an unsupervised method, the instance manager won't trigger a reconciliation loop of the operator after the primary server has been restarted.\r\nThis patch is doing that by patching the status with a known phase and reason.\r\nFix: #6409E2e: https://github.com/EnterpriseDB/cloudnative-pg/actions/runs/12467288471\n---\nE2e tests are green\n---\n/test limit=local"
    },
    {
        "title": "[Bug]: drain e2e can fail when pvcs can be mounted on different nodes",
        "id": 2753096221,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nYAML manifest\n### What happened?\nIn the e2e for the drain we are waiting firstly that there are no pods on the drained node, and secondly that the new primary pod is pending.\nThere is a chance that the pod is scheduled on a different node before our second check is run, so that the primary pod is never actually pending for that check, failing the test.\n[Related code](https://github.com/cloudnative-pg/cloudnative-pg/blob/6d2e5aa6e901a149fb4ac9b7994c1e62a4c1eb4f/tests/e2e/drain_node_test.go#L453-L467)\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nYAML manifest\n### What happened?\nIn the e2e for the drain we are waiting firstly that there are no pods on the drained node, and secondly that the new primary pod is pending.\nThere is a chance that the pod is scheduled on a different node before our second check is run, so that the primary pod is never actually pending for that check, failing the test.\n[Related code](https://github.com/cloudnative-pg/cloudnative-pg/blob/6d2e5aa6e901a149fb4ac9b7994c1e62a4c1eb4f/tests/e2e/drain_node_test.go#L453-L467)\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: E2e CreateOnDemandBackupViaKubectlPlugin is sensible to network problems",
        "id": 2752901250,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\n`CreateOnDemandBackupViaKubectlPlugin` is run once in the e2e. Occasionally, in case of network issues in the apiserver, this can fail. We could wrap it in a retry.\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\n`CreateOnDemandBackupViaKubectlPlugin` is run once in the e2e. Occasionally, in case of network issues in the apiserver, this can fail. We could wrap it in a retry.\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: declarative database management e2e test leaves unnecessary logs",
        "id": 2752836368,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\ntrunk (main)\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhenever the e2e run the declarative db management one leaves logs in the ci artifacts, even when they succeed.\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\ntrunk (main)\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhenever the e2e run the declarative db management one leaves logs in the ci artifacts, even when they succeed.\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: slow update of postgresql configuration",
        "id": 2752799051,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nIn case of a postgresql configuration update, in case of a decrease of a hot standby sensible parameter with an unsupervised method, it is possible for the primary to be updated while the standbys get stuck for a while at the older value until a reconciliation cycle happens.\nThat reconciliation cycle needs to be kickstarted earlier.\n[Relevant code section](https://github.com/cloudnative-pg/cloudnative-pg/blob/27fb8a6662dead86aec744f5ba7dc4268f15dd40/internal/management/controller/instance_controller.go#L1038-L1055)\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nIn case of a postgresql configuration update, in case of a decrease of a hot standby sensible parameter with an unsupervised method, it is possible for the primary to be updated while the standbys get stuck for a while at the older value until a reconciliation cycle happens.\nThat reconciliation cycle needs to be kickstarted earlier.\n[Relevant code section](https://github.com/cloudnative-pg/cloudnative-pg/blob/27fb8a6662dead86aec744f5ba7dc4268f15dd40/internal/management/controller/instance_controller.go#L1038-L1055)\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: volume snapsnot resources not collected in the tests",
        "id": 2752677894,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\ntrunk (main)\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nThe test contexts collected at the end of the e2e contain no information about the volume snapshots. They would be useful to have additional info while debugging snapshot tests failures.\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\ntrunk (main)\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nThe test contexts collected at the end of the e2e contain no information about the volume snapshots. They would be useful to have additional info while debugging snapshot tests failures.\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductI'm working on it :)"
    },
    {
        "title": "wip(pooler): fix custom ro edge cases",
        "id": 2752489826,
        "state": "open",
        "first": "Closes #5485 \r\n# Note for reviewers\r\n- PR is on hold; we need to decide how to approach the problem",
        "messages": "Closes #5485 \r\n# Note for reviewers\r\n- PR is on hold; we need to decide how to approach the problem"
    },
    {
        "title": "[Bug]: spammy log in the operator on volume snapshot backup",
        "id": 2752471938,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\ntrunk (main)\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhen logging a `Waiting for VolumeSnapshot to be provisioned` message, the `targetPod` is currently the entire pod definition, instead of simply the name of the pod. This makes the logs hard to read.\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\ntrunk (main)\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhen logging a `Waiting for VolumeSnapshot to be provisioned` message, the `targetPod` is currently the entire pod definition, instead of simply the name of the pod. This makes the logs hard to read.\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductWorking on it :)"
    },
    {
        "title": "[Bug]: duplicated fields in different log events",
        "id": 2749642392,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nWhen parsing log events from cnpg-cluster we're running into a issue where the JSON parser failes because of duplicate field in various log events.\nIn this particular case we identified that the fields logging_pod and logger are duplicated.\nlogging_pod seems to be duplicated only in instance-manager events. It is also striking that the logger field is also duplicated in an instance-manager event.\nWhere loggig_pod seems to be always the same, for logger we observed different values like \"logger\":\"roles_reconciler\" and \"logger\":\"instance-manager\"\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-12-19T08:13:25.165193242Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"cnpg-logging-test-cluster-2\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-logging-test-cluster\",\"namespace\":\"cnpg-cluster-loggin-test\"},\"namespace\":\"cnpg-cluster-loggin-test\",\"name\":\"cnpg-logging-test-cluster\",\"reconcileID\":\"5d9ff88c-bd5c-465e-af20-de28fceb09d5\",\"logging_pod\":\"cnpg-logging-test-cluster-2\",\"filename\":\"/controller/certificates/client-ca.crt\",\"secret\":\"cnpg-logging-test-cluster-ca\"}\n{\"level\":\"info\",\"ts\":\"2024-12-19T08:13:25.005347352Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logger\":\"instance-manager\",\"logging_pod\":\"cnpg-logging-test-cluster-2\"}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nWhen parsing log events from cnpg-cluster we're running into a issue where the JSON parser failes because of duplicate field in various log events.\nIn this particular case we identified that the fields logging_pod and logger are duplicated.\nlogging_pod seems to be duplicated only in instance-manager events. It is also striking that the logger field is also duplicated in an instance-manager event.\nWhere loggig_pod seems to be always the same, for logger we observed different values like \"logger\":\"roles_reconciler\" and \"logger\":\"instance-manager\"\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-12-19T08:13:25.165193242Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"cnpg-logging-test-cluster-2\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-logging-test-cluster\",\"namespace\":\"cnpg-cluster-loggin-test\"},\"namespace\":\"cnpg-cluster-loggin-test\",\"name\":\"cnpg-logging-test-cluster\",\"reconcileID\":\"5d9ff88c-bd5c-465e-af20-de28fceb09d5\",\"logging_pod\":\"cnpg-logging-test-cluster-2\",\"filename\":\"/controller/certificates/client-ca.crt\",\"secret\":\"cnpg-logging-test-cluster-ca\"}\n{\"level\":\"info\",\"ts\":\"2024-12-19T08:13:25.005347352Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logger\":\"instance-manager\",\"logging_pod\":\"cnpg-logging-test-cluster-2\"}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Support recent barman cloud compression",
        "id": 2748826545,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nProduction databases have a trade-off on backup size and CPU usage. Not all algorithms have the same ratios, and newer ones can even perform better for free.  When we were testing for HBase we found ZSTD and Lz4 to be good for live runs (I haven't tested xz for wal compression). When these requests have come up previously, the answers have been that Barman Cloud doesn't support other compression algos. I think that has changed with recent versions: https://github.com/EnterpriseDB/barman/releases/tag/release%2F3.12.0\n### Describe the solution you'd like\n- Update to 3.12.1 for barman\n- Then add zstd, lz4, and xz as possible compression algorithms for backup and wal\n### Describe alternatives you've considered\nSnappy doesn't compress very well, and gzip and bzip2 are both very poor trade offs for larger datasets in my experience.\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nProduction databases have a trade-off on backup size and CPU usage. Not all algorithms have the same ratios, and newer ones can even perform better for free.  When we were testing for HBase we found ZSTD and Lz4 to be good for live runs (I haven't tested xz for wal compression). When these requests have come up previously, the answers have been that Barman Cloud doesn't support other compression algos. I think that has changed with recent versions: https://github.com/EnterpriseDB/barman/releases/tag/release%2F3.12.0\n### Describe the solution you'd like\n- Update to 3.12.1 for barman\n- Then add zstd, lz4, and xz as possible compression algorithms for backup and wal\n### Describe alternatives you've considered\nSnappy doesn't compress very well, and gzip and bzip2 are both very poor trade offs for larger datasets in my experience.\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductHello @elliottneilclark \nCan you please add this issue also to https://github.com/cloudnative-pg/plugin-barman-cloud to keep a record of this ? Will be very appreciated, giving the fact that we are switching to the plugin format  :D \nRegards,"
    },
    {
        "title": "[Bug]: Readiness checks fail when primary is healthy / accepting connections / not in recovery",
        "id": 2748730619,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njeffreymealo@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nYAML manifest\n### What happened?\n- `kubectl cnpg status` hangs indefinitely\n- metrics for the primary are incomplete/unavailable\nI expect if the primary is healthy, accepting connections, and in a good state that the readiness probe will succeed.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"metadata\":{\"annotations\":{},\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"spec\":{\"affinity\":{\"nodeSelector\":{\"kubernetes.azure.com/agentpool\":\"pgcommon\",\"kubernetes.io/arch\":\"arm64\"},\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/postgres\",\"operator\":\"Exists\"}]},\"backup\":{\"barmanObjectStore\":{\"azureCredentials\":{\"connectionString\":{\"key\":\"AZURE_STORAGE_CONNECTION_STRING\",\"name\":\"postgres-backup-azure-creds\"}},\"data\":{\"compression\":\"snappy\",\"jobs\":2},\"destinationPath\":\"https://gisualproduction.blob.core.windows.net/postgres-backup\",\"wal\":{\"maxParallel\":2}},\"retentionPolicy\":\"30d\",\"target\":\"prefer-standby\",\"volumeSnapshot\":{\"className\":\"csi-azure-disk\",\"online\":true,\"onlineConfiguration\":{\"immediateCheckpoint\":false,\"waitForArchive\":true},\"snapshotOwnerReference\":\"cluster\"}},\"bootstrap\":{\"initdb\":{\"database\":\"gisual\",\"owner\":\"gisual\",\"secret\":{\"name\":\"postgres-gisual-db-credentials\"}}},\"enablePDB\":true,\"enableSuperuserAccess\":true,\"env\":[{\"name\":\"ENABLE_AZURE_PVC_UPDATES\",\"value\":\"true\"}],\"failoverDelay\":30,\"imageCatalogRef\":{\"apiGroup\":\"postgresql.cnpg.io\",\"kind\":\"ImageCatalog\",\"major\":17,\"name\":\"custom-postgres-images\"},\"imagePullSecrets\":[{\"name\":\"docker-registry-credentials\"}],\"instances\":3,\"monitoring\":{\"enablePodMonitor\":true},\"postgresql\":{\"parameters\":{\"autovacuum_max_workers\":\"2\",\"autovacuum_naptime\":\"20s\",\"autovacuum_vacuum_cost_limit\":\"1000\",\"autovacuum_vacuum_scale_factor\":\"0.1\",\"checkpoint_completion_target\":\"0.9\",\"checkpoint_timeout\":\"15min\",\"default_statistics_target\":\"100\",\"effective_cache_size\":\"4191MB\",\"effective_io_concurrency\":\"1000\",\"idle_in_transaction_session_timeout\":\"30min\",\"jit\":\"off\",\"lock_timeout\":\"1800s\",\"maintenance_work_mem\":\"341MB\",\"max_connections\":\"100\",\"max_parallel_maintenance_workers\":\"2\",\"max_parallel_workers\":\"4\",\"max_parallel_workers_per_gather\":\"2\",\"max_wal_size\":\"4GB\",\"max_worker_processes\":\"4\",\"min_wal_size\":\"1GB\",\"random_page_cost\":\"1.1\",\"shared_buffers\":\"1397MB\",\"statement_timeout\":\"7200s\",\"wal_buffers\":\"16MB\",\"wal_compression\":\"lz4\",\"work_mem\":\"4988kB\"}},\"primaryUpdateStrategy\":\"unsupervised\",\"resources\":{\"limits\":{\"cpu\":\"2628m\",\"memory\":\"5458Mi\"},\"requests\":{\"cpu\":\"2628m\",\"memory\":\"5458Mi\"}},\"storage\":{\"size\":\"258Gi\",\"storageClass\":\"6400-iops-212mb-tp-premium-v2\"},\"superuserSecret\":{\"name\":\"postgres-superuser-secret\"},\"switchoverDelay\":30,\"walStorage\":{\"size\":\"96Gi\",\"storageClass\":\"6400-iops-212mb-tp-premium-v2\"}}}\n    kubectl.kubernetes.io/restartedAt: \"2024-12-11T14:01:24-05:00\"\n  creationTimestamp: \"2024-12-10T22:08:24Z\"\n  generation: 5\n  name: production-pg-common\n  namespace: db\n  resourceVersion: \"840866221\"\n  uid: f02e579d-1d49-4cc2-ac9f-81a43fb28fe9\nspec:\n  affinity:\n    nodeSelector:\n      kubernetes.azure.com/agentpool: pgcommon\n      kubernetes.io/arch: arm64\n    podAntiAffinityType: preferred\n    tolerations:\n    - effect: NoSchedule\n      key: node-role.kubernetes.io/postgres\n      operator: Exists\n  backup:\n    barmanObjectStore:\n      azureCredentials:\n        connectionString:\n          key: AZURE_STORAGE_CONNECTION_STRING\n          name: postgres-backup-azure-creds\n      data:\n        compression: snappy\n        jobs: 2\n      destinationPath: https://xxxx.blob.core.windows.net/postgres-backup\n      wal:\n        maxParallel: 2\n    retentionPolicy: 30d\n    target: prefer-standby\n    volumeSnapshot:\n      className: csi-azure-disk\n      online: true\n      onlineConfiguration:\n        immediateCheckpoint: false\n        waitForArchive: true\n      snapshotOwnerReference: cluster\n  bootstrap:\n    initdb:\n      database: gisual\n      encoding: UTF8\n      localeCType: C\n      localeCollate: C\n      owner: gisual\n      secret:\n        name: postgres-gisual-db-credentials\n  enablePDB: true\n  enableSuperuserAccess: true\n  env:\n  - name: ENABLE_AZURE_PVC_UPDATES\n    value: \"true\"\n  failoverDelay: 30\n  imageCatalogRef:\n    apiGroup: postgresql.cnpg.io\n    kind: ImageCatalog\n    major: 17\n    name: custom-postgres-images\n  imagePullSecrets:\n  - name: docker-registry-credentials\n  instances: 3\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n    - key: queries\n      name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: true\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: \"on\"\n      archive_timeout: 5min\n      autovacuum_max_workers: \"2\"\n      autovacuum_naptime: 20s\n      autovacuum_vacuum_cost_limit: \"1000\"\n      autovacuum_vacuum_scale_factor: \"0.1\"\n      checkpoint_completion_target: \"0.9\"\n      checkpoint_timeout: 15min\n      default_statistics_target: \"100\"\n      dynamic_shared_memory_type: posix\n      effective_cache_size: 4191MB\n      effective_io_concurrency: \"1000\"\n      full_page_writes: \"on\"\n      idle_in_transaction_session_timeout: 30min\n      jit: \"off\"\n      lock_timeout: 1800s\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: \"0\"\n      log_rotation_size: \"0\"\n      log_truncate_on_rotation: \"false\"\n      logging_collector: \"on\"\n      maintenance_work_mem: 341MB\n      max_connections: \"100\"\n      max_parallel_maintenance_workers: \"2\"\n      max_parallel_workers: \"4\"\n      max_parallel_workers_per_gather: \"2\"\n      max_replication_slots: \"32\"\n      max_wal_size: 4GB\n      max_worker_processes: \"4\"\n      min_wal_size: 1GB\n      random_page_cost: \"1.1\"\n      shared_buffers: 1397MB\n      shared_memory_type: mmap\n      shared_preload_libraries: \"\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      statement_timeout: 7200s\n      wal_buffers: 16MB\n      wal_compression: lz4\n      wal_keep_size: 512MB\n      wal_level: logical\n      wal_log_hints: \"on\"\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n      work_mem: 4988kB\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: restart\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  resources:\n    limits:\n      cpu: 2628m\n      memory: 5458Mi\n    requests:\n      cpu: 2628m\n      memory: 5458Mi\n  smartShutdownTimeout: 180\n  startDelay: 3600\n  stopDelay: 1800\n  storage:\n    resizeInUseVolumes: true\n    size: 258Gi\n    storageClass: 6400-iops-212mb-tp-premium-v2\n  superuserSecret:\n    name: postgres-superuser-secret\n  switchoverDelay: 30\n  walStorage:\n    resizeInUseVolumes: true\n    size: 96Gi\n    storageClass: 6400-iops-212mb-tp-premium-v2\nstatus:\n  availableArchitectures:\n  - goArch: amd64\n    hash: 575b8d5080a718a1b1c8e6febcb6ccfde6cf546aa1a253acd7336226494ba784\n  - goArch: arm64\n    hash: bab50cc05e920db8bd118118323ef8003201dd3ba0642bbdee87cfdde1672e3e\n  certificates:\n    clientCASecret: production-pg-common-ca\n    expirations:\n      production-pg-common-ca: 2025-03-10 22:03:24 +0000 UTC\n      production-pg-common-replication: 2025-03-10 22:03:24 +0000 UTC\n      production-pg-common-server: 2025-03-10 22:03:24 +0000 UTC\n    replicationTLSSecret: production-pg-common-replication\n    serverAltDNSNames:\n    - production-pg-common-rw\n    - production-pg-common-rw.db\n    - production-pg-common-rw.db.svc\n    - production-pg-common-rw.db.svc.cluster.local\n    - production-pg-common-r\n    - production-pg-common-r.db\n    - production-pg-common-r.db.svc\n    - production-pg-common-r.db.svc.cluster.local\n    - production-pg-common-ro\n    - production-pg-common-ro.db\n    - production-pg-common-ro.db.svc\n    - production-pg-common-ro.db.svc.cluster.local\n    serverCASecret: production-pg-common-ca\n    serverTLSSecret: production-pg-common-server\n  cloudNativePGCommitHash: 3f96930d\n  cloudNativePGOperatorHash: 575b8d5080a718a1b1c8e6febcb6ccfde6cf546aa1a253acd7336226494ba784\n  conditions:\n  - lastTransitionTime: \"2024-12-11T21:37:17Z\"\n    message: Cluster is Ready\n    reason: ClusterIsReady\n    status: \"True\"\n    type: Ready\n  - lastTransitionTime: \"2024-12-11T18:38:35Z\"\n    message: Continuous archiving is working\n    reason: ContinuousArchivingSuccess\n    status: \"True\"\n    type: ContinuousArchiving\n  - lastTransitionTime: \"2024-12-18T20:00:00Z\"\n    message: 'Failed to create snapshot: failed to take snapshot of the volume /subscriptions/19ea338e-67f7-4a2f-a165-6ccdb9c1aecb/resourceGroups/mc_application_production-app-cluster_eastus/providers/Microsoft.Compute/disks/pvc-44483902-6492-4bb0-a1e9-3a4fd791a6da:\n      \"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"'\n    reason: LastBackupFailed\n    status: \"False\"\n    type: LastBackupSucceeded\n  configMapResourceVersion:\n    metrics:\n      cnpg-default-monitoring: \"824255109\"\n  currentPrimary: production-pg-common-3\n  currentPrimaryTimestamp: \"2024-12-11T18:38:32.062347Z\"\n  firstRecoverabilityPoint: \"2024-12-11T00:03:28Z\"\n  firstRecoverabilityPointByMethod:\n    barmanObjectStore: \"2024-12-11T00:03:28Z\"\n    volumeSnapshot: \"2024-12-11T00:09:40Z\"\n  healthyPVC:\n  - production-pg-common-1\n  - production-pg-common-1-wal\n  - production-pg-common-2\n  - production-pg-common-2-wal\n  - production-pg-common-3\n  - production-pg-common-3-wal\n  image: docker.xxxxx.net/custom-postgres17:17.2@sha256:9d93740c5fcfa7b3113815d6118f8a0afe35dcef38775dae2b93f6d7badedc91\n  instanceNames:\n  - production-pg-common-1\n  - production-pg-common-2\n  - production-pg-common-3\n  instances: 3\n  instancesReportedState:\n    production-pg-common-1:\n      isPrimary: false\n    production-pg-common-2:\n      isPrimary: false\n      timeLineID: 2\n    production-pg-common-3:\n      isPrimary: true\n      timeLineID: 2\n  instancesStatus:\n    healthy:\n    - production-pg-common-2\n    - production-pg-common-3\n    replicating:\n    - production-pg-common-1\n  lastSuccessfulBackup: \"2024-12-18T19:04:26Z\"\n  lastSuccessfulBackupByMethod:\n    barmanObjectStore: \"2024-12-18T00:08:10Z\"\n    volumeSnapshot: \"2024-12-18T19:04:26Z\"\n  latestGeneratedNode: 3\n  managedRolesStatus: {}\n  phase: Cluster in healthy state\n  poolerIntegrations:\n    pgBouncerIntegration:\n      secrets:\n      - production-pg-common-pooler\n  pvcCount: 6\n  readService: production-pg-common-r\n  readyInstances: 2\n  secretsResourceVersion:\n    clientCaSecretVersion: \"824255069\"\n    replicationSecretVersion: \"824255071\"\n    serverCaSecretVersion: \"824255069\"\n    serverSecretVersion: \"824255070\"\n    superuserSecretVersion: \"824019503\"\n  switchReplicaClusterStatus: {}\n  targetPrimary: production-pg-common-3\n  targetPrimaryTimestamp: \"2024-12-11T18:54:08.404015Z\"\n  timelineID: 2\n  topology:\n    instances:\n      production-pg-common-1: {}\n      production-pg-common-2: {}\n      production-pg-common-3: {}\n    nodesUsed: 3\n    successfullyExtracted: true\n  writeService: production-pg-common-rw\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:36:42.6804172Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"38dde046-8bac-4f48-9bbc-5421c9dd5f2d\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:37:12.829048134Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"8b7b8a51-4b6b-4c3a-a329-6ee62ed434d3\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:37:42.97729912Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"893e6af2-d47e-47af-818b-2b8491975a9d\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:38:13.142224766Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"8f044ee3-d748-4528-b331-036f28e5833f\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:38:43.301316301Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"fd50d828-3db8-40f1-8a42-3c95481c8f2d\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:39:13.447363434Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"3eb4044e-a601-482a-8faf-77b3223ca777\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:39:43.601544485Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"8db8ce61-c501-4743-8481-5f3db60d30b4\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:40:13.751990604Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"3eab9855-a206-40f3-b739-ee2484fef421\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:40:43.911698484Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"576818f3-1b02-49d4-bc5f-3d23d3611d0d\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:41:14.076655739Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"d44b95cd-97c6-45d5-a28c-12a462d35d41\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:41:44.230566662Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"f173c087-b924-44ed-b2f3-94d486be1e61\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:42:14.383416658Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"71b1ba94-9a21-44e8-89b0-7b2323264f5d\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:42:44.547937354Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"c64dc793-fdf0-4a0c-8040-759cedc2a232\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:43:14.709137225Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"7511caa7-4838-4968-85f7-7b0be3e8d9c3\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:43:44.8918728Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"1aaab653-b4e2-4f6b-9813-e509451f65a5\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:44:15.055810219Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"25d90f69-e135-4d56-8d92-97aeb52037c0\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:44:45.215848484Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"cb85b375-4b0f-40b2-ab0d-c504de9f67bf\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:45:15.367350072Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"58dbac9a-84fd-4573-a895-45fb2ab9d462\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njeffreymealo@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nYAML manifest\n### What happened?\n- `kubectl cnpg status` hangs indefinitely\n- metrics for the primary are incomplete/unavailable\nI expect if the primary is healthy, accepting connections, and in a good state that the readiness probe will succeed.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"metadata\":{\"annotations\":{},\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"spec\":{\"affinity\":{\"nodeSelector\":{\"kubernetes.azure.com/agentpool\":\"pgcommon\",\"kubernetes.io/arch\":\"arm64\"},\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/postgres\",\"operator\":\"Exists\"}]},\"backup\":{\"barmanObjectStore\":{\"azureCredentials\":{\"connectionString\":{\"key\":\"AZURE_STORAGE_CONNECTION_STRING\",\"name\":\"postgres-backup-azure-creds\"}},\"data\":{\"compression\":\"snappy\",\"jobs\":2},\"destinationPath\":\"https://gisualproduction.blob.core.windows.net/postgres-backup\",\"wal\":{\"maxParallel\":2}},\"retentionPolicy\":\"30d\",\"target\":\"prefer-standby\",\"volumeSnapshot\":{\"className\":\"csi-azure-disk\",\"online\":true,\"onlineConfiguration\":{\"immediateCheckpoint\":false,\"waitForArchive\":true},\"snapshotOwnerReference\":\"cluster\"}},\"bootstrap\":{\"initdb\":{\"database\":\"gisual\",\"owner\":\"gisual\",\"secret\":{\"name\":\"postgres-gisual-db-credentials\"}}},\"enablePDB\":true,\"enableSuperuserAccess\":true,\"env\":[{\"name\":\"ENABLE_AZURE_PVC_UPDATES\",\"value\":\"true\"}],\"failoverDelay\":30,\"imageCatalogRef\":{\"apiGroup\":\"postgresql.cnpg.io\",\"kind\":\"ImageCatalog\",\"major\":17,\"name\":\"custom-postgres-images\"},\"imagePullSecrets\":[{\"name\":\"docker-registry-credentials\"}],\"instances\":3,\"monitoring\":{\"enablePodMonitor\":true},\"postgresql\":{\"parameters\":{\"autovacuum_max_workers\":\"2\",\"autovacuum_naptime\":\"20s\",\"autovacuum_vacuum_cost_limit\":\"1000\",\"autovacuum_vacuum_scale_factor\":\"0.1\",\"checkpoint_completion_target\":\"0.9\",\"checkpoint_timeout\":\"15min\",\"default_statistics_target\":\"100\",\"effective_cache_size\":\"4191MB\",\"effective_io_concurrency\":\"1000\",\"idle_in_transaction_session_timeout\":\"30min\",\"jit\":\"off\",\"lock_timeout\":\"1800s\",\"maintenance_work_mem\":\"341MB\",\"max_connections\":\"100\",\"max_parallel_maintenance_workers\":\"2\",\"max_parallel_workers\":\"4\",\"max_parallel_workers_per_gather\":\"2\",\"max_wal_size\":\"4GB\",\"max_worker_processes\":\"4\",\"min_wal_size\":\"1GB\",\"random_page_cost\":\"1.1\",\"shared_buffers\":\"1397MB\",\"statement_timeout\":\"7200s\",\"wal_buffers\":\"16MB\",\"wal_compression\":\"lz4\",\"work_mem\":\"4988kB\"}},\"primaryUpdateStrategy\":\"unsupervised\",\"resources\":{\"limits\":{\"cpu\":\"2628m\",\"memory\":\"5458Mi\"},\"requests\":{\"cpu\":\"2628m\",\"memory\":\"5458Mi\"}},\"storage\":{\"size\":\"258Gi\",\"storageClass\":\"6400-iops-212mb-tp-premium-v2\"},\"superuserSecret\":{\"name\":\"postgres-superuser-secret\"},\"switchoverDelay\":30,\"walStorage\":{\"size\":\"96Gi\",\"storageClass\":\"6400-iops-212mb-tp-premium-v2\"}}}\n    kubectl.kubernetes.io/restartedAt: \"2024-12-11T14:01:24-05:00\"\n  creationTimestamp: \"2024-12-10T22:08:24Z\"\n  generation: 5\n  name: production-pg-common\n  namespace: db\n  resourceVersion: \"840866221\"\n  uid: f02e579d-1d49-4cc2-ac9f-81a43fb28fe9\nspec:\n  affinity:\n    nodeSelector:\n      kubernetes.azure.com/agentpool: pgcommon\n      kubernetes.io/arch: arm64\n    podAntiAffinityType: preferred\n    tolerations:\n    - effect: NoSchedule\n      key: node-role.kubernetes.io/postgres\n      operator: Exists\n  backup:\n    barmanObjectStore:\n      azureCredentials:\n        connectionString:\n          key: AZURE_STORAGE_CONNECTION_STRING\n          name: postgres-backup-azure-creds\n      data:\n        compression: snappy\n        jobs: 2\n      destinationPath: https://xxxx.blob.core.windows.net/postgres-backup\n      wal:\n        maxParallel: 2\n    retentionPolicy: 30d\n    target: prefer-standby\n    volumeSnapshot:\n      className: csi-azure-disk\n      online: true\n      onlineConfiguration:\n        immediateCheckpoint: false\n        waitForArchive: true\n      snapshotOwnerReference: cluster\n  bootstrap:\n    initdb:\n      database: gisual\n      encoding: UTF8\n      localeCType: C\n      localeCollate: C\n      owner: gisual\n      secret:\n        name: postgres-gisual-db-credentials\n  enablePDB: true\n  enableSuperuserAccess: true\n  env:\n  - name: ENABLE_AZURE_PVC_UPDATES\n    value: \"true\"\n  failoverDelay: 30\n  imageCatalogRef:\n    apiGroup: postgresql.cnpg.io\n    kind: ImageCatalog\n    major: 17\n    name: custom-postgres-images\n  imagePullSecrets:\n  - name: docker-registry-credentials\n  instances: 3\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n    - key: queries\n      name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: true\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: \"on\"\n      archive_timeout: 5min\n      autovacuum_max_workers: \"2\"\n      autovacuum_naptime: 20s\n      autovacuum_vacuum_cost_limit: \"1000\"\n      autovacuum_vacuum_scale_factor: \"0.1\"\n      checkpoint_completion_target: \"0.9\"\n      checkpoint_timeout: 15min\n      default_statistics_target: \"100\"\n      dynamic_shared_memory_type: posix\n      effective_cache_size: 4191MB\n      effective_io_concurrency: \"1000\"\n      full_page_writes: \"on\"\n      idle_in_transaction_session_timeout: 30min\n      jit: \"off\"\n      lock_timeout: 1800s\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: \"0\"\n      log_rotation_size: \"0\"\n      log_truncate_on_rotation: \"false\"\n      logging_collector: \"on\"\n      maintenance_work_mem: 341MB\n      max_connections: \"100\"\n      max_parallel_maintenance_workers: \"2\"\n      max_parallel_workers: \"4\"\n      max_parallel_workers_per_gather: \"2\"\n      max_replication_slots: \"32\"\n      max_wal_size: 4GB\n      max_worker_processes: \"4\"\n      min_wal_size: 1GB\n      random_page_cost: \"1.1\"\n      shared_buffers: 1397MB\n      shared_memory_type: mmap\n      shared_preload_libraries: \"\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      statement_timeout: 7200s\n      wal_buffers: 16MB\n      wal_compression: lz4\n      wal_keep_size: 512MB\n      wal_level: logical\n      wal_log_hints: \"on\"\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n      work_mem: 4988kB\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: restart\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  resources:\n    limits:\n      cpu: 2628m\n      memory: 5458Mi\n    requests:\n      cpu: 2628m\n      memory: 5458Mi\n  smartShutdownTimeout: 180\n  startDelay: 3600\n  stopDelay: 1800\n  storage:\n    resizeInUseVolumes: true\n    size: 258Gi\n    storageClass: 6400-iops-212mb-tp-premium-v2\n  superuserSecret:\n    name: postgres-superuser-secret\n  switchoverDelay: 30\n  walStorage:\n    resizeInUseVolumes: true\n    size: 96Gi\n    storageClass: 6400-iops-212mb-tp-premium-v2\nstatus:\n  availableArchitectures:\n  - goArch: amd64\n    hash: 575b8d5080a718a1b1c8e6febcb6ccfde6cf546aa1a253acd7336226494ba784\n  - goArch: arm64\n    hash: bab50cc05e920db8bd118118323ef8003201dd3ba0642bbdee87cfdde1672e3e\n  certificates:\n    clientCASecret: production-pg-common-ca\n    expirations:\n      production-pg-common-ca: 2025-03-10 22:03:24 +0000 UTC\n      production-pg-common-replication: 2025-03-10 22:03:24 +0000 UTC\n      production-pg-common-server: 2025-03-10 22:03:24 +0000 UTC\n    replicationTLSSecret: production-pg-common-replication\n    serverAltDNSNames:\n    - production-pg-common-rw\n    - production-pg-common-rw.db\n    - production-pg-common-rw.db.svc\n    - production-pg-common-rw.db.svc.cluster.local\n    - production-pg-common-r\n    - production-pg-common-r.db\n    - production-pg-common-r.db.svc\n    - production-pg-common-r.db.svc.cluster.local\n    - production-pg-common-ro\n    - production-pg-common-ro.db\n    - production-pg-common-ro.db.svc\n    - production-pg-common-ro.db.svc.cluster.local\n    serverCASecret: production-pg-common-ca\n    serverTLSSecret: production-pg-common-server\n  cloudNativePGCommitHash: 3f96930d\n  cloudNativePGOperatorHash: 575b8d5080a718a1b1c8e6febcb6ccfde6cf546aa1a253acd7336226494ba784\n  conditions:\n  - lastTransitionTime: \"2024-12-11T21:37:17Z\"\n    message: Cluster is Ready\n    reason: ClusterIsReady\n    status: \"True\"\n    type: Ready\n  - lastTransitionTime: \"2024-12-11T18:38:35Z\"\n    message: Continuous archiving is working\n    reason: ContinuousArchivingSuccess\n    status: \"True\"\n    type: ContinuousArchiving\n  - lastTransitionTime: \"2024-12-18T20:00:00Z\"\n    message: 'Failed to create snapshot: failed to take snapshot of the volume /subscriptions/19ea338e-67f7-4a2f-a165-6ccdb9c1aecb/resourceGroups/mc_application_production-app-cluster_eastus/providers/Microsoft.Compute/disks/pvc-44483902-6492-4bb0-a1e9-3a4fd791a6da:\n      \"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"'\n    reason: LastBackupFailed\n    status: \"False\"\n    type: LastBackupSucceeded\n  configMapResourceVersion:\n    metrics:\n      cnpg-default-monitoring: \"824255109\"\n  currentPrimary: production-pg-common-3\n  currentPrimaryTimestamp: \"2024-12-11T18:38:32.062347Z\"\n  firstRecoverabilityPoint: \"2024-12-11T00:03:28Z\"\n  firstRecoverabilityPointByMethod:\n    barmanObjectStore: \"2024-12-11T00:03:28Z\"\n    volumeSnapshot: \"2024-12-11T00:09:40Z\"\n  healthyPVC:\n  - production-pg-common-1\n  - production-pg-common-1-wal\n  - production-pg-common-2\n  - production-pg-common-2-wal\n  - production-pg-common-3\n  - production-pg-common-3-wal\n  image: docker.xxxxx.net/custom-postgres17:17.2@sha256:9d93740c5fcfa7b3113815d6118f8a0afe35dcef38775dae2b93f6d7badedc91\n  instanceNames:\n  - production-pg-common-1\n  - production-pg-common-2\n  - production-pg-common-3\n  instances: 3\n  instancesReportedState:\n    production-pg-common-1:\n      isPrimary: false\n    production-pg-common-2:\n      isPrimary: false\n      timeLineID: 2\n    production-pg-common-3:\n      isPrimary: true\n      timeLineID: 2\n  instancesStatus:\n    healthy:\n    - production-pg-common-2\n    - production-pg-common-3\n    replicating:\n    - production-pg-common-1\n  lastSuccessfulBackup: \"2024-12-18T19:04:26Z\"\n  lastSuccessfulBackupByMethod:\n    barmanObjectStore: \"2024-12-18T00:08:10Z\"\n    volumeSnapshot: \"2024-12-18T19:04:26Z\"\n  latestGeneratedNode: 3\n  managedRolesStatus: {}\n  phase: Cluster in healthy state\n  poolerIntegrations:\n    pgBouncerIntegration:\n      secrets:\n      - production-pg-common-pooler\n  pvcCount: 6\n  readService: production-pg-common-r\n  readyInstances: 2\n  secretsResourceVersion:\n    clientCaSecretVersion: \"824255069\"\n    replicationSecretVersion: \"824255071\"\n    serverCaSecretVersion: \"824255069\"\n    serverSecretVersion: \"824255070\"\n    superuserSecretVersion: \"824019503\"\n  switchReplicaClusterStatus: {}\n  targetPrimary: production-pg-common-3\n  targetPrimaryTimestamp: \"2024-12-11T18:54:08.404015Z\"\n  timelineID: 2\n  topology:\n    instances:\n      production-pg-common-1: {}\n      production-pg-common-2: {}\n      production-pg-common-3: {}\n    nodesUsed: 3\n    successfullyExtracted: true\n  writeService: production-pg-common-rw\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:36:42.6804172Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"38dde046-8bac-4f48-9bbc-5421c9dd5f2d\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:37:12.829048134Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"8b7b8a51-4b6b-4c3a-a329-6ee62ed434d3\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:37:42.97729912Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"893e6af2-d47e-47af-818b-2b8491975a9d\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:38:13.142224766Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"8f044ee3-d748-4528-b331-036f28e5833f\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:38:43.301316301Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"fd50d828-3db8-40f1-8a42-3c95481c8f2d\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:39:13.447363434Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"3eb4044e-a601-482a-8faf-77b3223ca777\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:39:43.601544485Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"8db8ce61-c501-4743-8481-5f3db60d30b4\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:40:13.751990604Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"3eab9855-a206-40f3-b739-ee2484fef421\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:40:43.911698484Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"576818f3-1b02-49d4-bc5f-3d23d3611d0d\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:41:14.076655739Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"d44b95cd-97c6-45d5-a28c-12a462d35d41\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:41:44.230566662Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"f173c087-b924-44ed-b2f3-94d486be1e61\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:42:14.383416658Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"71b1ba94-9a21-44e8-89b0-7b2323264f5d\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:42:44.547937354Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"c64dc793-fdf0-4a0c-8040-759cedc2a232\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:43:14.709137225Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"7511caa7-4838-4968-85f7-7b0be3e8d9c3\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:43:44.8918728Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"1aaab653-b4e2-4f6b-9813-e509451f65a5\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:44:15.055810219Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"25d90f69-e135-4d56-8d92-97aeb52037c0\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:44:45.215848484Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"cb85b375-4b0f-40b2-ab0d-c504de9f67bf\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n{\"level\":\"info\",\"ts\":\"2024-12-18T20:45:15.367350072Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"production-pg-common\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"production-pg-common\",\"reconcileID\":\"58dbac9a-84fd-4573-a895-45fb2ab9d462\",\"name\":\"production-pg-common-1\",\"error\":\"Get \\\"https://10.130.18.37:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductOut of curiosity--can you access the primary pod and run psql in an exec'd shell on the container? If so, can you check whether the pg_database_size query is running (and continues to run indefinitely with the same PID)? And guessing the db is using a substantial amount of the 258Gi? (Check with `df -h` as the pg_database_size query is likely timing out.)\n```\nSELECT pid,\n       usename,\n       datname,\n       client_addr,\n       state,\n       query_start,\n       query\nFROM pg_stat_activity\nWHERE state = 'active';\n```\nWe saw this on 1.23.4. The [same issue exists in 1.24.0](https://github.com/cloudnative-pg/cloudnative-pg/pull/5689) where the status probe calls pg_database_size and does not return in time to show readiness (e.g. /pg/status failing). If you can still access the service locally with psql, then it's the probe's failure, not the database failing. Even if you try to upgrade to 1.24.1, you'll likely see the cluster will not upgrade because it is not healthy. We have an ugly workaround that has worked for every instance we've encountered. Happy to share it if you have the same isssue--confirmed with the outputs from the above check.\n---\n@usiegj00: Sorry just saw your post now.\n- I can confirm I was able to access the primary via psql.\n- I did not check if the `pg_database_size` query was running.\n- I can confirm that health checks were failing (and that the HTTP endpoint was also hanging).\nI haven't tried upgrading the operator yet. I may need that workaround.\n---\n@gbartolini This is one of the more troubling bugs we're seeing frequently. It's alarming that it can cause many operations of the operator to fail, as it thinks the cluster is in a bad state.\n---\n@jmealo -- the **ugly** workaround goes like this:\n1. Your instance manager call /pg/status is failing because the underlying query for `pg_database_size` is taking too long. Verify with the query above. \n2. Replace the `pg_database_size` queries with dummies that return 1 immediately. For example, psql on the master node and run the create or replace below.\n3. On the primary, edit the /var/lib/postgresql/data/pgdata/custom.conf and append `search_path = 'public, pg_catalog'` (see below).\n4. Reload the configuration `SELECT pg_reload_conf();`\n5. Check the new search path `SHOW search_path; -- should show \"public, pg_catalog\"`\n6. Check the pg_database_size replacement `SELECT pg_database_size('mydb');  -- should now return 1`\n7. Kill the stuck queries you found above with `pg_terminate_backend`.\n8. The status for the pod should now be ready. If you reallly want to debug, you can cp a static busybox to the pod and locally curl /pg/status to see it go from failing to ready.\n9. Your upgrade will commence for this cluster if it was pending, or you can upgrade your operator to trigger the cluster upgrade.\n10. Delete the shim `pg_database_size` functions. \n11. Copy the old custom.conf back.\nReplacement Queries\n---\n```sql\nCREATE OR REPLACE FUNCTION public.pg_database_size(name)\nRETURNS bigint LANGUAGE sql IMMUTABLE AS $$\n  SELECT 1::bigint\n$$;\nCREATE OR REPLACE FUNCTION public.pg_database_size(oid)\nRETURNS bigint LANGUAGE sql IMMUTABLE AS $$\n  SELECT 1::bigint\n$$;\n```\nEdit custom.conf\n---\n```bash\ncd /var/lib/postgresql/data/pgdata/\necho \"search_path = 'public, pg_catalog'\" > prepend.conf\ncp custom.conf custom.conf-orig\ncat prepend.conf custom.conf-orig > custom.conf\n```\nRemove the Queries\n---\n```sql\nDROP FUNCTION IF EXISTS public.pg_database_size(name);\nDROP FUNCTION IF EXISTS public.pg_database_size(oid);\n```\n---\n@usiegj00 is the readiness probe not explicitly setting search_path? this seems potentially insecure... typically these sorts of injections shouldn't be allowed\ncf. https://github.com/timescale/pgspot for a nice list of best practices here\n---\nreadiness probe will never be perfect, and is also pretty critical. i would suggest our focus should be on debuggability - if a readiness probe fails, what additional information would give more clarity?\nif this is a timeout-based thing where k8s fails the check if it doesn't respond within some time threshold then there absolutely should be a metric by default on whatever number k8s is looking at to make that decision\nbased on the comment here about pg_database_size taking too long, another troubleshooting idea we might consider is enabling SQL tracing for the readiness probes with log_min_duration_statement=0 for that connection only\nthen you could check logs to see how long the queries for this session are taking\n---\n**One of the root causes have been identified:** #6761 \nI haven't closed this issue because there are outstanding defects identified in the probes and connection/context/concurrency handling that need to be addressed."
    },
    {
        "title": "[Bug]: dangling files in `base` directory after pod was killed by OOM or when run out of space",
        "id": 2743969889,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nNot sure if it's some cnpg-specific postgresql configuration or postgresql flaw (I cannot believe it, but...) \ud83e\udd37 \nThe story: I started restoring a relatively small db, on original old server it occupies ~16gb of disk space.\nDue to mistake I didn't allocate enough (it was 10Gb initially) and the `pg_restore` process was aborted, since cnpg controller has killed postgresql process due to insufficient disk space.\nTo restore I used `pg_restore` with `--single-transaction` flag.\nNow - I have quite a number of files like\n```\n$ ls -la /var/lib/postgresql/data/pgdata/base/8609796/9007730\n-rw------- 1 postgres tape 1073741824 Dec 17 05:09 /var/lib/postgresql/data/pgdata/base/8609796/9007730\n```\nwhich represent tables.\nAnd the problem is: since it was pg_restore running in a single transaction - as expected the target database is empty.\nBut `/var/lib/postgresql/data/pgdata/base/8609796` occupies almost 10Gb of abandoned table files.\nIf I run `select * from pg_class` - I cannot see `9007730` or any other dangling files, so they stay unaccounted.\nHence a question: I fail to believe postgresql itself can have such a terrible design flaw per-se, can it be something about how cnpg manages/configures postgresql?\n### Cluster resource\n<details>\nCluster and Pooler yaml definitions</summary>\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  annotations:\n  creationTimestamp: \"2024-12-09T04:09:39Z\"\n  generation: 2\n  name: db\n  namespace: <redacted>\n  resourceVersion: \"14000397\"\n  uid: 819d2929-ef97-438b-8e5b-02ee840ac35f\nspec:\n  affinity:\n    podAntiAffinityType: required\n  backup:\n    barmanObjectStore:\n      data:\n        compression: bzip2\n      destinationPath: s3://<redacted>\n      endpointURL: https://<redacted>\n      s3Credentials:\n        accessKeyId:\n          key: ACCESS_KEY\n          name: cnpg-db-backup-credentials\n        secretAccessKey:\n          key: SECRET_KEY\n          name: cnpg-db-backup-credentials\n      serverName: db\n      wal:\n        compression: bzip2\n    retentionPolicy: 3d\n    target: prefer-standby\n  bootstrap:\n    initdb:\n      database: app\n      postInitApplicationSQL:\n      - ALTER user app <redacted>\n      - CREATE SCHEMA <redacted>\n      - ALTER SCHEMA <redacted> OWNER TO app;\n  enablePDB: false\n  imageName: ghcr.io/cloudnative-pg/postgis:17-3.5-2\n  inheritedMetadata:\n    annotations:\n      prometheus.io/port: \"9187\"\n      prometheus.io/scrape: \"true\"\n  instances: 3\n  managed:\n    services:\n      disabledDefaultServices:\n      - r\n      - ro\n  postgresql:\n    parameters:\n      statement_timeout: 30min\n      lock_timeout: 30min\n      idle_in_transaction_session_timeout: 30min\n      idle_session_timeout: 30min\n    synchronous:\n      method: any\n      number: 1\n  primaryUpdateMethod: switchover\n  resources:\n    limits:\n      cpu: \"1\"\n      memory: 500Mi\n    requests:\n      cpu: 1m\n      memory: 80Mi\n  storage:\n    size: 1000Mi\n    storageClass: <redacted>\n---\napiVersion: postgresql.cnpg.io/v1\nkind: Pooler\nmetadata:\n  creationTimestamp: \"2024-12-09T04:09:39Z\"\n  generation: 2\n  name: db-pooler\n  namespace: <redacted>\n  resourceVersion: \"14154801\"\n  uid: 672b3b7c-8bc8-4c1c-a23d-0317df6ab57f\nspec:\n  cluster:\n    name: db\n  instances: 2\n  pgbouncer:\n    parameters:\n      client_idle_timeout: \"1800\"\n      default_pool_size: \"100\"\n      idle_transaction_timeout: \"1800\"\n      tcp_keepcnt: \"10\"\n      tcp_keepidle: \"300\"\n      tcp_keepintvl: \"20\"\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: \"9127\"\n        prometheus.io/scrape: \"true\"\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                cnpg.io/cluster: db\n                cnpg.io/podRole: pooler\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: pgbouncer\n        resources:\n          limits:\n            cpu: \"1\"\n            memory: 128Mi\n          requests:\n            cpu: 1m\n            memory: 16Mi\n      initContainers:\n      - name: bootstrap-controller\n        resources:\n          limits:\n            cpu: \"1\"\n            memory: 96Mi\n          requests:\n            cpu: 1m\n            memory: 96Mi\n  type: rw\n</details>\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nNot sure if it's some cnpg-specific postgresql configuration or postgresql flaw (I cannot believe it, but...) \ud83e\udd37 \nThe story: I started restoring a relatively small db, on original old server it occupies ~16gb of disk space.\nDue to mistake I didn't allocate enough (it was 10Gb initially) and the `pg_restore` process was aborted, since cnpg controller has killed postgresql process due to insufficient disk space.\nTo restore I used `pg_restore` with `--single-transaction` flag.\nNow - I have quite a number of files like\n```\n$ ls -la /var/lib/postgresql/data/pgdata/base/8609796/9007730\n-rw------- 1 postgres tape 1073741824 Dec 17 05:09 /var/lib/postgresql/data/pgdata/base/8609796/9007730\n```\nwhich represent tables.\nAnd the problem is: since it was pg_restore running in a single transaction - as expected the target database is empty.\nBut `/var/lib/postgresql/data/pgdata/base/8609796` occupies almost 10Gb of abandoned table files.\nIf I run `select * from pg_class` - I cannot see `9007730` or any other dangling files, so they stay unaccounted.\nHence a question: I fail to believe postgresql itself can have such a terrible design flaw per-se, can it be something about how cnpg manages/configures postgresql?\n### Cluster resource\n<details>\nCluster and Pooler yaml definitions</summary>\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  annotations:\n  creationTimestamp: \"2024-12-09T04:09:39Z\"\n  generation: 2\n  name: db\n  namespace: <redacted>\n  resourceVersion: \"14000397\"\n  uid: 819d2929-ef97-438b-8e5b-02ee840ac35f\nspec:\n  affinity:\n    podAntiAffinityType: required\n  backup:\n    barmanObjectStore:\n      data:\n        compression: bzip2\n      destinationPath: s3://<redacted>\n      endpointURL: https://<redacted>\n      s3Credentials:\n        accessKeyId:\n          key: ACCESS_KEY\n          name: cnpg-db-backup-credentials\n        secretAccessKey:\n          key: SECRET_KEY\n          name: cnpg-db-backup-credentials\n      serverName: db\n      wal:\n        compression: bzip2\n    retentionPolicy: 3d\n    target: prefer-standby\n  bootstrap:\n    initdb:\n      database: app\n      postInitApplicationSQL:\n      - ALTER user app <redacted>\n      - CREATE SCHEMA <redacted>\n      - ALTER SCHEMA <redacted> OWNER TO app;\n  enablePDB: false\n  imageName: ghcr.io/cloudnative-pg/postgis:17-3.5-2\n  inheritedMetadata:\n    annotations:\n      prometheus.io/port: \"9187\"\n      prometheus.io/scrape: \"true\"\n  instances: 3\n  managed:\n    services:\n      disabledDefaultServices:\n      - r\n      - ro\n  postgresql:\n    parameters:\n      statement_timeout: 30min\n      lock_timeout: 30min\n      idle_in_transaction_session_timeout: 30min\n      idle_session_timeout: 30min\n    synchronous:\n      method: any\n      number: 1\n  primaryUpdateMethod: switchover\n  resources:\n    limits:\n      cpu: \"1\"\n      memory: 500Mi\n    requests:\n      cpu: 1m\n      memory: 80Mi\n  storage:\n    size: 1000Mi\n    storageClass: <redacted>\n---\napiVersion: postgresql.cnpg.io/v1\nkind: Pooler\nmetadata:\n  creationTimestamp: \"2024-12-09T04:09:39Z\"\n  generation: 2\n  name: db-pooler\n  namespace: <redacted>\n  resourceVersion: \"14154801\"\n  uid: 672b3b7c-8bc8-4c1c-a23d-0317df6ab57f\nspec:\n  cluster:\n    name: db\n  instances: 2\n  pgbouncer:\n    parameters:\n      client_idle_timeout: \"1800\"\n      default_pool_size: \"100\"\n      idle_transaction_timeout: \"1800\"\n      tcp_keepcnt: \"10\"\n      tcp_keepidle: \"300\"\n      tcp_keepintvl: \"20\"\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: \"9127\"\n        prometheus.io/scrape: \"true\"\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                cnpg.io/cluster: db\n                cnpg.io/podRole: pooler\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: pgbouncer\n        resources:\n          limits:\n            cpu: \"1\"\n            memory: 128Mi\n          requests:\n            cpu: 1m\n            memory: 16Mi\n      initContainers:\n      - name: bootstrap-controller\n        resources:\n          limits:\n            cpu: \"1\"\n            memory: 96Mi\n          requests:\n            cpu: 1m\n            memory: 96Mi\n  type: rw\n</details>\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductUPDATE: just in case - I also asked it in the postgresql general mail list: https://www.postgresql.org/message-id/CABbRoj6EmGOzvk3O2r5RwT_Yn%3DB53D%3DZXP3pNf-3c8DpB10W1Q%40mail.gmail.com\n---\nOk, it looks like it's really not unexpected and was discussed before https://www.postgresql.org/message-id/4ae62ff0-f33e-2a26-79ff-dcaa39ee92ff%40erven.at\nHence a suggestion: cnpg could run some query like \n```sql\nWITH files AS (SELECT $$base/$$||(SELECT oid::text FROM pg_database WHERE datname = current_database())||$$/$$||pg_ls_dir AS file FROM pg_ls_dir($$base/$$||(SELECT oid::text FROM pg_database WHERE datname = current_database()))), orphans AS (SELECT * FROM files WHERE file !~ $$PG_VERSION|pg_filenode.map|pg_internal.init$$ AND regexp_replace(file, $$[._].*$$, $$$$) NOT IN (SELECT pg_relation_filepath(oid) FROM pg_class WHERE pg_relation_filepath(oid) IS NOT NULL)) SELECT * FROM orphans, pg_stat_file(orphans.file) ORDER BY file;\n```\nand export prometheus metric as a flag or a counter of abandoned files? Or may be even clean up (:-D )\n---\n@zerkms: That seems like a reasonable suggestion on the surface. I'll be adding this to my monitoring to see how often this occurs. In my limited testing, this may be inexpensive enough to include in my normal metric queries.\nIIRC, this should only happen if the file system runs out of space entirely while Postgres is running, so running it in all cases seems odd, even if it's cheap.\n\ud83e\udd14 **A diagnostic set of metrics / health check queries that folks can opt-into could be helpful!**\nFor me, on a small instance (<1TB):\n```\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|QUERY PLAN                                                                                                                                                                                  |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|Sort  (cost=108.17..109.42 rows=500 width=73) (actual time=2.724..2.725 rows=0 loops=1)                                                                                                     |\n|  Output: files.file, pg_stat_file.size, pg_stat_file.access, pg_stat_file.modification, pg_stat_file.change, pg_stat_file.creation, pg_stat_file.isdir                                     |\n|  Sort Key: files.file                                                                                                                                                                      |\n|  Sort Method: quicksort  Memory: 25kB                                                                                                                                                      |\n|  Buffers: shared hit=20                                                                                                                                                                    |\n|  CTE files                                                                                                                                                                                 |\n|    ->  Function Scan on pg_catalog.pg_ls_dir  (cost=2.07..19.58 rows=1000 width=32) (actual time=0.587..0.740 rows=988 loops=1)                                                            |\n|          Output: ((('base/'::text || (InitPlan 1).col1) || '/'::text) || pg_ls_dir.pg_ls_dir)                                                                                              |\n|          Function Call: pg_ls_dir(('base/'::text || (InitPlan 2).col1))                                                                                                                    |\n|          Buffers: shared hit=2                                                                                                                                                             |\n|          InitPlan 1                                                                                                                                                                        |\n|            ->  Seq Scan on pg_catalog.pg_database  (cost=0.00..1.03 rows=1 width=32) (actual time=0.027..0.028 rows=1 loops=1)                                                             |\n|                  Output: (pg_database.oid)::text                                                                                                                                           |\n|                  Filter: (pg_database.datname = current_database())                                                                                                                        |\n|                  Rows Removed by Filter: 3                                                                                                                                                 |\n|                  Buffers: shared hit=1                                                                                                                                                     |\n|          InitPlan 2                                                                                                                                                                        |\n|            ->  Seq Scan on pg_catalog.pg_database pg_database_1  (cost=0.00..1.03 rows=1 width=32) (actual time=0.016..0.017 rows=1 loops=1)                                               |\n|                  Output: (pg_database_1.oid)::text                                                                                                                                         |\n|                  Filter: (pg_database_1.datname = current_database())                                                                                                                      |\n|                  Rows Removed by Filter: 3                                                                                                                                                 |\n|                  Buffers: shared hit=1                                                                                                                                                     |\n|  ->  Nested Loop  (cost=28.68..66.18 rows=500 width=73) (actual time=2.714..2.715 rows=0 loops=1)                                                                                          |\n|        Output: files.file, pg_stat_file.size, pg_stat_file.access, pg_stat_file.modification, pg_stat_file.change, pg_stat_file.creation, pg_stat_file.isdir                               |\n|        Buffers: shared hit=20                                                                                                                                                              |\n|        ->  CTE Scan on files  (cost=28.68..56.18 rows=500 width=32) (actual time=2.714..2.714 rows=0 loops=1)                                                                              |\n|              Output: files.file                                                                                                                                                            |\n|              Filter: ((files.file !~ 'PG_VERSION|pg_filenode.map|pg_internal.init'::text) AND (NOT (ANY (regexp_replace(files.file, '[._].*'::text, ''::text) = (hashed SubPlan 4).col1))))|\n|              Rows Removed by Filter: 988                                                                                                                                                   |\n|              Buffers: shared hit=20                                                                                                                                                        |\n|              SubPlan 4                                                                                                                                                                     |\n|                ->  Seq Scan on pg_catalog.pg_class  (cost=0.00..27.16 rows=608 width=32) (actual time=0.032..0.322 rows=463 loops=1)                                                       |\n|                      Output: pg_relation_filepath((pg_class.oid)::regclass)                                                                                                                |\n|                      Filter: (pg_relation_filepath((pg_class.oid)::regclass) IS NOT NULL)                                                                                                  |\n|                      Rows Removed by Filter: 158                                                                                                                                           |\n|                      Buffers: shared hit=18                                                                                                                                                |\n|        ->  Function Scan on pg_catalog.pg_stat_file  (cost=0.00..0.01 rows=1 width=41) (never executed)                                                                                    |\n|              Output: pg_stat_file.size, pg_stat_file.access, pg_stat_file.modification, pg_stat_file.change, pg_stat_file.creation, pg_stat_file.isdir                                     |\n|              Function Call: pg_stat_file(files.file)                                                                                                                                       |\n|Query Identifier: -1807929810228504465                                                                                                                                                      |\n|Planning Time: 0.211 ms                                                                                                                                                                     |\n|Execution Time: 2.766 ms                                                                                                                                                                    |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n```\n---\n> this should only happen if the file system runs out of space entirely while Postgres is running\nIt happens after a postgresql process has died (regardless of the reason) during transactional DDL."
    },
    {
        "title": "[Feature]: Use HaProxy as LoadBalancer for cluster",
        "id": 2739988111,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\n*During a failover, a new primary will be elected\n   - Failover itself is done by cnpg operator not HAProxy\n   -  Server may not fail cleanly:\n      * Old primary server could be a lingering around\n      * Failovers can be happening consecutively\n*HAPproxy make sure that write are accepted when there is a only primary server.\nFrontends can be extended to failback to the primary if there is no healthy replica\n### Describe the solution you'd like\nPgBouncer and HAProxy act as the gateway layer in each cluster. Each tenant acquires client-side connections from HAProxy instead of Postgres directly. PgBouncer holds a pool of maximum server-side connections to Postgres, allocating those across multiple tenants to prevent Postgres connection starvation. From here, HAProxy  forwards queries to PgBouncer, which load balances across Postgres primary and read replicas.\n[Performance isolation in a multi-tenant database environment](https://blog.cloudflare.com/performance-isolation-in-a-multi-tenant-database-environment/)\n![Deployment Topology](https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7twYWGaewKPLe7xRCpnIBG/f47ea6a51eaf09d7e44982a9128365a0/6.png)\n### Describe alternatives you've considered\nusing pgpool and pgbouncer\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\n*During a failover, a new primary will be elected\n   - Failover itself is done by cnpg operator not HAProxy\n   -  Server may not fail cleanly:\n      * Old primary server could be a lingering around\n      * Failovers can be happening consecutively\n*HAPproxy make sure that write are accepted when there is a only primary server.\nFrontends can be extended to failback to the primary if there is no healthy replica\n### Describe the solution you'd like\nPgBouncer and HAProxy act as the gateway layer in each cluster. Each tenant acquires client-side connections from HAProxy instead of Postgres directly. PgBouncer holds a pool of maximum server-side connections to Postgres, allocating those across multiple tenants to prevent Postgres connection starvation. From here, HAProxy  forwards queries to PgBouncer, which load balances across Postgres primary and read replicas.\n[Performance isolation in a multi-tenant database environment](https://blog.cloudflare.com/performance-isolation-in-a-multi-tenant-database-environment/)\n![Deployment Topology](https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7twYWGaewKPLe7xRCpnIBG/f47ea6a51eaf09d7e44982a9128365a0/6.png)\n### Describe alternatives you've considered\nusing pgpool and pgbouncer\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: mTLS only with external user CA",
        "id": 2736434679,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI have an existing CA that distributes certificates across an organization, but I do not have access to the key for the CA. I would like to setup cnpg-managed postgres to authenticate clients with that CA and also disallow password authentication.\nPostgres uses SSL_load_client_CA_file() to load the ssl_ca_file, so if I could append my client CA to the one CNPG uses by default, I could let cnpg continue to manage certificates for replication.\nWhile I can add a hostssl line with CNPG to I can't remove the default host line that allows default (password) authentication.\n### Describe the solution you'd like\nI think I would need two things:\n1. Adding something like 'additionalClientCASecret' which would point to a secret containing one (or more) additional CA certificates that would get appended to the ssl_ca_file that postgres loads.\n2. Some way of disabling/removing the default entry in pg_hba.conf\n### Describe alternatives you've considered\nAlternatively, I could give up on using CNPG for this database and manually configure a postgres instance.\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI have an existing CA that distributes certificates across an organization, but I do not have access to the key for the CA. I would like to setup cnpg-managed postgres to authenticate clients with that CA and also disallow password authentication.\nPostgres uses SSL_load_client_CA_file() to load the ssl_ca_file, so if I could append my client CA to the one CNPG uses by default, I could let cnpg continue to manage certificates for replication.\nWhile I can add a hostssl line with CNPG to I can't remove the default host line that allows default (password) authentication.\n### Describe the solution you'd like\nI think I would need two things:\n1. Adding something like 'additionalClientCASecret' which would point to a secret containing one (or more) additional CA certificates that would get appended to the ssl_ca_file that postgres loads.\n2. Some way of disabling/removing the default entry in pg_hba.conf\n### Describe alternatives you've considered\nAlternatively, I could give up on using CNPG for this database and manually configure a postgres instance.\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Password-based authentication for pgbouncer clients",
        "id": 2736211651,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nHi,\nI am creating initdb with auto generated password to use in client app airflow, according to documentation https://cloudnative-pg.io/documentation/1.22/connection_pooling/#authentication.\nI have added steps described in above url mentioned in ```PostInitSQL``` , but pg cluster creation is failing with error ```airflow (initdb)``` is not available.\nCan someone please advise , how to execute postInitSQL ? Executing Manually is not practical.\n```\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: pgcluster\n  namespace: airflow\nspec:\n  instances: 3\n  bootstrap:\n    initdb:\n      database: airflow\n      owner: airflow\n      #secret: airflow\n      encoding: 'UTF8'\n      postInitSQL:\n        - CREATE ROLE cnpg_pooler_pgbouncer WITH LOGIN\n        - GRANT CONNECT ON DATABASE airflow TO cnpg_pooler_pgbouncer\n        - CREATE OR REPLACE FUNCTION public.user_search(uname TEXT)\n            RETURNS TABLE (usename name, passwd text)\n            LANGUAGE sql SECURITY DEFINER AS\n            'SELECT usename, passwd FROM pg_catalog.pg_shadow WHERE usename=$1;'\n        - REVOKE ALL ON FUNCTION public.user_search(text) FROM public\n        - GRANT EXECUTE ON FUNCTION public.user_search(text) TO cnpg_pooler_pgbouncer\n  imageCatalogRef:\n    apiGroup: postgresql.cnpg.io\n    kind: ClusterImageCatalog\n    name: postgresql\n    major: 16\n  storage:\n    storageClass: standard\n    size: 200Gi\n  monitoring:\n    enablePodMonitor: true\n  postgresql:\n    parameters:\n      pg_stat_statements.max: \"10000\"\n      pg_stat_statements.track: all\n    pg_hba:  \n      - host all all samehost trust\n      - local all all peer\n      - host airflow airflow ip trust\n      - hostssl airflow airflow ip password\n  enableSuperuserAccess: true\n  resources:\n    requests:\n      memory: \"512Mi\"\n      cpu: \"1\"\n---\napiVersion: postgresql.cnpg.io/v1\nkind: Pooler\nmetadata:\n  name: pgpooler\n  namespace: airflow\nspec:\n  cluster:\n    name: pgcluster\n  instances: 3\n  type: rw\n  pgbouncer:\n    poolMode: session\n    parameters:\n      max_client_conn: \"1000\"\n      default_pool_size: \"10\"\n  template:\n    spec:\n      containers:\n      - name: pgbouncer\n        image: cloudnative-pg/pgbouncer:1.22.1-14\n```\n### Describe the solution you'd like\nMy expectation is that cluster must create initDB first and execute PostinitSQL.\n### Describe alternatives you've considered\nna\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nHi,\nI am creating initdb with auto generated password to use in client app airflow, according to documentation https://cloudnative-pg.io/documentation/1.22/connection_pooling/#authentication.\nI have added steps described in above url mentioned in ```PostInitSQL``` , but pg cluster creation is failing with error ```airflow (initdb)``` is not available.\nCan someone please advise , how to execute postInitSQL ? Executing Manually is not practical.\n```\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: pgcluster\n  namespace: airflow\nspec:\n  instances: 3\n  bootstrap:\n    initdb:\n      database: airflow\n      owner: airflow\n      #secret: airflow\n      encoding: 'UTF8'\n      postInitSQL:\n        - CREATE ROLE cnpg_pooler_pgbouncer WITH LOGIN\n        - GRANT CONNECT ON DATABASE airflow TO cnpg_pooler_pgbouncer\n        - CREATE OR REPLACE FUNCTION public.user_search(uname TEXT)\n            RETURNS TABLE (usename name, passwd text)\n            LANGUAGE sql SECURITY DEFINER AS\n            'SELECT usename, passwd FROM pg_catalog.pg_shadow WHERE usename=$1;'\n        - REVOKE ALL ON FUNCTION public.user_search(text) FROM public\n        - GRANT EXECUTE ON FUNCTION public.user_search(text) TO cnpg_pooler_pgbouncer\n  imageCatalogRef:\n    apiGroup: postgresql.cnpg.io\n    kind: ClusterImageCatalog\n    name: postgresql\n    major: 16\n  storage:\n    storageClass: standard\n    size: 200Gi\n  monitoring:\n    enablePodMonitor: true\n  postgresql:\n    parameters:\n      pg_stat_statements.max: \"10000\"\n      pg_stat_statements.track: all\n    pg_hba:  \n      - host all all samehost trust\n      - local all all peer\n      - host airflow airflow ip trust\n      - hostssl airflow airflow ip password\n  enableSuperuserAccess: true\n  resources:\n    requests:\n      memory: \"512Mi\"\n      cpu: \"1\"\n---\napiVersion: postgresql.cnpg.io/v1\nkind: Pooler\nmetadata:\n  name: pgpooler\n  namespace: airflow\nspec:\n  cluster:\n    name: pgcluster\n  instances: 3\n  type: rw\n  pgbouncer:\n    poolMode: session\n    parameters:\n      max_client_conn: \"1000\"\n      default_pool_size: \"10\"\n  template:\n    spec:\n      containers:\n      - name: pgbouncer\n        image: cloudnative-pg/pgbouncer:1.22.1-14\n```\n### Describe the solution you'd like\nMy expectation is that cluster must create initDB first and execute PostinitSQL.\n### Describe alternatives you've considered\nna\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductWhy are you running the `postInitSQL` queries in the first place? The operator handles that when you have a pooler.\n---\nAlso, I believe there's some problem with the `pg_hba` section. Pardon my directness here, but it will help me understand better: how familiar are you with Postgres's HBA system and, more broadly, with Postgres?\n---\nHey,\nI am using postgres for the first time. Let me detail what i need and have tried so far.\nI need a postgres with custom db and user to use in apache airflow.\nFirst i\u2019ve tried without pooler but postgres is overwhelmed and throwing error `slots aren\u2019t available to accept connection`.\nThen i\u2019ve created pooler, however it didn\u2019t create pooler connection in secret. So we have made a new connection based on uri from pgcluater-app secret. ```Example: postgresql://airflow:password@pgpooler.airflow:5432/airflow```,but our application failing intermittently with \u2018SSL connection has been closed unexpectedly\u2019\nAll the above are done without adding ```PostIntiSQL and Pg_hba``` then i was reading up documentation i cited in my question and trying to follow that.\nQ. When pooler is enabled , Do we need to use pooler or pgcluster-rw connection in client app ? In case we use pgcluster-rw connection in client app will operator automatically handles connection pooling via pgbouncer ?\n```**pgcluster -** postgresql://airflow:password@pgcluster-rw.edwairflow:5432/airflow    **pooler -** postgresql://airflow:password@pgpooler.edwairflow:5432/airflow ```\nSorry for the long text, just want to provide you context."
    },
    {
        "title": "feat: require confirmation before deleting the last alive instance",
        "id": 2733044382,
        "state": "open",
        "first": "This bring a little \"protection\" to prevent destroy too easily the last running instance of a cluster. A prompt will be display if its the last instance running be unattended if `--force` is pass to destroy command.",
        "messages": "This bring a little \"protection\" to prevent destroy too easily the last running instance of a cluster. A prompt will be display if its the last instance running be unattended if `--force` is pass to destroy command.I did some test this morning, this looks ok for me like this with the pvc annotations @leonardoce \ud83d\udc4d\ud83c\udffb"
    },
    {
        "title": "[Docs]: Prefix the cnpg resource type in kubectl examples",
        "id": 2730567509,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThe resource type in Kubernetes, for example `cluster`, need only be unique when combined with the API group. This is a problem because several Kubernetes CRDs from various projects seems to use the name `cluster`  (and you may find some that would use the name `backup` too, etc).\nIf a Kubernetes cluster has several resource types with the same name, then the `kubectl get` command will list (only) the one that comes first in the retrieved API discovery document. This may be a surprising result.\nTo be on the safe side the resource type can be qualified when using it in a `kubectl` command.\nIn others words:\nThis may be ambiguous:\n```bash\nkubectl get cluster     # Show all Pg Clusters (or?)\n```\n.. but this is precise (and shows intent);\n```bash\nkubectl get cluster.postgresql.cnpg.io   # Show all Pg Clusters\n```\nBottom line: I believe the qualified form should be evangelized as a kind of a best practice somewhere in the documentation .. to make sure scripts are future proof. \n### Describe what additions need to be done to the documentation\nPut a comment somewhere about using _qualified_ resource type names in `kubectl`  commands. .. if you want to be safe.\n### Describe what pages need to change in the documentation, if any\n_No response_\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThe resource type in Kubernetes, for example `cluster`, need only be unique when combined with the API group. This is a problem because several Kubernetes CRDs from various projects seems to use the name `cluster`  (and you may find some that would use the name `backup` too, etc).\nIf a Kubernetes cluster has several resource types with the same name, then the `kubectl get` command will list (only) the one that comes first in the retrieved API discovery document. This may be a surprising result.\nTo be on the safe side the resource type can be qualified when using it in a `kubectl` command.\nIn others words:\nThis may be ambiguous:\n```bash\nkubectl get cluster     # Show all Pg Clusters (or?)\n```\n.. but this is precise (and shows intent);\n```bash\nkubectl get cluster.postgresql.cnpg.io   # Show all Pg Clusters\n```\nBottom line: I believe the qualified form should be evangelized as a kind of a best practice somewhere in the documentation .. to make sure scripts are future proof. \n### Describe what additions need to be done to the documentation\nPut a comment somewhere about using _qualified_ resource type names in `kubectl`  commands. .. if you want to be safe.\n### Describe what pages need to change in the documentation, if any\n_No response_\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductPR just created.\n---\nLooking into this, the documentation is meant to be used to be used with the cnpg operator, if you have more resources with the same name, the documentation need to assume you know this and now how to handle that because at this point, we can presume you know what you have in your cluster.\nOn the other hand, probably what we can do is add a shortcut in the resources and not put the full name? if we add this kind of full path in the documentation people may assume is always required and that's not the real case.\nProbably there's a lot more to discuss here about how we can create a shorter and unique way to request the objects\n---\nYes of course, we can assume that people know exactly what happens when then use `kubectl get ...`. However, depending on what is installed and who deployed it in the cluster, they might not know what is exactly inside.\nSimple example : \n```\n\u279c  /tmp k get backups\nNo resources found in default namespace.\n\u279c  /tmp k get backups.postgresql.cnpg.io \nNAME                  AGE     CLUSTER      METHOD              PHASE       ERROR\nbackup-test    3h32m   postgresql   barmanObjectStore   completed  \n```\n\"Just\" because a new CRD has been created and that kubectl uses alphabetical order to retrieve objects.\nOther tools, like Velero use Backup CRD as well.\nIt can be confusing. Thus, in a second thought, it might not be necessary to add all the prefixes in the documentation. Maybe a _WARNING BOX_ will be enough.\n> Probably there's a lot more to discuss here about how we can create a shorter and unique way to request the objects\nYep. Sure !\nCNPGCluster ? CNPGBackup ? CNPGPoolers ? \ud83d\ude03\n---\nAnother way to go about this is to launch a PR to the kubectl folks asking them to make `kubectl` barf and abort if multiple resource types match. This will then allow users to continue to use the lazy syntax (the short one) without the current risk.\nHere is the PR:   https://github.com/kubernetes/kubectl/issues/1704\nThis is not to say that the current PR about a \"something\" in the cnpg docs about this topic isn't justified too. It is."
    },
    {
        "title": "PodDisruptionBudget is warning for primary instance",
        "id": 2730566441,
        "state": "open",
        "first": "### Discussed in https://github.com/cloudnative-pg/cloudnative-pg/discussions/3865\n<div type='discussions-op-text'>\n<sup>Originally posted by **grahamboyle** February 23, 2024</sup>\nHi\r\nI'm new to the K8s and Postgres world\r\nI've created a database with 3 instances called pg-graham-dev3\r\nTwo pod disruption budgets get created by CNPG when I do this: \r\npg-graham-dev3 and pg-graham-dev3-primary\r\npg-graham-dev3 is fine, min available is 1 and allowed disruptions is 1\r\npg-graham-dev3-primary is in warning , min available is 1 and allowed disruptions is 0\r\n\"Insufficient Pods\" is the alert, I don't understand why its saying that where there are two replicas available.\r\nAny advice on what's wrong?\r\n```\r\nkind: PodDisruptionBudget\r\napiVersion: policy/v1\r\nmetadata:\r\n  annotations:\r\n    cnpg.io/operatorVersion: 1.22.1\r\n  name: pg-graham-dev3-primary\r\n  generation: 1\r\n  namespace: cloudnativepg\r\n  ownerReferences:\r\n    - apiVersion: postgresql.cnpg.io/v1\r\n      kind: Cluster\r\n      name: pg-graham-dev3\r\n      controller: true\r\n  labels:\r\n    cnpg.io/cluster: pg-graham-dev3\r\nspec:\r\n  minAvailable: 1\r\n  selector:\r\n    matchLabels:\r\n      cnpg.io/cluster: pg-graham-dev3\r\n      role: primary\r\nstatus:\r\n  observedGeneration: 1\r\n  disruptionsAllowed: 0\r\n  currentHealthy: 1\r\n  desiredHealthy: 1\r\n  expectedPods: 1\r\n  conditions:\r\n    - type: DisruptionAllowed\r\n      status: 'False'\r\n      observedGeneration: 1\r\n      lastTransitionTime: '2024-02-22T15:23:56Z'\r\n      reason: InsufficientPods\r\n      message: ''\r\n```\r\n</div>",
        "messages": "### Discussed in https://github.com/cloudnative-pg/cloudnative-pg/discussions/3865\n<div type='discussions-op-text'>\n<sup>Originally posted by **grahamboyle** February 23, 2024</sup>\nHi\r\nI'm new to the K8s and Postgres world\r\nI've created a database with 3 instances called pg-graham-dev3\r\nTwo pod disruption budgets get created by CNPG when I do this: \r\npg-graham-dev3 and pg-graham-dev3-primary\r\npg-graham-dev3 is fine, min available is 1 and allowed disruptions is 1\r\npg-graham-dev3-primary is in warning , min available is 1 and allowed disruptions is 0\r\n\"Insufficient Pods\" is the alert, I don't understand why its saying that where there are two replicas available.\r\nAny advice on what's wrong?\r\n```\r\nkind: PodDisruptionBudget\r\napiVersion: policy/v1\r\nmetadata:\r\n  annotations:\r\n    cnpg.io/operatorVersion: 1.22.1\r\n  name: pg-graham-dev3-primary\r\n  generation: 1\r\n  namespace: cloudnativepg\r\n  ownerReferences:\r\n    - apiVersion: postgresql.cnpg.io/v1\r\n      kind: Cluster\r\n      name: pg-graham-dev3\r\n      controller: true\r\n  labels:\r\n    cnpg.io/cluster: pg-graham-dev3\r\nspec:\r\n  minAvailable: 1\r\n  selector:\r\n    matchLabels:\r\n      cnpg.io/cluster: pg-graham-dev3\r\n      role: primary\r\nstatus:\r\n  observedGeneration: 1\r\n  disruptionsAllowed: 0\r\n  currentHealthy: 1\r\n  desiredHealthy: 1\r\n  expectedPods: 1\r\n  conditions:\r\n    - type: DisruptionAllowed\r\n      status: 'False'\r\n      observedGeneration: 1\r\n      lastTransitionTime: '2024-02-22T15:23:56Z'\r\n      reason: InsufficientPods\r\n      message: ''\r\n```\r\n</div>"
    },
    {
        "title": "[Bug]: Segfault in pgbouncer",
        "id": 2728970609,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nzerkms@zerkms.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nIt just happened several times on a new cnpg cluster. I also run around 10 more cnpg clusters in the same kubernetes cluster, and it was only 2 pods from one of them that were affected.\nNothing particularly interesting in logs before this core:\n```\ndropping database '<dbname-redacted>' as it does not exist anymore or inactive auto-database\n```\n\u261d this is the log record 90ms before the core dump \ud83d\udc47 \n```\ngithub.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\n\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/pgbouncer/run.runSubCommand\n\tinternal/cmd/manager/pgbouncer/run/cmd.go:144\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/pgbouncer/run.NewCmd.func2\n\tinternal/cmd/manager/pgbouncer/run/cmd.go:74\ngithub.com/spf13/cobra.(*Command).execute\n\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\ngithub.com/spf13/cobra.(*Command).ExecuteC\n\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\ngithub.com/spf13/cobra.(*Command).Execute\n\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\nmain.main\n\tcmd/manager/main.go:68\nruntime.main\n\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\n```\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nzerkms@zerkms.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nIt just happened several times on a new cnpg cluster. I also run around 10 more cnpg clusters in the same kubernetes cluster, and it was only 2 pods from one of them that were affected.\nNothing particularly interesting in logs before this core:\n```\ndropping database '<dbname-redacted>' as it does not exist anymore or inactive auto-database\n```\n\u261d this is the log record 90ms before the core dump \ud83d\udc47 \n```\ngithub.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\n\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/pgbouncer/run.runSubCommand\n\tinternal/cmd/manager/pgbouncer/run/cmd.go:144\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/pgbouncer/run.NewCmd.func2\n\tinternal/cmd/manager/pgbouncer/run/cmd.go:74\ngithub.com/spf13/cobra.(*Command).execute\n\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\ngithub.com/spf13/cobra.(*Command).ExecuteC\n\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\ngithub.com/spf13/cobra.(*Command).Execute\n\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\nmain.main\n\tcmd/manager/main.go:68\nruntime.main\n\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\n```\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductThank you for reporting this issue. We'll need additional details to investigate further, as the attached log and message don't provide enough information to pinpoint the problem. Could you please provide the following?\n* Cluster and Pooler Definition: Sharing the relevant YAML configuration for the Cluster and Pooler will help us understand the setup.\n* Crash Reproducibility: Let us know if the crash is consistently reproducible. If so, what steps or workload triggers the issue?\n* Core File: If a core dump is available, having access to it would be extremely helpful.\nPlease ensure any sensitive information is removed before sharing.\nThis information will allow us to understand the context better and reproduce the issue.\n---\n<details>\n<summary>1. Cluster and Pooler yaml definitions</summary>\n```yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  annotations:\n  creationTimestamp: \"2024-12-09T04:09:39Z\"\n  generation: 2\n  name: db\n  namespace: <redacted>\n  resourceVersion: \"14000397\"\n  uid: 819d2929-ef97-438b-8e5b-02ee840ac35f\nspec:\n  affinity:\n    podAntiAffinityType: required\n  backup:\n    barmanObjectStore:\n      data:\n        compression: bzip2\n      destinationPath: s3://<redacted>\n      endpointURL: https://<redacted>\n      s3Credentials:\n        accessKeyId:\n          key: ACCESS_KEY\n          name: cnpg-db-backup-credentials\n        secretAccessKey:\n          key: SECRET_KEY\n          name: cnpg-db-backup-credentials\n      serverName: db\n      wal:\n        compression: bzip2\n    retentionPolicy: 3d\n    target: prefer-standby\n  bootstrap:\n    initdb:\n      database: app\n      postInitApplicationSQL:\n      - ALTER user app <redacted>\n      - CREATE SCHEMA <redacted>\n      - ALTER SCHEMA <redacted> OWNER TO app;\n  enablePDB: false\n  imageName: ghcr.io/cloudnative-pg/postgis:17-3.5-2\n  inheritedMetadata:\n    annotations:\n      prometheus.io/port: \"9187\"\n      prometheus.io/scrape: \"true\"\n  instances: 3\n  managed:\n    services:\n      disabledDefaultServices:\n      - r\n      - ro\n  postgresql:\n    parameters:\n      statement_timeout: 30min\n      lock_timeout: 30min\n      idle_in_transaction_session_timeout: 30min\n      idle_session_timeout: 30min\n    synchronous:\n      method: any\n      number: 1\n  primaryUpdateMethod: switchover\n  resources:\n    limits:\n      cpu: \"1\"\n      memory: 500Mi\n    requests:\n      cpu: 1m\n      memory: 80Mi\n  storage:\n    size: 1000Mi\n    storageClass: <redacted>\n---\napiVersion: postgresql.cnpg.io/v1\nkind: Pooler\nmetadata:\n  creationTimestamp: \"2024-12-09T04:09:39Z\"\n  generation: 2\n  name: db-pooler\n  namespace: <redacted>\n  resourceVersion: \"14154801\"\n  uid: 672b3b7c-8bc8-4c1c-a23d-0317df6ab57f\nspec:\n  cluster:\n    name: db\n  instances: 2\n  pgbouncer:\n    parameters:\n      client_idle_timeout: \"1800\"\n      default_pool_size: \"100\"\n      idle_transaction_timeout: \"1800\"\n      tcp_keepcnt: \"10\"\n      tcp_keepidle: \"300\"\n      tcp_keepintvl: \"20\"\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: \"9127\"\n        prometheus.io/scrape: \"true\"\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                cnpg.io/cluster: db\n                cnpg.io/podRole: pooler\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: pgbouncer\n        resources:\n          limits:\n            cpu: \"1\"\n            memory: 128Mi\n          requests:\n            cpu: 1m\n            memory: 16Mi\n      initContainers:\n      - name: bootstrap-controller\n        resources:\n          limits:\n            cpu: \"1\"\n            memory: 96Mi\n          requests:\n            cpu: 1m\n            memory: 96Mi\n  type: rw\n```\n</details>\n2. It's random, I cannot see any pattern when it happens. More often though it's right after the connection to db is closed: `msg\":\"dropping database '<redacted>' as it does not exist anymore or inactive auto-database`. **BUT** I can also see cases when there are no events logged just before core dump (in close proximity).\n3. No, tbh I don't even know how to enable them\nI run quite a number of cnpg clusters, and this one is the only problematic one. They all are created using the same template for the `Cluster` resource, **BUT** this one is the only one that has multiple postgresql databases created (unmanaged, I created them manually with `CREATE DATABASE`).\n---\n**UPDATE**\nI also found the following interesting behaviour: as you can see I have 2 instances of pools. And when one of the pgbouncer pods segfaulted, the other pod has crashed within one minute too, with a different message\ncrash without segfault\n```json\n{\"level\":\"info\",\"ts\":\"2024-12-11T02:48:10.706380554Z\",\"msg\":\"record\",\"logger\":\"pgbouncer-manager\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-12-11 02:48:10.706 UTC\",\"pid\":\"16\",\"level\":\"FATAL\",\"msg\":\"@src/objects.c:420 in function put_in_order(): put_in_order: found existing elem\"}}\n{\"level\":\"error\",\"ts\":\"2024-12-11T02:48:10.70742981Z\",\"msg\":\"pgbouncer process exited with errors\",\"logger\":\"pgbouncer-manager\",\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/pgbouncer/run.runSubCommand\\n\\tinternal/cmd/manager/pgbouncer/run/cmd.go:144\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/pgbouncer/run.NewCmd.func2\\n\\tinternal/cmd/manager/pgbouncer/run/cmd.go:74\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\n{\"level\":\"error\",\"ts\":\"2024-12-11T02:48:10.707575711Z\",\"msg\":\"Error while running manager\",\"logger\":\"pgbouncer-manager\",\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/pgbouncer/run.NewCmd.func2\\n\\tinternal/cmd/manager/pgbouncer/run/cmd.go:75\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\n```\ncrash with segfault\n```json\n{\"level\":\"info\",\"ts\":\"2024-12-11T01:49:10.58213842Z\",\"msg\":\"record\",\"logger\":\"pgbouncer-manager\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-12-11 01:49:10.581 UTC\",\"pid\":\"16\",\"level\":\"WARNING\",\"msg\":\"dropping database '<redacted>' as it does not exist anymore or inactive auto-database\"}}\n{\"level\":\"error\",\"ts\":\"2024-12-11T01:49:10.701797437Z\",\"msg\":\"pgbouncer process exited with errors\",\"logger\":\"pgbouncer-manager\",\"error\":\"signal: segmentation fault (core dumped)\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/pgbouncer/run.runSubCommand\\n\\tinternal/cmd/manager/pgbouncer/run/cmd.go:144\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/pgbouncer/run.NewCmd.func2\\n\\tinternal/cmd/manager/pgbouncer/run/cmd.go:74\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\n{\"level\":\"error\",\"ts\":\"2024-12-11T01:49:10.701906708Z\",\"msg\":\"Error while running manager\",\"logger\":\"pgbouncer-manager\",\"error\":\"signal: segmentation fault (core dumped)\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/pgbouncer/run.NewCmd.func2\\n\\tinternal/cmd/manager/pgbouncer/run/cmd.go:75\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\n```\n---\n```\n$ pgbouncer --version\nPgBouncer 1.23.0\nlibevent 2.1.8-stable\nadns: evdns2\ntls: OpenSSL 1.1.1n  15 Mar 2022\n```\nand given https://github.com/pgbouncer/pgbouncer/releases/tag/pgbouncer_1_23_1-fixed --- see that this very problem was a bug fixed in 1.23.1.\n---\nsame here"
    },
    {
        "title": "docs: Remove deprecated Pod Security Policies from docs",
        "id": 2728422793,
        "state": "open",
        "first": "Removing deprecated Pod Security Policies information from security docs. \r\nCloses #5996",
        "messages": "Removing deprecated Pod Security Policies information from security docs. \r\nCloses #5996This shouldn't be backported\n---\n> This shouldn't be backported\r\nIs that just the label? It doesn't look like I have permissions to add or remove labels on this PR. Is there another way to prevent the backport?\n---\nWe should be at the point of wrapping this up. Are there any outstanding objections?\r\nI don't want this to slip the next release...\r\n@sharifmshaker please make sure Jonathan is happy with your changes.\r\nAlso, please can you make sure the lines fit in 80 columns? It's a rule of sorts for our docs.\n---\nI think this is only waiting for your comments @sxd , can you please take a look?"
    },
    {
        "title": "[Bug]:  cnpg tatus hangs indefinitely if an instance is on a node that is  <NotReady>",
        "id": 2725662366,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nsandro.dentella@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nHelm\n### What happened?\n# Command `cnpg status` hangs when a cluster node is NotReady\n## Description\nThe `kubectl cnpg status` command hangs indefinitely when trying to get information about a PostgreSQL cluster that has instances on a NotReady node. This prevents operators from getting diagnostic information about the cluster status precisely when it might be most needed.\n## Steps to Reproduce\n1. Have a CNPG cluster with instances spread across multiple nodes (with node anti-affinity configured)\n2. Make one of the nodes hosting a PostgreSQL instance NotReady (e.g., by stopping kubelet/rke2-agent)\n3. Run `kubectl cnpg status <cluster-name>`\n4. The command hangs indefinitely\n## Current Behavior\nThe command hangs without providing any output or timeout, even though:\n- The cluster status is available via `kubectl get cluster`\n- Some instances are still healthy and accessible\n- Other kubectl commands continue to work normally\n## Expected Behavior\nThe command should either:\n- Show partial information about the reachable instances\n- Fail gracefully with a timeout and error message\n- Show status with clear indication of which instances are unreachable\n## Environment\n- CloudNativePG version: 1.24\n- Kubernetes version: 1.30.4\n- kubectl-cnpg plugin version: 1.24.0\n## Cluster Configuration\n```yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: pg-wikijs\n  namespace: wikijs\nspec:\n  instances: 2\n  affinity:\n    enablePodAntiAffinity: true\n    podAntiAffinityType: preferred\n    tolerations:\n    - effect: NoSchedule\n      key: app\n      operator: Equal\n      value: postgresql\n    topologyKey: kubernetes.io/hostname\n  # ... other relevant configs ...\nstatus:\n  instancesStatus:\n    healthy:\n    - pg-wikijs-1\n  readyInstances: 1\n  phase: Waiting for the instances to become active\n  phaseReason: Some instances are not yet active. Please wait.\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nsandro.dentella@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nHelm\n### What happened?\n# Command `cnpg status` hangs when a cluster node is NotReady\n## Description\nThe `kubectl cnpg status` command hangs indefinitely when trying to get information about a PostgreSQL cluster that has instances on a NotReady node. This prevents operators from getting diagnostic information about the cluster status precisely when it might be most needed.\n## Steps to Reproduce\n1. Have a CNPG cluster with instances spread across multiple nodes (with node anti-affinity configured)\n2. Make one of the nodes hosting a PostgreSQL instance NotReady (e.g., by stopping kubelet/rke2-agent)\n3. Run `kubectl cnpg status <cluster-name>`\n4. The command hangs indefinitely\n## Current Behavior\nThe command hangs without providing any output or timeout, even though:\n- The cluster status is available via `kubectl get cluster`\n- Some instances are still healthy and accessible\n- Other kubectl commands continue to work normally\n## Expected Behavior\nThe command should either:\n- Show partial information about the reachable instances\n- Fail gracefully with a timeout and error message\n- Show status with clear indication of which instances are unreachable\n## Environment\n- CloudNativePG version: 1.24\n- Kubernetes version: 1.30.4\n- kubectl-cnpg plugin version: 1.24.0\n## Cluster Configuration\n```yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: pg-wikijs\n  namespace: wikijs\nspec:\n  instances: 2\n  affinity:\n    enablePodAntiAffinity: true\n    podAntiAffinityType: preferred\n    tolerations:\n    - effect: NoSchedule\n      key: app\n      operator: Equal\n      value: postgresql\n    topologyKey: kubernetes.io/hostname\n  # ... other relevant configs ...\nstatus:\n  instancesStatus:\n    healthy:\n    - pg-wikijs-1\n  readyInstances: 1\n  phase: Waiting for the instances to become active\n  phaseReason: Some instances are not yet active. Please wait.\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductI'm hitting this too. What's weird is the readiness endpoint on my primary fails to respond/hangs, `cnpg status` hangs. I'm not sure why the readiness endpoint fails, because I'm currently connected to the instance without issue.\n---\n@sandroden: There's a typo in the title `tatus` -> `status`. Folks might open duplicate issues if they search for `status` :)\nRelates to: #6362"
    },
    {
        "title": "Declarative extension management (`Database` CRD)",
        "id": 2722997707,
        "state": "open",
        "first": "Integrate the `Database` resource to manage PostgreSQL extensions inside a database, supporting:\n- [`CREATE EXTENSION`](https://www.postgresql.org/docs/current/sql-createextension.html)\n- [`DROP EXTENSION`](https://www.postgresql.org/docs/current/sql-dropextension.html)\n- [`ALTER EXTENSION`](https://www.postgresql.org/docs/current/sql-alterextension.html), limited to `UPDATE TO` and `SET SCHEMA` commands",
        "messages": "Integrate the `Database` resource to manage PostgreSQL extensions inside a database, supporting:\n- [`CREATE EXTENSION`](https://www.postgresql.org/docs/current/sql-createextension.html)\n- [`DROP EXTENSION`](https://www.postgresql.org/docs/current/sql-dropextension.html)\n- [`ALTER EXTENSION`](https://www.postgresql.org/docs/current/sql-alterextension.html), limited to `UPDATE TO` and `SET SCHEMA` commands"
    },
    {
        "title": "[Bug]: Internal error occurred: failed calling webhook \"mcluster.cnpg.io\"",
        "id": 2718093382,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nAfter installing the operator using `kubectl apply --server-side -f \\\n  https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.24/releases/cnpg-1.24.1.yaml` and then attempting to create a cluster with a simple yaml\n```yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: cnpg-test\nspec:\n  instances: 3\n  storage:\n    size: 1Gi\n```\nCauses this error\n```\nError from server (InternalError): error when creating \"psql_instance.yaml\": Internal error occurred: failed calling webhook \"mcluster.cnpg.io\": failed to call webhook: Post \"https://cnpg-webhook-service.cnpg-system.svc:443/mutate-postgresql-cnpg-io-v1-cluster?timeout=10s\": context deadline exceeded\n```\nThis is the first time I am using this operator. I checked other issues and most of the similar issues revolve around certificates.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: cnpg-test\nspec:\n  instances: 3\n  storage:\n    size: 1Gi\n```\n### Relevant log output\n```shell\nError from server (InternalError): error when creating \"psql_instance.yaml\": Internal error occurred: failed calling webhook \"mcluster.cnpg.io\": failed to call webhook: Post \"https://cnpg-webhook-service.cnpg-system.svc:443/mutate-postgresql-cnpg-io-v1-cluster?timeout=10s\": context deadline exceeded\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nAfter installing the operator using `kubectl apply --server-side -f \\\n  https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.24/releases/cnpg-1.24.1.yaml` and then attempting to create a cluster with a simple yaml\n```yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: cnpg-test\nspec:\n  instances: 3\n  storage:\n    size: 1Gi\n```\nCauses this error\n```\nError from server (InternalError): error when creating \"psql_instance.yaml\": Internal error occurred: failed calling webhook \"mcluster.cnpg.io\": failed to call webhook: Post \"https://cnpg-webhook-service.cnpg-system.svc:443/mutate-postgresql-cnpg-io-v1-cluster?timeout=10s\": context deadline exceeded\n```\nThis is the first time I am using this operator. I checked other issues and most of the similar issues revolve around certificates.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: cnpg-test\nspec:\n  instances: 3\n  storage:\n    size: 1Gi\n```\n### Relevant log output\n```shell\nError from server (InternalError): error when creating \"psql_instance.yaml\": Internal error occurred: failed calling webhook \"mcluster.cnpg.io\": failed to call webhook: Post \"https://cnpg-webhook-service.cnpg-system.svc:443/mutate-postgresql-cnpg-io-v1-cluster?timeout=10s\": context deadline exceeded\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductHi @shezaan-hashgraph! Welcome to CloudNativePG!\nBy any chance, did you check here? https://cloudnative-pg.io/documentation/current/troubleshooting/#networking\n---\n@gbartolini I did have network policies created by some operators like argocd. I did however, use this example network policy: https://cloudnative-pg.io/documentation/current/samples/networkpolicy-example.yaml after your message but I still run into the exact same issue when trying to create a cluster per this slightly modified manifest.\n```yaml\n---\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: cluster-example\nspec:\n  instances: 3\n    size: 1Gi\n```\n---\n@shezaan-hashgraph : were you able to resolve this issue?\n---\n@bhanotjyoti I was unable to identify any network policies that would cause this to happen. So to answer your question, no I was not able to solve this problem yet.\nIt would be nice if a network policy could be supplied to prevent this if there is any indicator that the policies are the problem.\n---\nIt might not be the best solution, but I solved this by changing `failurePolicy` to `Ignore`.\nBecause the cluster is not created due to the timeout, I temporarily set it to ignore connection and timeout errors.\n```console\n% kubectl patch validatingwebhookconfiguration cnpg-validating-webhook-configuration -p '{\"webhooks\": [{\"name\": \"vcluster.cnpg.io\",\"failurePolicy\": \"Ignore\"}]}'\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/cnpg-validating-webhook-configuration patched\n% kubectl patch mutatingwebhookconfiguration cnpg-mutating-webhook-configuration -p '{\"webhooks\": [{\"name\": \"mcluster.cnpg.io\",\"failurePolicy\": \"Ignore\"}]}' \nmutatingwebhookconfiguration.admissionregistration.k8s.io/cnpg-mutating-webhook-configuration patched\n```\nThen, I could create the cluster.\n```console\n% kubectl apply -f cluster-example.yaml                                                                                                                                                          \ncluster.postgresql.cnpg.io/cluster-example created\n```\n---\nsame issue here with argocd & CNPG\n---\nAlso just ran into this with FluxCD"
    },
    {
        "title": "pooler fails auth after major version Upgrade from pg16 to pg17",
        "id": 2716250414,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Amazon EKS\n### How did you install the operator?\nYAML manifest\n### What happened?\nAfter performing an upgrade from postgres 16 to postgres 17, pgbouncer auth no longer works.\nAny time a connection is attempted through pgbouncer I get these error:\n```\nplatform-pgbouncer-5477c79474-5vpg7 pgbouncer {\"level\":\"info\",\"ts\":\"2024-12-03T20:54:33.463442798Z\",\"msg\":\"record\",\"logger\":\"pgbouncer-manager\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-12-03 20:54:33.463 UTC\",\"pid\":\"12\",\"level\":\"LOG\",\"msg\":\"S-0x55fa419e7b50: mydb/cnpg_pooler_pgbouncer@172.20.172.156:5432 new connection to server (from 10.100.10.210:50356)\"}}\nplatform-pgbouncer-5477c79474-5vpg7 pgbouncer {\"level\":\"info\",\"ts\":\"2024-12-03T20:54:33.467846655Z\",\"msg\":\"record\",\"logger\":\"pgbouncer-manager\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-12-03 20:54:33.467 UTC\",\"pid\":\"12\",\"level\":\"LOG\",\"msg\":\"S-0x55fa419e7b50: mydb/cnpg_pooler_pgbouncer@172.20.172.156:5432 SSL established: TLSv1.3/TLS_AES_256_GCM_SHA384/ECDH=prime256v1\"}}\nplatform-pgbouncer-5477c79474-5vpg7 pgbouncer {\"level\":\"info\",\"ts\":\"2024-12-03T20:54:33.474514074Z\",\"msg\":\"record\",\"logger\":\"pgbouncer-manager\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-12-03 20:54:33.474 UTC\",\"pid\":\"12\",\"level\":\"LOG\",\"msg\":\"S-0x55fa419e7b50: mydb/cnpg_pooler_pgbouncer@172.20.172.156:5432 closing because: error response from auth_query (age=0s)\"}}\nplatform-pgbouncer-5477c79474-5vpg7 pgbouncer {\"level\":\"info\",\"ts\":\"2024-12-03T20:54:33.474557783Z\",\"msg\":\"record\",\"logger\":\"pgbouncer-manager\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-12-03 20:54:33.474 UTC\",\"pid\":\"12\",\"level\":\"LOG\",\"msg\":\"C-0x55fa419cda50: mydb/(nouser)@10.100.9.65:53784 closing because: bouncer config error (age=0s)\"}}\nplatform-pgbouncer-5477c79474-5vpg7 pgbouncer {\"level\":\"info\",\"ts\":\"2024-12-03T20:54:33.474573431Z\",\"msg\":\"record\",\"logger\":\"pgbouncer-manager\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-12-03 20:54:33.474 UTC\",\"pid\":\"12\",\"level\":\"WARNING\",\"msg\":\"C-0x55fa419cda50: mydb/(nouser)@10.100.9.65:53784 pooler error: bouncer config error\"}}\nplatform-pgbouncer-5477c79474-5vpg7 pgbouncer {\"level\":\"info\",\"ts\":\"2024-12-03T21:54:33.948862615Z\",\"msg\":\"record\",\"logger\":\"pgbouncer-manager\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-12-03 21:54:33.948 UTC\",\"pid\":\"12\",\"level\":\"WARNING\",\"msg\":\"dropping database 'mydb' as it does not exist anymore or inactive auto-database\"}}\n```\nI seem to have narrowed it down to a permissions failure, but I haven't been able to fully understand what is missing. seems to only be in the context of the \"mydb\" db. It works if I `\\c postgres` to the main postgres db. When I start up a brand new database with the same config, it works fine. Doing this on the previous database works fine. \n```\nmydb=> SELECT usename, passwd FROM public.user_search('mydb');\nERROR:  permission denied for view pg_shadow\nCONTEXT:  SQL function \"user_search\" statement 1\n```\nI tested this by execing into the pgbouncer pod and connected to postgres like so \n```\npsql \"user=cnpg_pooler_pgbouncer dbname=mydb host=platform-db-rw sslcert=/controller/configs/authUser/tls.crt sslkey=/controller/configs/authUser/tls.key sslrootcert=/controller/configs/server-tls/ca.crt sslmode=prefer\"\n```\nAgain, everything works fine on the cluster that I'm bootstrapping from in the same namespace. I point pgbouncer at the old cluster and it goes back to normal. I have also tried deleting the pooler resource before standing up the pg17 instance, but it fails in the same way when I add the pooler resource back in pointing at the new instance.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  annotations:\n    argocd.argoproj.io/sync-wave: \"-2\"\n  labels:\n    app: mydb\n    app.kubernetes.io/instance: ephemeral_ephemeral-dylan\n    component: postgres\n  name: platform-db-17\n  namespace: ephemeral-dylan\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: karpenter.k8s.aws/instance-category\n            operator: In\n            values:\n            - r\n    podAntiAffinityType: preferred\n  bootstrap:\n    initdb:\n      database: mydb\n      encoding: UTF8\n      import:\n        databases:\n        - mydb\n        source:\n          externalCluster: platform-db\n        type: microservice\n      localeCType: C\n      localeCollate: C\n      owner: mydb\n  enablePDB: true\n  enableSuperuserAccess: true\n  externalClusters:\n  - connectionParameters:\n      dbname: postgres\n      host: platform-db-rw\n      user: postgres\n    name: platform-db\n    password:\n      key: password\n      name: platform-db-superuser\n  failoverDelay: 0\n  imageName: ghcr.io/cloudnative-pg/postgresql:17.0-24\n  instances: 1\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n    - key: queries\n      name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: false\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: \"on\"\n      archive_timeout: 5min\n      dynamic_shared_memory_type: posix\n      full_page_writes: \"on\"\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: \"0\"\n      log_rotation_size: \"0\"\n      log_truncate_on_rotation: \"false\"\n      logging_collector: \"on\"\n      max_parallel_workers: \"32\"\n      max_replication_slots: \"32\"\n      max_worker_processes: \"32\"\n      pg_stat_statements.track: all\n      pgaudit.log: \"\"\n      pgaudit.log_parameter: \"off\"\n      pgaudit.log_relation: \"off\"\n      shared_memory_type: mmap\n      shared_preload_libraries: \"\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      wal_keep_size: 512MB\n      wal_level: logical\n      wal_log_hints: \"on\"\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: restart\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  resources:\n    requests:\n      cpu: \"2\"\n      memory: 4Gi\n  smartShutdownTimeout: 180\n  startDelay: 3600\n  stopDelay: 1800\n  storage:\n    resizeInUseVolumes: true\n    size: 10Gi\n    storageClass: ebs-ephemeral\n  switchoverDelay: 3600\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Amazon EKS\n### How did you install the operator?\nYAML manifest\n### What happened?\nAfter performing an upgrade from postgres 16 to postgres 17, pgbouncer auth no longer works.\nAny time a connection is attempted through pgbouncer I get these error:\n```\nplatform-pgbouncer-5477c79474-5vpg7 pgbouncer {\"level\":\"info\",\"ts\":\"2024-12-03T20:54:33.463442798Z\",\"msg\":\"record\",\"logger\":\"pgbouncer-manager\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-12-03 20:54:33.463 UTC\",\"pid\":\"12\",\"level\":\"LOG\",\"msg\":\"S-0x55fa419e7b50: mydb/cnpg_pooler_pgbouncer@172.20.172.156:5432 new connection to server (from 10.100.10.210:50356)\"}}\nplatform-pgbouncer-5477c79474-5vpg7 pgbouncer {\"level\":\"info\",\"ts\":\"2024-12-03T20:54:33.467846655Z\",\"msg\":\"record\",\"logger\":\"pgbouncer-manager\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-12-03 20:54:33.467 UTC\",\"pid\":\"12\",\"level\":\"LOG\",\"msg\":\"S-0x55fa419e7b50: mydb/cnpg_pooler_pgbouncer@172.20.172.156:5432 SSL established: TLSv1.3/TLS_AES_256_GCM_SHA384/ECDH=prime256v1\"}}\nplatform-pgbouncer-5477c79474-5vpg7 pgbouncer {\"level\":\"info\",\"ts\":\"2024-12-03T20:54:33.474514074Z\",\"msg\":\"record\",\"logger\":\"pgbouncer-manager\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-12-03 20:54:33.474 UTC\",\"pid\":\"12\",\"level\":\"LOG\",\"msg\":\"S-0x55fa419e7b50: mydb/cnpg_pooler_pgbouncer@172.20.172.156:5432 closing because: error response from auth_query (age=0s)\"}}\nplatform-pgbouncer-5477c79474-5vpg7 pgbouncer {\"level\":\"info\",\"ts\":\"2024-12-03T20:54:33.474557783Z\",\"msg\":\"record\",\"logger\":\"pgbouncer-manager\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-12-03 20:54:33.474 UTC\",\"pid\":\"12\",\"level\":\"LOG\",\"msg\":\"C-0x55fa419cda50: mydb/(nouser)@10.100.9.65:53784 closing because: bouncer config error (age=0s)\"}}\nplatform-pgbouncer-5477c79474-5vpg7 pgbouncer {\"level\":\"info\",\"ts\":\"2024-12-03T20:54:33.474573431Z\",\"msg\":\"record\",\"logger\":\"pgbouncer-manager\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-12-03 20:54:33.474 UTC\",\"pid\":\"12\",\"level\":\"WARNING\",\"msg\":\"C-0x55fa419cda50: mydb/(nouser)@10.100.9.65:53784 pooler error: bouncer config error\"}}\nplatform-pgbouncer-5477c79474-5vpg7 pgbouncer {\"level\":\"info\",\"ts\":\"2024-12-03T21:54:33.948862615Z\",\"msg\":\"record\",\"logger\":\"pgbouncer-manager\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-12-03 21:54:33.948 UTC\",\"pid\":\"12\",\"level\":\"WARNING\",\"msg\":\"dropping database 'mydb' as it does not exist anymore or inactive auto-database\"}}\n```\nI seem to have narrowed it down to a permissions failure, but I haven't been able to fully understand what is missing. seems to only be in the context of the \"mydb\" db. It works if I `\\c postgres` to the main postgres db. When I start up a brand new database with the same config, it works fine. Doing this on the previous database works fine. \n```\nmydb=> SELECT usename, passwd FROM public.user_search('mydb');\nERROR:  permission denied for view pg_shadow\nCONTEXT:  SQL function \"user_search\" statement 1\n```\nI tested this by execing into the pgbouncer pod and connected to postgres like so \n```\npsql \"user=cnpg_pooler_pgbouncer dbname=mydb host=platform-db-rw sslcert=/controller/configs/authUser/tls.crt sslkey=/controller/configs/authUser/tls.key sslrootcert=/controller/configs/server-tls/ca.crt sslmode=prefer\"\n```\nAgain, everything works fine on the cluster that I'm bootstrapping from in the same namespace. I point pgbouncer at the old cluster and it goes back to normal. I have also tried deleting the pooler resource before standing up the pg17 instance, but it fails in the same way when I add the pooler resource back in pointing at the new instance.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  annotations:\n    argocd.argoproj.io/sync-wave: \"-2\"\n  labels:\n    app: mydb\n    app.kubernetes.io/instance: ephemeral_ephemeral-dylan\n    component: postgres\n  name: platform-db-17\n  namespace: ephemeral-dylan\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: karpenter.k8s.aws/instance-category\n            operator: In\n            values:\n            - r\n    podAntiAffinityType: preferred\n  bootstrap:\n    initdb:\n      database: mydb\n      encoding: UTF8\n      import:\n        databases:\n        - mydb\n        source:\n          externalCluster: platform-db\n        type: microservice\n      localeCType: C\n      localeCollate: C\n      owner: mydb\n  enablePDB: true\n  enableSuperuserAccess: true\n  externalClusters:\n  - connectionParameters:\n      dbname: postgres\n      host: platform-db-rw\n      user: postgres\n    name: platform-db\n    password:\n      key: password\n      name: platform-db-superuser\n  failoverDelay: 0\n  imageName: ghcr.io/cloudnative-pg/postgresql:17.0-24\n  instances: 1\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n    - key: queries\n      name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: false\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: \"on\"\n      archive_timeout: 5min\n      dynamic_shared_memory_type: posix\n      full_page_writes: \"on\"\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: \"0\"\n      log_rotation_size: \"0\"\n      log_truncate_on_rotation: \"false\"\n      logging_collector: \"on\"\n      max_parallel_workers: \"32\"\n      max_replication_slots: \"32\"\n      max_worker_processes: \"32\"\n      pg_stat_statements.track: all\n      pgaudit.log: \"\"\n      pgaudit.log_parameter: \"off\"\n      pgaudit.log_relation: \"off\"\n      shared_memory_type: mmap\n      shared_preload_libraries: \"\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      wal_keep_size: 512MB\n      wal_level: logical\n      wal_log_hints: \"on\"\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: restart\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  resources:\n    requests:\n      cpu: \"2\"\n      memory: 4Gi\n  smartShutdownTimeout: 180\n  startDelay: 3600\n  stopDelay: 1800\n  storage:\n    resizeInUseVolumes: true\n    size: 10Gi\n    storageClass: ebs-ephemeral\n  switchoverDelay: 3600\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conductsame here\n---\n@kamikaze - have you found any workaround methods that work? I'd love to eventually get this working even if it's a workaround, unfortunately I haven't found one. Even manually updating the permissions I cannot figure out how to fix it.\n---\n> @kamikaze - have you found any workaround methods that work? I'd love to eventually get this working even if it's a workaround, unfortunately I haven't found one. Even manually updating the permissions I cannot figure out how to fix it.\nnope :(\n---\nI was receiving the type of error after a DB migration. \nI saw that my function user_search was owned by the \"app\" user, when I deleted and recreated it to be owned by postgres user it started working again. Could you check if that is your case?\n---\nEDIT: wait no you're right! it's the owner of the function being differnet within the app database. I was looking at the postgres database function. It also looks like it was missing access priviliges for pgbouncer...\n---\nOk so the full workaround to fix this is:\n1. Connect to the database\n  ```\n  kubectl cnpg psql <new_db_name>\n ```\n2. Connect to the app db\n  ```\n  \\c <app_db>\n  ```\n4. Check the function and verify that it's not owned by `postgres` and it is missing permissions for the pooler.\n  ```\n  \\df+ user_search\n  ```\n6. Drop the function\n  ```\n  DROP FUNCTION user_search\n  ```\n7. Recreate the function (per https://cloudnative-pg.io/documentation/1.16/connection_pooling/#authentication)\n  ```\n  CREATE OR REPLACE FUNCTION user_search(uname TEXT)\n    RETURNS TABLE (usename name, passwd text)\n    LANGUAGE sql SECURITY DEFINER AS\n    'SELECT usename, passwd FROM pg_shadow WHERE usename=$1;';\n  REVOKE ALL ON FUNCTION user_search(text)\n    FROM public;\n  GRANT EXECUTE ON FUNCTION user_search(text)\n    TO cnpg_pooler_pgbouncer;\n  ```\n---\nNote that dropping the function may be enough, as the operator should recreate it if it is absent.\nIt may be necessary to redeploy the poolers to trigger a reconciliation, but I'm not sure."
    },
    {
        "title": "[Bug]: self-healing mode disables before replica fully catches up",
        "id": 2716174949,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.4\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nWe have cluster:\n- replicas: 2\n- minSyncReplicas: 1\n- maxSyncReplicas: 1\nwe performed rolling update, which couldn't complete due to unrelated reasons. Once we fixed issue, replica (former master) was able to proceed to pg_rewind + catching up with WAL recovery from archive. Replica was down for significant time, so catching up took some time as well.\nWhen replica logged:\n```\nconsistent recovery state reached at 93B8/C2FEB338\ndatabase system is ready to accept read-only connections\n```\nit was immediately added  to `synchronous_standby_names` on primary (I assume that is when it became Ready and self-healing mode was disabled), which caused downtime: replica was still recovering from files and was significantly lagging and didn't switch to streaming mode yet. On primary all queries ended up in `IPC:SyncRep` state and client app start to time out.\n~20 minutes later  replica caught up:\n```\nend-of-wal-stream flag found.Exiting with error once to let Postgres try switching to streaming replication\nstarted streaming WAL from primary at 93BE/17000000 on timeline 31\n```\nand everything returned to normal.\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\nR - replica, P-primary\nR 19:27:12 consistent recovery state reached at 93B8/C2FEB338\nR 19:27:12 database system is ready to accept read-only connections\nP 19:27:12 parameter \"synchronous_standby_names\" changed to \"ANY 1 (\"pod-1\")\" \n...\nR <many>: restored log file \"0000001F000093BC00000018\" from archive\nR <many>: restored log file \"0000001F000093BC00000019\" from archive\n...\nR 19:44:51 end-of-wal-stream flag found.Exiting with error once to let Postgres try switching to streaming replication\nR 19:44:51 started streaming WAL from primary at 93BE/17000000 on timeline 31\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.4\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nWe have cluster:\n- replicas: 2\n- minSyncReplicas: 1\n- maxSyncReplicas: 1\nwe performed rolling update, which couldn't complete due to unrelated reasons. Once we fixed issue, replica (former master) was able to proceed to pg_rewind + catching up with WAL recovery from archive. Replica was down for significant time, so catching up took some time as well.\nWhen replica logged:\n```\nconsistent recovery state reached at 93B8/C2FEB338\ndatabase system is ready to accept read-only connections\n```\nit was immediately added  to `synchronous_standby_names` on primary (I assume that is when it became Ready and self-healing mode was disabled), which caused downtime: replica was still recovering from files and was significantly lagging and didn't switch to streaming mode yet. On primary all queries ended up in `IPC:SyncRep` state and client app start to time out.\n~20 minutes later  replica caught up:\n```\nend-of-wal-stream flag found.Exiting with error once to let Postgres try switching to streaming replication\nstarted streaming WAL from primary at 93BE/17000000 on timeline 31\n```\nand everything returned to normal.\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\nR - replica, P-primary\nR 19:27:12 consistent recovery state reached at 93B8/C2FEB338\nR 19:27:12 database system is ready to accept read-only connections\nP 19:27:12 parameter \"synchronous_standby_names\" changed to \"ANY 1 (\"pod-1\")\" \n...\nR <many>: restored log file \"0000001F000093BC00000018\" from archive\nR <many>: restored log file \"0000001F000093BC00000019\" from archive\n...\nR 19:44:51 end-of-wal-stream flag found.Exiting with error once to let Postgres try switching to streaming replication\nR 19:44:51 started streaming WAL from primary at 93BE/17000000 on timeline 31\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductThanks for opening up this ticket. We can certainly try and optimise this behaviour, but the main recommendation sticks: sync rep should come with an additional replica for fallback.\nCan you please try with the new syntax for `dataDurability` introduced in 1.25 from `main` or testing RC1 when it is released?\n---\n> Can you please try with the new syntax for dataDurability\nmy understanding is that `dataDurability` just brings back same self-healing mode \"legacy\" replica configuration had, so it will behave the same.\nI can think 2 of fixes:\n- replica won't become ready until it is in streaming mode\n- CNPG sets `synchronous_commit = local` and keeps it that way until `minSyncReplicas` are both ready AND in streaming mode. It will need to track their mode somehow.\n---\nThe problem is that `synchronous_commit` can be defined at the transaction level. So option 2 is not feasible. We need to operate on the `synchronous_standby` option.\nWe are now in release mode, we will take this back once 1.25 is out. Thanks for the feedback."
    },
    {
        "title": ":warning: Action Required: Replace Deprecated gcr.io/kubebuilder/kube-rbac-proxy",
        "id": 2708109782,
        "state": "open",
        "first": "## Description\n:warning: **The image `gcr.io/kubebuilder/kube-rbac-proxy` is deprecated and will become unavailable.**\n**You must move as soon as possible, sometime from early 2025, the GCR will go away.**\n> _Unfortunately, we're unable to provide any guarantees regarding timelines or potential extensions at this time. Images provided under GRC will be unavailable from March 18, 2025, [as per announcement](https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr). However, `gcr.io/kubebuilder/`may be unavailable before this date due [to efforts to deprecate infrastructure](https://github.com/kubernetes/k8s.io/issues/2647)._\n- **If your project uses `gcr.io/kubebuilder/kube-rbac-proxy`, it will be affected.**\n  Your project may fail to work if the image cannot be pulled. **You must take action as soon as possible.**\n- However, if your project is **no longer using this image**, **no action is required**, and you can close this issue.\n## Using the image `gcr.io/kubebuilder/kube-rbac-proxy`?\n[kube-rbac-proxy](https://github.com/brancz/kube-rbac-proxy) was historically used to protect the metrics endpoint. However, its usage has been discontinued in [Kubebuilder](https://github.com/kubernetes-sigs/kubebuilder). The default scaffold now leverages the [`WithAuthenticationAndAuthorization`](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/metrics/filters#WithAuthenticationAndAuthorization) feature provided by [Controller-Runtime](https://github.com/kubernetes-sigs/controller-runtime).\nThis feature provides integrated support for securing metrics endpoints by embedding authentication (`authn`) and authorization (`authz`) mechanisms directly into the controller manager's metrics server, replacing the need for (https://github.com/brancz/kube-rbac-proxy) to secure metrics endpoints.\n### What To Do?\n**You must replace the deprecated image `gcr.io/kubebuilder/kube-rbac-proxy` with an alternative approach. For example:**\n- Update your project to use [`WithAuthenticationAndAuthorization`](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/metrics/filters#WithAuthenticationAndAuthorization):\n  > _You can fully upgrade your project to use the latest scaffolds provided by the tool or manually make the necessary changes. Refer to the [FAQ and Discussion](https://github.com/kubernetes-sigs/kubebuilder/discussions/3907) for detailed instructions on how to manually update your project and test the changes._\n- Alternatively, replace the image with another trusted source at your own risk, as its usage has been discontinued in Kubebuilder.\n**For further information, suggestions, and guidance:**\n- \ud83d\udcd6 [FAQ and Discussion](https://github.com/kubernetes-sigs/kubebuilder/discussions/3907)\n- \ud83d\udcac Join the Slack channel: [#kubebuilder](https://communityinviter.com/apps/kubernetes/community#kubebuilder).\n> **NOTE:** _This issue was opened automatically as part of our efforts to identify projects that might be affected and to raise awareness about this change within the community. If your project is no longer using this image, **feel free to close this issue**._\nWe sincerely apologize for any inconvenience this may cause.\n**Thank you for your cooperation and understanding!** :pray:",
        "messages": "## Description\n:warning: **The image `gcr.io/kubebuilder/kube-rbac-proxy` is deprecated and will become unavailable.**\n**You must move as soon as possible, sometime from early 2025, the GCR will go away.**\n> _Unfortunately, we're unable to provide any guarantees regarding timelines or potential extensions at this time. Images provided under GRC will be unavailable from March 18, 2025, [as per announcement](https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr). However, `gcr.io/kubebuilder/`may be unavailable before this date due [to efforts to deprecate infrastructure](https://github.com/kubernetes/k8s.io/issues/2647)._\n- **If your project uses `gcr.io/kubebuilder/kube-rbac-proxy`, it will be affected.**\n  Your project may fail to work if the image cannot be pulled. **You must take action as soon as possible.**\n- However, if your project is **no longer using this image**, **no action is required**, and you can close this issue.\n## Using the image `gcr.io/kubebuilder/kube-rbac-proxy`?\n[kube-rbac-proxy](https://github.com/brancz/kube-rbac-proxy) was historically used to protect the metrics endpoint. However, its usage has been discontinued in [Kubebuilder](https://github.com/kubernetes-sigs/kubebuilder). The default scaffold now leverages the [`WithAuthenticationAndAuthorization`](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/metrics/filters#WithAuthenticationAndAuthorization) feature provided by [Controller-Runtime](https://github.com/kubernetes-sigs/controller-runtime).\nThis feature provides integrated support for securing metrics endpoints by embedding authentication (`authn`) and authorization (`authz`) mechanisms directly into the controller manager's metrics server, replacing the need for (https://github.com/brancz/kube-rbac-proxy) to secure metrics endpoints.\n### What To Do?\n**You must replace the deprecated image `gcr.io/kubebuilder/kube-rbac-proxy` with an alternative approach. For example:**\n- Update your project to use [`WithAuthenticationAndAuthorization`](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/metrics/filters#WithAuthenticationAndAuthorization):\n  > _You can fully upgrade your project to use the latest scaffolds provided by the tool or manually make the necessary changes. Refer to the [FAQ and Discussion](https://github.com/kubernetes-sigs/kubebuilder/discussions/3907) for detailed instructions on how to manually update your project and test the changes._\n- Alternatively, replace the image with another trusted source at your own risk, as its usage has been discontinued in Kubebuilder.\n**For further information, suggestions, and guidance:**\n- \ud83d\udcd6 [FAQ and Discussion](https://github.com/kubernetes-sigs/kubebuilder/discussions/3907)\n- \ud83d\udcac Join the Slack channel: [#kubebuilder](https://communityinviter.com/apps/kubernetes/community#kubebuilder).\n> **NOTE:** _This issue was opened automatically as part of our efforts to identify projects that might be affected and to raise awareness about this change within the community. If your project is no longer using this image, **feel free to close this issue**._\nWe sincerely apologize for any inconvenience this may cause.\n**Thank you for your cooperation and understanding!** :pray:Thanks @camilamacedo86 !!!\nWe don't use the image and we will close it, thank you so much!\n---\nHi @sxd \nI can check it usage in : \nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/main/config/default/manager_auth_proxy_patch.yaml#L11-L25\nSo, could you please verify?\nThank you.\n---\nHi @camilamacedo86  !\nYes it is mentioned there but we never use that file, I've created an issue https://github.com/cloudnative-pg/cloudnative-pg/issues/6223 to remove those old files.\nThank you so much!\n---\nHI @sxd \nAre you sure?\nWhen you run `make deploy` the patch will be made. \nAnyway, if you need help please feel free to reach us in the slack\n---\nHi @camilamacedo86 \nYup, pretty sure, we stopped the use of kube-rbac-proxy years ago, will keep the issue updated with all the other sub-issues linked here, will reach out to keep you posted!\nRegards!\n---\nHi @camilamacedo86 \nWe made it! both tasks related are now finished =)\nRegards!"
    },
    {
        "title": "Example alert BackendsWaiting has wrong description",
        "id": 2705210649,
        "state": "open",
        "first": "The example alert [BackendsWaiting](https://github.com/cloudnative-pg/cloudnative-pg/blob/18f1c062b4166556ddbbdfd0684a22c22fd8b822/docs/src/samples/monitoring/alerts.yaml#L16) claims that backends are waiting for more than 5 minutes. But if you have a look at the metric:\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/18f1c062b4166556ddbbdfd0684a22c22fd8b822/config/manager/default-monitoring.yaml#L56\nyou see that it actually counts the number of locks that are waiting for other locks. It does not involve any notion of seconds. That might be confusing and missleading. I also don't know if 300 is a good choice as a threshold in this case.",
        "messages": "The example alert [BackendsWaiting](https://github.com/cloudnative-pg/cloudnative-pg/blob/18f1c062b4166556ddbbdfd0684a22c22fd8b822/docs/src/samples/monitoring/alerts.yaml#L16) claims that backends are waiting for more than 5 minutes. But if you have a look at the metric:\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/18f1c062b4166556ddbbdfd0684a22c22fd8b822/config/manager/default-monitoring.yaml#L56\nyou see that it actually counts the number of locks that are waiting for other locks. It does not involve any notion of seconds. That might be confusing and missleading. I also don't know if 300 is a good choice as a threshold in this case."
    },
    {
        "title": "[Bug]: Cloudnative PG v1.24 remote replica creation is failing with bad certificate error.",
        "id": 2701549042,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\namod_kakade@yahoo.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nHi, \nI am using the latest 1.24 documentation (https://cloudnative-pg.io/documentation/1.24/replica_cluster/) to test distributed topology or remote replica cluster. \nI have created a primary database named cnpg-primary-db on one k8s cluster. I am trying to create a remote replica on another k8s cluster named cnpg-replica-db.\nAs per the documentation I have created the secrets for the ca.crt, tls.crt and tls.key in replica cluster. \nHowever when I create the remote replica cluster the pgbasebackup POD fails to connect to primary due to error \n{\"level\":\"info\",\"ts\":\"2024-11-28T09:31:36Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"cnpg-replica-db-1-pgbasebackup\",\"err\":\"failed to connect to `user=streaming_replica database=postgres`: 10.107.147.173:5432 (cnpg-replica-db-rw.tenant01.svc.cluster.local): dial error: dial tcp 10.107.147.173:5432: connect: connection refused\"}\nWhen I check the primary DB I get following error on DB POD\n{\"level\":\"info\",\"ts\":\"2024-11-28T08:33:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-primary-db-1\",\"record\":{\"log_time\":\"2024-11-28 08:33:04.743 UTC\",\"process_id\":\"1799240\",\"connection_from\":\"192.168.1.1:22069\",\"session_id\":\"67482ac0.1b7448\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-28 08:33:04 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"08P01\",\"message\":\"could not accept SSL connection: sslv3 alert bad certificate\",\"backend_type\":\"not initialized\",\"query_id\":\"0\"}}\nI have tried recreating the CA.CRT however I am getting the same error. I could not locate clear documentation explaining this issue or steps to copy the certificates across.\nAppreciate if someone can help here\nRegards\nFor your information, I have following replica YAML\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: cnpg-replica-db\n  namespace: tenant01\nspec:\n  instances: 3\n  bootstrap:\n    pg_basebackup:\n      source: cnpg-primary-db\n  replica:\n    enabled: true\n    source: cnpg-primary-db\n  storage:\n    size: 1Gi\n  externalClusters:\n  - name: cnpg-primary-db\n    connectionParameters:\n      host: cnpg-replica-db-rw.tenant01.svc.cluster.local\n      user: streaming_replica\n      namespace: tenant01\n      sslmode: verify-full\n      dbname: postgres\n    sslKey:\n      name: cnpg-primary-db-replication\n      key: tls.key\n    sslCert:\n      name: cnpg-primary-db-replication\n      key: tls.crt\n    sslRootCert:\n      name: cnpg-primary-db-ca\n      key: ca.crt\n& Following is Primary DB yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name:cnpg-primary-db\nspec:\n  instances: 3\n  storage:\n    size: 1Gi\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\nLog from replica cluster\n{\"level\":\"info\",\"ts\":\"2024-11-28T09:31:36Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"cnpg-replica-db-1-pgbasebackup\",\"err\":\"failed to connect to `user=streaming_replica database=postgres`: 10.107.147.173:5432 (cnpg-replica-db-rw.tenant01.svc.cluster.local): dial error: dial tcp 10.107.147.173:5432: connect: connection refused\"}\nLog from primary DB cluster\n{\"level\":\"info\",\"ts\":\"2024-11-28T08:33:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-primary-db-1\",\"record\":{\"log_time\":\"2024-11-28 08:33:04.743 UTC\",\"process_id\":\"1799240\",\"connection_from\":\"192.168.1.1:22069\",\"session_id\":\"67482ac0.1b7448\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-28 08:33:04 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"08P01\",\"message\":\"could not accept SSL connection: sslv3 alert bad certificate\",\"backend_type\":\"not initialized\",\"query_id\":\"0\"}}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\namod_kakade@yahoo.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nHi, \nI am using the latest 1.24 documentation (https://cloudnative-pg.io/documentation/1.24/replica_cluster/) to test distributed topology or remote replica cluster. \nI have created a primary database named cnpg-primary-db on one k8s cluster. I am trying to create a remote replica on another k8s cluster named cnpg-replica-db.\nAs per the documentation I have created the secrets for the ca.crt, tls.crt and tls.key in replica cluster. \nHowever when I create the remote replica cluster the pgbasebackup POD fails to connect to primary due to error \n{\"level\":\"info\",\"ts\":\"2024-11-28T09:31:36Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"cnpg-replica-db-1-pgbasebackup\",\"err\":\"failed to connect to `user=streaming_replica database=postgres`: 10.107.147.173:5432 (cnpg-replica-db-rw.tenant01.svc.cluster.local): dial error: dial tcp 10.107.147.173:5432: connect: connection refused\"}\nWhen I check the primary DB I get following error on DB POD\n{\"level\":\"info\",\"ts\":\"2024-11-28T08:33:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-primary-db-1\",\"record\":{\"log_time\":\"2024-11-28 08:33:04.743 UTC\",\"process_id\":\"1799240\",\"connection_from\":\"192.168.1.1:22069\",\"session_id\":\"67482ac0.1b7448\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-28 08:33:04 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"08P01\",\"message\":\"could not accept SSL connection: sslv3 alert bad certificate\",\"backend_type\":\"not initialized\",\"query_id\":\"0\"}}\nI have tried recreating the CA.CRT however I am getting the same error. I could not locate clear documentation explaining this issue or steps to copy the certificates across.\nAppreciate if someone can help here\nRegards\nFor your information, I have following replica YAML\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: cnpg-replica-db\n  namespace: tenant01\nspec:\n  instances: 3\n  bootstrap:\n    pg_basebackup:\n      source: cnpg-primary-db\n  replica:\n    enabled: true\n    source: cnpg-primary-db\n  storage:\n    size: 1Gi\n  externalClusters:\n  - name: cnpg-primary-db\n    connectionParameters:\n      host: cnpg-replica-db-rw.tenant01.svc.cluster.local\n      user: streaming_replica\n      namespace: tenant01\n      sslmode: verify-full\n      dbname: postgres\n    sslKey:\n      name: cnpg-primary-db-replication\n      key: tls.key\n    sslCert:\n      name: cnpg-primary-db-replication\n      key: tls.crt\n    sslRootCert:\n      name: cnpg-primary-db-ca\n      key: ca.crt\n& Following is Primary DB yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name:cnpg-primary-db\nspec:\n  instances: 3\n  storage:\n    size: 1Gi\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\nLog from replica cluster\n{\"level\":\"info\",\"ts\":\"2024-11-28T09:31:36Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"cnpg-replica-db-1-pgbasebackup\",\"err\":\"failed to connect to `user=streaming_replica database=postgres`: 10.107.147.173:5432 (cnpg-replica-db-rw.tenant01.svc.cluster.local): dial error: dial tcp 10.107.147.173:5432: connect: connection refused\"}\nLog from primary DB cluster\n{\"level\":\"info\",\"ts\":\"2024-11-28T08:33:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-primary-db-1\",\"record\":{\"log_time\":\"2024-11-28 08:33:04.743 UTC\",\"process_id\":\"1799240\",\"connection_from\":\"192.168.1.1:22069\",\"session_id\":\"67482ac0.1b7448\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-28 08:33:04 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"08P01\",\"message\":\"could not accept SSL connection: sslv3 alert bad certificate\",\"backend_type\":\"not initialized\",\"query_id\":\"0\"}}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Enable PITR from Backup object",
        "id": 2698276855,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIf you attempt to configure recovery by combining a `Backup` object (method `volumeSnapshot`) with WAL archive in an object store, the PITR process fails. \nIf you instead refer to the `volumeSnapshots` from the `Backup` object directly in the recovery stanzas, PITR works correctly.\nLikewise, PITR from object store should also work fine (I haven't actually tested this).\nIt seems everything should be in place to perform PITR from a `Backup` object, regardless of method used to take the backup, as long as it is combined with a WAL archive in an object store.\n### Describe the solution you'd like\nReferring to a simple `Backup` object feels simpler and less error-prone than looking up the `volumeSnapshots`, making sure to combine the correct snapshots for storage/walStorage, and generally get the rest of the configuration correct.\n```\n  bootstrap:\n    recovery:\n      backup:\n        name: backup-20241127004242\n      recoveryTarget:\n        targetTime: \"2024-11-27T02:30:00\"\n      source: wal-archive\n  externalClusters:\n    - name: wal-archive\n      barmanObjectStore:\n        destinationPath: gs://wal-archive\n        googleCredentials:\n          gkeEnvironment: true\n```\n### Describe alternatives you've considered\nIt is possible to do PITR by using the volumeSnapshots directly, but that feels more complicated and error prone.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIf you attempt to configure recovery by combining a `Backup` object (method `volumeSnapshot`) with WAL archive in an object store, the PITR process fails. \nIf you instead refer to the `volumeSnapshots` from the `Backup` object directly in the recovery stanzas, PITR works correctly.\nLikewise, PITR from object store should also work fine (I haven't actually tested this).\nIt seems everything should be in place to perform PITR from a `Backup` object, regardless of method used to take the backup, as long as it is combined with a WAL archive in an object store.\n### Describe the solution you'd like\nReferring to a simple `Backup` object feels simpler and less error-prone than looking up the `volumeSnapshots`, making sure to combine the correct snapshots for storage/walStorage, and generally get the rest of the configuration correct.\n```\n  bootstrap:\n    recovery:\n      backup:\n        name: backup-20241127004242\n      recoveryTarget:\n        targetTime: \"2024-11-27T02:30:00\"\n      source: wal-archive\n  externalClusters:\n    - name: wal-archive\n      barmanObjectStore:\n        destinationPath: gs://wal-archive\n        googleCredentials:\n          gkeEnvironment: true\n```\n### Describe alternatives you've considered\nIt is possible to do PITR by using the volumeSnapshots directly, but that feels more complicated and error prone.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductSubscribing for updates."
    },
    {
        "title": "[Feature]: Trigger the failover process independent from node-monitor-grace-period",
        "id": 2694445913,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI already started this issue as a question (#6146) in the community to understand how the process of failover in case of node failure works in background. p.s. Sorry that I created also an issue (#6154 ) out of that discussion, my bad.\nAfter some tests I could confirm that the reason why the failover process in case of node failure is being triggered after some 40s was because even if we set the tolerationSeconds parameter to 0, there is another internal parameter in **KubeControllerManager** which is called **node-monitor-grace-period** which is set to **40s**, and its configuration/change is not supported.\nThat means that with the actual solution since the failover trigger is depended on the node-monitor-grace-period it is not possible to use cloudnativePG for the use-cases where a failover time smaller than 40-45s in case of node failure will be required. (Which is the case for my application also)\nAs mentioned in the initial ticket I wanted to replace mssql with postgres and with mssql I had failover times between 25-30 seconds.\n### Describe the solution you'd like\nI think a good solution would be to implement a custom timeout mechanism for the failover inside the cloudnativepg and not be dependent on node-monitor-grace-period parameter, in order to be able to configure also failover times smaller than 40s. \n### Describe alternatives you've considered\nI tried out to change the value of the node-monitor-grace-period to 20s in the kube controller manager and the failover time was than as expected some 20s smaller, but as I mentioned changing this value is not supported normally and I had to do it with an unsupportedConfigOverride\n`\n$ oc edit kubecontrollermanagers cluster -n openshift-config\nspec:\n  unsupportedConfigOverrides:\n    extendedArguments:\n      node-monitor-grace-period:\n      - 20s\n`\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI already started this issue as a question (#6146) in the community to understand how the process of failover in case of node failure works in background. p.s. Sorry that I created also an issue (#6154 ) out of that discussion, my bad.\nAfter some tests I could confirm that the reason why the failover process in case of node failure is being triggered after some 40s was because even if we set the tolerationSeconds parameter to 0, there is another internal parameter in **KubeControllerManager** which is called **node-monitor-grace-period** which is set to **40s**, and its configuration/change is not supported.\nThat means that with the actual solution since the failover trigger is depended on the node-monitor-grace-period it is not possible to use cloudnativePG for the use-cases where a failover time smaller than 40-45s in case of node failure will be required. (Which is the case for my application also)\nAs mentioned in the initial ticket I wanted to replace mssql with postgres and with mssql I had failover times between 25-30 seconds.\n### Describe the solution you'd like\nI think a good solution would be to implement a custom timeout mechanism for the failover inside the cloudnativepg and not be dependent on node-monitor-grace-period parameter, in order to be able to configure also failover times smaller than 40s. \n### Describe alternatives you've considered\nI tried out to change the value of the node-monitor-grace-period to 20s in the kube controller manager and the failover time was than as expected some 20s smaller, but as I mentioned changing this value is not supported normally and I had to do it with an unsupportedConfigOverride\n`\n$ oc edit kubecontrollermanagers cluster -n openshift-config\nspec:\n  unsupportedConfigOverrides:\n    extendedArguments:\n      node-monitor-grace-period:\n      - 20s\n`\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct+1 as I have seen the same behavior. Waiting 40 seconds during a node failure is too long"
    },
    {
        "title": "feat: new PgRole CRD and controller for CRD-based role reconciliation",
        "id": 2682841268,
        "state": "open",
        "first": "Closes #5940",
        "messages": "Closes #5940"
    },
    {
        "title": "Failover taking too long",
        "id": 2682773708,
        "state": "open",
        "first": "### Discussed in https://github.com/cloudnative-pg/cloudnative-pg/discussions/6146\n<div type='discussions-op-text'>\n<sup>Originally posted by **albionb96** November 21, 2024</sup>\nHi guys,\r\nwe were using another solution for our database until now, and lately we wanted to have a look at the coudnativePG solution.\r\nThe first thing that we tested was the failover time. \r\nWe killed the Kubernetes node where the cluster primary was living in order to simulate a node failure, and we observed from a client for how long this client cannot send inserts into db.\r\nWith our old solution we had a failover time between 25-35 seconds (during this time the application was not able to insert data  into db).\r\nWhen executing the same failover tests with cloudnativePG we got a failover time somewhere between 40-80seconds.\r\n(If the controller manager pod would be living in the same node like the primary this time could be much longer.)\r\nWhat we noticed is that the WAL receivers are timed out exactly after 5seconds but it takes some 30-37 additional seconds  (which sums up to 35-42 seconds after the node failure, if we calculate the 5s timeout we had for wal receivers) for the cluster to even just start the failover process in the first place. At least we do not see anything in the logs related to the failover during this time. And although we tried to modify some of the config parameters we didn't achieve better results.\r\nI thought to myself that maybe this time might be impacted from something hardcoded, and after looking a little bit at the source code I doubt that this time is coming from the cluster status timeout defined here, but I am not a golang dev or expert, so I don't know:\r\n![image](https://github.com/user-attachments/assets/3bc3fcbc-04b4-45d8-a4ba-462674472fc2)\r\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/72e2f5c490fd17346f1f9ff539589a8d9f01690c/internal/cmd/manager/instance/status/cmd.go#L121\r\nSomewhere I read that in case of node failure the cloudnativepg looks at the node status if it is not-ready or unreachable, and then based on tolerations decides to kill the primary pod and start the failover process. If that is true I now that by default the node-monitor-grace-period is 40s that means that it takes 40s for the kube controller manager to mark node as unhealthy, and that config cannot be adapted. If that is true why is the cloudnativePG relying on that and not triggering the failover process based on its cluster status results/timeouts?\r\nMy questions is: Is it possible to reduce somehow the failover time when the node of primary is killed or fails? Can we achieve the old range we had 25-35 seconds? Any idea?\r\nSome more details:\r\nBelow we see the logs from the client which was not able to write to db for around 51 seconds during the failover:\r\n`Connection is Valid: 2024-11-19T15:02:17.141189046+01:00Sql: INSERT INTO time (time) VALUES (to_timestamp('2024-11-19T15:02:17.141189046+01:00', 'YYYY-MM-DDThh24:mi:ss')::timestamp with time zone at time zone 'Etc/UTC');\r\nConnection is Valid: **2024-11-19T15:02:18.141194779+01:00**Sql: INSERT INTO time (time) VALUES (to_timestamp('2024-11-19T15:02:18.141194779+01:00', 'YYYY-MM-DDThh24:mi:ss')::timestamp with time zone at time zone 'Etc/UTC');\r\n2024-11-19T14:02:29.150Z  WARN 1 --- [   scheduling-1] com.zaxxer.hikari.pool.PoolBase          : HikariPool-1 - Failed to validate connection org.postgresql.jdbc.PgConnection@3fc0c66f (This connection has been closed.). Possibly consider using a shorter maxLifetime value.\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 10009ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:19.141219498+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:29.155141629+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:31.156196969+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:33.156514975+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:35.156847851+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:37.157299363+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:39.157611065+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:41.157907340+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:43.158215925+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:45.158519959+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:47.158822915+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:49.159112575+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:51.159612646+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:53.159916412+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:55.160769376+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:57.161084043+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:59.161413739+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:03:01.161729031+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:03:03.162039029+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:03:05.162364989+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:03:07.162657327+01:00\r\nConnection is Valid: **2024-11-19T15:03:09.162953329+01:00**Sql: INSERT INTO time (time) VALUES (to_timestamp('2024-11-19T15:03:09.162953329+01:00', 'YYYY-MM-DDThh24:mi:ss')::timestamp with time zone at time zone 'Etc/UTC');\r\nOutage duration: 51\r\nConnection is Valid: 2024-11-19T15:03:09.331016425+01:00Sql: INSERT INTO time (time) VALUES (to_timestamp('2024-11-19T15:03:09.331016425+01:00', 'YYYY-MM-DDThh24:mi:ss')::timestamp with time zone at time zone 'Etc/UTC');\r\n`\r\nBelow we see the logs of the cluster replica which will be elected as primary after the failover.\r\nWe see that the wal receivers started to terminate 5seconds after the failure but it took around 37 additional seconds too see some further logs regarding the failover process itself:\r\n`{\"level\":\"info\",\"ts\":\"`**2024-11-19T14:02:23.147275668**Z`\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cluster-sample-1\",\"record\":{\"log_time\":\"2024-11-19 14:02:23.147 UTC\",\"process_id\":\"130\",\"session_id\":\"673c9568.82\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-11-19 13:40:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"08006\",\"message\":**\"terminating walreceiver** due to timeout\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:02:23.172584863Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:178\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:02:23.303569229Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:178\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:02:23.408597375Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cluster-sample-1\",\"record\":{\"log_time\":\"2024-11-19 14:02:23.408 UTC\",\"process_id\":\"43\",\"session_id\":\"673c9567.2b\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-11-19 13:40:55 UTC\",\"virtual_transaction_id\":\"148/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"invalid record length at 0/E0083D8: expected at least 24, got 0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:02:58.375493965Z\",\"logger\":\"Replicator\",\"msg\":\"synchronizing replication slots\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"err\":\"getting replication slot status from primary: failed to connect to `user=streaming_replica database=postgres`: 172.30.33.8:5432 (cluster-sample-rw): dial error: dial tcp 172.30.33.8:5432: connect: connection refused\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:02:58.376720004Z\",\"logger\":\"Replicator\",\"msg\":\"synchronizing replication slots\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"err\":\"getting replication slot status from primary: failed to connect to `user=streaming_replica database=postgres`: 172.30.33.8:5432 (cluster-sample-rw): dial error: dial tcp 172.30.33.8:5432: connect: connection refused\"}\r\n{\"level\":\"debug\",\"ts\":\"**2024-11-19T14:03:00.430552602Z**\",\"logger\":\"external_servers_reconciler\",\"msg\":\"starting up the external servers reconciler\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"575b16c0-d8c4-48a1-b3af-c29fff36cf1c\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:56\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.430567114Z\",\"msg\":\"Reconciling Cluster\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"5b2e572b-ae50-4134-9c2f-0844f9f2ceda\",\"caller\":\"internal/management/controller/instance_controller.go:115\",\"logging_pod\":\"cluster-sample-1\",\"cluster\":{\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.430617394Z\",\"msg\":\"Reconciling custom monitoring queries\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"5b2e572b-ae50-4134-9c2f-0844f9f2ceda\",\"caller\":\"internal/management/controller/instance_controller.go:807\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.430674265Z\",\"logger\":\"tbs_reconciler\",\"msg\":\"skipping the tablespace reconciler in replicas\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"bda1796f-7c85-407d-bf7d-3b9f7a6cf9b5\",\"caller\":\"internal/management/controller/tablespaces/reconciler.go:52\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.440898957Z\",\"logger\":\"tbs_reconciler\",\"msg\":\"skipping the tablespace reconciler in replicas\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4f42d5ab-b60c-4025-8837-f959b277137d\",\"caller\":\"internal/management/controller/tablespaces/reconciler.go:52\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.440916345Z\",\"logger\":\"external_servers_reconciler\",\"msg\":\"starting up the external servers reconciler\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"94d16774-2ee8-4d11-97a6-1b6c42da40a5\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:56\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.455005845Z\",\"logger\":\"external_servers_reconciler\",\"msg\":\"starting up the external servers reconciler\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"9f1e441b-8e61-4364-94a4-2c0b5d24bc5c\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:56\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.455086917Z\",\"logger\":\"tbs_reconciler\",\"msg\":\"skipping the tablespace reconciler in replicas\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"0000e57f-6943-453b-8239-bb9dd88ba92b\",\"caller\":\"internal/management/controller/tablespaces/reconciler.go:52\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.464739371Z\",\"logger\":\"tbs_reconciler\",\"msg\":\"skipping the tablespace reconciler in replicas\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"e1bdbf8f-4c97-41eb-b7f1-98348b7eaa8b\",\"caller\":\"internal/management/controller/tablespaces/reconciler.go:52\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.46486145Z\",\"logger\":\"external_servers_reconciler\",\"msg\":\"starting up the external servers reconciler\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"e830b45c-fcdf-4214-890a-da3429522b03\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:56\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.476379816Z\",\"logger\":\"tbs_reconciler\",\"msg\":\"skipping the tablespace reconciler in replicas\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4ca5d71d-f9b4-4a3a-aa65-9e75d819ba6f\",\"caller\":\"internal/management/controller/tablespaces/reconciler.go:52\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.476433933Z\",\"logger\":\"external_servers_reconciler\",\"msg\":\"starting up the external servers reconciler\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"3e0bf7f3-9a00-4043-8b79-590f86b500d2\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:56\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.495962846Z\",\"msg\":\"Reconciling Cluster\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4a2d265e-5a49-458b-bae6-f1dd8cc957ff\",\"caller\":\"internal/management/controller/instance_controller.go:115\",\"logging_pod\":\"cluster-sample-1\",\"cluster\":{\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.496001228Z\",\"msg\":\"Reconciling custom monitoring queries\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4a2d265e-5a49-458b-bae6-f1dd8cc957ff\",\"caller\":\"internal/management/controller/instance_controller.go:807\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.497249192Z\",\"logger\":\"Replicator\",\"msg\":\"synchronizing replication slots\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"err\":\"getting replication slot status from primary: failed to connect to `user=streaming_replica database=postgres`: 172.30.33.8:5432 (cluster-sample-rw): dial error: dial tcp 172.30.33.8:5432: connect: connection refused\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.551899561Z\",\"msg\":\"Setting myself as primary\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4a2d265e-5a49-458b-bae6-f1dd8cc957ff\",\"logging_pod\":\"cluster-sample-1\",\"phase\":\"Failing over\",\"currentTimestamp\":\"2024-11-19T14:03:00.551882Z\",\"targetPrimaryTimestamp\":\"2024-11-19T14:03:00.463507Z\",\"currentPrimaryTimestamp\":\"2024-11-19T13:35:09.903398Z\",\"msPassedSinceTargetPrimaryTimestamp\":88,\"msPassedSinceCurrentPrimaryTimestamp\":1670648,\"msDifferenceBetweenCurrentAndTargetPrimary\":-1670560}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.551924762Z\",\"msg\":\"I'm the target primary, wait for the wal_receiver to be terminated\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4a2d265e-5a49-458b-bae6-f1dd8cc957ff\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.554814361Z\",\"msg\":\"I'm the target primary, applying WALs and promoting my instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4a2d265e-5a49-458b-bae6-f1dd8cc957ff\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.554833716Z\",\"msg\":\"Extracting pg_controldata information\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4a2d265e-5a49-458b-bae6-f1dd8cc957ff\",\"logging_pod\":\"cluster-sample-1\",\"reason\":\"promote\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.555859512Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1700\\nCatalog version number:               202406281\\nDatabase system identifier:           7438970221660221468\\nDatabase cluster state:               in archive recovery\\npg_control last modified:             Tue 19 Nov 2024 02:00:56 PM UTC\\nLatest checkpoint location:           0/E002458\\nLatest checkpoint's REDO location:    0/D0150D8\\nLatest checkpoint's REDO WAL file:    00000002000000000000000D\\nLatest checkpoint's TimeLineID:       2\\nLatest checkpoint's PrevTimeLineID:   2\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:2380\\nLatest checkpoint's NextOID:          24578\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        730\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  2380\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Tue 19 Nov 2024 02:00:09 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/E004618\\nMin recovery ending loc's timeline:   2\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            394ee91a692b830e68552a0f1ced84562d6115a83a3d2179e7ca9d5f34dffb03\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.555881163Z\",\"msg\":\"Promoting instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4a2d265e-5a49-458b-bae6-f1dd8cc957ff\",\"logging_pod\":\"cluster-sample-1\",\"pgctl_options\":[\"-D\",\"/var/lib/postgresql/data/pgdata\",\"-w\",\"promote\",\"-t 40000000\"]}\r\n`\r\nBelow we see the logs from the controller manager, which pod doesn't log anything at all during the first 42 seconds, after the node failure. It starts logging something about the failover process at the same time as the logs in the replica that we have seen.\r\n`\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T13:40:57.085473677Z\",\"msg\":\"Cluster is healthy\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"cc46950c-dbd8-4ff7-9473-87c616ad1206\"}\r\n{\"level\":\"info\",\"ts\":\"**2024-11-19T14:03:00.419148934Z**\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"name\":\"cluster-sample-2\",\"error\":\"Get \\\"https://10.244.5.186:8000/pg/status\\\": dial tcp 10.244.5.186:8000: i/o timeout\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.429902684Z\",\"msg\":\"Current primary isn't healthy, initiating a failover\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.42992148Z\",\"msg\":\"pod status (1 of 3)\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"name\":\"cluster-sample-1\",\"currentLsn\":\"\",\"receivedLsn\":\"0/E0083D8\",\"replayLsn\":\"0/E0083D8\",\"isPrimary\":false,\"isPodReady\":true,\"pendingRestart\":false,\"pendingRestartForDecrease\":false,\"statusCollectionError\":null}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.429937552Z\",\"msg\":\"pod status (2 of 3)\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"name\":\"cluster-sample-3\",\"currentLsn\":\"\",\"receivedLsn\":\"0/E0083D8\",\"replayLsn\":\"0/E0083D8\",\"isPrimary\":false,\"isPodReady\":true,\"pendingRestart\":false,\"pendingRestartForDecrease\":false,\"statusCollectionError\":null}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.429944256Z\",\"msg\":\"pod status (3 of 3)\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"name\":\"cluster-sample-2\",\"currentLsn\":\"\",\"receivedLsn\":\"\",\"replayLsn\":\"\",\"isPrimary\":false,\"isPodReady\":true,\"pendingRestart\":false,\"pendingRestartForDecrease\":false,\"statusCollectionError\":\"Get \\\"https://10.244.5.186:8000/pg/status\\\": dial tcp 10.244.5.186:8000: i/o timeout\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.430038243Z\",\"msg\":\"Cluster is not healthy\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.45246805Z\",\"msg\":\"Failing over\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"newPrimary\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.452497233Z\",\"msg\":\"pod status (1 of 3)\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"name\":\"cluster-sample-1\",\"currentLsn\":\"\",\"receivedLsn\":\"0/E0083D8\",\"replayLsn\":\"0/E0083D8\",\"isPrimary\":false,\"isPodReady\":true,\"pendingRestart\":false,\"pendingRestartForDecrease\":false,\"statusCollectionError\":null}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.452513933Z\",\"msg\":\"pod status (2 of 3)\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"name\":\"cluster-sample-3\",\"currentLsn\":\"\",\"receivedLsn\":\"0/E0083D8\",\"replayLsn\":\"0/E0083D8\",\"isPrimary\":false,\"isPodReady\":true,\"pendingRestart\":false,\"pendingRestartForDecrease\":false,\"statusCollectionError\":null}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.452520661Z\",\"msg\":\"pod status (3 of 3)\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"name\":\"cluster-sample-2\",\"currentLsn\":\"\",\"receivedLsn\":\"\",\"replayLsn\":\"\",\"isPrimary\":false,\"isPodReady\":true,\"pendingRestart\":false,\"pendingRestartForDecrease\":false,\"statusCollectionError\":\"Get \\\"https://10.244.5.186:8000/pg/status\\\": dial tcp 10.244.5.186:8000: i/o timeout\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.475254228Z\",\"msg\":\"Waiting for the new primary to notice the promotion request\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"newPrimary\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.537044226Z\",\"msg\":\"There is a switchover or a failover in progress, waiting for the operation to complete\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"470d2f40-97a4-4262-90ea-d75074d539d2\",\"currentPrimary\":\"cluster-sample-2\",\"targetPrimary\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.592989434Z\",\"msg\":\"There is a switchover or a failover in progress, waiting for the operation to complete\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"78b742d6-3da3-4ed1-9fb2-1d270e23bdb2\",\"currentPrimary\":\"cluster-sample-2\",\"targetPrimary\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:03.575714289Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a92cad56-4eec-4a17-9f55-9b5f8ebf59cc\",\"name\":\"cluster-sample-2\",\"error\":\"Get \\\"https://10.244.5.186:8000/pg/status\\\": dial tcp 10.244.5.186:8000: i/o timeout\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:05.708673906Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"065e7936-51e9-4979-a4a9-4eff0cc315bc\",\"name\":\"cluster-sample-2\",\"error\":\"Get \\\"https://10.244.5.186:8000/pg/status\\\": dial tcp 10.244.5.186:8000: i/o timeout\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:05.718818785Z\",\"msg\":\"Updating pvc metadata\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"065e7936-51e9-4979-a4a9-4eff0cc315bc\",\"pvc\":\"cluster-sample-1\",\"reconciler\":\"instance-role\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:05.727137941Z\",\"msg\":\"Updating pvc metadata\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"065e7936-51e9-4979-a4a9-4eff0cc315bc\",\"pvc\":\"cluster-sample-1-wal\",\"reconciler\":\"instance-role\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:05.735681781Z\",\"msg\":\"Updating pvc metadata\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"065e7936-51e9-4979-a4a9-4eff0cc315bc\",\"pvc\":\"cluster-sample-2\",\"reconciler\":\"instance-role\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:05.747854475Z\",\"msg\":\"Updating pvc metadata\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"065e7936-51e9-4979-a4a9-4eff0cc315bc\",\"pvc\":\"cluster-sample-2-wal\",\"reconciler\":\"instance-role\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:05.753609429Z\",\"msg\":\"Setting primary label\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"065e7936-51e9-4979-a4a9-4eff0cc315bc\",\"pod\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:05.76791717Z\",\"msg\":\"Setting replica label\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"065e7936-51e9-4979-a4a9-4eff0cc315bc\",\"pod\":\"cluster-sample-2\"}\r\n`</div>",
        "messages": "### Discussed in https://github.com/cloudnative-pg/cloudnative-pg/discussions/6146\n<div type='discussions-op-text'>\n<sup>Originally posted by **albionb96** November 21, 2024</sup>\nHi guys,\r\nwe were using another solution for our database until now, and lately we wanted to have a look at the coudnativePG solution.\r\nThe first thing that we tested was the failover time. \r\nWe killed the Kubernetes node where the cluster primary was living in order to simulate a node failure, and we observed from a client for how long this client cannot send inserts into db.\r\nWith our old solution we had a failover time between 25-35 seconds (during this time the application was not able to insert data  into db).\r\nWhen executing the same failover tests with cloudnativePG we got a failover time somewhere between 40-80seconds.\r\n(If the controller manager pod would be living in the same node like the primary this time could be much longer.)\r\nWhat we noticed is that the WAL receivers are timed out exactly after 5seconds but it takes some 30-37 additional seconds  (which sums up to 35-42 seconds after the node failure, if we calculate the 5s timeout we had for wal receivers) for the cluster to even just start the failover process in the first place. At least we do not see anything in the logs related to the failover during this time. And although we tried to modify some of the config parameters we didn't achieve better results.\r\nI thought to myself that maybe this time might be impacted from something hardcoded, and after looking a little bit at the source code I doubt that this time is coming from the cluster status timeout defined here, but I am not a golang dev or expert, so I don't know:\r\n![image](https://github.com/user-attachments/assets/3bc3fcbc-04b4-45d8-a4ba-462674472fc2)\r\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/72e2f5c490fd17346f1f9ff539589a8d9f01690c/internal/cmd/manager/instance/status/cmd.go#L121\r\nSomewhere I read that in case of node failure the cloudnativepg looks at the node status if it is not-ready or unreachable, and then based on tolerations decides to kill the primary pod and start the failover process. If that is true I now that by default the node-monitor-grace-period is 40s that means that it takes 40s for the kube controller manager to mark node as unhealthy, and that config cannot be adapted. If that is true why is the cloudnativePG relying on that and not triggering the failover process based on its cluster status results/timeouts?\r\nMy questions is: Is it possible to reduce somehow the failover time when the node of primary is killed or fails? Can we achieve the old range we had 25-35 seconds? Any idea?\r\nSome more details:\r\nBelow we see the logs from the client which was not able to write to db for around 51 seconds during the failover:\r\n`Connection is Valid: 2024-11-19T15:02:17.141189046+01:00Sql: INSERT INTO time (time) VALUES (to_timestamp('2024-11-19T15:02:17.141189046+01:00', 'YYYY-MM-DDThh24:mi:ss')::timestamp with time zone at time zone 'Etc/UTC');\r\nConnection is Valid: **2024-11-19T15:02:18.141194779+01:00**Sql: INSERT INTO time (time) VALUES (to_timestamp('2024-11-19T15:02:18.141194779+01:00', 'YYYY-MM-DDThh24:mi:ss')::timestamp with time zone at time zone 'Etc/UTC');\r\n2024-11-19T14:02:29.150Z  WARN 1 --- [   scheduling-1] com.zaxxer.hikari.pool.PoolBase          : HikariPool-1 - Failed to validate connection org.postgresql.jdbc.PgConnection@3fc0c66f (This connection has been closed.). Possibly consider using a shorter maxLifetime value.\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 10009ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:19.141219498+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:29.155141629+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:31.156196969+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:33.156514975+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:35.156847851+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:37.157299363+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:39.157611065+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:41.157907340+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:43.158215925+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:45.158519959+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:47.158822915+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:49.159112575+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:51.159612646+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:53.159916412+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:55.160769376+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:57.161084043+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:02:59.161413739+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:03:01.161729031+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:03:03.162039029+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:03:05.162364989+01:00\r\nProblem: HikariPool-1 - Connection is not available, request timed out after 2000ms (total=0, active=0, idle=0, waiting=0), Failure: 2024-11-19T15:03:07.162657327+01:00\r\nConnection is Valid: **2024-11-19T15:03:09.162953329+01:00**Sql: INSERT INTO time (time) VALUES (to_timestamp('2024-11-19T15:03:09.162953329+01:00', 'YYYY-MM-DDThh24:mi:ss')::timestamp with time zone at time zone 'Etc/UTC');\r\nOutage duration: 51\r\nConnection is Valid: 2024-11-19T15:03:09.331016425+01:00Sql: INSERT INTO time (time) VALUES (to_timestamp('2024-11-19T15:03:09.331016425+01:00', 'YYYY-MM-DDThh24:mi:ss')::timestamp with time zone at time zone 'Etc/UTC');\r\n`\r\nBelow we see the logs of the cluster replica which will be elected as primary after the failover.\r\nWe see that the wal receivers started to terminate 5seconds after the failure but it took around 37 additional seconds too see some further logs regarding the failover process itself:\r\n`{\"level\":\"info\",\"ts\":\"`**2024-11-19T14:02:23.147275668**Z`\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cluster-sample-1\",\"record\":{\"log_time\":\"2024-11-19 14:02:23.147 UTC\",\"process_id\":\"130\",\"session_id\":\"673c9568.82\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-11-19 13:40:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"08006\",\"message\":**\"terminating walreceiver** due to timeout\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:02:23.172584863Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:178\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:02:23.303569229Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:178\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:02:23.408597375Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cluster-sample-1\",\"record\":{\"log_time\":\"2024-11-19 14:02:23.408 UTC\",\"process_id\":\"43\",\"session_id\":\"673c9567.2b\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-11-19 13:40:55 UTC\",\"virtual_transaction_id\":\"148/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"invalid record length at 0/E0083D8: expected at least 24, got 0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:02:58.375493965Z\",\"logger\":\"Replicator\",\"msg\":\"synchronizing replication slots\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"err\":\"getting replication slot status from primary: failed to connect to `user=streaming_replica database=postgres`: 172.30.33.8:5432 (cluster-sample-rw): dial error: dial tcp 172.30.33.8:5432: connect: connection refused\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:02:58.376720004Z\",\"logger\":\"Replicator\",\"msg\":\"synchronizing replication slots\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"err\":\"getting replication slot status from primary: failed to connect to `user=streaming_replica database=postgres`: 172.30.33.8:5432 (cluster-sample-rw): dial error: dial tcp 172.30.33.8:5432: connect: connection refused\"}\r\n{\"level\":\"debug\",\"ts\":\"**2024-11-19T14:03:00.430552602Z**\",\"logger\":\"external_servers_reconciler\",\"msg\":\"starting up the external servers reconciler\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"575b16c0-d8c4-48a1-b3af-c29fff36cf1c\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:56\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.430567114Z\",\"msg\":\"Reconciling Cluster\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"5b2e572b-ae50-4134-9c2f-0844f9f2ceda\",\"caller\":\"internal/management/controller/instance_controller.go:115\",\"logging_pod\":\"cluster-sample-1\",\"cluster\":{\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.430617394Z\",\"msg\":\"Reconciling custom monitoring queries\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"5b2e572b-ae50-4134-9c2f-0844f9f2ceda\",\"caller\":\"internal/management/controller/instance_controller.go:807\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.430674265Z\",\"logger\":\"tbs_reconciler\",\"msg\":\"skipping the tablespace reconciler in replicas\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"bda1796f-7c85-407d-bf7d-3b9f7a6cf9b5\",\"caller\":\"internal/management/controller/tablespaces/reconciler.go:52\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.440898957Z\",\"logger\":\"tbs_reconciler\",\"msg\":\"skipping the tablespace reconciler in replicas\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4f42d5ab-b60c-4025-8837-f959b277137d\",\"caller\":\"internal/management/controller/tablespaces/reconciler.go:52\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.440916345Z\",\"logger\":\"external_servers_reconciler\",\"msg\":\"starting up the external servers reconciler\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"94d16774-2ee8-4d11-97a6-1b6c42da40a5\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:56\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.455005845Z\",\"logger\":\"external_servers_reconciler\",\"msg\":\"starting up the external servers reconciler\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"9f1e441b-8e61-4364-94a4-2c0b5d24bc5c\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:56\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.455086917Z\",\"logger\":\"tbs_reconciler\",\"msg\":\"skipping the tablespace reconciler in replicas\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"0000e57f-6943-453b-8239-bb9dd88ba92b\",\"caller\":\"internal/management/controller/tablespaces/reconciler.go:52\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.464739371Z\",\"logger\":\"tbs_reconciler\",\"msg\":\"skipping the tablespace reconciler in replicas\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"e1bdbf8f-4c97-41eb-b7f1-98348b7eaa8b\",\"caller\":\"internal/management/controller/tablespaces/reconciler.go:52\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.46486145Z\",\"logger\":\"external_servers_reconciler\",\"msg\":\"starting up the external servers reconciler\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"e830b45c-fcdf-4214-890a-da3429522b03\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:56\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.476379816Z\",\"logger\":\"tbs_reconciler\",\"msg\":\"skipping the tablespace reconciler in replicas\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4ca5d71d-f9b4-4a3a-aa65-9e75d819ba6f\",\"caller\":\"internal/management/controller/tablespaces/reconciler.go:52\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.476433933Z\",\"logger\":\"external_servers_reconciler\",\"msg\":\"starting up the external servers reconciler\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"3e0bf7f3-9a00-4043-8b79-590f86b500d2\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:56\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.495962846Z\",\"msg\":\"Reconciling Cluster\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4a2d265e-5a49-458b-bae6-f1dd8cc957ff\",\"caller\":\"internal/management/controller/instance_controller.go:115\",\"logging_pod\":\"cluster-sample-1\",\"cluster\":{\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-11-19T14:03:00.496001228Z\",\"msg\":\"Reconciling custom monitoring queries\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4a2d265e-5a49-458b-bae6-f1dd8cc957ff\",\"caller\":\"internal/management/controller/instance_controller.go:807\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.497249192Z\",\"logger\":\"Replicator\",\"msg\":\"synchronizing replication slots\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"err\":\"getting replication slot status from primary: failed to connect to `user=streaming_replica database=postgres`: 172.30.33.8:5432 (cluster-sample-rw): dial error: dial tcp 172.30.33.8:5432: connect: connection refused\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.551899561Z\",\"msg\":\"Setting myself as primary\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4a2d265e-5a49-458b-bae6-f1dd8cc957ff\",\"logging_pod\":\"cluster-sample-1\",\"phase\":\"Failing over\",\"currentTimestamp\":\"2024-11-19T14:03:00.551882Z\",\"targetPrimaryTimestamp\":\"2024-11-19T14:03:00.463507Z\",\"currentPrimaryTimestamp\":\"2024-11-19T13:35:09.903398Z\",\"msPassedSinceTargetPrimaryTimestamp\":88,\"msPassedSinceCurrentPrimaryTimestamp\":1670648,\"msDifferenceBetweenCurrentAndTargetPrimary\":-1670560}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.551924762Z\",\"msg\":\"I'm the target primary, wait for the wal_receiver to be terminated\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4a2d265e-5a49-458b-bae6-f1dd8cc957ff\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.554814361Z\",\"msg\":\"I'm the target primary, applying WALs and promoting my instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4a2d265e-5a49-458b-bae6-f1dd8cc957ff\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.554833716Z\",\"msg\":\"Extracting pg_controldata information\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4a2d265e-5a49-458b-bae6-f1dd8cc957ff\",\"logging_pod\":\"cluster-sample-1\",\"reason\":\"promote\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.555859512Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1700\\nCatalog version number:               202406281\\nDatabase system identifier:           7438970221660221468\\nDatabase cluster state:               in archive recovery\\npg_control last modified:             Tue 19 Nov 2024 02:00:56 PM UTC\\nLatest checkpoint location:           0/E002458\\nLatest checkpoint's REDO location:    0/D0150D8\\nLatest checkpoint's REDO WAL file:    00000002000000000000000D\\nLatest checkpoint's TimeLineID:       2\\nLatest checkpoint's PrevTimeLineID:   2\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:2380\\nLatest checkpoint's NextOID:          24578\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        730\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  2380\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Tue 19 Nov 2024 02:00:09 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/E004618\\nMin recovery ending loc's timeline:   2\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            394ee91a692b830e68552a0f1ced84562d6115a83a3d2179e7ca9d5f34dffb03\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.555881163Z\",\"msg\":\"Promoting instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"cluster-sample-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"4a2d265e-5a49-458b-bae6-f1dd8cc957ff\",\"logging_pod\":\"cluster-sample-1\",\"pgctl_options\":[\"-D\",\"/var/lib/postgresql/data/pgdata\",\"-w\",\"promote\",\"-t 40000000\"]}\r\n`\r\nBelow we see the logs from the controller manager, which pod doesn't log anything at all during the first 42 seconds, after the node failure. It starts logging something about the failover process at the same time as the logs in the replica that we have seen.\r\n`\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T13:40:57.085473677Z\",\"msg\":\"Cluster is healthy\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"cc46950c-dbd8-4ff7-9473-87c616ad1206\"}\r\n{\"level\":\"info\",\"ts\":\"**2024-11-19T14:03:00.419148934Z**\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"name\":\"cluster-sample-2\",\"error\":\"Get \\\"https://10.244.5.186:8000/pg/status\\\": dial tcp 10.244.5.186:8000: i/o timeout\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.429902684Z\",\"msg\":\"Current primary isn't healthy, initiating a failover\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.42992148Z\",\"msg\":\"pod status (1 of 3)\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"name\":\"cluster-sample-1\",\"currentLsn\":\"\",\"receivedLsn\":\"0/E0083D8\",\"replayLsn\":\"0/E0083D8\",\"isPrimary\":false,\"isPodReady\":true,\"pendingRestart\":false,\"pendingRestartForDecrease\":false,\"statusCollectionError\":null}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.429937552Z\",\"msg\":\"pod status (2 of 3)\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"name\":\"cluster-sample-3\",\"currentLsn\":\"\",\"receivedLsn\":\"0/E0083D8\",\"replayLsn\":\"0/E0083D8\",\"isPrimary\":false,\"isPodReady\":true,\"pendingRestart\":false,\"pendingRestartForDecrease\":false,\"statusCollectionError\":null}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.429944256Z\",\"msg\":\"pod status (3 of 3)\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"name\":\"cluster-sample-2\",\"currentLsn\":\"\",\"receivedLsn\":\"\",\"replayLsn\":\"\",\"isPrimary\":false,\"isPodReady\":true,\"pendingRestart\":false,\"pendingRestartForDecrease\":false,\"statusCollectionError\":\"Get \\\"https://10.244.5.186:8000/pg/status\\\": dial tcp 10.244.5.186:8000: i/o timeout\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.430038243Z\",\"msg\":\"Cluster is not healthy\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.45246805Z\",\"msg\":\"Failing over\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"newPrimary\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.452497233Z\",\"msg\":\"pod status (1 of 3)\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"name\":\"cluster-sample-1\",\"currentLsn\":\"\",\"receivedLsn\":\"0/E0083D8\",\"replayLsn\":\"0/E0083D8\",\"isPrimary\":false,\"isPodReady\":true,\"pendingRestart\":false,\"pendingRestartForDecrease\":false,\"statusCollectionError\":null}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.452513933Z\",\"msg\":\"pod status (2 of 3)\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"name\":\"cluster-sample-3\",\"currentLsn\":\"\",\"receivedLsn\":\"0/E0083D8\",\"replayLsn\":\"0/E0083D8\",\"isPrimary\":false,\"isPodReady\":true,\"pendingRestart\":false,\"pendingRestartForDecrease\":false,\"statusCollectionError\":null}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.452520661Z\",\"msg\":\"pod status (3 of 3)\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"name\":\"cluster-sample-2\",\"currentLsn\":\"\",\"receivedLsn\":\"\",\"replayLsn\":\"\",\"isPrimary\":false,\"isPodReady\":true,\"pendingRestart\":false,\"pendingRestartForDecrease\":false,\"statusCollectionError\":\"Get \\\"https://10.244.5.186:8000/pg/status\\\": dial tcp 10.244.5.186:8000: i/o timeout\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.475254228Z\",\"msg\":\"Waiting for the new primary to notice the promotion request\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a773349e-854e-4e7a-959d-c2c667e4ca1d\",\"newPrimary\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.537044226Z\",\"msg\":\"There is a switchover or a failover in progress, waiting for the operation to complete\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"470d2f40-97a4-4262-90ea-d75074d539d2\",\"currentPrimary\":\"cluster-sample-2\",\"targetPrimary\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:00.592989434Z\",\"msg\":\"There is a switchover or a failover in progress, waiting for the operation to complete\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"78b742d6-3da3-4ed1-9fb2-1d270e23bdb2\",\"currentPrimary\":\"cluster-sample-2\",\"targetPrimary\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:03.575714289Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"a92cad56-4eec-4a17-9f55-9b5f8ebf59cc\",\"name\":\"cluster-sample-2\",\"error\":\"Get \\\"https://10.244.5.186:8000/pg/status\\\": dial tcp 10.244.5.186:8000: i/o timeout\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:05.708673906Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"065e7936-51e9-4979-a4a9-4eff0cc315bc\",\"name\":\"cluster-sample-2\",\"error\":\"Get \\\"https://10.244.5.186:8000/pg/status\\\": dial tcp 10.244.5.186:8000: i/o timeout\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:05.718818785Z\",\"msg\":\"Updating pvc metadata\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"065e7936-51e9-4979-a4a9-4eff0cc315bc\",\"pvc\":\"cluster-sample-1\",\"reconciler\":\"instance-role\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:05.727137941Z\",\"msg\":\"Updating pvc metadata\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"065e7936-51e9-4979-a4a9-4eff0cc315bc\",\"pvc\":\"cluster-sample-1-wal\",\"reconciler\":\"instance-role\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:05.735681781Z\",\"msg\":\"Updating pvc metadata\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"065e7936-51e9-4979-a4a9-4eff0cc315bc\",\"pvc\":\"cluster-sample-2\",\"reconciler\":\"instance-role\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:05.747854475Z\",\"msg\":\"Updating pvc metadata\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"065e7936-51e9-4979-a4a9-4eff0cc315bc\",\"pvc\":\"cluster-sample-2-wal\",\"reconciler\":\"instance-role\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:05.753609429Z\",\"msg\":\"Setting primary label\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"065e7936-51e9-4979-a4a9-4eff0cc315bc\",\"pod\":\"cluster-sample-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-11-19T14:03:05.76791717Z\",\"msg\":\"Setting replica label\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-sample\",\"namespace\":\"openshift-operators\"},\"namespace\":\"openshift-operators\",\"name\":\"cluster-sample\",\"reconcileID\":\"065e7936-51e9-4979-a4a9-4eff0cc315bc\",\"pod\":\"cluster-sample-2\"}\r\n`</div>Thank you for opening this ticket and for your time spent investigating. We will look into this and check all timeouts.\nFWIW, we highly discourage running the controller on the same nodes where Postgres runs. Postgres should be isolated from the rest, and proper anti-affinity rules should be in place: https://cloudnative-pg.io/documentation/current/architecture/#reserving-nodes-for-postgresql-workloads\n---\n@gbartolini Thank you for your reply. If you need anything else from me just let me know. I believe you saw also the other ticket with a feature request #6168 , because from my lasts tests I believe that the reason for the failover taking so long is because the failover on case of node failure is dependent on the **node-monitor-grace-period** parameter which is by default in Kubernetes 40s and cannot be changed in a supported way.\nActually putting the controller in the same node as a postgres workloads was only an early experiment, all the later failovertests were executed, when the controller pod was deployed in another node.\nBur are you saying that not only the controller pod, but also all workloads other than the postgres workloads should not be deployed to the three Kubernetes nodes where the postgres workloads live?\nThat means that it is also recommended to reseize these Kubernetes worker nodes differently from the others, and also have only one postgres cluster for multiple (or all) apps inside a Kubernetes cluster?\n---\n> [@gbartolini](https://github.com/gbartolini) Thank you for your reply. If you need anything else from me just let me know. I believe you saw also the other ticket with a feature request [#6168](https://github.com/cloudnative-pg/cloudnative-pg/issues/6168) , because from my lasts tests I believe that the reason for the failover taking so long is because the failover on case of node failure is dependent on the **node-monitor-grace-period** parameter which is by default in Kubernetes 40s and cannot be changed in a supported way.\nWell ... we are talking about a Kubernetes node failure. And our priority is to ensure that there is a consistent view in Kubernetes of not just the cluster, but everything around it, starting from services (through which apps connect). You don't want Kubernetes and the failover manager to have different views on the cluster (this is why we don't have a separate failover manager in CNPG).\nCan you provide more information on the environment you have used for these tests and the actual steps to reproduce the issue exactly?\n> Actually putting the controller in the same node as a postgres workloads was only an early experiment, all the later failovertests were executed, when the controller pod was deployed in another node.\nOk. Thanks.\n> Bur are you saying that not only the controller pod, but also all workloads other than the postgres workloads should not be deployed to the three Kubernetes nodes where the postgres workloads live? That means that it is also recommended to reseize these Kubernetes worker nodes differently from the others, and also have only one postgres cluster for multiple (or all) apps inside a Kubernetes cluster?\nYes. That is in line with our principle of \"cattle vs pets vs elephant herds\". PostgreSQL must be treated in a kind of special way, if you want to have best results. Dedicating nodes for Postgres workloads shouldn't be a problem today, thanks to logical (affinity) and physical (taints and labels) separation of workloads that makes them more similar to cattle than pets. This opens up for bare metal Postgres nodes with directly attached storage.\nI covered this in my last blog article: https://www.cncf.io/blog/2024/11/20/cloud-neutral-postgres-databases-with-kubernetes-and-cloudnativepg/\nThanks,\nGabriele\n---\n@gbartolini I am running openshift 4.15 on prem on top of vSphere cluster (vmware), I have multiple worker nodes and 3 master nodes all of them are in the end effect Virtual Machines on vSphere.\n1) I installed the latest cloudnativepg operator from the openshift operator market, and deployed a postgres cluster without changing much from the default configuration., except that I set the tolerationSeconds parameter to 0, since I don't want to wait another additional 300sec before the failover starts, that's documented here: https://cloudnative-pg.io/documentation/1.20/failure_modes/#:~:text=will%20happen%20after-,tolerationSeconds,-.\n2) In the end effect I have a postgres cluster with three replicas each of them deployed in different worker nodes (on those worker nodes there are still other workloads which are not related to postgres), and the controller manager pod of cloudnativepg is running on a fourth node.\n3) After everything is up and running I start my simple client pod which enters data (e.g. timestamp info) to database each second.\n4) After sometime I look in which worker node the primary replica of postgres cluster is living and I go on my vSphere cluster and shut-down exactly that virtualmachine which represents the worker node which hosts the primary replica.\n5) It takes something between 40-60seconds until my client is again able to write into it.\n6) As explained in this ticket and other linked tickets, I was assuming that this time is coming from the **node-monitor-grace-period** parameter which is by default in Openshift 40s and cannot be changed in a supported way. That's why I made a test and changed this parameter with an unsupportedConfigOverride\n$ oc edit kubecontrollermanagers cluster -n openshift-config spec: unsupportedConfigOverrides: extendedArguments: node-monitor-grace-period: - 20s\nAfter this change the failover time was ca. 20s smaller.\nKubernetes may have its own reasons why it waits by default 40s+5mins until the pods are notified that their node is gone. But if we want to have an application which is high available then those default values can never be true. We are using for example NATS as message broker they are also very cloud native from very beginning and they have a very fast failover time, based on their own timeouts. They have also a kind of controller and as soon as the controller notices that a replica is not healthy it will initiate some changes. They are not building the failover mechanism based on what some Kubernetes standards try to introduce as a silver bullet. The custom timeout would be very similar to readinessProbe or livnessProbe which are faster than the node failure mechanism for now.\nYou said the cloudnativepg is special and should be treated special, why should it be different this time, why should we use only the default solution that Kubernetes offers us in this particular case?\n---\n@gbartolini @mnencia @leonardoce Are there any new updates regarding this issue?\n---\nWe're seeing the same behavior and are interested in faster failover times as well. Any plan or updates?"
    },
    {
        "title": "[Bug]: walStorage pvc will not grow",
        "id": 2681224879,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nscirner@provlabs.io\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nHelm\n### What happened?\nI configured backups to gcs for the first time and immediately the walStorage pvc filled up. The steady state was 1gb used and after configuring the backup it grew to the limit of 10gb. The pod is now continuously crashing the following.\n```\n{\"level\":\"info\",\"ts\":\"2024-11-21T22:32:52.250211742Z\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logger\":\"instance-manager\",\"logging_pod\":\"timescaledb-cluster-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-21T22:32:52.273202361Z\",\"msg\":\"Detected low-disk space condition, avoid starting the instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"timescaledb-cluster-1\"}\n```\nThe operator is continuously logging the following.\n```\n{\"level\":\"info\",\"ts\":\"2024-11-21T22:35:41.842835454Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"timescaledb-cluster\",\"namespace\":\"database\"},\"namespace\":\"database\",\"name\":\"timescaledb-cluster\",\"reconcileID\":\"2ee140e4-a3ec-4996-9181-3b69e34ff69c\",\"name\":\"timescaledb-cluster-1\",\"error\":\"Get \\\"https://10.2.0.47:8000/pg/status\\\": dial tcp 10.2.0.47:8000: connect: connection refused\"}\n{\"level\":\"info\",\"ts\":\"2024-11-21T22:35:41.842995404Z\",\"logger\":\"ensure_sufficient_disk_space\",\"msg\":\"Insufficient disk space detected in a pod. PostgreSQL cannot proceed until the PVC group is enlarged\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"timescaledb-cluster\",\"namespace\":\"database\"},\"namespace\":\"database\",\"name\":\"timescaledb-cluster\",\"reconcileID\":\"2ee140e4-a3ec-4996-9181-3b69e34ff69c\",\"instanceNames\":[\"timescaledb-cluster-1\"]}\n```\nThe storage class allows expansion because I just grew the storage volume earlier.\n```\nkubectl get storageclasses.storage.k8s.io standard-rwo -o jsonpath='{$.allowVolumeExpansion}'\ntrue\n```\nI tried to get into the Slack community to ask this question, but the link on GitHub seems to be out of date.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  labels:\n    app.kubernetes.io/instance: timescaledb-cluster\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: timescaledb-cluster\n    app.kubernetes.io/part-of: cloudnative-pg\n    argocd.redacted.io/app: timescaledb\n    helm.sh/chart: cluster-0.1.0\n  name: timescaledb-cluster\n  namespace: database\nspec:\n  affinity:\n    podAntiAffinityType: preferred\n    topologyKey: topology.kubernetes.io/zone\n  backup:\n    barmanObjectStore:\n      data:\n        compression: gzip\n        encryption: AES256\n        jobs: 2\n      destinationPath: gs://redacted-test-cnpg-backup/timescaledb-cluster\n      googleCredentials:\n        applicationCredentials:\n          key: APPLICATION_CREDENTIALS\n          name: cnpg-backup-sa\n      wal:\n        compression: gzip\n        encryption: AES256\n        maxParallel: 1\n    retentionPolicy: 30d\n    target: prefer-standby\n  bootstrap:\n    initdb:\n      database: app\n      encoding: UTF8\n      localeCType: C\n      localeCollate: C\n      owner: app\n      postInitApplicationSQL:\n      - CREATE EXTENSION IF NOT EXISTS timescaledb;\n  enablePDB: true\n  enableSuperuserAccess: true\n  failoverDelay: 0\n  imageCatalogRef:\n    apiGroup: postgresql.cnpg.io\n    kind: ImageCatalog\n    major: 16\n    name: timescaledb-cluster-timescaledb-ha\n  imagePullPolicy: IfNotPresent\n  instances: 1\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n    - key: queries\n      name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: false\n  postgresGID: 1000\n  postgresUID: 1000\n  postgresql:\n    parameters:\n      archive_mode: \"on\"\n      archive_timeout: 5min\n      dynamic_shared_memory_type: posix\n      full_page_writes: \"on\"\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: \"0\"\n      log_rotation_size: \"0\"\n      log_truncate_on_rotation: \"false\"\n      logging_collector: \"on\"\n      max_parallel_workers: \"32\"\n      max_replication_slots: \"32\"\n      max_worker_processes: \"32\"\n      shared_memory_type: mmap\n      shared_preload_libraries: \"\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      wal_keep_size: 512MB\n      wal_level: logical\n      wal_log_hints: \"on\"\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n    shared_preload_libraries:\n    - timescaledb\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: switchover\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  smartShutdownTimeout: 180\n  startDelay: 3600\n  stopDelay: 1800\n  storage:\n    resizeInUseVolumes: true\n    size: 120Gi\n    storageClass: standard-rwo\n  switchoverDelay: 3600\n  walStorage:\n    resizeInUseVolumes: true\n    size: 20Gi\n    storageClass: standard-rwo\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nscirner@provlabs.io\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nHelm\n### What happened?\nI configured backups to gcs for the first time and immediately the walStorage pvc filled up. The steady state was 1gb used and after configuring the backup it grew to the limit of 10gb. The pod is now continuously crashing the following.\n```\n{\"level\":\"info\",\"ts\":\"2024-11-21T22:32:52.250211742Z\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logger\":\"instance-manager\",\"logging_pod\":\"timescaledb-cluster-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-21T22:32:52.273202361Z\",\"msg\":\"Detected low-disk space condition, avoid starting the instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"timescaledb-cluster-1\"}\n```\nThe operator is continuously logging the following.\n```\n{\"level\":\"info\",\"ts\":\"2024-11-21T22:35:41.842835454Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"timescaledb-cluster\",\"namespace\":\"database\"},\"namespace\":\"database\",\"name\":\"timescaledb-cluster\",\"reconcileID\":\"2ee140e4-a3ec-4996-9181-3b69e34ff69c\",\"name\":\"timescaledb-cluster-1\",\"error\":\"Get \\\"https://10.2.0.47:8000/pg/status\\\": dial tcp 10.2.0.47:8000: connect: connection refused\"}\n{\"level\":\"info\",\"ts\":\"2024-11-21T22:35:41.842995404Z\",\"logger\":\"ensure_sufficient_disk_space\",\"msg\":\"Insufficient disk space detected in a pod. PostgreSQL cannot proceed until the PVC group is enlarged\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"timescaledb-cluster\",\"namespace\":\"database\"},\"namespace\":\"database\",\"name\":\"timescaledb-cluster\",\"reconcileID\":\"2ee140e4-a3ec-4996-9181-3b69e34ff69c\",\"instanceNames\":[\"timescaledb-cluster-1\"]}\n```\nThe storage class allows expansion because I just grew the storage volume earlier.\n```\nkubectl get storageclasses.storage.k8s.io standard-rwo -o jsonpath='{$.allowVolumeExpansion}'\ntrue\n```\nI tried to get into the Slack community to ask this question, but the link on GitHub seems to be out of date.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  labels:\n    app.kubernetes.io/instance: timescaledb-cluster\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: timescaledb-cluster\n    app.kubernetes.io/part-of: cloudnative-pg\n    argocd.redacted.io/app: timescaledb\n    helm.sh/chart: cluster-0.1.0\n  name: timescaledb-cluster\n  namespace: database\nspec:\n  affinity:\n    podAntiAffinityType: preferred\n    topologyKey: topology.kubernetes.io/zone\n  backup:\n    barmanObjectStore:\n      data:\n        compression: gzip\n        encryption: AES256\n        jobs: 2\n      destinationPath: gs://redacted-test-cnpg-backup/timescaledb-cluster\n      googleCredentials:\n        applicationCredentials:\n          key: APPLICATION_CREDENTIALS\n          name: cnpg-backup-sa\n      wal:\n        compression: gzip\n        encryption: AES256\n        maxParallel: 1\n    retentionPolicy: 30d\n    target: prefer-standby\n  bootstrap:\n    initdb:\n      database: app\n      encoding: UTF8\n      localeCType: C\n      localeCollate: C\n      owner: app\n      postInitApplicationSQL:\n      - CREATE EXTENSION IF NOT EXISTS timescaledb;\n  enablePDB: true\n  enableSuperuserAccess: true\n  failoverDelay: 0\n  imageCatalogRef:\n    apiGroup: postgresql.cnpg.io\n    kind: ImageCatalog\n    major: 16\n    name: timescaledb-cluster-timescaledb-ha\n  imagePullPolicy: IfNotPresent\n  instances: 1\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n    - key: queries\n      name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: false\n  postgresGID: 1000\n  postgresUID: 1000\n  postgresql:\n    parameters:\n      archive_mode: \"on\"\n      archive_timeout: 5min\n      dynamic_shared_memory_type: posix\n      full_page_writes: \"on\"\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: \"0\"\n      log_rotation_size: \"0\"\n      log_truncate_on_rotation: \"false\"\n      logging_collector: \"on\"\n      max_parallel_workers: \"32\"\n      max_replication_slots: \"32\"\n      max_worker_processes: \"32\"\n      shared_memory_type: mmap\n      shared_preload_libraries: \"\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      wal_keep_size: 512MB\n      wal_level: logical\n      wal_log_hints: \"on\"\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n    shared_preload_libraries:\n    - timescaledb\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: switchover\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  smartShutdownTimeout: 180\n  startDelay: 3600\n  stopDelay: 1800\n  storage:\n    resizeInUseVolumes: true\n    size: 120Gi\n    storageClass: standard-rwo\n  switchoverDelay: 3600\n  walStorage:\n    resizeInUseVolumes: true\n    size: 20Gi\n    storageClass: standard-rwo\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductI manually resized the wal pvc and deleted the pod in order to get it to startup correctly and I fixed the underlying backup problem by adding a role to the SA so that continuous wal can now get archived, but this leaves me wondering what if the DB is unable to connect to GCS for even a short period of time, won't the wal storage fill up very quickly since the segments will stay on disk until they can get backed up and once the storage fills up the node will crash continuously? Even if the GCS connection can be re-established, the disk will be so full the node will not start.\n---\nI hit this issue as well. I had to manually resize the PVCs.\n---\nHi, is there any update on this issue please ? I have the same problem. When I enable backup, the WAL pvc fills up quickly (10GiB).\n---\n@EBMBA: Can you check if you have multiple backups piling up?\n- `SELECT * FROM pg_stat_activity` on the primary or replica that your backups are targeting should give you a good idea.\n- `kubectl get backups -n <namespace>` \n- Are any of your pods marked as not-ready?\n---\nI'm going to try to turn the backups back on and debug it more closely."
    },
    {
        "title": "[Feature]: export number of connections to pgbouncer used and available",
        "id": 2681126161,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nAt the moment it's impossible to estimate how well connection pool is doing.\nFor instance in crunchydata-pgo there are two metrics:\n* ccp_connection_stats_total\n* ccp_connection_stats_max_connections\nWhich correspondingly expose max number of connections, and current. So you can monitor the ratio.\nWith current cnpg implementation I'm not sure I can find anything similar.\n### Describe the solution you'd like\nTwo metrics in pgbouncer exporter that expose number of connections configured and currently in use.\n### Describe alternatives you've considered\nThere are no.\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nAt the moment it's impossible to estimate how well connection pool is doing.\nFor instance in crunchydata-pgo there are two metrics:\n* ccp_connection_stats_total\n* ccp_connection_stats_max_connections\nWhich correspondingly expose max number of connections, and current. So you can monitor the ratio.\nWith current cnpg implementation I'm not sure I can find anything similar.\n### Describe the solution you'd like\nTwo metrics in pgbouncer exporter that expose number of connections configured and currently in use.\n### Describe alternatives you've considered\nThere are no.\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductOk, I found there is `cnpg_pgbouncer_lists_used_clients` and `cnpg_pgbouncer_lists_free_clients`.\nI think it's what I asked for and not sure how I overlooked it \ud83e\udd37\n---\nActually, it might be not it:\n```\nfree_clients\nCount of free clients. These are clients that are disconnected, but PgBouncer keeps the memory around that was allocated for them so it can be reused for a future clients to avoid allocations.\n```"
    },
    {
        "title": "[Feature]: Support additional volume mounts",
        "id": 2680915877,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWe have data stored in S3. We can mount S3 as an additional mount volume to the pod to bulk load data\n### Describe the solution you'd like\nSupport additional volume mounts. Zalando's Postgres-operator supports this option \n### Describe alternatives you've considered\nAdd mount s3 as part of the container binary and run mount s3 as root\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWe have data stored in S3. We can mount S3 as an additional mount volume to the pod to bulk load data\n### Describe the solution you'd like\nSupport additional volume mounts. Zalando's Postgres-operator supports this option \n### Describe alternatives you've considered\nAdd mount s3 as part of the container binary and run mount s3 as root\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductCould you please elaborate more on the reasons why you need multiple volumes?\nThe direction we will take is to delegate features like this to the new CNPG-I plugin interface.\n---\nhey @andyndang @gbartolini how is it going? I want to mount some custom stuffs into /docker-entrypoint-initdb.d, like:\n![Image](https://github.com/user-attachments/assets/58d7d049-ae0e-4286-901b-d62828bb2e32)\n---\nBasically, I want to execute an migrate.sh script in the **docker-entrypoint-initdb.d**.\nBut it seems that cloudnative-pg does not run it.\n![Image](https://github.com/user-attachments/assets/9f9a409f-d70a-4d95-a04a-8525856cd67e)"
    },
    {
        "title": "[Bug]: cnpg status cannot show physical backup status initiated by cnpg",
        "id": 2680436488,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njeffreymealo@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nYAML manifest\n### What happened?\n**PostgreSQL Implementation Detail:**\nBackups started with `pg_backup_start` do not appear in the `pg_stat_progress_basebackup` view.\n**Behavior observed:**\n- Manually created backups of type `barmanObjectStore` do not appear under Physical Backups in the `kubectl cnpg status` view.\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njeffreymealo@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nYAML manifest\n### What happened?\n**PostgreSQL Implementation Detail:**\nBackups started with `pg_backup_start` do not appear in the `pg_stat_progress_basebackup` view.\n**Behavior observed:**\n- Manually created backups of type `barmanObjectStore` do not appear under Physical Backups in the `kubectl cnpg status` view.\n### Cluster resource\n```shell\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductI think that we could set the `application_name` to include the name of the backup so we could extract the in-progress backups using a query to check against running queries. I haven't found a way to monitor the progress of these backups though.\n---\n**Somewhat related:** https://github.com/cloudnative-pg/cloudnative-pg/pull/5998\n---\n@gbartolini: I think we should remove this from the status screen or fix it."
    },
    {
        "title": "[Docs]: WorkerNode Failure scenario clarification when using OpenEBS LocalPV",
        "id": 2674892601,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThe CNPG documentation on [worker node failure](https://cloudnative-pg.io/documentation/current/failure_modes/#worker-node-failure) though says \"A new pod will be created on a different worker node from a physical backup of the primary. The default value for that parameter in a Kubernetes cluster is 5 minutes. Self-healing will happen after tolerationSeconds.\" Additionally in the [self-healing section of the documentation](https://cloudnative-pg.io/documentation/current/failure_modes/#self-healing) is says \"The former primary will use pg_rewind to synchronise itself with the new one if its PVC is available; otherwise, a new standby will be created from a backup of the current primary.\"\nBased on experimentation with OpenEBS local PV, I believe this section perhaps doesn't cover the Local PV use case. The documentation seems to assume some kind of replicated storage or networked PV solution.\n### Describe what additions need to be done to the documentation\nIn the event of Node Failure with OpenEBS LocalPV, the PG pod on the failed node is expected to be stuck in terminating state well past any grace periods and toleration seconds.  The pod on the failed node would only get rescheduled if the PVC bound to the failed node comes back online.  In practice the only way to get the pod to reschedule and Cluster resource status and instances visible via kubectl cnpg status <cluster-name> to sync correctly is if you manually delete the pod and its PVC at the same time.\n### Describe what pages need to change in the documentation, if any\n1. https://github.com/cloudnative-pg/cloudnative-pg/blob/c556f597345d386cf5cb8e6a0f4a68b162e6f4ad/docs/src/failure_modes.md#worker-node-failure\n2. https://github.com/cloudnative-pg/cloudnative-pg/blob/c556f597345d386cf5cb8e6a0f4a68b162e6f4ad/docs/src/failure_modes.md#worker-node-failure\n### Describe what pages need to be removed from the documentation, if any\nNot Applicable\n### Additional context\nI am running three instances on a CNPG cluster  with the following package versions:\n- CNPG: v1.24\n- OpenEBS: v4.1.1 with Local PV Hostpath\nDiscussed on the CNPG Slack \ud83e\uddf5  [here](https://postgresteam.slack.com/archives/C0501L0QP1N/p1728560908414279)\n### Backport?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThe CNPG documentation on [worker node failure](https://cloudnative-pg.io/documentation/current/failure_modes/#worker-node-failure) though says \"A new pod will be created on a different worker node from a physical backup of the primary. The default value for that parameter in a Kubernetes cluster is 5 minutes. Self-healing will happen after tolerationSeconds.\" Additionally in the [self-healing section of the documentation](https://cloudnative-pg.io/documentation/current/failure_modes/#self-healing) is says \"The former primary will use pg_rewind to synchronise itself with the new one if its PVC is available; otherwise, a new standby will be created from a backup of the current primary.\"\nBased on experimentation with OpenEBS local PV, I believe this section perhaps doesn't cover the Local PV use case. The documentation seems to assume some kind of replicated storage or networked PV solution.\n### Describe what additions need to be done to the documentation\nIn the event of Node Failure with OpenEBS LocalPV, the PG pod on the failed node is expected to be stuck in terminating state well past any grace periods and toleration seconds.  The pod on the failed node would only get rescheduled if the PVC bound to the failed node comes back online.  In practice the only way to get the pod to reschedule and Cluster resource status and instances visible via kubectl cnpg status <cluster-name> to sync correctly is if you manually delete the pod and its PVC at the same time.\n### Describe what pages need to change in the documentation, if any\n1. https://github.com/cloudnative-pg/cloudnative-pg/blob/c556f597345d386cf5cb8e6a0f4a68b162e6f4ad/docs/src/failure_modes.md#worker-node-failure\n2. https://github.com/cloudnative-pg/cloudnative-pg/blob/c556f597345d386cf5cb8e6a0f4a68b162e6f4ad/docs/src/failure_modes.md#worker-node-failure\n### Describe what pages need to be removed from the documentation, if any\nNot Applicable\n### Additional context\nI am running three instances on a CNPG cluster  with the following package versions:\n- CNPG: v1.24\n- OpenEBS: v4.1.1 with Local PV Hostpath\nDiscussed on the CNPG Slack \ud83e\uddf5  [here](https://postgresteam.slack.com/archives/C0501L0QP1N/p1728560908414279)\n### Backport?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: Cluster in healthy state despite \"WAL archive check failed\"",
        "id": 2671771914,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\ntrunk (main)\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nYAML manifest\n### What happened?\nI deleted and replaced a Cluster with a `spec.backup.barmanObjectStore` without emptying the old WAL archive. [According to the docs](https://cloudnative-pg.io/documentation/1.20/recovery/#restoring-into-a-cluster-with-a-backup-section), this should result in a failed state of the Cluster:\n> The operator includes a safety check to ensure a cluster will not overwrite a storage bucket that contained information. A cluster that would overwrite existing storage will remain in state Setting up primary with Pods in an Error state. The pod logs will show: ERROR: WAL archive check failed for server recoveredCluster: Expected empty archive\nHowever, the cluster started without reporting any errors and reached \"healthy state\". No new WALs were written to the object store though. Logs reported \"WAL archive check failed for the server\" only after the Cluster was in healthy state, and this did not change the state of the cluster nor were any events created.\nNB: I'm using the postgis image below. But the same problem appears to have occurred when we were using the default image.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: cluster-signalen\n  namespace: signalen-common\nspec:\n  instances: 2\n  imageName: ghcr.io/cloudnative-pg/postgis:17-3.5\n  storage:\n    storageClass: signalen-postgres\n    size: 10Gi\n  walStorage:\n    storageClass: signalen-postgres\n    size: 10Gi\n  backup:\n    volumeSnapshot:\n      className: csi-azuredisk\n    barmanObjectStore:\n      destinationPath: https://xxx\n      azureCredentials:\n        storageAccount:\n          name: azure-creds\n          key: AZURE_STORAGE_ACCOUNT\n        storageKey:\n          name: azure-creds\n          key: AZURE_STORAGE_KEY\n      wal:\n        compression: bzip2\n  resources:\n    requests:\n      memory: \"256Mi\"\n      cpu: \"100m\"\n    limits:\n      memory: \"512Mi\"\n      cpu: \"200m\"\n  monitoring:\n    enablePodMonitor: true\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-11-19T10:39:55.492270222Z\",\"logger\":\"wal-archive\",\"msg\":\"barman-cloud-check-wal-archive checking the first wal\",\"logging_pod\":\"cluster-signalen-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-19T10:39:56.737635531Z\",\"logger\":\"barman-cloud-check-wal-archive\",\"msg\":\"2024-11-19 10:39:56,737 [696] ERROR: WAL archive check failed for server cluster-signalen: Expected empty archive\",\"pipe\":\"stderr\",\"logging_pod\":\"cluster-signalen-1\"}\n{\"level\":\"error\",\"ts\":\"2024-11-19T10:39:56.896669111Z\",\"logger\":\"wal-archive\",\"msg\":\"Error invoking barman-cloud-check-wal-archive\",\"logging_pod\":\"cluster-signalen-1\",\"options\":[\"--cloud-provider\",\"azure-blob-storage\",\"https://cgtestpostgresbackup.blob.core.windows.net/signalen\",\"cluster-signalen\"],\"exitCode\":-1,\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241030141148-670a0f16f836/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/barman-cloud/pkg/walarchive.(*BarmanArchiver).CheckWalArchiveDestination\\n\\tpkg/mod/github.com/cloudnative-pg/barman-cloud@v0.0.0-20241105055149-ae6c2408bd14/pkg/walarchive/cmd.go:178\\ngithub.com/cloudnative-pg/barman-cloud/pkg/archiver.(*WALArchiver).CheckWalArchiveDestination\\n\\tpkg/mod/github.com/cloudnative-pg/barman-cloud@v0.0.0-20241105055149-ae6c2408bd14/pkg/archiver/archiver.go:131\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.checkWalArchive\\n\\tinternal/cmd/manager/walarchive/cmd.go:314\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.run\\n\\tinternal/cmd/manager/walarchive/cmd.go:197\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:84\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.3/x64/src/runtime/proc.go:272\"}\n{\"level\":\"error\",\"ts\":\"2024-11-19T10:39:56.896802536Z\",\"logger\":\"wal-archive\",\"msg\":\"while barman-cloud-check-wal-archive\",\"logging_pod\":\"cluster-signalen-1\",\"error\":\"unexpected failure invoking barman-cloud-wal-archive: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241030141148-670a0f16f836/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.checkWalArchive\\n\\tinternal/cmd/manager/walarchive/cmd.go:315\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.run\\n\\tinternal/cmd/manager/walarchive/cmd.go:197\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:84\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.3/x64/src/runtime/proc.go:272\"}\n{\"level\":\"error\",\"ts\":\"2024-11-19T10:39:56.896886127Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"cluster-signalen-1\",\"error\":\"unexpected failure invoking barman-cloud-wal-archive: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241030141148-670a0f16f836/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.3/x64/src/runtime/proc.go:272\"}\n{\"level\":\"info\",\"ts\":\"2024-11-19T10:39:57.003390311Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cluster-signalen-1\",\"record\":{\"log_time\":\"2024-11-19 10:39:57.003 UTC\",\"process_id\":\"35\",\"session_id\":\"673c69ce.23\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-19 10:34:54 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/00000001000000000000004F\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\ntrunk (main)\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nYAML manifest\n### What happened?\nI deleted and replaced a Cluster with a `spec.backup.barmanObjectStore` without emptying the old WAL archive. [According to the docs](https://cloudnative-pg.io/documentation/1.20/recovery/#restoring-into-a-cluster-with-a-backup-section), this should result in a failed state of the Cluster:\n> The operator includes a safety check to ensure a cluster will not overwrite a storage bucket that contained information. A cluster that would overwrite existing storage will remain in state Setting up primary with Pods in an Error state. The pod logs will show: ERROR: WAL archive check failed for server recoveredCluster: Expected empty archive\nHowever, the cluster started without reporting any errors and reached \"healthy state\". No new WALs were written to the object store though. Logs reported \"WAL archive check failed for the server\" only after the Cluster was in healthy state, and this did not change the state of the cluster nor were any events created.\nNB: I'm using the postgis image below. But the same problem appears to have occurred when we were using the default image.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: cluster-signalen\n  namespace: signalen-common\nspec:\n  instances: 2\n  imageName: ghcr.io/cloudnative-pg/postgis:17-3.5\n  storage:\n    storageClass: signalen-postgres\n    size: 10Gi\n  walStorage:\n    storageClass: signalen-postgres\n    size: 10Gi\n  backup:\n    volumeSnapshot:\n      className: csi-azuredisk\n    barmanObjectStore:\n      destinationPath: https://xxx\n      azureCredentials:\n        storageAccount:\n          name: azure-creds\n          key: AZURE_STORAGE_ACCOUNT\n        storageKey:\n          name: azure-creds\n          key: AZURE_STORAGE_KEY\n      wal:\n        compression: bzip2\n  resources:\n    requests:\n      memory: \"256Mi\"\n      cpu: \"100m\"\n    limits:\n      memory: \"512Mi\"\n      cpu: \"200m\"\n  monitoring:\n    enablePodMonitor: true\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-11-19T10:39:55.492270222Z\",\"logger\":\"wal-archive\",\"msg\":\"barman-cloud-check-wal-archive checking the first wal\",\"logging_pod\":\"cluster-signalen-1\"}\n{\"level\":\"info\",\"ts\":\"2024-11-19T10:39:56.737635531Z\",\"logger\":\"barman-cloud-check-wal-archive\",\"msg\":\"2024-11-19 10:39:56,737 [696] ERROR: WAL archive check failed for server cluster-signalen: Expected empty archive\",\"pipe\":\"stderr\",\"logging_pod\":\"cluster-signalen-1\"}\n{\"level\":\"error\",\"ts\":\"2024-11-19T10:39:56.896669111Z\",\"logger\":\"wal-archive\",\"msg\":\"Error invoking barman-cloud-check-wal-archive\",\"logging_pod\":\"cluster-signalen-1\",\"options\":[\"--cloud-provider\",\"azure-blob-storage\",\"https://cgtestpostgresbackup.blob.core.windows.net/signalen\",\"cluster-signalen\"],\"exitCode\":-1,\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241030141148-670a0f16f836/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/barman-cloud/pkg/walarchive.(*BarmanArchiver).CheckWalArchiveDestination\\n\\tpkg/mod/github.com/cloudnative-pg/barman-cloud@v0.0.0-20241105055149-ae6c2408bd14/pkg/walarchive/cmd.go:178\\ngithub.com/cloudnative-pg/barman-cloud/pkg/archiver.(*WALArchiver).CheckWalArchiveDestination\\n\\tpkg/mod/github.com/cloudnative-pg/barman-cloud@v0.0.0-20241105055149-ae6c2408bd14/pkg/archiver/archiver.go:131\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.checkWalArchive\\n\\tinternal/cmd/manager/walarchive/cmd.go:314\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.run\\n\\tinternal/cmd/manager/walarchive/cmd.go:197\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:84\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.3/x64/src/runtime/proc.go:272\"}\n{\"level\":\"error\",\"ts\":\"2024-11-19T10:39:56.896802536Z\",\"logger\":\"wal-archive\",\"msg\":\"while barman-cloud-check-wal-archive\",\"logging_pod\":\"cluster-signalen-1\",\"error\":\"unexpected failure invoking barman-cloud-wal-archive: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241030141148-670a0f16f836/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.checkWalArchive\\n\\tinternal/cmd/manager/walarchive/cmd.go:315\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.run\\n\\tinternal/cmd/manager/walarchive/cmd.go:197\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:84\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.3/x64/src/runtime/proc.go:272\"}\n{\"level\":\"error\",\"ts\":\"2024-11-19T10:39:56.896886127Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"cluster-signalen-1\",\"error\":\"unexpected failure invoking barman-cloud-wal-archive: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241030141148-670a0f16f836/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.3/x64/src/runtime/proc.go:272\"}\n{\"level\":\"info\",\"ts\":\"2024-11-19T10:39:57.003390311Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cluster-signalen-1\",\"record\":{\"log_time\":\"2024-11-19 10:39:57.003 UTC\",\"process_id\":\"35\",\"session_id\":\"673c69ce.23\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-11-19 10:34:54 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/00000001000000000000004F\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductExperiencing the same problem unfortunately. Using an image with base cloud native pg 17.0 to install pg-stat-kcache and set-user. However don't think that's the issue.\n---\nIt seems, I've got the same issue (running `cloudnative-pgvecto.rs:16.5-v0.3.0` image). I recreated a cluster, but forgot to delete backups on S3.\nUpon startup, `barman-cloud-check-wal-archive` was called, which indeed throws an `Expected empty archive` error, as per the documentation, because the bucket has some old backups. Unfortunately the cluster is already in a healthy state, as the author of this issue already stated. It lead to a lengthy debug ending up here...\nIt would have been nice, if the cluster initialization would simply fail with an error of \"backup destination is not empty.\"\nIt seems, the [barman-cloud-check-wal-archive](https://github.com/cloudnative-pg/cloudnative-pg/blob/v1.25.0-rc1/pkg/management/postgres/archiver/archiver.go#L319) needs to be called earlier, at some cluster initialization stage."
    },
    {
        "title": "[Feature]: kubectl cnpg psql connecting to pooler",
        "id": 2667810673,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nEasily connecting to a configured pooler as we do for instances, `kubectl cnpg psql <name>`.\n### Describe the solution you'd like\nWe could have a boolean flag to say that the name should be instead used as that of a `pooler`: `kubectl cnpg psql --pooler <name>`.\n### Describe alternatives you've considered\nCan't think of any other.\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nEasily connecting to a configured pooler as we do for instances, `kubectl cnpg psql <name>`.\n### Describe the solution you'd like\nWe could have a boolean flag to say that the name should be instead used as that of a `pooler`: `kubectl cnpg psql --pooler <name>`.\n### Describe alternatives you've considered\nCan't think of any other.\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: 4,800 requests per hour from CNPG on an S3 bucket used for backups unexpected",
        "id": 2666673452,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nSelf-managed: k0s\n### How did you install the operator?\nHelm\n### What happened?\nAWS shows tens of millions of requests in our postgres backup bucket, which is unexpected.\nThis adds up to about $500/mo in costs.\nAny insights as to why this would occur?\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: some-cluster\n  namespace: appmana\nspec:\n  instances: 3\n  storage:\n    size: 50Gi\n  imageName: 11111.dkr.ecr.us-west-2.amazonaws.com/postgresql:15.6\n  imagePullSecrets:\n    - name: harbor\n    - name: ecr-credentials\n  affinity:\n    nodeSelector:\n      kubernetes.io/os: linux\n  postgresql:\n    parameters:\n      shared_buffers: \"128MB\"\n  resources:\n    requests:\n      memory: \"512Mi\"\n    limits:\n      hugepages-2Mi: \"512Mi\"\n  bootstrap:\n    initdb:\n      database: appmana\n      owner: appmana\n      secret:\n        name: owner\n  enableSuperuserAccess: true\n  superuserSecret:\n    name: superuser\n  backup:\n    barmanObjectStore:\n      destinationPath: s3://some-bucket\n      s3Credentials:\n        accessKeyId:\n          name: s3-user\n          key: ACCESS_KEY_ID\n        secretAccessKey:\n          name: s3-user\n          key: ACCESS_SECRET_KEY\n    retentionPolicy: \"30d\"\n---\napiVersion: postgresql.cnpg.io/v1\nkind: ScheduledBackup\nmetadata:\n  name: some-cluster\n  namespace: appmana\nspec:\n  schedule: \"0 0 0 * * *\"\n  backupOwnerReference: self\n  cluster:\n    name: some-cluster\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nSelf-managed: k0s\n### How did you install the operator?\nHelm\n### What happened?\nAWS shows tens of millions of requests in our postgres backup bucket, which is unexpected.\nThis adds up to about $500/mo in costs.\nAny insights as to why this would occur?\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: some-cluster\n  namespace: appmana\nspec:\n  instances: 3\n  storage:\n    size: 50Gi\n  imageName: 11111.dkr.ecr.us-west-2.amazonaws.com/postgresql:15.6\n  imagePullSecrets:\n    - name: harbor\n    - name: ecr-credentials\n  affinity:\n    nodeSelector:\n      kubernetes.io/os: linux\n  postgresql:\n    parameters:\n      shared_buffers: \"128MB\"\n  resources:\n    requests:\n      memory: \"512Mi\"\n    limits:\n      hugepages-2Mi: \"512Mi\"\n  bootstrap:\n    initdb:\n      database: appmana\n      owner: appmana\n      secret:\n        name: owner\n  enableSuperuserAccess: true\n  superuserSecret:\n    name: superuser\n  backup:\n    barmanObjectStore:\n      destinationPath: s3://some-bucket\n      s3Credentials:\n        accessKeyId:\n          name: s3-user\n          key: ACCESS_KEY_ID\n        secretAccessKey:\n          name: s3-user\n          key: ACCESS_SECRET_KEY\n    retentionPolicy: \"30d\"\n---\napiVersion: postgresql.cnpg.io/v1\nkind: ScheduledBackup\nmetadata:\n  name: some-cluster\n  namespace: appmana\nspec:\n  schedule: \"0 0 0 * * *\"\n  backupOwnerReference: self\n  cluster:\n    name: some-cluster\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductDisabling backups correctly works around the issue, although you can see why I wouldn't want to do that.\nMultiple clusters share the same bucket, does this matter?\n---\nAs you said that stopping the backup resolved your issue, it might be linked to the archive mode.\nIf your instance/database is heavily used, it can lead to a lot of WAL files generated (which is ok, and the default behaviour of PostgreSQL) then send to S3 buckets as part of the archive process.\nHave you got any log that can help ? PostgreSQL instance logs ?\nHow many WAL files have you got in your bucket ?"
    },
    {
        "title": "[Bug]: Can't get role secret from imported DB to show up",
        "id": 2659233519,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nlgromb@oci.fr\n### Version\n1.23.4\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhen trying to perform a major upgrade from 15 to 16 (or 17), I followed the \"new cluster\" route.\nMy old-cluster does have a role which is superuser and is used by an app to create and manage multiple databases inside PG (ODOO ERP).\nWhen trying to upgrade, I set up a new cluster (see below), to import the DBs and Odoo roles from the old-cluster.\nThe new-cluster is successfully importing the DBs and upgrades them, also imports the odoo role. I can still connect using odoo with its password, but what bothers me is that the new generated secrets from CNPG is referring to the \"app\" role for new-cluster, but as intended, app wouldn't be used, and I'm trying to find a way to automatically connect Odoo to the new-cluster (as I'm using env referring secret keys in Odoo deployments).\nSince \"app\" is the only role present in the secret and have no rights to manage the imported dbs, the secret is, for me, totally useless.\nWhat would be the right way to finish the upgrade ? I want to delete old-cluster at the end, but that would delete the \"good\" secret containing the odoo credentials. I thought of cloning the existing secret but it won't be managed anymore by CNPG.\nIn the below cluster CRD, I wasn't sure if the managed stanza was utilized since I've specified a role already imported by bootstrap:.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: import-upgrade-pg-cluster\n  namespace: odoo-pgupgrade\nspec:\n  imageName: ghcr.io/cloudnative-pg/postgresql:16\n  bootstrap:\n    initdb:\n      import:\n        type: monolith\n        databases:\n          - testok\n          - test-upgrade\n        roles:\n          - odoo\n        source:\n          externalCluster: pgupgrade-pg-dev\n  managed:\n    roles:\n    - name: odoo\n      ensure: present\n      login: true\n      superuser: true\n  postgresql:\n    parameters:\n      shared_buffers: \"512MB\"\n  resources:\n    requests:\n      cpu: 10m\n      memory: 512Mi\n    limits:\n      cpu: 1000m\n      memory: 4Gi\n  enableSuperuserAccess: true\n  instances: 1\n  storage:\n    size: 15Gi\n  externalClusters:\n    - name: pgupgrade-pg-dev\n      connectionParameters:\n        host: pgupgrade-pg-dev-rw\n        user: odoo\n        dbname: postgres\n      password:\n        name: pgupgrade-pg-dev-app\n        key: password\n```\n### Relevant log output\n```shell\npostgres=# \\du\n                                 List of roles\n     Role name     |                         Attributes\n-------------------+------------------------------------------------------------\n app               |\n odoo              | Superuser\n postgres          | Superuser, Create role, Create DB, Replication, Bypass RLS\n streaming_replica | Replication\npostgres=# \\l\n                                                    List of databases\n     Name     |  Owner   | Encoding | Locale Provider | Collate | Ctype | ICU Locale | ICU Rules |   Access privileges\n--------------+----------+----------+-----------------+---------+-------+------------+-----------+-----------------------\n app          | app      | UTF8     | libc            | C       | C     |            |           |\n postgres     | postgres | UTF8     | libc            | C       | C     |            |           |\n template0    | postgres | UTF8     | libc            | C       | C     |            |           | =c/postgres          +\n              |          |          |                 |         |       |            |           | postgres=CTc/postgres\n template1    | postgres | UTF8     | libc            | C       | C     |            |           | =c/postgres          +\n              |          |          |                 |         |       |            |           | postgres=CTc/postgres\n test-upgrade | odoo     | UTF8     | libc            | C       | C     |            |           |\n testok       | odoo     | UTF8     | libc            | C       | C     |            |           |\n(6 rows)\npostgres=#\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nlgromb@oci.fr\n### Version\n1.23.4\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhen trying to perform a major upgrade from 15 to 16 (or 17), I followed the \"new cluster\" route.\nMy old-cluster does have a role which is superuser and is used by an app to create and manage multiple databases inside PG (ODOO ERP).\nWhen trying to upgrade, I set up a new cluster (see below), to import the DBs and Odoo roles from the old-cluster.\nThe new-cluster is successfully importing the DBs and upgrades them, also imports the odoo role. I can still connect using odoo with its password, but what bothers me is that the new generated secrets from CNPG is referring to the \"app\" role for new-cluster, but as intended, app wouldn't be used, and I'm trying to find a way to automatically connect Odoo to the new-cluster (as I'm using env referring secret keys in Odoo deployments).\nSince \"app\" is the only role present in the secret and have no rights to manage the imported dbs, the secret is, for me, totally useless.\nWhat would be the right way to finish the upgrade ? I want to delete old-cluster at the end, but that would delete the \"good\" secret containing the odoo credentials. I thought of cloning the existing secret but it won't be managed anymore by CNPG.\nIn the below cluster CRD, I wasn't sure if the managed stanza was utilized since I've specified a role already imported by bootstrap:.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: import-upgrade-pg-cluster\n  namespace: odoo-pgupgrade\nspec:\n  imageName: ghcr.io/cloudnative-pg/postgresql:16\n  bootstrap:\n    initdb:\n      import:\n        type: monolith\n        databases:\n          - testok\n          - test-upgrade\n        roles:\n          - odoo\n        source:\n          externalCluster: pgupgrade-pg-dev\n  managed:\n    roles:\n    - name: odoo\n      ensure: present\n      login: true\n      superuser: true\n  postgresql:\n    parameters:\n      shared_buffers: \"512MB\"\n  resources:\n    requests:\n      cpu: 10m\n      memory: 512Mi\n    limits:\n      cpu: 1000m\n      memory: 4Gi\n  enableSuperuserAccess: true\n  instances: 1\n  storage:\n    size: 15Gi\n  externalClusters:\n    - name: pgupgrade-pg-dev\n      connectionParameters:\n        host: pgupgrade-pg-dev-rw\n        user: odoo\n        dbname: postgres\n      password:\n        name: pgupgrade-pg-dev-app\n        key: password\n```\n### Relevant log output\n```shell\npostgres=# \\du\n                                 List of roles\n     Role name     |                         Attributes\n-------------------+------------------------------------------------------------\n app               |\n odoo              | Superuser\n postgres          | Superuser, Create role, Create DB, Replication, Bypass RLS\n streaming_replica | Replication\npostgres=# \\l\n                                                    List of databases\n     Name     |  Owner   | Encoding | Locale Provider | Collate | Ctype | ICU Locale | ICU Rules |   Access privileges\n--------------+----------+----------+-----------------+---------+-------+------------+-----------+-----------------------\n app          | app      | UTF8     | libc            | C       | C     |            |           |\n postgres     | postgres | UTF8     | libc            | C       | C     |            |           |\n template0    | postgres | UTF8     | libc            | C       | C     |            |           | =c/postgres          +\n              |          |          |                 |         |       |            |           | postgres=CTc/postgres\n template1    | postgres | UTF8     | libc            | C       | C     |            |           | =c/postgres          +\n              |          |          |                 |         |       |            |           | postgres=CTc/postgres\n test-upgrade | odoo     | UTF8     | libc            | C       | C     |            |           |\n testok       | odoo     | UTF8     | libc            | C       | C     |            |           |\n(6 rows)\npostgres=#\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: Operator deletes PodMonitor that it didn't create due to lack of owner reference check",
        "id": 2659060770,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nYAML manifest\n### What happened?\n1. https://github.com/cloudnative-pg/cloudnative-pg/issues/6108 , decide to set `monitoring.enablePodMonitor: false`. In the same commit, create a `PodMonitor` to be applied manually, with the same name (i.e. the cluster name) and namespace.\n2. Apply both changes simultaneously\n3. Wonder why I don't have a `PodMonitor`\n4. Go spelunking in the code: https://github.com/cloudnative-pg/cloudnative-pg/blob/3c2c3f695eafbf1a0db96954f1796ff37cbd0733/internal/controller/cluster_create.go#L959 gets a reference to the `PodMonitor` based on namespace and name only, *not* looking at the owner reference. Therefore https://github.com/cloudnative-pg/cloudnative-pg/blob/3c2c3f695eafbf1a0db96954f1796ff37cbd0733/internal/controller/cluster_create.go#L973 indeed finds the `PodMonitor` (that it doesn't have an owner reference on) and deletes it.\nI worked around the issue by adding a short suffix to the `PodMonitor` name so that it wouldn't be deleted, but this isn't ideal because it changes the job label that Prometheus attaches to it.\n### Cluster resource\n```shell\nN/A\n```\n### Relevant log output\n```shell\nN/A\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nYAML manifest\n### What happened?\n1. https://github.com/cloudnative-pg/cloudnative-pg/issues/6108 , decide to set `monitoring.enablePodMonitor: false`. In the same commit, create a `PodMonitor` to be applied manually, with the same name (i.e. the cluster name) and namespace.\n2. Apply both changes simultaneously\n3. Wonder why I don't have a `PodMonitor`\n4. Go spelunking in the code: https://github.com/cloudnative-pg/cloudnative-pg/blob/3c2c3f695eafbf1a0db96954f1796ff37cbd0733/internal/controller/cluster_create.go#L959 gets a reference to the `PodMonitor` based on namespace and name only, *not* looking at the owner reference. Therefore https://github.com/cloudnative-pg/cloudnative-pg/blob/3c2c3f695eafbf1a0db96954f1796ff37cbd0733/internal/controller/cluster_create.go#L973 indeed finds the `PodMonitor` (that it doesn't have an owner reference on) and deletes it.\nI worked around the issue by adding a short suffix to the `PodMonitor` name so that it wouldn't be deleted, but this isn't ideal because it changes the job label that Prometheus attaches to it.\n### Cluster resource\n```shell\nN/A\n```\n### Relevant log output\n```shell\nN/A\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: Prometheus v2.52+ fails to scrape metrics: Error on ingesting samples with different value but same timestamp",
        "id": 2658980518,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nI enabled the podMonitor and got a bug similar to https://github.com/prometheus/prometheus/issues/14089 .\nI was able to workaround the issue by creating a PodMonitor manually (not the one created by the operator) and setting `honorTimestamps: false`.\nIt seems that Prometheus thinks that the metrics are reporting the same timestamp and then it (v2.52+) refuses to accept the scrape.\n### Cluster resource\n```shell\nN/A\n```\n### Relevant log output\n```shell\nN/A\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nI enabled the podMonitor and got a bug similar to https://github.com/prometheus/prometheus/issues/14089 .\nI was able to workaround the issue by creating a PodMonitor manually (not the one created by the operator) and setting `honorTimestamps: false`.\nIt seems that Prometheus thinks that the metrics are reporting the same timestamp and then it (v2.52+) refuses to accept the scrape.\n### Cluster resource\n```shell\nN/A\n```\n### Relevant log output\n```shell\nN/A\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductHave you checked this issue : https://github.com/cloudnative-pg/cloudnative-pg/issues/4621 ?"
    },
    {
        "title": "[Feature]: Enhanced Point In Time Recovery for existing databases",
        "id": 2658078146,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently, the Point In Time Recovery feature allows for creating a new database from a specific point in time. However, there is a need for the ability to revert an existing database to a previous point in time, without changing its current backup configuration. This feature would be beneficial for users who need to recover data from a specific point in time, but do not want to create a new database or modify the existing backup configuration\n### Describe the solution you'd like\nThe solution I propose is to add a new feature to CloudNativePG that allows for reverting an existing database to a previous point in time. This feature would utilize the existing backup configuration to restore the database to the desired point in time.\nTo automate this process, I suggest adding a new Kubernetes Job resource that can be triggered to perform the point in time recovery. This Job would take the name of the database and the desired point in time as input, and would use the existing backup configuration to restore the database to the specified point in time. The Job would also handle any necessary cleanup or rollback operations in case of failure.\nThis solution would provide a more efficient and scalable way to recover data from a specific point in time, without requiring any changes to the existing backup configuration. Additionally, automating this process using a Kubernetes Job would provide a more efficient and scalable solution, especially for large databases or clusters.\n### Describe alternatives you've considered\nA more complex alternative approach would be to create a custom Kubernetes operator that can perform point in time recovery on demand. This operator would watch for custom resources that specify the database name and desired point in time, and would initiate the restore process using the existing backup configuration. This approach would provide the most flexibility and automation, but would also require the most development effort.\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently, the Point In Time Recovery feature allows for creating a new database from a specific point in time. However, there is a need for the ability to revert an existing database to a previous point in time, without changing its current backup configuration. This feature would be beneficial for users who need to recover data from a specific point in time, but do not want to create a new database or modify the existing backup configuration\n### Describe the solution you'd like\nThe solution I propose is to add a new feature to CloudNativePG that allows for reverting an existing database to a previous point in time. This feature would utilize the existing backup configuration to restore the database to the desired point in time.\nTo automate this process, I suggest adding a new Kubernetes Job resource that can be triggered to perform the point in time recovery. This Job would take the name of the database and the desired point in time as input, and would use the existing backup configuration to restore the database to the specified point in time. The Job would also handle any necessary cleanup or rollback operations in case of failure.\nThis solution would provide a more efficient and scalable way to recover data from a specific point in time, without requiring any changes to the existing backup configuration. Additionally, automating this process using a Kubernetes Job would provide a more efficient and scalable solution, especially for large databases or clusters.\n### Describe alternatives you've considered\nA more complex alternative approach would be to create a custom Kubernetes operator that can perform point in time recovery on demand. This operator would watch for custom resources that specify the database name and desired point in time, and would initiate the restore process using the existing backup configuration. This approach would provide the most flexibility and automation, but would also require the most development effort.\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct+1"
    },
    {
        "title": "[Bug]: Bootstrapping a new cluster using pg_basebackup from replica",
        "id": 2657843433,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nmaxpain177@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nBootstrapping using the `pg_basebackup` method doesn't work when using a `replica` IP instead of `primary` in the `externalClusters` section.\nThe `pg_basebackup` job successfully dumps the database but is stuck with the following logs:\n```\nspecified neither \\\"primary_conninfo\\\" nor \\\"restore_command\\\"\",\"hint\":\"The database server will regularly poll the pg_wal subdirectory to check for files placed there\n```\n`waiting for WAL to become available at 14B/3C002000`\nThere are no problems when bootstrapping from `primary`.\n### Cluster resource\n```yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: fc-dev-pg\nspec:\n  instances: 1\n  imageName: ghcr.io/cloudnative-pg/postgresql:17.0\n  bootstrap:\n    pg_basebackup:\n      source: fc-pg\n      database: fastcup\n      owner: fastcup\n      secret:\n        name: fc-dev-pg\n  externalClusters:\n    - name: fc-pg\n      connectionParameters:\n        # Production replica svc\n        host: fc-pg-ro.fastcup\n        user: streaming_replica\n        sslmode: verify-full\n      sslKey:\n        name: fc-pg-replication\n        key: tls.key\n      sslCert:\n        name: fc-pg-replication\n        key: tls.crt\n      sslRootCert:\n        name: fc-pg-ca\n        key: ca.crt\n  storage:\n    size: 1Ti\n    storageClass: nvme-lvm-local\n  postgresql:\n    parameters:\n      max_connections: \"300\"\n      shared_buffers: \"16GB\"\n      effective_cache_size: \"48GB\"\n      maintenance_work_mem: \"2GB\"\n      checkpoint_completion_target: \"0.9\"\n      wal_buffers: \"16MB\"\n      default_statistics_target: \"100\"\n      random_page_cost: \"1.1\"\n      effective_io_concurrency: \"200\"\n      work_mem: \"20971kB\"\n      huge_pages: \"try\"\n      min_wal_size: \"2GB\"\n      max_wal_size: \"8GB\"\n      max_worker_processes: \"16\"\n      max_parallel_workers_per_gather: \"4\"\n      max_parallel_workers: \"16\"\n      max_parallel_maintenance_workers: \"4\"\n      pg_stat_statements.max: \"10000\"\n      pg_stat_statements.track: all\n      pg_stat_statements.track_utility: \"true\"\n      pg_stat_statements.save: \"false\"\n      track_io_timing: \"on\"\n```\n### Relevant log output\n[pgbasebackup.log](https://github.com/user-attachments/files/17744632/pgbasebackup.log)\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nmaxpain177@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nBootstrapping using the `pg_basebackup` method doesn't work when using a `replica` IP instead of `primary` in the `externalClusters` section.\nThe `pg_basebackup` job successfully dumps the database but is stuck with the following logs:\n```\nspecified neither \\\"primary_conninfo\\\" nor \\\"restore_command\\\"\",\"hint\":\"The database server will regularly poll the pg_wal subdirectory to check for files placed there\n```\n`waiting for WAL to become available at 14B/3C002000`\nThere are no problems when bootstrapping from `primary`.\n### Cluster resource\n```yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: fc-dev-pg\nspec:\n  instances: 1\n  imageName: ghcr.io/cloudnative-pg/postgresql:17.0\n  bootstrap:\n    pg_basebackup:\n      source: fc-pg\n      database: fastcup\n      owner: fastcup\n      secret:\n        name: fc-dev-pg\n  externalClusters:\n    - name: fc-pg\n      connectionParameters:\n        # Production replica svc\n        host: fc-pg-ro.fastcup\n        user: streaming_replica\n        sslmode: verify-full\n      sslKey:\n        name: fc-pg-replication\n        key: tls.key\n      sslCert:\n        name: fc-pg-replication\n        key: tls.crt\n      sslRootCert:\n        name: fc-pg-ca\n        key: ca.crt\n  storage:\n    size: 1Ti\n    storageClass: nvme-lvm-local\n  postgresql:\n    parameters:\n      max_connections: \"300\"\n      shared_buffers: \"16GB\"\n      effective_cache_size: \"48GB\"\n      maintenance_work_mem: \"2GB\"\n      checkpoint_completion_target: \"0.9\"\n      wal_buffers: \"16MB\"\n      default_statistics_target: \"100\"\n      random_page_cost: \"1.1\"\n      effective_io_concurrency: \"200\"\n      work_mem: \"20971kB\"\n      huge_pages: \"try\"\n      min_wal_size: \"2GB\"\n      max_wal_size: \"8GB\"\n      max_worker_processes: \"16\"\n      max_parallel_workers_per_gather: \"4\"\n      max_parallel_workers: \"16\"\n      max_parallel_maintenance_workers: \"4\"\n      pg_stat_statements.max: \"10000\"\n      pg_stat_statements.track: all\n      pg_stat_statements.track_utility: \"true\"\n      pg_stat_statements.save: \"false\"\n      track_io_timing: \"on\"\n```\n### Relevant log output\n[pgbasebackup.log](https://github.com/user-attachments/files/17744632/pgbasebackup.log)\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductYou're not using the IP address as you comment, but the RO service\n```\n- name: fc-pg\n      connectionParameters:\n        # Production replica svc\n        host: fc-pg-ro.fastcup\n```\nThis worries me. The RO service is not a connection to one instance.\nThen, while it is possible to run base backup on a replica, there are caveats.\nhttps://www.postgresql.org/docs/current/app-pgbasebackup.html\n>Note that there are some limitations in taking a backup from a standby:\n>\n>The backup history file is not created in the database cluster backed up.\n>\n>pg_basebackup cannot force the standby to switch to a new WAL file at the end of backup. When you are using -X none, if write activity on the primary is low, pg_basebackup may need to wait a long time for the last WAL file required for the backup to be switched and archived. In this case, it may be useful to run pg_switch_wal on the primary in order to trigger an immediate WAL file switch.\n>\n>If the standby is promoted to be primary during backup, the backup fails.\n>\n>All WAL records required for the backup must contain sufficient full-page writes, which requires you to enable full_page_writes on the primary.\n```"
    },
    {
        "title": "[Feature]: Add postInitSQL to pg_basebackup and recovery sections",
        "id": 2655823614,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nHello.\nCurrently, SQL scripts can be executed only when bootstrapping a cluster using the `initdb` method. They cannot be executed when using the `recovery` and `pg_basebackup` methods.\nWe often bootstrap dev/staging clusters using `pg_basebackup` and want to execute some SQL scripts to clean sensitive data from the backup.\n### Describe the solution you'd like\nAdd `postInitSQL`, `postInitApplicationSQL` and `postInitTemplateSQL` to the `recovery` and `pg_basebackup` sections.\n### Describe alternatives you've considered\nManually executing scripts.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nHello.\nCurrently, SQL scripts can be executed only when bootstrapping a cluster using the `initdb` method. They cannot be executed when using the `recovery` and `pg_basebackup` methods.\nWe often bootstrap dev/staging clusters using `pg_basebackup` and want to execute some SQL scripts to clean sensitive data from the backup.\n### Describe the solution you'd like\nAdd `postInitSQL`, `postInitApplicationSQL` and `postInitTemplateSQL` to the `recovery` and `pg_basebackup` sections.\n### Describe alternatives you've considered\nManually executing scripts.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: initdb doesn't wait for secret",
        "id": 2653485074,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ndmills@qumulo.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nThe initdb option to create a new DB doesn't appear to wait for the secret to exist before starting. Then later if the secret is created the owner is not created unless you separately use a role.\nWe use external-secrets to save secrets in a secure vault, but if we deploy the external-secret and the cluster at the same time they both start working even though external-secrets may have not created the Kubernetes secret. So you end up with a DB that doesn't have a owner can the user cannot connect. If you recreate the cluster once the secret exists everything proceeds normally\n### Cluster resource\n```shell\n---\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: db\n  namespace: dv\nspec:\n  instances: 2\n  storage:\n    size: 10Gi\n  bootstrap:\n    initdb:\n      database: db\n      owner: db\n      secret:\n        name: db-secret\n---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: db-secret\n  namespace: db\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: secretstore\n    kind: ClusterSecretStore\n  target:\n    name: db-secret\n    template:\n      type: kubernetes.io/basic-auth\n      data:\n        username: \"{{ .login }}\"\n        password: \"{{ .password }}\"\n        host: \"{{ .host }}\"\n  dataFrom:\n    - extract:\n        key: db-secret\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ndmills@qumulo.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nThe initdb option to create a new DB doesn't appear to wait for the secret to exist before starting. Then later if the secret is created the owner is not created unless you separately use a role.\nWe use external-secrets to save secrets in a secure vault, but if we deploy the external-secret and the cluster at the same time they both start working even though external-secrets may have not created the Kubernetes secret. So you end up with a DB that doesn't have a owner can the user cannot connect. If you recreate the cluster once the secret exists everything proceeds normally\n### Cluster resource\n```shell\n---\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: db\n  namespace: dv\nspec:\n  instances: 2\n  storage:\n    size: 10Gi\n  bootstrap:\n    initdb:\n      database: db\n      owner: db\n      secret:\n        name: db-secret\n---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: db-secret\n  namespace: db\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: secretstore\n    kind: ClusterSecretStore\n  target:\n    name: db-secret\n    template:\n      type: kubernetes.io/basic-auth\n      data:\n        username: \"{{ .login }}\"\n        password: \"{{ .password }}\"\n        host: \"{{ .host }}\"\n  dataFrom:\n    - extract:\n        key: db-secret\n```\n### Relevant log output\n```shell\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductIt would be helpful to have the full logs of both the operator and the primary instance (while there is no 'owner')"
    },
    {
        "title": "[BUG]: cnpg plugin overrides `-v`",
        "id": 2652868574,
        "state": "open",
        "first": "According to `kubectl options`, `The following options can be passed to any command:` `-v, --v=0: number for the log level verbosity`. Unfortunately this plugin overrides that:\n```\n$ kubectl cnpg status -v=10 cluster                                                                                                                                                                                                       Error: clusters.postgresql.cnpg.io \"cluster\" not found\n```\nContrast that with:\n`kubectl get cluster -v=10 cluster` which prints the expected traces.",
        "messages": "According to `kubectl options`, `The following options can be passed to any command:` `-v, --v=0: number for the log level verbosity`. Unfortunately this plugin overrides that:\n```\n$ kubectl cnpg status -v=10 cluster                                                                                                                                                                                                       Error: clusters.postgresql.cnpg.io \"cluster\" not found\n```\nContrast that with:\n`kubectl get cluster -v=10 cluster` which prints the expected traces."
    },
    {
        "title": "[Feature]: Report replica datadir size during provisioning in logs",
        "id": 2651319386,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nPG in general has not a good observability when pg_basebackup provisions the replica. Typically we use `df -h` just to get datadir size to have a rough understanding of how much data was already transferred. From the metrics perspective, you can already get it via PVC usage, but it's kind of cumbersome especially when you are in an incident.\nHaving a simple log entry would suffice to understand that the pg_basebackup is progressing correctly.\n### Describe the solution you'd like\nWhen the replica is provisioning via pg_basebackup, write every 10s disk usage (human readable) for all attached datadir PVCs.\nConfiguration - I think this should not bother anyone much, if they don't like to use this, but for sure the interval can be configurable with a possibility of disabling it.\n### Describe alternatives you've considered\n(vanilla PG might have it one day)\nObserve via metrics for PVC usage - cumbersome, especially in stressful situations.\n### Additional context\n(If you would consider this viable and 'good first issue', I'd gladly participate in the development).\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nPG in general has not a good observability when pg_basebackup provisions the replica. Typically we use `df -h` just to get datadir size to have a rough understanding of how much data was already transferred. From the metrics perspective, you can already get it via PVC usage, but it's kind of cumbersome especially when you are in an incident.\nHaving a simple log entry would suffice to understand that the pg_basebackup is progressing correctly.\n### Describe the solution you'd like\nWhen the replica is provisioning via pg_basebackup, write every 10s disk usage (human readable) for all attached datadir PVCs.\nConfiguration - I think this should not bother anyone much, if they don't like to use this, but for sure the interval can be configurable with a possibility of disabling it.\n### Describe alternatives you've considered\n(vanilla PG might have it one day)\nObserve via metrics for PVC usage - cumbersome, especially in stressful situations.\n### Additional context\n(If you would consider this viable and 'good first issue', I'd gladly participate in the development).\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "feat: managed role should only grant role for missing in inRoles",
        "id": 2648482539,
        "state": "open",
        "first": "This match ensure that managed role only grant missing roles in database\r\nfor the user. If roles are missing in InRoles, managed role will not revoke the roles\r\nfrom the declared user. This is for the changes in postgres 16\r\nabout [role enhancement](https://github.com/postgres/postgres/commit/e5b8a4c098ad6add39626a14475148872cd687e0)\r\nWhen a non-super user Alice create a new role bob, Alice will be added as the member of\r\nBob.\r\nCloses: #6024",
        "messages": "This match ensure that managed role only grant missing roles in database\r\nfor the user. If roles are missing in InRoles, managed role will not revoke the roles\r\nfrom the declared user. This is for the changes in postgres 16\r\nabout [role enhancement](https://github.com/postgres/postgres/commit/e5b8a4c098ad6add39626a14475148872cd687e0)\r\nWhen a non-super user Alice create a new role bob, Alice will be added as the member of\r\nBob.\r\nCloses: #6024e2e rune: https://github.com/EnterpriseDB/cloudnative-pg/actions/runs/11774679765"
    },
    {
        "title": "[Feature]: Evaluate transition to `ReadWriteOncePod` access mode for persistent volumes",
        "id": 2639940066,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nKubernetes 1.29 introduces GA support for `ReadWriteOncePod` access mode of persistent volumes: _the volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it._\nSee https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes.\nWe fix access mode to `ReadWriteOnce` (possibly even in PVC templates - see #5145).\n### Describe the solution you'd like\nSee if we can simply add it as the default mode starting from version 1.25 (at that point, all stable Kubernetes versions will have `ReadWriteOncePod` available). Make sure that we provide a way to fallback to `ReadWriteOnce` in case storage classes do not support it. Shall we add an `accessMode` property to any _storage_ block?\n### Describe alternatives you've considered\nN/A\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nKubernetes 1.29 introduces GA support for `ReadWriteOncePod` access mode of persistent volumes: _the volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it._\nSee https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes.\nWe fix access mode to `ReadWriteOnce` (possibly even in PVC templates - see #5145).\n### Describe the solution you'd like\nSee if we can simply add it as the default mode starting from version 1.25 (at that point, all stable Kubernetes versions will have `ReadWriteOncePod` available). Make sure that we provide a way to fallback to `ReadWriteOnce` in case storage classes do not support it. Shall we add an `accessMode` property to any _storage_ block?\n### Describe alternatives you've considered\nN/A\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductI quickly tried changing the access mode to `ReadWriteOncePod` and it did not work locally, as I feared:\n```\nfailed to provision volume with StorageClass \"standard\": Only support ReadWriteOnce access mode\nWaiting for a volume to be created either by the external provisioner 'rancher.io/local-path' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.\n```\n---\nSetting the access mode as ReadWriteOncePod would prevent using a debug container to inspect a crash-looping pod. We must keep this in mind when evaluating if we should change the default behavior."
    },
    {
        "title": "[Feature]: allow cross-namespace Database and Role configuration",
        "id": 2638501088,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe new CRD for Database and the upcoming Role CRD use `LocalObjectReference` for the cluster setting. This makes it awkward to share a Postgres cluster among applications in different namespaces.\n### Describe the solution you'd like\nIt would be nice if a Cluster in another namespace can be referenced in Database and Role objects. This should be disabled by default for security reasons, but could be enabled using an option on the Cluster resource (separately for Database and Role resources).\n### Describe alternatives you've considered\nDeploy all resources in the same namespaces. This is awkward for us since we have a large number of applications in separate namespace. They should be kept separate and use separate deployment pipelines.\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe new CRD for Database and the upcoming Role CRD use `LocalObjectReference` for the cluster setting. This makes it awkward to share a Postgres cluster among applications in different namespaces.\n### Describe the solution you'd like\nIt would be nice if a Cluster in another namespace can be referenced in Database and Role objects. This should be disabled by default for security reasons, but could be enabled using an option on the Cluster resource (separately for Database and Role resources).\n### Describe alternatives you've considered\nDeploy all resources in the same namespaces. This is awkward for us since we have a large number of applications in separate namespace. They should be kept separate and use separate deployment pipelines.\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductI'd also like to see this, especially in usecase where i'll have hundreds few MB big databases for dynamically created instances of application.\nHaving one \"central\" cluster would be way easier in terms of backups management etc.\nonly downside i can think of is that currently there's an issue to restore a single DB from backup. it would be very helpful to be able to restore it per DB instead of per cluster.\n---\n+1 Currently having to do some jank to work around this.\nBeing able to have a single cluster and then use crd's in seperate namespaces to deploy databases would be ideal. With role creation also being namespace independant re https://github.com/cloudnative-pg/cloudnative-pg/issues/6498 so that access can be isolated and the entire db/role lifecycle can be contained within each namespaces individual deployment."
    },
    {
        "title": "[Bug]: WAL archive throwing error \"failed to get envs: cache miss\"",
        "id": 2634687088,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nkristian@grimsby.us\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nYAML manifest\n### What happened?\nWAL archiving is failing and building up huge storage.\nManual trigger archiving:\n`/controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001`\nLog\n```\n{\"level\":\"error\",\"ts\":\"2024-11-05T07:44:36.161495727Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"pg17-1\",\"error\":\"failed to get envs: cache miss\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\n{\"level\":\"info\",\"ts\":\"2024-11-05T07:44:36.164279597Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg17-1\",\"record\":{\"log_time\":\"2024-11-05 07:44:36.164 UTC\",\"process_id\":\"40\",\"session_id\":\"6729cbe7.28\",\"session_line_num\":\"17\",\"session_start_time\":\"2024-11-05 07:40:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n{\"level\":\"error\",\"ts\":\"2024-11-05T07:44:37.517214415Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"pg17-1\",\"error\":\"failed to get envs: cache miss\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\n{\"level\":\"info\",\"ts\":\"2024-11-05T07:44:37.519603713Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg17-1\",\"record\":{\"log_time\":\"2024-11-05 07:44:37.519 UTC\",\"process_id\":\"40\",\"session_id\":\"6729cbe7.28\",\"session_line_num\":\"18\",\"session_start_time\":\"2024-11-05 07:40:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n{\"level\":\"error\",\"ts\":\"2024-11-05T07:44:38.870712509Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"pg17-1\",\"error\":\"failed to get envs: cache miss\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\n{\"level\":\"info\",\"ts\":\"2024-11-05T07:44:38.873442578Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg17-1\",\"record\":{\"log_time\":\"2024-11-05 07:44:38.873 UTC\",\"process_id\":\"40\",\"session_id\":\"6729cbe7.28\",\"session_line_num\":\"19\",\"session_start_time\":\"2024-11-05 07:40:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-05T07:44:38.873464219Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg17-1\",\"record\":{\"log_time\":\"2024-11-05 07:44:38.873 UTC\",\"process_id\":\"40\",\"session_id\":\"6729cbe7.28\",\"session_line_num\":\"20\",\"session_start_time\":\"2024-11-05 07:40:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"WARNING\",\"sql_state_code\":\"01000\",\"message\":\"archiving write-ahead log file \\\"000000010000000000000001\\\" failed too many times, will try again later\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n{\"level\":\"error\",\"ts\":\"2024-11-05T07:45:23.832793235Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"pg17-1\",\"error\":\"failed to get envs: cache miss\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\n{\"level\":\"info\",\"ts\":\"2024-11-05T07:45:23.835194034Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg17-1\",\"record\":{\"log_time\":\"2024-11-05 07:45:23.835 UTC\",\"process_id\":\"40\",\"session_id\":\"6729cbe7.28\",\"session_line_num\":\"21\",\"session_start_time\":\"2024-11-05 07:40:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n{\"level\":\"error\",\"ts\":\"2024-11-05T07:45:25.180963222Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"pg17-1\",\"error\":\"failed to get envs: cache miss\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\n{\"level\":\"info\",\"ts\":\"2024-11-05T07:45:25.183441946Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg17-1\",\"record\":{\"log_time\":\"2024-11-05 07:45:25.183 UTC\",\"process_id\":\"40\",\"session_id\":\"6729cbe7.28\",\"session_line_num\":\"22\",\"session_start_time\":\"2024-11-05 07:40:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n{\"level\":\"error\",\"ts\":\"2024-11-05T07:45:26.518112485Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"pg17-1\",\"error\":\"failed to get envs: cache miss\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\n{\"level\":\"info\",\"ts\":\"2024-11-05T07:45:26.520657271Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg17-1\",\"record\":{\"log_time\":\"2024-11-05 07:45:26.520 UTC\",\"process_id\":\"40\",\"session_id\":\"6729cbe7.28\",\"session_line_num\":\"23\",\"session_start_time\":\"2024-11-05 07:40:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-05T07:45:26.520679513Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg17-1\",\"record\":{\"log_time\":\"2024-11-05 07:45:26.520 UTC\",\"process_id\":\"40\",\"session_id\":\"6729cbe7.28\",\"session_line_num\":\"24\",\"session_start_time\":\"2024-11-05 07:40:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"WARNING\",\"sql_state_code\":\"01000\",\"message\":\"archiving write-ahead log file \\\"000000010000000000000001\\\" failed too many times, will try again later\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n```\nPrettified error\n```\n{\n  \"level\": \"error\",\n  \"ts\": \"2024-11-05T07:45:23.832793235Z\",\n  \"logger\": \"wal-archive\",\n  \"msg\": \"failed to run wal-archive command\",\n  \"logging_pod\": \"pg17-1\",\n  \"error\": \"failed to get envs: cache miss\",\n  \"stacktrace\": \"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"\n}\n```\nIt seems to be related to the cache trying to retrieve the WALArchiveKey, but couldn't find any documentation about this and how to debug.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nkristian@grimsby.us\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nYAML manifest\n### What happened?\nWAL archiving is failing and building up huge storage.\nManual trigger archiving:\n`/controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001`\nLog\n```\n{\"level\":\"error\",\"ts\":\"2024-11-05T07:44:36.161495727Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"pg17-1\",\"error\":\"failed to get envs: cache miss\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\n{\"level\":\"info\",\"ts\":\"2024-11-05T07:44:36.164279597Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg17-1\",\"record\":{\"log_time\":\"2024-11-05 07:44:36.164 UTC\",\"process_id\":\"40\",\"session_id\":\"6729cbe7.28\",\"session_line_num\":\"17\",\"session_start_time\":\"2024-11-05 07:40:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n{\"level\":\"error\",\"ts\":\"2024-11-05T07:44:37.517214415Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"pg17-1\",\"error\":\"failed to get envs: cache miss\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\n{\"level\":\"info\",\"ts\":\"2024-11-05T07:44:37.519603713Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg17-1\",\"record\":{\"log_time\":\"2024-11-05 07:44:37.519 UTC\",\"process_id\":\"40\",\"session_id\":\"6729cbe7.28\",\"session_line_num\":\"18\",\"session_start_time\":\"2024-11-05 07:40:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n{\"level\":\"error\",\"ts\":\"2024-11-05T07:44:38.870712509Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"pg17-1\",\"error\":\"failed to get envs: cache miss\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\n{\"level\":\"info\",\"ts\":\"2024-11-05T07:44:38.873442578Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg17-1\",\"record\":{\"log_time\":\"2024-11-05 07:44:38.873 UTC\",\"process_id\":\"40\",\"session_id\":\"6729cbe7.28\",\"session_line_num\":\"19\",\"session_start_time\":\"2024-11-05 07:40:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-05T07:44:38.873464219Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg17-1\",\"record\":{\"log_time\":\"2024-11-05 07:44:38.873 UTC\",\"process_id\":\"40\",\"session_id\":\"6729cbe7.28\",\"session_line_num\":\"20\",\"session_start_time\":\"2024-11-05 07:40:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"WARNING\",\"sql_state_code\":\"01000\",\"message\":\"archiving write-ahead log file \\\"000000010000000000000001\\\" failed too many times, will try again later\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n{\"level\":\"error\",\"ts\":\"2024-11-05T07:45:23.832793235Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"pg17-1\",\"error\":\"failed to get envs: cache miss\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\n{\"level\":\"info\",\"ts\":\"2024-11-05T07:45:23.835194034Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg17-1\",\"record\":{\"log_time\":\"2024-11-05 07:45:23.835 UTC\",\"process_id\":\"40\",\"session_id\":\"6729cbe7.28\",\"session_line_num\":\"21\",\"session_start_time\":\"2024-11-05 07:40:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n{\"level\":\"error\",\"ts\":\"2024-11-05T07:45:25.180963222Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"pg17-1\",\"error\":\"failed to get envs: cache miss\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\n{\"level\":\"info\",\"ts\":\"2024-11-05T07:45:25.183441946Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg17-1\",\"record\":{\"log_time\":\"2024-11-05 07:45:25.183 UTC\",\"process_id\":\"40\",\"session_id\":\"6729cbe7.28\",\"session_line_num\":\"22\",\"session_start_time\":\"2024-11-05 07:40:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n{\"level\":\"error\",\"ts\":\"2024-11-05T07:45:26.518112485Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"pg17-1\",\"error\":\"failed to get envs: cache miss\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\n{\"level\":\"info\",\"ts\":\"2024-11-05T07:45:26.520657271Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg17-1\",\"record\":{\"log_time\":\"2024-11-05 07:45:26.520 UTC\",\"process_id\":\"40\",\"session_id\":\"6729cbe7.28\",\"session_line_num\":\"23\",\"session_start_time\":\"2024-11-05 07:40:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n{\"level\":\"info\",\"ts\":\"2024-11-05T07:45:26.520679513Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg17-1\",\"record\":{\"log_time\":\"2024-11-05 07:45:26.520 UTC\",\"process_id\":\"40\",\"session_id\":\"6729cbe7.28\",\"session_line_num\":\"24\",\"session_start_time\":\"2024-11-05 07:40:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"WARNING\",\"sql_state_code\":\"01000\",\"message\":\"archiving write-ahead log file \\\"000000010000000000000001\\\" failed too many times, will try again later\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n```\nPrettified error\n```\n{\n  \"level\": \"error\",\n  \"ts\": \"2024-11-05T07:45:23.832793235Z\",\n  \"logger\": \"wal-archive\",\n  \"msg\": \"failed to run wal-archive command\",\n  \"logging_pod\": \"pg17-1\",\n  \"error\": \"failed to get envs: cache miss\",\n  \"stacktrace\": \"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"\n}\n```\nIt seems to be related to the cache trying to retrieve the WALArchiveKey, but couldn't find any documentation about this and how to debug.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductSame here:\n- self manage k3s, \n- flux\n- cn-pg:   helmchart/cloudnative-pg-cloudnative-pg\t0.22.1 \n- longhorn storage: helmchart/longhorn-longhorn            \t1.7.2\n- synology s3\ncluster: pg image: ghcr.io/cloudnative-pg/postgresql:17.2\n`{\n  \"level\":\"error\",\n  \"ts\":\"2024-12-05T22:22:40.920203941Z\",\n  \"logger\":\"wal-archive\",\n  \"msg\":\"failed to run wal-archive command\",\n  \"logging_pod\":\"keycloak-db-1\",\n  \"error\":\"failed to get envs: cache miss\",\n  \"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"\n}`\n---\n> Same here:\n> \n> * self manage k3s,\n> * flux\n> * cn-pg:   helmchart/cloudnative-pg-cloudnative-pg\t0.22.1\n> * longhorn storage: helmchart/longhorn-longhorn            \t1.7.2\n> * synology s3\n> \n> cluster: pg image: ghcr.io/cloudnative-pg/postgresql:17.2\n> \n> `{ \"level\":\"error\", \"ts\":\"2024-12-05T22:22:40.920203941Z\", \"logger\":\"wal-archive\", \"msg\":\"failed to run wal-archive command\", \"logging_pod\":\"keycloak-db-1\", \"error\":\"failed to get envs: cache miss\", \"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\" }`\ncould fix it....\nwas my bad, did put the s3 creds into the wrong namespace.  so was simply missing the s3 keys....\n---\nI'm just starting to learn Kubernetes. I had the same issue as @aloxlamm and I misunderstood how secrets work. I assumed you needed the username [from Minio] + the created id+key [for that user]. But the s3 creds were to be added to a \"secret\" (to the clusters namespace). Hence my S3 credentials were borked.\nSo, the issue as far as I know is related to missing/wrong key(s) for s3. But the error is quite unhelpful in deducing that.\n---\nSame here. The error is really hard to understand.\n---\n+1. Experiencing the same  issue. Promoting a replica solves it for now."
    },
    {
        "title": "[Bug]: status cmd still broken due to NetworkPolicy",
        "id": 2632721322,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.4\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: k0s\n### How did you install the operator?\nHelm\n### What happened?\nfollow up to: https://github.com/cloudnative-pg/cloudnative-pg/issues/5247\n---\nAs soon as I apply the following NetworkPolicy, the status command does not report backup status, primary instance status, etc.. It seems like, the requests going through a proxy are not included in the policy and therefor blocked. I've been executing status using a root-level kubernetes account which is why I ruled out RBAC issues. As soon as I remove the network policy, it works.\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-operator\nspec:\n  podSelector:\n    matchLabels:\n      cnpg.io/cluster: cluster-example # The label value must be the cluster name\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              kubernetes.io/metadata.name: cnpg-system # Namespace where the operator is deployed\n          podSelector:\n            matchLabels:\n              app.kubernetes.io/name: cloudnative-pg # Matches the Operator pod\n      ports:\n        - port: 8000\n        - port: 5432\n```\n### Cluster resource\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/main/docs/src/samples/cluster-example-catalog.yaml\n### Relevant log output\n_No response_\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.4\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: k0s\n### How did you install the operator?\nHelm\n### What happened?\nfollow up to: https://github.com/cloudnative-pg/cloudnative-pg/issues/5247\n---\nAs soon as I apply the following NetworkPolicy, the status command does not report backup status, primary instance status, etc.. It seems like, the requests going through a proxy are not included in the policy and therefor blocked. I've been executing status using a root-level kubernetes account which is why I ruled out RBAC issues. As soon as I remove the network policy, it works.\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-operator\nspec:\n  podSelector:\n    matchLabels:\n      cnpg.io/cluster: cluster-example # The label value must be the cluster name\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              kubernetes.io/metadata.name: cnpg-system # Namespace where the operator is deployed\n          podSelector:\n            matchLabels:\n              app.kubernetes.io/name: cloudnative-pg # Matches the Operator pod\n      ports:\n        - port: 8000\n        - port: 5432\n```\n### Cluster resource\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/main/docs/src/samples/cluster-example-catalog.yaml\n### Relevant log output\n_No response_\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductHi @till \nI think I was able to reproduce your issue, but can you show me the output of your status command?\nRegards,\n---\nSame issue here, hopefully with the same cause. Ingress netpol on the cluster NS causes the failure, with this removed it works fine:\n```yaml\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: cnpg-system\n      podSelector:\n        matchLabels:\n          app.kubernetes.io/name: cloudnative-pg\n    ports:\n    - port: 8000\n      protocol: TCP\n    - port: 5432\n      protocol: TCP\n    - port: 9187\n      protocol: TCP\n  podSelector:\n    matchExpressions:\n    - key: cnpg.io/cluster\n      operator: Exists\n```\n`cnpg status` takes just under 5 minutes to return almost nothing. (#6068 makes debugging much harder)\n```\nCluster Summary\nName                 namespace/cluster-psql\nPostgreSQL Image:    ghcr.io/cloudnative-pg/postgresql:17.0\nPrimary instance:    cluster-psql-2\nPrimary start time:  2024-11-12 15:48:07 +0000 UTC (uptime 1h42m17s)\nStatus:              Cluster in healthy state \nInstances:           2\nReady instances:     2\nSize:                618M\nContinuous Backup status\nFirst Point of Recoverability:  Not Available\nNo Primary instance found\nPhysical backups\nPrimary instance not found\nStreaming Replication status\nPrimary instance not found\nInstances status\nName              Current LSN  Replication role  Status  QoS                 Manager Version  Node\n----              -----------  ----------------  ------  ---                 ---------------  ----\ncluster-psql-2  -            -                 -       ServiceUnavailable  Burstable        -  worker-b\ncluster-psql-3  -            -                 -       ServiceUnavailable  Burstable        -  worker-p\nError(s) extracting status\n-----------------------------------\nthe server is currently unable to handle the request (get pods https:cluster-psql-2:8000)\nthe server is currently unable to handle the request (get pods https:cluster-psql-3:8000)\n```\n---\n@sxd Is there anything else you need?\n---\n@till yes is enough, but I haven't had time which is the required policy to allow the subresources pods/proxy to be called and executed\n---\n@sxd I am not exactly familiar with the proxy part (yet), can you point me to where this happens? I'll try to investigate as well.\nBtw, are you referring using the `kube-proxy` process that (usually) runs in `kube-system`? I guess then the policy would need to include something like `k8s-app=kube-proxy` and the `kube-system` namespace?\n---\nIt took me a bit to get back to this, but I think I managed to make it work.\nI created a 2 node cluster which is called `db-dec6-4` (in namespace `db-dec6-4`), and this is what the policy looks like like:\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: db-dec6-4\n  namespace: db-dec6-4\n  generation: 1\n  labels:\n    app: db-dec6-4\n    organization: some-name\nspec:\n  podSelector:\n    matchLabels:\n      cnpg.io/cluster: db-dec6-4\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              organization: some-name\n        - namespaceSelector:\n            matchExpressions:\n              - key: kubernetes.io/metadata.name\n                operator: In\n                values:\n                  - kube-system\n                  - cnpg-system\n        - podSelector:\n            matchLabels:\n              cnpg.io/cluster: db-dec6-4\n  policyTypes:\n    - Ingress\n```\nBe nice if someone else could verify that. Then I am also happy to PR this with comments inline maybe.\nThe idea is the following:\n1. allow requests from any namespace labeled `organization: some-name` (this is related to our setup, we tag namespaces of an org/team)\n2. allow requests from namespace `kube-system` (this was needed to make the status command work)\n3. from namespace `cnpg-system` (generally for the operator)\n4. from any pod that belongs to the `db-dec6-4` cluster (this was needed in order to get it to run/boostrap completely)\n(conditions are `OR` as far as I can tell)\nFor rule 4, if I didn't add it, it would basically timeout in init/bootstrap and was unable to go through the `-rw` service to complete the setup.\nThis is the output of the `kubectl-cnpg status` command:\n```\nCluster Summary\nName                 db-dec6-4/db-dec6-4\nSystem ID:           7445407285926371351\nPostgreSQL Image:    ghcr.io/cloudnative-pg/postgresql:16.4-39-bookworm@sha256:d653f7ac4b2e95aa1559ff812a62a98965e2ccc88cc27551def41c18119404f2\nPrimary instance:    db-dec6-4-1\nPrimary start time:  2024-12-06 21:03:00 +0000 UTC (uptime 58s)\nStatus:              Cluster in healthy state \nInstances:           2\nReady instances:     2\nSize:                94M\nCurrent Write LSN:   0/404DE90 (Timeline: 1 - WAL File: 000000010000000000000004)\nContinuous Backup status\nFirst Point of Recoverability:  Not Available\nWorking WAL archiving:          OK\nWALs waiting to be archived:    0\nLast Archived WAL:              000000010000000000000003.00000028.backup   @   2024-12-06T21:03:25.838189Z\nLast Failed WAL:                -\nPhysical backups\nName  Phase  Started at  Total  Transferred  Progress  Tablespaces\n----  -----  ----------  -----  -----------  --------  -----------\nStreaming Replication status\nReplication Slots Enabled\nName         Sent LSN   Write LSN  Flush LSN  Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority  Replication Slot\n----         --------   ---------  ---------  ----------  ---------  ---------  ----------  -----      ----------  -------------  ----------------\ndb-dec6-4-2  0/404DE90  0/404DE90  0/404DE90  0/404DE90   00:00:00   00:00:00   00:00:00    streaming  async       0              active\nInstances status\nName         Current LSN  Replication role  Status  QoS        Manager Version  Node\n----         -----------  ----------------  ------  ---        ---------------  ----\ndb-dec6-4-1  0/404DE90    Primary           OK      Burstable  1.23.5           node-006.testing\ndb-dec6-4-2  0/404DE90    Standby (async)   OK      Burstable  1.23.5           node-004.testing\n```\nGuess I could add the ports into it, but honestly not yet sure what they all are. I'd still like a high-level diagram that shows what all runs in a pod, etc. and what is needed where.\n---\nI looked into this a bit since I had similar issues.\nTo get the cluster working correctly I needed the following NetworkPolicies\nOne for the `cnpg-system` namespace which is necessary for the cnpg clusters to be able to call the webhook on the controller on changes and to post metrics. \n```yaml\n# Allow cnpg clusters to access operator\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-cnpg-operator-access\n  namespace: cnpg-system\nspec:\n  podSelector:\n    matchLabels:\n      app.kubernetes.io/name: cloudnative-pg\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels: {} # your desired labels here\n    - ports:\n        - port: 9443 # webhook-server\n          protocol: TCP\n        - port: 8080 # metrics\n          protocol: TCP\n  policyTypes:\n    - Ingress\n```\nSecond, for the `cnpg` cluster itself in whatever namespace you deploy the cluster into. This allows the controller to access necessary ports to be able to operate. Metrics on 9187, Kubernetes pod status on port 8000, and postgres interface itself on 5432. Here I also allow traffic from `kube-system` namespace.\n```yaml\n# Allow operator to access cnpg cluster\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-cnpg-operator-access\n  namespace: <your-namespace-here>\nspec:\n  podSelector:\n    matchExpressions:\n      - key: cnpg.io/cluster\n        operator: Exists\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchExpressions:\n              - key: kubernetes.io/metadata.name\n                operator: In\n                values:\n                  - kube-system\n                  - cnpg-system\n      ports:\n        - port: 9187 # metrics-manager\n          protocol: TCP\n        - port: 8000 # kubernetes status\n          protocol: TCP\n        - port: 5432 # postgres\n          protocol: TCP\n  policyTypes:\n    - Ingress\n```\nAll of the above allows full functionality of CloudNative-PG  but the `kubectl cnpg status` command still does not work for me at least. From my understanding `kubectl cnpg status` proxies network requests via the cluster to check status.\nThat is why we added `kube-system` to the namespaces that is allowed to reach our cnpg cluster pods. The proxied traffic should in my mind arrive from that namespace (the cluster I am testing in is using Cilium in this case). But, unfortunately it does not work.\nI have not narrowed it down much but this works. I.e. *Allow all traffic from any source to the cnpg cluster*\n``` yaml\n# Allow operator to access cnpg cluster\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-cnpg-operator-access\n  namespace: <your-namespace-here>\nspec:\n  podSelector:\n    matchExpressions:\n      - key: cnpg.io/cluster\n        operator: Exists\n  ingress: {}\n  policyTypes:\n    - Ingress\n```\nBut for some reason this does not work. I.e *Allow all traffic from any pod on any namespace*\n```yaml\n# Allow operator to access cnpg cluster\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-cnpg-operator-access\n  namespace: <your-namespace-here>\nspec:\n  podSelector:\n    matchExpressions:\n      - key: cnpg.io/cluster\n        operator: Exists\n  ingress:\n    - from:\n        - namespaceSelector: {}\n  policyTypes:\n    - Ingress\n```\nThis is where my available skillset ends. I do not know how I can allow the `kubectl cnpg status` command without simply allowing all traffic since the traffic needed is somehow not originating from any pod in any namespace. At least not in my setup.\n---\nOk, I got something that I think is good enough. I can keep full access but only for the status port (port 8000) and lock down the rest. So final working setup for me would be.\nAllow access to cnpg operator from individual cnpg clusters\n```yaml\n# Allow cnpg clusters to access operator\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-cnpg-operator-access\n  namespace: cnpg-system\nspec:\n  podSelector:\n    matchLabels:\n      app.kubernetes.io/name: cloudnative-pg\n  ingress:\n    - ports:\n        - port: 9443 # webhook-server\n          protocol: TCP\n        - port: 8080 # metrics\n          protocol: TCP\n  policyTypes:\n    - Ingress\n```\nAllow operator to reach cluster\n```yaml\n# Allow operator to reach cluster in our namespace\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-cnpg-operator-access\n  namespace: <your-namespace-here>\nspec:\n  podSelector:\n    matchExpressions:\n      - key: cnpg.io/cluster\n        operator: Exists\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              name: cnpg-system\n      ports:\n        - port: 9187 # metrics-manager\n          protocol: TCP\n        - port: 5432 # postgresql\n          protocol: TCP\n  policyTypes:\n    - Ingress\n```\nAnd finally, allow operator and kubectl cnpg command access to cluster status.\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-cnpg-cluster-status\n  namespace: <your-namespace-here>\nspec:\n  podSelector:\n    matchExpressions:\n      - key: cnpg.io/cluster\n        operator: Exists\n  ingress:\n    - ports:\n        - port: 8000 # status\n          protocol: TCP\n  policyTypes:\n    - Ingress\n```"
    },
    {
        "title": "[Feature]: Skip revoke role from user if not declared in inRoles of managed role",
        "id": 2631742257,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\n In manage role,the user role is followed by the rule that add by declare and revoke when non-declare.  \nBut, since pg 16, this feature change the behavor of non-super user to create role , see this feature \nAllow roles that create other roles to automatically inherit the new role's rights or the ability to  SET ROLE to the new role\nThe behavor now when a non-super user `admin` going to create a role Bob, Bob will be added as the parent role of `admin`. \ncreate role Bob  is equals to `create role Bob role `admin` \nIf the `admin` is non-super user and declared in managed role , Bob will be grant to `admin` and later will be revoked from admin. \n### Describe the solution you'd like\nAs revoke role has more restriction, and usually a role is added will be used, how about\nwhen a role is defined in inRoles or managed role, we grant the role for the user. \nignore the role which is missing in managed role but exists in database, NO revoke. \n### Describe alternatives you've considered\nn/a\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\n In manage role,the user role is followed by the rule that add by declare and revoke when non-declare.  \nBut, since pg 16, this feature change the behavor of non-super user to create role , see this feature \nAllow roles that create other roles to automatically inherit the new role's rights or the ability to  SET ROLE to the new role\nThe behavor now when a non-super user `admin` going to create a role Bob, Bob will be added as the parent role of `admin`. \ncreate role Bob  is equals to `create role Bob role `admin` \nIf the `admin` is non-super user and declared in managed role , Bob will be grant to `admin` and later will be revoked from admin. \n### Describe the solution you'd like\nAs revoke role has more restriction, and usually a role is added will be used, how about\nwhen a role is defined in inRoles or managed role, we grant the role for the user. \nignore the role which is missing in managed role but exists in database, NO revoke. \n### Describe alternatives you've considered\nn/a\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: VolumeSnapshot retention policy",
        "id": 2629336821,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently in CNPG when you have volume snapshots enabled the `.spec.backup.retentionPolicy` is ignored.  This leads to having to implement your own retention policy or manually delete older snapshots as they grow unbounded in the cluster over time.\n### Describe the solution you'd like\nAdd functionality to remove snapshots older than what is set in the `.spec.backup.rententionPolicy` in the same manner as what is being done with Barman backups.\nWhen we have snapshots older than `.spec.backup.retentionPolicy` is set to, delete snapshots for the specific instance.\n### Describe alternatives you've considered\nWe currently are in the process of adding some logic into a customer controller to watch and handle CNPG backups that are older than what is set in `.spec.backup.rententionPolicy` for each deployed `Cluster`.  This would basically look at all `Backups` and if any are older than the retention policy value then it would delete the `Backup` and ensure that the `VolumeSnapshot` and `VolumeSnapshotContents` are deleted from the cluster.\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently in CNPG when you have volume snapshots enabled the `.spec.backup.retentionPolicy` is ignored.  This leads to having to implement your own retention policy or manually delete older snapshots as they grow unbounded in the cluster over time.\n### Describe the solution you'd like\nAdd functionality to remove snapshots older than what is set in the `.spec.backup.rententionPolicy` in the same manner as what is being done with Barman backups.\nWhen we have snapshots older than `.spec.backup.retentionPolicy` is set to, delete snapshots for the specific instance.\n### Describe alternatives you've considered\nWe currently are in the process of adding some logic into a customer controller to watch and handle CNPG backups that are older than what is set in `.spec.backup.rententionPolicy` for each deployed `Cluster`.  This would basically look at all `Backups` and if any are older than the retention policy value then it would delete the `Backup` and ensure that the `VolumeSnapshot` and `VolumeSnapshotContents` are deleted from the cluster.\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductI understand this might be something that CNPG doesn't want to handle since it deals with so many factors based off the users cluster configuration.  For example if they have `deletionPolicy` set to `Retain` in the `VolumeSnapshotClass` you could delete the volume from the cluster but it wouldn't be deleted from the underlying storage provider.\nMaybe this is something that is out of scope of CNPG and should be set as a responsibility to the end user based off their needs and their storage provider?\n---\n> I understand this might be something that CNPG doesn't want to handle since it deals with so many factors based off the users cluster configuration. For example if they have `deletionPolicy` set to `Retain` in the `VolumeSnapshotClass` you could delete the volume from the cluster but it wouldn't be deleted from the underlying storage provider.\nWe too are looking to find a good solution for this use case. For the cluster configuration variance I think the `deletionPolicy` should be ignored since the dangling snapshot artifact is an accepted outcome within the `VolumeSnapshot` design. If that's ignored is there other issues that may arise from allowing CNPG to trim old backups as needed?\n---\nI think CNPG should implement this rather than something else. VolumeSnapshots are generic snapshots that may be used for many different purposes, so you typically don't want a cluster-wide retention policy for VolumeSnaphots. Rather, you'd want a retention policy for ScheduledBackups/Backups, which are custom resources from CNPG. It only makes sense that CNPG implements retention for these."
    },
    {
        "title": "[Refactor]: make the relevant parts of the managed roles code Public for reuse",
        "id": 2627021323,
        "state": "no reaction",
        "first": "",
        "messages": ""
    },
    {
        "title": "[Bug]: Documentation still reports pod security policies",
        "id": 2626929878,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nThe security page still reports pod security policies - which have been deprecated. See: https://cloudnative-pg.io/documentation/current/security/#pod-security-policies\nWe should change it into security contexts.\nI exported the ones for the postgres container:\n```yaml\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      privileged: false\n      readOnlyRootFilesystem: true\n      runAsNonRoot: true\n      seccompProfile:\n        type: RuntimeDefault\n```\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nThe security page still reports pod security policies - which have been deprecated. See: https://cloudnative-pg.io/documentation/current/security/#pod-security-policies\nWe should change it into security contexts.\nI exported the ones for the postgres container:\n```yaml\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      privileged: false\n      readOnlyRootFilesystem: true\n      runAsNonRoot: true\n      seccompProfile:\n        type: RuntimeDefault\n```\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductI would be interested to pick this up as a first issue. I have work for this in a PR https://github.com/cloudnative-pg/cloudnative-pg/pull/6303, but am not sure if I need this issue as a ticket first as described in the [contributor doc](https://github.com/cloudnative-pg/cloudnative-pg/blob/480f80593e922d01be27be25e71db8992c57bc9a/contribute/README.md). I did not see a ticket for the work here https://github.com/orgs/cloudnative-pg/projects/2 and thought maybe one needed to be created first."
    },
    {
        "title": "[Bug]: Error: controller with name instance-cluster already exists",
        "id": 2626257247,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nwolvie@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nDuring cluster maintenance, one of my nodes got resource starvation and started to evict pods, my db cluster was affected by the eviction, but the new pods couldn't start with an error\n```\nError: controller with name instance-cluster already exists. Controller names must be unique to avoid multiple controllers reporting to the same metric\n```\nYou can find the full [error log here](https://github.com/user-attachments/files/17584988/error.txt)\n### Cluster resource\n```\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: gitea-db\n  namespace: gitea\nspec:\n  instances: 1\n  bootstrap:\n    initdb:\n      database: gitea\n      encoding: UTF8\n      localeCType: en_US.UTF-8\n      localeCollate: en_US.UTF-8\n  storage:\n    size: 20Gi\n    storageClass: longhorn\n  walStorage:\n    size: 2Gi\n    storageClass: longhorn\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n              - key: node-role.kubernetes.io/worker\n                operator: In\n                values:\n                  - worker\n  monitoring:\n    enablePodMonitor: false\n```\n### Relevant log output\n```\n{\n  \"level\": \"error\",\n  \"ts\": \"2024-10-31T08:09:51.842501799Z\",\n  \"msg\": \"unable to create controller\",\n  \"logger\": \"instance-manager\",\n  \"logging_pod\": \"gitea-db-1\",\n  \"error\": \"controller with name instance-cluster already exists. Controller names must be unique to avoid multiple controllers reporting to the same metric\",\n  \"stacktrace\": \"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.runSubCommand\\n\\tinternal/cmd/manager/instance/run/cmd.go:201\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.NewCmd.func2.1\\n\\tinternal/cmd/manager/instance/run/cmd.go:104\\nk8s.io/client-go/util/retry.OnError.func1\\n\\tpkg/mod/k8s.io/client-go@v0.31.1/util/retry/util.go:51\\nk8s.io/apimachinery/pkg/util/wait.runConditionWithCrashProtection\\n\\tpkg/mod/k8s.io/apimachinery@v0.31.1/pkg/util/wait/wait.go:145\\nk8s.io/apimachinery/pkg/util/wait.ExponentialBackoff\\n\\tpkg/mod/k8s.io/apimachinery@v0.31.1/pkg/util/wait/backoff.go:461\\nk8s.io/client-go/util/retry.OnError\\n\\tpkg/mod/k8s.io/client-go@v0.31.1/util/retry/util.go:50\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.NewCmd.func2\\n\\tinternal/cmd/manager/instance/run/cmd.go:103\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"\n}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nwolvie@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nDuring cluster maintenance, one of my nodes got resource starvation and started to evict pods, my db cluster was affected by the eviction, but the new pods couldn't start with an error\n```\nError: controller with name instance-cluster already exists. Controller names must be unique to avoid multiple controllers reporting to the same metric\n```\nYou can find the full [error log here](https://github.com/user-attachments/files/17584988/error.txt)\n### Cluster resource\n```\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: gitea-db\n  namespace: gitea\nspec:\n  instances: 1\n  bootstrap:\n    initdb:\n      database: gitea\n      encoding: UTF8\n      localeCType: en_US.UTF-8\n      localeCollate: en_US.UTF-8\n  storage:\n    size: 20Gi\n    storageClass: longhorn\n  walStorage:\n    size: 2Gi\n    storageClass: longhorn\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n              - key: node-role.kubernetes.io/worker\n                operator: In\n                values:\n                  - worker\n  monitoring:\n    enablePodMonitor: false\n```\n### Relevant log output\n```\n{\n  \"level\": \"error\",\n  \"ts\": \"2024-10-31T08:09:51.842501799Z\",\n  \"msg\": \"unable to create controller\",\n  \"logger\": \"instance-manager\",\n  \"logging_pod\": \"gitea-db-1\",\n  \"error\": \"controller with name instance-cluster already exists. Controller names must be unique to avoid multiple controllers reporting to the same metric\",\n  \"stacktrace\": \"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.runSubCommand\\n\\tinternal/cmd/manager/instance/run/cmd.go:201\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.NewCmd.func2.1\\n\\tinternal/cmd/manager/instance/run/cmd.go:104\\nk8s.io/client-go/util/retry.OnError.func1\\n\\tpkg/mod/k8s.io/client-go@v0.31.1/util/retry/util.go:51\\nk8s.io/apimachinery/pkg/util/wait.runConditionWithCrashProtection\\n\\tpkg/mod/k8s.io/apimachinery@v0.31.1/pkg/util/wait/wait.go:145\\nk8s.io/apimachinery/pkg/util/wait.ExponentialBackoff\\n\\tpkg/mod/k8s.io/apimachinery@v0.31.1/pkg/util/wait/backoff.go:461\\nk8s.io/client-go/util/retry.OnError\\n\\tpkg/mod/k8s.io/client-go@v0.31.1/util/retry/util.go:50\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.NewCmd.func2\\n\\tinternal/cmd/manager/instance/run/cmd.go:103\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"\n}\n```\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductYup. I see the bug on my own servers too.\nCan't say what is the problem unfortunately, or even what is the cause?\nCan someone elaborate on this? \nIt would be nice to understand if the cluster recreation from backup isn't better option than waiting for a fix.\n---\nis there any workaround for this issue?\n---\nTo my understanding no. I also coudln't get through it, so I've attempted to recreate databases from backups. \nUnfortunately I couldn't get it back to work, as I'm not using default `app` database and `app` users. In such case I suppose the accesses to custom db and user have to be added as a custom script somwhere in the chart.\nBut instead of figuring it out, I simply recreated the contents using apps and migrations.\n---\nAfter applying a security patch in OVHcloud's managed kubernetes service, I also stumbled on this issue. I have not been able to fix it, every pod immediately went to this error.\nI have forced recreation of the cloudnative-pg cluster."
    },
    {
        "title": "[Docs]: Misleading documentation regarding Azure Storage",
        "id": 2622059526,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nStorage documentation mentioned that Azure support pvc expending only on specific region.\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/main/docs/src/storage.md#expanding-pvc-volumes-on-aks\nThat seem not more the case\nhttps://learn.microsoft.com/en-us/azure/aks/azure-disk-csi#resize-a-persistent-volume-without-downtime\nPheraps the option ENABLE_AZURE_PVC_UPDATES can be deprecated or remove to simply the code ?\n### Describe what additions need to be done to the documentation\n_No response_\n### Describe what pages need to change in the documentation, if any\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/main/docs/src/storage.md#expanding-pvc-volumes-on-aks\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nStorage documentation mentioned that Azure support pvc expending only on specific region.\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/main/docs/src/storage.md#expanding-pvc-volumes-on-aks\nThat seem not more the case\nhttps://learn.microsoft.com/en-us/azure/aks/azure-disk-csi#resize-a-persistent-volume-without-downtime\nPheraps the option ENABLE_AZURE_PVC_UPDATES can be deprecated or remove to simply the code ?\n### Describe what additions need to be done to the documentation\n_No response_\n### Describe what pages need to change in the documentation, if any\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/main/docs/src/storage.md#expanding-pvc-volumes-on-aks\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductI'm curious about this. I passed on using the helm chart to install the CNPG operator based on being unable to set the environment variables (as far as I know, I may have missed the option)."
    },
    {
        "title": "refactor: roles controller",
        "id": 2621993855,
        "state": "open",
        "first": "Closes #5997 \r\n**IMPORTANT** for reviewers\r\nThe CRD parts of this PR are being moved to https://github.com/cloudnative-pg/cloudnative-pg/pull/6155\r\nThis branch is to contain ONLY the refactoring bits.",
        "messages": "Closes #5997 \r\n**IMPORTANT** for reviewers\r\nThe CRD parts of this PR are being moved to https://github.com/cloudnative-pg/cloudnative-pg/pull/6155\r\nThis branch is to contain ONLY the refactoring bits./test\n---\n/test depth=push"
    },
    {
        "title": "[Bug]: Operator memory consumption",
        "id": 2621708771,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nOLM\n### What happened?\nWhen installed in cluster wide, memory consuption of the operator is out of control. On a (huge) production K8S cluster it went above 8Gib, had really huge startup / reponse times, probes were failing regularly, ...\nI reproduced it quite easily :\n- install a stock Rancher with monitoring\n- install OLM \n- install the operator with OLM\n- create 300 (huge) configmaps and 300 (huge) secrets on a random namespace\n- delete the 600 objects\nHere is the test result :\n![Image](https://github.com/user-attachments/assets/701e56e1-984d-4e94-b160-3c59710ebd64)\nMemory went from 60MiB to 462 MiB just by creating those uncorrelated ConfigMaps and Secrets, and back to 70 MiB by deleting them\nAs we already implemented internally a solution for mandatory labels and annotations from the operator to the clusters, we just put in place a cache key for the selectors to reduce the impact. I've copied our implementation here if you want to have a look : https://github.com/fyannk/cloudnative-pg/tree/dev/clusterwide\nWith this, absolutely no impacts on operator's memory was seen when managing uncorrelated objects :\n![Image](https://github.com/user-attachments/assets/41f7267f-50f0-41aa-b3cc-7af2cc327d5f)\nWith this, all those stock kubernetes objects MUST have an annotation to be visible and managed by the operator :\n- ConfigMap\n- Deployment\n- Job\n- PersistentVolumeClaim\n- Pod\n- Role\n- RoleBinding\n- Secret\n- Service\n- ServiceAccount\nNo need for any annotation for CNPG's CRD objects (we don't really want to filter those) nor for VolumeSnapshot (I still need to have a look at this object)\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [x] I have read the troubleshooting guide and I think this is a new bug.\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.30\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nOLM\n### What happened?\nWhen installed in cluster wide, memory consuption of the operator is out of control. On a (huge) production K8S cluster it went above 8Gib, had really huge startup / reponse times, probes were failing regularly, ...\nI reproduced it quite easily :\n- install a stock Rancher with monitoring\n- install OLM \n- install the operator with OLM\n- create 300 (huge) configmaps and 300 (huge) secrets on a random namespace\n- delete the 600 objects\nHere is the test result :\n![Image](https://github.com/user-attachments/assets/701e56e1-984d-4e94-b160-3c59710ebd64)\nMemory went from 60MiB to 462 MiB just by creating those uncorrelated ConfigMaps and Secrets, and back to 70 MiB by deleting them\nAs we already implemented internally a solution for mandatory labels and annotations from the operator to the clusters, we just put in place a cache key for the selectors to reduce the impact. I've copied our implementation here if you want to have a look : https://github.com/fyannk/cloudnative-pg/tree/dev/clusterwide\nWith this, absolutely no impacts on operator's memory was seen when managing uncorrelated objects :\n![Image](https://github.com/user-attachments/assets/41f7267f-50f0-41aa-b3cc-7af2cc327d5f)\nWith this, all those stock kubernetes objects MUST have an annotation to be visible and managed by the operator :\n- ConfigMap\n- Deployment\n- Job\n- PersistentVolumeClaim\n- Pod\n- Role\n- RoleBinding\n- Secret\n- Service\n- ServiceAccount\nNo need for any annotation for CNPG's CRD objects (we don't really want to filter those) nor for VolumeSnapshot (I still need to have a look at this object)\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [x] I agree to follow this project's Code of ConductHi @fyannk \nDo you have a tool or script to create these configmaps/secrets you can share? \nRegards!\n---\nSure thing @sxd \nI've added an adapted version here : https://github.com/fyannk/cloudnative-pg/commit/2efe4ebe2a9161552702dc0cae591ad1bef72b2d\nFor more context, most operators have faced this issue :\n- https://github.com/cert-manager/cert-manager/issues/4722\n- https://github.com/prometheus-operator/prometheus-operator/issues/5410\n- https://github.com/kyverno/kyverno/issues/1832"
    },
    {
        "title": "[Docs]: update the managed-roles documentation with the CRD based roles",
        "id": 2613693308,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThere is already documentation for managed roles.\nWe should explain the differences between the existing cluster-based managed roles and the new CRD based roles and reconciler.\n### Describe what additions need to be done to the documentation\n_No response_\n### Describe what pages need to change in the documentation, if any\ndocs/src/declarative_role_management.md\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThere is already documentation for managed roles.\nWe should explain the differences between the existing cluster-based managed roles and the new CRD based roles and reconciler.\n### Describe what additions need to be done to the documentation\n_No response_\n### Describe what pages need to change in the documentation, if any\ndocs/src/declarative_role_management.md\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nNo\n### Code of Conduct\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Add `cnpg_backups_total` metric",
        "id": 2612448371,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThis metric is very useful to monitor the number of backups. Sometimes the expired backups don't get deleted, including the failed backups.\r\nCurrently the only way to check backups count is using `backups.postgresql.cnpg.io` CR\n### Describe the solution you'd like\nBased on barman-exporter project https://github.com/marcinhlybin/prometheus-barman-exporter\n### Describe alternatives you've considered\ncnpg has a lack of backup metrics.\r\nNo any project with barman exporter written in golang, due a barman nature.\r\nhttps://github.com/marcinhlybin/prometheus-barman-exporter project uses python and not actively maintained.\r\nPerhaps the things will be changed with cnpg-i, but not in a near future.\r\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThis metric is very useful to monitor the number of backups. Sometimes the expired backups don't get deleted, including the failed backups.\r\nCurrently the only way to check backups count is using `backups.postgresql.cnpg.io` CR\n### Describe the solution you'd like\nBased on barman-exporter project https://github.com/marcinhlybin/prometheus-barman-exporter\n### Describe alternatives you've considered\ncnpg has a lack of backup metrics.\r\nNo any project with barman exporter written in golang, due a barman nature.\r\nhttps://github.com/marcinhlybin/prometheus-barman-exporter project uses python and not actively maintained.\r\nPerhaps the things will be changed with cnpg-i, but not in a near future.\r\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductWe are heavily changing the way backup and recovery works in CNPG, and introducing a pluggable interface. I am putting this aside for now."
    },
    {
        "title": "[Feat]: add CRD and basic controller",
        "id": 2610693783,
        "state": "no reaction",
        "first": "",
        "messages": ""
    },
    {
        "title": "[Bug]: Cannot use PG 17 replication slot failover (dbname must be specified in primary_conninfo)",
        "id": 2603915581,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nI'm trying to test the new failover slots into PG 17. I am able to turn on hot_standby_feedback and sync_replication_slots but the slots never synchronize. The replica is logging an error from the \"slotsync worker\" that says that dbname must be specified in the primary_conninfo in order to enable failover sync. [According to the PG docs](https://www.postgresql.org/docs/current/runtime-config-replication.html#GUC-PRIMARY-CONNINFO) the dbname parameter is used exclusively for slot synchronization, so the issue might be relatively straightforward to solve.\n### Cluster resource\n_No response_\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-10-21T23:57:29.986731099Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"best-cluster-1\",\"record\":{\"log_time\":\"2024-10-21 23:57:29.986 UTC\",\"process_id\":\"4376\",\"session_id\":\"6716ea69.1118\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-10-21 23:57:29 UTC\",\"virtual_transaction_id\":\"92/0\",\"transaction_id\":\"0\",\"error_severity\":\"ERROR\",\"sql_state_code\":\"22023\",\"message\":\"replication slot synchronization requires \\\"dbname\\\" to be specified in \\\"primary_conninfo\\\"\",\"backend_type\":\"slotsync worker\",\"query_id\":\"0\"}}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nI'm trying to test the new failover slots into PG 17. I am able to turn on hot_standby_feedback and sync_replication_slots but the slots never synchronize. The replica is logging an error from the \"slotsync worker\" that says that dbname must be specified in the primary_conninfo in order to enable failover sync. [According to the PG docs](https://www.postgresql.org/docs/current/runtime-config-replication.html#GUC-PRIMARY-CONNINFO) the dbname parameter is used exclusively for slot synchronization, so the issue might be relatively straightforward to solve.\n### Cluster resource\n_No response_\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-10-21T23:57:29.986731099Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"best-cluster-1\",\"record\":{\"log_time\":\"2024-10-21 23:57:29.986 UTC\",\"process_id\":\"4376\",\"session_id\":\"6716ea69.1118\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-10-21 23:57:29 UTC\",\"virtual_transaction_id\":\"92/0\",\"transaction_id\":\"0\",\"error_severity\":\"ERROR\",\"sql_state_code\":\"22023\",\"message\":\"replication slot synchronization requires \\\"dbname\\\" to be specified in \\\"primary_conninfo\\\"\",\"backend_type\":\"slotsync worker\",\"query_id\":\"0\"}}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductAbsolutely! @jfrconley \nThis may also be interesting relation for some https://github.com/EnterpriseDB/pg_failover_slots/issues/53\n![Image](https://github.com/user-attachments/assets/849585a8-748e-4f31-a414-7669dbbb8a96)"
    },
    {
        "title": "[Bug]: transient \"walStorage cannot be disabled once the cluster is created\"",
        "id": 2596939479,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ngb12335@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nKubernetes - talos on hcloud\r\n```\r\nkubectl get nodes                               \r\nNAME                           STATUS   ROLES           AGE     VERSION\r\npreprod-talos-controlplane-1   Ready    control-plane   53d     v1.30.3\r\npreprod-talos-controlplane-2   Ready    control-plane   53d     v1.30.3\r\npreprod-talos-controlplane-3   Ready    control-plane   53d     v1.30.3\r\npreprod-talos-ingress-1        Ready    ingress         53d     v1.30.3\r\npreprod-talos-ingress-2        Ready    ingress         53d     v1.30.3\r\npreprod-talos-worker-1         Ready    worker          53d     v1.30.3\r\npreprod-talos-worker-2         Ready    worker          53d     v1.30.3\r\ntalos-database-1               Ready    database        4d13h   v1.30.3\r\ntalos-database-2               Ready    database        4d13h   v1.30.3\r\ntalos-database-3               Ready    database        49m     v1.30.3\r\n```\r\nThe database was created like `HelmRelease` with FluxCD:\r\n```\r\napiVersion: helm.toolkit.fluxcd.io/v2beta1\r\nkind: HelmRelease\r\nmetadata:\r\n  name: main\r\n  namespace: postgres\r\nspec:\r\n  chart:\r\n    spec:\r\n      chart: cluster\r\n      sourceRef:\r\n        kind: HelmRepository\r\n        name: cnpg\r\n        namespace: cnpg-system\r\n      version: '*'\r\n  install:\r\n    createNamespace: false\r\n  interval: 1m0s\r\n  values:\r\n    cluster:\r\n      storage:\r\n        size: 100Gi\r\n        storageClass: hcloud-volumes\r\n      affinity:\r\n        nodeSelector:\r\n          node-role.kubernetes.io/database: \"\"\r\n```\r\nNow I am getting:\r\n```\r\npostgres                 \thelmrelease/main              \t0.0.11            \tFalse    \tFalse\tHelm upgrade failed for release postgres/main with chart cluster@0.0.11: cannot patch \"main-cluster\" with kind Cluster: admission webhook \"vcluster.cnpg.io\" denied the request: Cluster.cluster.cnpg.io \"main-cluster\" is invalid: spec.walStorage: Invalid value: \"null\": walStorage cannot be disabled once the cluster is created\r\n```\r\nI believe the spec for helm chart was changed and when upgrading to the new version the error appeared.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    meta.helm.sh/release-name: main\r\n    meta.helm.sh/release-namespace: postgres\r\n  creationTimestamp: \"2024-08-27T15:40:23Z\"\r\n  generation: 3\r\n  labels:\r\n    app.kubernetes.io/instance: main\r\n    app.kubernetes.io/managed-by: Helm\r\n    app.kubernetes.io/name: cluster\r\n    app.kubernetes.io/part-of: cloudnative-pg\r\n    helm.sh/chart: cluster-0.0.10\r\n    helm.toolkit.fluxcd.io/name: main\r\n    helm.toolkit.fluxcd.io/namespace: postgres\r\n  name: main-cluster\r\n  namespace: postgres\r\n  resourceVersion: \"31072319\"\r\n  uid: 4dead356-4acc-4d0d-a98d-d18784cb7e82\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n    topologyKey: topology.kubernetes.io/zone\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enablePDB: true\r\n  enableSuperuserAccess: true\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:15.2\r\n  imagePullPolicy: IfNotPresent\r\n  instances: 3\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: false\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      full_page_writes: \"on\"\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_level: logical\r\n      wal_log_hints: \"on\"\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: switchover\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    synchronizeReplicas:\r\n      enabled: true\r\n    updateInterval: 30\r\n  resources: {}\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 100Gi\r\n    storageClass: hcloud-volumes\r\n  switchoverDelay: 3600\r\n  walStorage:\r\n    resizeInUseVolumes: true\r\n    size: 1Gi\r\nstatus:\r\n  availableArchitectures:\r\n  - goArch: amd64\r\n    hash: 575b8d5080a718a1b1c8e6febcb6ccfde6cf546aa1a253acd7336226494ba784\r\n  - goArch: arm64\r\n    hash: bab50cc05e920db8bd118118323ef8003201dd3ba0642bbdee87cfdde1672e3e\r\n  certificates:\r\n    clientCASecret: main-cluster-ca\r\n    expirations:\r\n      main-cluster-ca: 2024-11-25 15:35:23 +0000 UTC\r\n      main-cluster-replication: 2024-11-25 15:35:23 +0000 UTC\r\n      main-cluster-server: 2024-11-25 15:35:23 +0000 UTC\r\n    replicationTLSSecret: main-cluster-replication\r\n    serverAltDNSNames:\r\n    - main-cluster-rw\r\n    - main-cluster-rw.postgres\r\n    - main-cluster-rw.postgres.svc\r\n    - main-cluster-rw.postgres.svc.cluster.local\r\n    - main-cluster-r\r\n    - main-cluster-r.postgres\r\n    - main-cluster-r.postgres.svc\r\n    - main-cluster-r.postgres.svc.cluster.local\r\n    - main-cluster-ro\r\n    - main-cluster-ro.postgres\r\n    - main-cluster-ro.postgres.svc\r\n    - main-cluster-ro.postgres.svc.cluster.local\r\n    serverCASecret: main-cluster-ca\r\n    serverTLSSecret: main-cluster-server\r\n  cloudNativePGCommitHash: 3f96930d\r\n  cloudNativePGOperatorHash: 575b8d5080a718a1b1c8e6febcb6ccfde6cf546aa1a253acd7336226494ba784\r\n  conditions:\r\n  - lastTransitionTime: \"2024-10-17T11:51:12Z\"\r\n    message: Cluster is Ready\r\n    reason: ClusterIsReady\r\n    status: \"True\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-10-17T11:50:22Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"952073\"\r\n  currentPrimary: main-cluster-2\r\n  currentPrimaryTimestamp: \"2024-10-17T11:50:21.398133Z\"\r\n  healthyPVC:\r\n  - main-cluster-1\r\n  - main-cluster-1-wal\r\n  - main-cluster-2\r\n  - main-cluster-2-wal\r\n  - main-cluster-3\r\n  - main-cluster-3-wal\r\n  image: ghcr.io/cloudnative-pg/postgresql:15.2\r\n  instanceNames:\r\n  - main-cluster-1\r\n  - main-cluster-2\r\n  - main-cluster-3\r\n  instances: 3\r\n  instancesReportedState:\r\n    main-cluster-1:\r\n      isPrimary: false\r\n      timeLineID: 2\r\n    main-cluster-2:\r\n      isPrimary: true\r\n      timeLineID: 2\r\n    main-cluster-3:\r\n      isPrimary: false\r\n      timeLineID: 2\r\n  instancesStatus:\r\n    healthy:\r\n    - main-cluster-1\r\n    - main-cluster-2\r\n    - main-cluster-3\r\n  latestGeneratedNode: 3\r\n  managedRolesStatus: {}\r\n  phase: Cluster in healthy state\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 6\r\n  readService: main-cluster-r\r\n  readyInstances: 3\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"31055627\"\r\n    clientCaSecretVersion: \"952036\"\r\n    replicationSecretVersion: \"952039\"\r\n    serverCaSecretVersion: \"952036\"\r\n    serverSecretVersion: \"952038\"\r\n    superuserSecretVersion: \"31055626\"\r\n  switchReplicaClusterStatus: {}\r\n  targetPrimary: main-cluster-2\r\n  targetPrimaryTimestamp: \"2024-10-17T11:50:19.283279Z\"\r\n  timelineID: 2\r\n  topology:\r\n    instances:\r\n      main-cluster-1: {}\r\n      main-cluster-2: {}\r\n      main-cluster-3: {}\r\n    nodesUsed: 3\r\n    successfullyExtracted: true\r\n  writeService: main-cluster-rw\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ngb12335@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nKubernetes - talos on hcloud\r\n```\r\nkubectl get nodes                               \r\nNAME                           STATUS   ROLES           AGE     VERSION\r\npreprod-talos-controlplane-1   Ready    control-plane   53d     v1.30.3\r\npreprod-talos-controlplane-2   Ready    control-plane   53d     v1.30.3\r\npreprod-talos-controlplane-3   Ready    control-plane   53d     v1.30.3\r\npreprod-talos-ingress-1        Ready    ingress         53d     v1.30.3\r\npreprod-talos-ingress-2        Ready    ingress         53d     v1.30.3\r\npreprod-talos-worker-1         Ready    worker          53d     v1.30.3\r\npreprod-talos-worker-2         Ready    worker          53d     v1.30.3\r\ntalos-database-1               Ready    database        4d13h   v1.30.3\r\ntalos-database-2               Ready    database        4d13h   v1.30.3\r\ntalos-database-3               Ready    database        49m     v1.30.3\r\n```\r\nThe database was created like `HelmRelease` with FluxCD:\r\n```\r\napiVersion: helm.toolkit.fluxcd.io/v2beta1\r\nkind: HelmRelease\r\nmetadata:\r\n  name: main\r\n  namespace: postgres\r\nspec:\r\n  chart:\r\n    spec:\r\n      chart: cluster\r\n      sourceRef:\r\n        kind: HelmRepository\r\n        name: cnpg\r\n        namespace: cnpg-system\r\n      version: '*'\r\n  install:\r\n    createNamespace: false\r\n  interval: 1m0s\r\n  values:\r\n    cluster:\r\n      storage:\r\n        size: 100Gi\r\n        storageClass: hcloud-volumes\r\n      affinity:\r\n        nodeSelector:\r\n          node-role.kubernetes.io/database: \"\"\r\n```\r\nNow I am getting:\r\n```\r\npostgres                 \thelmrelease/main              \t0.0.11            \tFalse    \tFalse\tHelm upgrade failed for release postgres/main with chart cluster@0.0.11: cannot patch \"main-cluster\" with kind Cluster: admission webhook \"vcluster.cnpg.io\" denied the request: Cluster.cluster.cnpg.io \"main-cluster\" is invalid: spec.walStorage: Invalid value: \"null\": walStorage cannot be disabled once the cluster is created\r\n```\r\nI believe the spec for helm chart was changed and when upgrading to the new version the error appeared.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    meta.helm.sh/release-name: main\r\n    meta.helm.sh/release-namespace: postgres\r\n  creationTimestamp: \"2024-08-27T15:40:23Z\"\r\n  generation: 3\r\n  labels:\r\n    app.kubernetes.io/instance: main\r\n    app.kubernetes.io/managed-by: Helm\r\n    app.kubernetes.io/name: cluster\r\n    app.kubernetes.io/part-of: cloudnative-pg\r\n    helm.sh/chart: cluster-0.0.10\r\n    helm.toolkit.fluxcd.io/name: main\r\n    helm.toolkit.fluxcd.io/namespace: postgres\r\n  name: main-cluster\r\n  namespace: postgres\r\n  resourceVersion: \"31072319\"\r\n  uid: 4dead356-4acc-4d0d-a98d-d18784cb7e82\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n    topologyKey: topology.kubernetes.io/zone\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enablePDB: true\r\n  enableSuperuserAccess: true\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:15.2\r\n  imagePullPolicy: IfNotPresent\r\n  instances: 3\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: false\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      full_page_writes: \"on\"\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_level: logical\r\n      wal_log_hints: \"on\"\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: switchover\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    synchronizeReplicas:\r\n      enabled: true\r\n    updateInterval: 30\r\n  resources: {}\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 100Gi\r\n    storageClass: hcloud-volumes\r\n  switchoverDelay: 3600\r\n  walStorage:\r\n    resizeInUseVolumes: true\r\n    size: 1Gi\r\nstatus:\r\n  availableArchitectures:\r\n  - goArch: amd64\r\n    hash: 575b8d5080a718a1b1c8e6febcb6ccfde6cf546aa1a253acd7336226494ba784\r\n  - goArch: arm64\r\n    hash: bab50cc05e920db8bd118118323ef8003201dd3ba0642bbdee87cfdde1672e3e\r\n  certificates:\r\n    clientCASecret: main-cluster-ca\r\n    expirations:\r\n      main-cluster-ca: 2024-11-25 15:35:23 +0000 UTC\r\n      main-cluster-replication: 2024-11-25 15:35:23 +0000 UTC\r\n      main-cluster-server: 2024-11-25 15:35:23 +0000 UTC\r\n    replicationTLSSecret: main-cluster-replication\r\n    serverAltDNSNames:\r\n    - main-cluster-rw\r\n    - main-cluster-rw.postgres\r\n    - main-cluster-rw.postgres.svc\r\n    - main-cluster-rw.postgres.svc.cluster.local\r\n    - main-cluster-r\r\n    - main-cluster-r.postgres\r\n    - main-cluster-r.postgres.svc\r\n    - main-cluster-r.postgres.svc.cluster.local\r\n    - main-cluster-ro\r\n    - main-cluster-ro.postgres\r\n    - main-cluster-ro.postgres.svc\r\n    - main-cluster-ro.postgres.svc.cluster.local\r\n    serverCASecret: main-cluster-ca\r\n    serverTLSSecret: main-cluster-server\r\n  cloudNativePGCommitHash: 3f96930d\r\n  cloudNativePGOperatorHash: 575b8d5080a718a1b1c8e6febcb6ccfde6cf546aa1a253acd7336226494ba784\r\n  conditions:\r\n  - lastTransitionTime: \"2024-10-17T11:51:12Z\"\r\n    message: Cluster is Ready\r\n    reason: ClusterIsReady\r\n    status: \"True\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-10-17T11:50:22Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"952073\"\r\n  currentPrimary: main-cluster-2\r\n  currentPrimaryTimestamp: \"2024-10-17T11:50:21.398133Z\"\r\n  healthyPVC:\r\n  - main-cluster-1\r\n  - main-cluster-1-wal\r\n  - main-cluster-2\r\n  - main-cluster-2-wal\r\n  - main-cluster-3\r\n  - main-cluster-3-wal\r\n  image: ghcr.io/cloudnative-pg/postgresql:15.2\r\n  instanceNames:\r\n  - main-cluster-1\r\n  - main-cluster-2\r\n  - main-cluster-3\r\n  instances: 3\r\n  instancesReportedState:\r\n    main-cluster-1:\r\n      isPrimary: false\r\n      timeLineID: 2\r\n    main-cluster-2:\r\n      isPrimary: true\r\n      timeLineID: 2\r\n    main-cluster-3:\r\n      isPrimary: false\r\n      timeLineID: 2\r\n  instancesStatus:\r\n    healthy:\r\n    - main-cluster-1\r\n    - main-cluster-2\r\n    - main-cluster-3\r\n  latestGeneratedNode: 3\r\n  managedRolesStatus: {}\r\n  phase: Cluster in healthy state\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 6\r\n  readService: main-cluster-r\r\n  readyInstances: 3\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"31055627\"\r\n    clientCaSecretVersion: \"952036\"\r\n    replicationSecretVersion: \"952039\"\r\n    serverCaSecretVersion: \"952036\"\r\n    serverSecretVersion: \"952038\"\r\n    superuserSecretVersion: \"31055626\"\r\n  switchReplicaClusterStatus: {}\r\n  targetPrimary: main-cluster-2\r\n  targetPrimaryTimestamp: \"2024-10-17T11:50:19.283279Z\"\r\n  timelineID: 2\r\n  topology:\r\n    instances:\r\n      main-cluster-1: {}\r\n      main-cluster-2: {}\r\n      main-cluster-3: {}\r\n    nodesUsed: 3\r\n    successfullyExtracted: true\r\n  writeService: main-cluster-rw\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductSome more details:\r\n```\r\nkubectl describe helmrelease -n postgres main\r\nName:         main\r\nNamespace:    postgres\r\nLabels:       kustomize.toolkit.fluxcd.io/name=postgres\r\n              kustomize.toolkit.fluxcd.io/namespace=flux-system\r\nAnnotations:  <none>\r\nAPI Version:  helm.toolkit.fluxcd.io/v2\r\nKind:         HelmRelease\r\nMetadata:\r\n  Creation Timestamp:  2024-08-27T15:32:22Z\r\n  Finalizers:\r\n    finalizers.fluxcd.io\r\n  Generation:        3\r\n  Resource Version:  31583450\r\n  UID:               021701d1-a84e-4534-83f2-0a651ebef289\r\nSpec:\r\n  Chart:\r\n    Spec:\r\n      Chart:               cluster\r\n      Reconcile Strategy:  ChartVersion\r\n      Source Ref:\r\n        Kind:       HelmRepository\r\n        Name:       cnpg\r\n        Namespace:  cnpg-system\r\n      Version:      *\r\n  Install:\r\n    Create Namespace:  false\r\n  Interval:            1m0s\r\n  Values:\r\n    Cluster:\r\n      Affinity:\r\n        Node Selector:\r\n          node-role.kubernetes.io/database:  \r\n      Storage:\r\n        Size:           100Gi\r\n        Storage Class:  hcloud-volumes\r\nStatus:\r\n  Conditions:\r\n    Last Transition Time:  2024-10-18T07:54:07Z\r\n    Message:               Failed to upgrade after 1 attempt(s)\r\n    Observed Generation:   3\r\n    Reason:                RetriesExceeded\r\n    Status:                True\r\n    Type:                  Stalled\r\n    Last Transition Time:  2024-10-18T07:54:07Z\r\n    Message:               Helm upgrade failed for release postgres/main with chart cluster@0.0.11: cannot patch \"main-cluster\" with kind Cluster: admission webhook \"vcluster.cnpg.io\" denied the request: Cluster.cluster.cnpg.io \"main-cluster\" is invalid: spec.walStorage: Invalid value: \"null\": walStorage cannot be disabled once the cluster is created\r\n    Observed Generation:   3\r\n    Reason:                UpgradeFailed\r\n    Status:                False\r\n    Type:                  Ready\r\n    Last Transition Time:  2024-09-03T10:05:38Z\r\n    Message:               Helm upgrade failed for release postgres/main with chart cluster@0.0.11: cannot patch \"main-cluster\" with kind Cluster: admission webhook \"vcluster.cnpg.io\" denied the request: Cluster.cluster.cnpg.io \"main-cluster\" is invalid: spec.walStorage: Invalid value: \"null\": walStorage cannot be disabled once the cluster is created\r\n    Observed Generation:   3\r\n    Reason:                UpgradeFailed\r\n    Status:                False\r\n    Type:                  Released\r\n  Failures:                1\r\n  Helm Chart:              cnpg-system/postgres-main\r\n  History:\r\n    Chart Name:                   cluster\r\n    Chart Version:                0.0.11\r\n    Config Digest:                sha256:e310e924e4feff740083df266cd31d7e41d90405a1859b9c83dbbe883326d0bd\r\n    Digest:                       sha256:957fe7984fd61c4d5d5646e76be0ac143f598a836ea9e518f1b9f9005e48b62b\r\n    First Deployed:               2024-08-27T15:40:23Z\r\n    Last Deployed:                2024-10-18T07:54:06Z\r\n    Name:                         main\r\n    Namespace:                    postgres\r\n    Status:                       failed\r\n    Version:                      4\r\n    Chart Name:                   cluster\r\n    Chart Version:                0.0.11\r\n    Config Digest:                sha256:5dae214a42b4059c5edd8bfbbbdeb22afef0bcab1f1c6dd0629cf20a3d0eabd4\r\n    Digest:                       sha256:db9f60bc9db82c1e513e0d7c1e54186e76240c98b9a03c05ae00d71add91e3f0\r\n    First Deployed:               2024-08-27T15:40:23Z\r\n    Last Deployed:                2024-09-03T10:05:38Z\r\n    Name:                         main\r\n    Namespace:                    postgres\r\n    Status:                       failed\r\n    Version:                      3\r\n    Chart Name:                   cluster\r\n    Chart Version:                0.0.10\r\n    Config Digest:                sha256:5dae214a42b4059c5edd8bfbbbdeb22afef0bcab1f1c6dd0629cf20a3d0eabd4\r\n    Digest:                       sha256:59969a2965d14f824c8a924d30b23409397a8e4f8e1b7ac0174352728aed1139\r\n    First Deployed:               2024-08-27T15:40:23Z\r\n    Last Deployed:                2024-08-28T18:01:35Z\r\n    Name:                         main\r\n    Namespace:                    postgres\r\n    Status:                       deployed\r\n    Version:                      2\r\n    Chart Name:                   cluster\r\n    Chart Version:                0.0.9\r\n    Config Digest:                sha256:5dae214a42b4059c5edd8bfbbbdeb22afef0bcab1f1c6dd0629cf20a3d0eabd4\r\n    Digest:                       sha256:8bcbc39015b9930f6dc6355d76c91a59e8a4cfce295638f4cbdc35675a99d21a\r\n    First Deployed:               2024-08-27T15:40:23Z\r\n    Last Deployed:                2024-08-27T15:40:23Z\r\n    Name:                         main\r\n    Namespace:                    postgres\r\n    Status:                       superseded\r\n    Version:                      1\r\n  Last Attempted Config Digest:   sha256:e310e924e4feff740083df266cd31d7e41d90405a1859b9c83dbbe883326d0bd\r\n  Last Attempted Generation:      3\r\n  Last Attempted Release Action:  upgrade\r\n  Last Attempted Revision:        0.0.11\r\n  Observed Generation:            3\r\n  Storage Namespace:              postgres\r\n  Upgrade Failures:               1\r\nEvents:                           <none>\r\n```\n---\nIt helped to explicitly set `walStorage` settings in the `values`\r\n```\r\napiVersion: helm.toolkit.fluxcd.io/v2beta1\r\nkind: HelmRelease\r\nmetadata:\r\n  name: main\r\n  namespace: postgres\r\nspec:\r\n  chart:\r\n    spec:\r\n      chart: cluster\r\n      sourceRef:\r\n        kind: HelmRepository\r\n        name: cnpg\r\n        namespace: cnpg-system\r\n      version: '*'\r\n  install:\r\n    createNamespace: false\r\n  interval: 1m0s\r\n  values:\r\n    cluster:\r\n      storage:\r\n        size: 100Gi\r\n        storageClass: hcloud-volumes\r\n      affinity:\r\n        nodeSelector:\r\n          node-role.kubernetes.io/database: \"\"\r\n      walStorage:\r\n        enabled: true\r\n        size: 1Gi\r\n```\r\nso it is the proof that defaults were changed between helm chart `cluster` between 0.0.9 and 0.0.10 version:\r\nhttps://github.com/cloudnative-pg/charts/commit/ad5b27593ddaa0815c7779a80255177bafcb232a\n---\nHello @gecube \nSo, let me know if I understood correctly, the webook block you of removing the walStorage field when is already set, right?\nRegards,\n---\n@sxd Hi! Yes, I could confirm. Probably need to make a test on the latest version of cnpg."
    },
    {
        "title": "[Feature]: Add secret template for credentials secret",
        "id": 2595138147,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIt's impossible to add specific labels and annotations to the generated `basic-auth` type secrets containing the credentials and connection information.\n### Describe the solution you'd like\nSomething like `spec.secretTemplates.appUserSecret.annotations` (also this would introduce the possibility for other (future) secrets be specified here, like `superUserSecret`). would be sufficient.\n### Describe alternatives you've considered\nAlternative 1:\r\nFirst workaround is to specify the annotations with the `inheritedMetadata` attribute from the Cluster spec. But this adds too much noise to any object related to this cluster. Also in my specific use case (allowing the secret to be replicated into another namespace) it is possibly leaking too much information into other workspaces. Since it is not possible to connect with the superuser password from the network, this is a threat, that IMHO is possible to accept.\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: ...\r\nspec:\r\n  inheritedMetadata:\r\n    annotations:\r\n      reflector.v1.k8s.emberstack.com/reflection-allowed: 'true'\r\n      reflector.v1.k8s.emberstack.com/reflection-allowed-namespaces: grafana\r\n   ...\r\n```\r\nAlternative 2:\r\nUse the possibility of the managed roles to create an additional role with also a self created secret. For example I now have a Grafana role.\n### Additional context\nThe use case is replicating the postgres app user secret to another workspace by utilizing kubernetes-reflector). After writing the \"Alternatives I considered\" section, I realized, that for my specific use case, a Grafana role in Postgres (with also only read only access) would be more useful, but I opened the issue because I think this would be a useful addition in general for other use cases (database residing in other workspace than application for example).\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIt's impossible to add specific labels and annotations to the generated `basic-auth` type secrets containing the credentials and connection information.\n### Describe the solution you'd like\nSomething like `spec.secretTemplates.appUserSecret.annotations` (also this would introduce the possibility for other (future) secrets be specified here, like `superUserSecret`). would be sufficient.\n### Describe alternatives you've considered\nAlternative 1:\r\nFirst workaround is to specify the annotations with the `inheritedMetadata` attribute from the Cluster spec. But this adds too much noise to any object related to this cluster. Also in my specific use case (allowing the secret to be replicated into another namespace) it is possibly leaking too much information into other workspaces. Since it is not possible to connect with the superuser password from the network, this is a threat, that IMHO is possible to accept.\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: ...\r\nspec:\r\n  inheritedMetadata:\r\n    annotations:\r\n      reflector.v1.k8s.emberstack.com/reflection-allowed: 'true'\r\n      reflector.v1.k8s.emberstack.com/reflection-allowed-namespaces: grafana\r\n   ...\r\n```\r\nAlternative 2:\r\nUse the possibility of the managed roles to create an additional role with also a self created secret. For example I now have a Grafana role.\n### Additional context\nThe use case is replicating the postgres app user secret to another workspace by utilizing kubernetes-reflector). After writing the \"Alternatives I considered\" section, I realized, that for my specific use case, a Grafana role in Postgres (with also only read only access) would be more useful, but I opened the issue because I think this would be a useful addition in general for other use cases (database residing in other workspace than application for example).\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductAn unresolved discussion I found with another (unspecific) use case: https://github.com/cloudnative-pg/cloudnative-pg/discussions/3653"
    },
    {
        "title": "[Bug]: Make `color` argument of kubectl cnpg a global variable",
        "id": 2591210033,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nIn the beginning there were colors only in the status command, but now we have colors in more than one command, we should have a global argument for this option since it's used in more than one place\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nIn the beginning there were colors only in the status command, but now we have colors in more than one command, we should have a global argument for this option since it's used in more than one place\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI'd love to take this up!"
    },
    {
        "title": "[Feature]: managing cnpg in GitOps workflows (FluxCD, ArgoCD ...)",
        "id": 2575593567,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIn kubernetes cluster manager using gitops tools like FluxCD, I faced a really annoying limiation, especially for restoring db cluster, which is the following:\r\nWhen defining a CNPG cluster resource for a db, we specify the initialization mode for the cluster of the database. There are two options:\r\n    initdb: This mode creates an empty database without any data.\r\n    Recovery: This mode restores the database from a backup in object storage.\r\nIn the initdb mode, if the database crashes at some point and we need to restore it, we would have to restore it manually. However, since the database is managed by FluxCD, it will automatically recreate the cluster and initialize it without any data during the next reconciliation. This makes restoration impossible in this scenario it may seem to work momentarily, but all data will be lost shortly after (next FluxCD reconcialiation).\r\nOn the other hand, using the recovery mode is a good solution for restoring databases after a failure. However, for newly created databases, the CNPG cluster needs to be initialized in initdb mode. If we do this, we encounter the same problem as before\u2014FluxCD will recreate an empty database, making it difficult to restore any existing data.\r\nAnother issue arises when the cluster recovers from a backup or is newly created. It must save new backups to a different path, as the old path contains backups from before the database crash. \r\n### Describe the solution you'd like\nMy goal is to manage the db and its restore in case of restore when changing the yaml of resource and go throught git and FluxCD.\r\nThe cnpg cluster must be more intellegent to create dynamic destination path for backup data, and also spec.bootstrap must have a schema that allow recovery from an empty backup and initialise db without data like initdb mode\n### Describe alternatives you've considered\nIt would be more efficient if we added an option in the cluster resource to dynamically create the backup destination folder by concatenating the database name with the date of its creation, such as postgres-db-09-10-2020. This way, the new backup folder (e.g., postgres-db-09-10-2020) would start empty, allowing the database to be successfully initialized.\r\nWith this approach, everything is managed automatically, without needing to change the cluster resource code in the Git repository whenever we want to restore a database.\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIn kubernetes cluster manager using gitops tools like FluxCD, I faced a really annoying limiation, especially for restoring db cluster, which is the following:\r\nWhen defining a CNPG cluster resource for a db, we specify the initialization mode for the cluster of the database. There are two options:\r\n    initdb: This mode creates an empty database without any data.\r\n    Recovery: This mode restores the database from a backup in object storage.\r\nIn the initdb mode, if the database crashes at some point and we need to restore it, we would have to restore it manually. However, since the database is managed by FluxCD, it will automatically recreate the cluster and initialize it without any data during the next reconciliation. This makes restoration impossible in this scenario it may seem to work momentarily, but all data will be lost shortly after (next FluxCD reconcialiation).\r\nOn the other hand, using the recovery mode is a good solution for restoring databases after a failure. However, for newly created databases, the CNPG cluster needs to be initialized in initdb mode. If we do this, we encounter the same problem as before\u2014FluxCD will recreate an empty database, making it difficult to restore any existing data.\r\nAnother issue arises when the cluster recovers from a backup or is newly created. It must save new backups to a different path, as the old path contains backups from before the database crash. \r\n### Describe the solution you'd like\nMy goal is to manage the db and its restore in case of restore when changing the yaml of resource and go throught git and FluxCD.\r\nThe cnpg cluster must be more intellegent to create dynamic destination path for backup data, and also spec.bootstrap must have a schema that allow recovery from an empty backup and initialise db without data like initdb mode\n### Describe alternatives you've considered\nIt would be more efficient if we added an option in the cluster resource to dynamically create the backup destination folder by concatenating the database name with the date of its creation, such as postgres-db-09-10-2020. This way, the new backup folder (e.g., postgres-db-09-10-2020) would start empty, allowing the database to be successfully initialized.\r\nWith this approach, everything is managed automatically, without needing to change the cluster resource code in the Git repository whenever we want to restore a database.\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductHello @riadelhajjaji-2001\r\nThis is not something to be solved at operator level, why you think the operator should solve that? every GitOps has it's own way to generated variables that can be used to replace, is not clear what wants to be solved here that cannot be solved using GitOps\r\nCan you clarify a little bit?\r\nThanks\n---\nHello @sxd \nThank you for your message!\nI'll describe a scenario:\nImagine we have a database in production, and it crashes due to a disaster. When the database cluster restarts, it initializes with initdb, which means the database is empty, even though other applications need the data. Another issue is that, after the crash, the database cannot perform a backup because the destination bucket already contains archived WALs from before the crash.\nIf I use GitOps tools to generate dynamic YAML, the GitOps tool will reconcile at regular intervals. At the end of each interval, it creates a new cluster with initdb and a new destination path for the backup. In my case, I\u2019m using a 5-minute interval, so every time the GitOps tool reconciles, it reinitializes the database cluster with initdb. which is a problem !\nThank you \ud83d\ude4f\n---\n@sxd Let me describe the scenario I run into which is closely related to @riadelhajjaji-2001 comment.\r\nMy Kubernetes cluster managed by [Flux](https://fluxcd.io/) is all happy, pg cluster running great, backups are running. Now there is a full kubernetes disaster and I need to re-provision my Kubernetes cluster. After I have the new Kubernetes cluster ready to go and before I tell Flux to put everything back, I need to change the CNPG Cluster manifest and commit/push it to Git before it is applied by Flux.\r\nIdeally I would like CNPG to always try to recover on first bootstrap from the latest version if there's a recovery found. It's the manual changing of the CNPG Cluster manifest during a kubernetes cluster restore that is hard to fit into a GitOps pattern.\r\nTo be more clear: Mutation of the CNPG `Cluster` resource within the remote Git repo before the GitOps tool can be kicked off to put everything back is a nuisance that would be nice to be solved.\r\nCPGO has this functionality it would be awesome if CNPG could have it too.\n---\n@sxd This for sure is something that should be solved at the operator level.\r\nEssentially CNPG is blocking reproducible cluster  (re)builds for ALL gitops tools out there.\r\nAs well as reproducible builds of in-cluster applications (aka partial cluster rebuilds)\n---\n+1 This is also our main problem with argo-cd at the moment. Even with preSync and postSync hooks, and a lot of helm shenanigans, it's not possible..\n---\nSame here in my case i faced real dr on hetzner k8s, luckily i was running pgdumps which saved me.\nThis should be managed by operator it should have a state management to know that this was old cluster with existing data in objectstore and if data is missing on postgress side it performs restore automatically (configurable)\n---\n@sxd Can we please actually get a response on the clarification you requested?"
    },
    {
        "title": "[Bug]: WAL archive not running every 5 mins",
        "id": 2574653283,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njasontse829@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nHelm\n### What happened?\nWAL archive not run every 5 mins. \r\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  creationTimestamp: '2024-10-08T09:06:14Z'\r\n  generation: 4\r\n  labels:\r\n    argocd.argoproj.io/instance: testprd.namespace\r\n  name: testprd\r\n  namespace: namespace\r\n  resourceVersion: 'xxxxxxxx'\r\n  uid: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\r\nspec:\r\n  affinity:\r\n    additionalPodAntiAffinity:\r\n      preferredDuringSchedulingIgnoredDuringExecution:\r\n        - podAffinityTerm:\r\n            labelSelector:\r\n              matchExpressions:\r\n                - key: namespace\r\n                  operator: In\r\n                  values:\r\n                    - testprd\r\n            topologyKey: deploy-zone\r\n          weight: 100\r\n        - podAffinityTerm:\r\n            labelSelector:\r\n              matchExpressions:\r\n                - key: namespace\r\n                  operator: In\r\n                  values:\r\n                    - testprd\r\n            topologyKey: kubernetes.io/hostname\r\n          weight: 90\r\n    nodeAffinity:\r\n      requiredDuringSchedulingIgnoredDuringExecution:\r\n        nodeSelectorTerms:\r\n          - matchExpressions:\r\n              - key: deploy-zone\r\n                operator: In\r\n                values:\r\n                  - zoneA\r\n                  - zoneB\r\n    podAntiAffinityType: preferred\r\n    tolerations:\r\n      - effect: NoSchedule\r\n        key: usage\r\n        operator: Equal\r\n        value: PGVector\r\n  backup:\r\n    barmanObjectStore:\r\n      data:\r\n        compression: gzip\r\n        encryption: AES256\r\n        jobs: 2\r\n      destinationPath: 's3://xxxxxprd/'\r\n      endpointCA:\r\n        key: <key>\r\n        name: <key>-secret\r\n      endpointURL: 'https://xxxxx.com:10443'\r\n      s3Credentials:\r\n        accessKeyId:\r\n          key: ACCESS_KEY_ID\r\n          name: creds\r\n        secretAccessKey:\r\n          key: ACCESS_SECRET_KEY\r\n          name: creds\r\n      wal:\r\n        compression: gzip\r\n        encryption: AES256\r\n        maxParallel: 1\r\n    retentionPolicy: 7d\r\n    target: prefer-standby\r\n  bootstrap:\r\n    initdb:\r\n      database: database\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: owner\r\n      postInitApplicationSQL:\r\n        - >-\r\n        - CREATE EXTENSION IF NOT EXISTS \"vectors\";\r\n        - ALTER DATABASE \"database\" SET SEARCH_PATH TO vectors;\r\n      secret:\r\n        name: testprd-owner\r\n  enablePDB: true\r\n  enableSuperuserAccess: false\r\n  failoverDelay: 0\r\n  imageName: '<images>'\r\n  instances: 3\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n      - key: queries\r\n        name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: 'on'\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_duration: 'on'\r\n      log_filename: postgres\r\n      log_min_duration_statement: '-1'\r\n      log_min_error_statement: error\r\n      log_min_messages: info\r\n      log_rotation_age: '0'\r\n      log_rotation_size: '0'\r\n      log_statement: mod\r\n      log_truncate_on_rotation: 'false'\r\n      logging_collector: 'on'\r\n      max_parallel_workers: '32'\r\n      max_replication_slots: '32'\r\n      max_worker_processes: '32'\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: ''\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_level: logical\r\n      wal_log_hints: 'on'\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    pg_hba:\r\n      - host replication replicatest all md5\r\n    shared_preload_libraries:\r\n      - vectors.so\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    synchronizeReplicas:\r\n      enabled: true\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      cpu: '8'\r\n      memory: 16Gi\r\n    requests:\r\n      cpu: '8'\r\n      memory: 16Gi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 30Gi\r\n    storageClass: trident-ontap-nas\r\n  switchoverDelay: 3600\r\n  tablespaces:\r\n    - name: data\r\n      owner:\r\n        name: owner\r\n      storage:\r\n        resizeInUseVolumes: true\r\n        size: 40Gi\r\n        storageClass: trident-ontap-nas\r\n      temporary: false\r\nstatus:\r\n  availableArchitectures:\r\n    - goArch: amd64\r\n      hash: <hash>\r\n    - goArch: arm64\r\n      hash: <hash>\r\n  certificates:\r\n    clientCASecret: testprd-ca\r\n    expirations:\r\n      testprd-ca: '2025-01-06 09:01:14 +0000 UTC'\r\n      testprd-replication: '2025-01-06 09:01:14 +0000 UTC'\r\n      testprd-server: '2025-01-06 09:01:14 +0000 UTC'\r\n    replicationTLSSecret: testprd-replication\r\n    serverAltDNSNames:\r\n      - testprd-rw\r\n      - testprd-rw.namespace\r\n      - testprd-rw.namespace.svc\r\n      - testprd-rw.namespace.svc.cluster.local\r\n      - testprd-r\r\n      - testprd-r.namespace\r\n      - testprd-r.namespace.svc\r\n      - testprd-r.namespace.svc.cluster.local\r\n      - testprd-ro\r\n      - testprd-ro.namespace\r\n      - testprd-ro.namespace.svc\r\n      - testprd-ro.namespace.svc.cluster.local\r\n    serverCASecret: testprd-ca\r\n    serverTLSSecret: testprd-server\r\n  cloudNativePGCommitHash: 5fe5bb6b\r\n  cloudNativePGOperatorHash: f20cbc18bb03eafc1b02c90a8872b8e7a199e63196e6ce546029ea2a503bb883\r\n  conditions:\r\n    - lastTransitionTime: '2024-10-08T09:12:03Z'\r\n      message: Cluster is Ready\r\n      reason: ClusterIsReady\r\n      status: 'True'\r\n      type: Ready\r\n    - lastTransitionTime: '2024-10-08T09:29:18Z'\r\n      message: Continuous archiving is working\r\n      reason: ContinuousArchivingSuccess\r\n      status: 'True'\r\n      type: ContinuousArchiving\r\n    - lastTransitionTime: '2024-10-09T03:00:09Z'\r\n      message: Backup was successful\r\n      reason: LastBackupSucceeded\r\n      status: 'True'\r\n      type: LastBackupSucceeded\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: '385609033'\r\n  currentPrimary: testprd-1\r\n  currentPrimaryTimestamp: '2024-10-08T09:08:01.239597Z'\r\n  firstRecoverabilityPoint: '2024-10-09T03:00:07Z'\r\n  firstRecoverabilityPointByMethod:\r\n    barmanObjectStore: '2024-10-09T03:00:07Z'\r\n  healthyPVC:\r\n    - testprd-1\r\n    - testprd-1-tbs-data\r\n    - testprd-2\r\n    - testprd-2-tbs-data\r\n    - testprd-3\r\n    - testprd-3-tbs-data\r\n  image: 'artifact.oocl.com/postgres-containers-15-modules-pgvector:15'\r\n  instanceNames:\r\n    - testprd-1\r\n    - testprd-2\r\n    - testprd-3\r\n  instances: 3\r\n  instancesReportedState:\r\n    testprd-1:\r\n      isPrimary: true\r\n      timeLineID: 1\r\n    testprd-2:\r\n      isPrimary: false\r\n      timeLineID: 1\r\n    testprd-3:\r\n      isPrimary: false\r\n      timeLineID: 1\r\n  instancesStatus:\r\n    healthy:\r\n      - testprd-1\r\n      - testprd-2\r\n      - testprd-3\r\n  lastSuccessfulBackup: '2024-10-09T03:00:07Z'\r\n  lastSuccessfulBackupByMethod:\r\n    barmanObjectStore: '2024-10-09T03:00:07Z'\r\n  latestGeneratedNode: 3\r\n  managedRolesStatus:\r\n    byStatus:\r\n      reconciled:\r\n        - owner\r\n      reserved:\r\n        - postgres\r\n        - streaming_replica\r\n    passwordStatus:\r\n      owner:\r\n        transactionID: 741\r\n  phase: Cluster in healthy state\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 6\r\n  readService: testprd-r\r\n  readyInstances: 3\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: '401300078'\r\n    barmanEndpointCA: '385669736'\r\n    clientCaSecretVersion: '401300089'\r\n    replicationSecretVersion: '401300092'\r\n    serverCaSecretVersion: '401300089'\r\n    serverSecretVersion: '401300091'\r\n  switchReplicaClusterStatus: {}\r\n  tablespacesStatus:\r\n    - name: testdata\r\n      owner: testowner\r\n      state: reconciled\r\n  targetPrimary: testprd-1\r\n  targetPrimaryTimestamp: '2024-10-08T09:06:14.308267Z'\r\n  timelineID: 1\r\n  topology:\r\n    instances:\r\n      testprd-1: {}\r\n      testprd-2: {}\r\n      testprd-3: {}\r\n    nodesUsed: 3\r\n    successfullyExtracted: true\r\n  writeService: testprd-rw\n```\n### Relevant log output\n```shell\nNo errors message related WAL archive.\r\n/var/lib/postgresql/data/pgdata/pg_wal\r\n-rw-------. 1 postgres 16777216 Sep 23 01:37 000000010000000000000001\r\n-rw-------. 1 postgres 16777216 Sep 23 01:39 000000010000000000000002\r\n-rw-------. 1 postgres 16777216 Sep 23 01:39 000000010000000000000003\r\n-rw-------. 1 postgres 16777216 Sep 23 01:41 000000010000000000000004\r\n-rw-------. 1 postgres 16777216 Sep 23 01:41 000000010000000000000005\r\n-rw-------. 1 postgres      338 Sep 23 01:41 000000010000000000000005.00000028.backup\r\n-rw-------. 1 postgres 16777216 Sep 23 01:46 000000010000000000000006\r\n-rw-------. 1 postgres 16777216 Sep 23 02:11 000000010000000000000007\r\n-rw-------. 1 postgres 16777216 Sep 23 03:01 000000010000000000000008\r\n-rw-------. 1 postgres 16777216 Sep 23 07:16 000000010000000000000009\r\n-rw-------. 1 postgres 16777216 Sep 23 07:16 00000001000000000000000A\r\n-rw-------. 1 postgres 16777216 Sep 23 07:21 00000001000000000000000B\r\n-rw-------. 1 postgres 16777216 Sep 23 07:26 00000001000000000000000C\r\n-rw-------. 1 postgres 16777216 Sep 23 07:31 00000001000000000000000D\r\n-rw-------. 1 postgres 16777216 Sep 23 07:56 00000001000000000000000E\r\n-rw-------. 1 postgres 16777216 Sep 23 08:01 00000001000000000000000F\r\n-rw-------. 1 postgres 16777216 Sep 24 03:02 000000010000000000000010\r\n-rw-------. 1 postgres 16777216 Sep 24 03:07 000000010000000000000011\r\ndrwx------. 2 postgres     4096 Sep 24 03:07 archive_status\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njasontse829@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nHelm\n### What happened?\nWAL archive not run every 5 mins. \r\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  creationTimestamp: '2024-10-08T09:06:14Z'\r\n  generation: 4\r\n  labels:\r\n    argocd.argoproj.io/instance: testprd.namespace\r\n  name: testprd\r\n  namespace: namespace\r\n  resourceVersion: 'xxxxxxxx'\r\n  uid: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\r\nspec:\r\n  affinity:\r\n    additionalPodAntiAffinity:\r\n      preferredDuringSchedulingIgnoredDuringExecution:\r\n        - podAffinityTerm:\r\n            labelSelector:\r\n              matchExpressions:\r\n                - key: namespace\r\n                  operator: In\r\n                  values:\r\n                    - testprd\r\n            topologyKey: deploy-zone\r\n          weight: 100\r\n        - podAffinityTerm:\r\n            labelSelector:\r\n              matchExpressions:\r\n                - key: namespace\r\n                  operator: In\r\n                  values:\r\n                    - testprd\r\n            topologyKey: kubernetes.io/hostname\r\n          weight: 90\r\n    nodeAffinity:\r\n      requiredDuringSchedulingIgnoredDuringExecution:\r\n        nodeSelectorTerms:\r\n          - matchExpressions:\r\n              - key: deploy-zone\r\n                operator: In\r\n                values:\r\n                  - zoneA\r\n                  - zoneB\r\n    podAntiAffinityType: preferred\r\n    tolerations:\r\n      - effect: NoSchedule\r\n        key: usage\r\n        operator: Equal\r\n        value: PGVector\r\n  backup:\r\n    barmanObjectStore:\r\n      data:\r\n        compression: gzip\r\n        encryption: AES256\r\n        jobs: 2\r\n      destinationPath: 's3://xxxxxprd/'\r\n      endpointCA:\r\n        key: <key>\r\n        name: <key>-secret\r\n      endpointURL: 'https://xxxxx.com:10443'\r\n      s3Credentials:\r\n        accessKeyId:\r\n          key: ACCESS_KEY_ID\r\n          name: creds\r\n        secretAccessKey:\r\n          key: ACCESS_SECRET_KEY\r\n          name: creds\r\n      wal:\r\n        compression: gzip\r\n        encryption: AES256\r\n        maxParallel: 1\r\n    retentionPolicy: 7d\r\n    target: prefer-standby\r\n  bootstrap:\r\n    initdb:\r\n      database: database\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: owner\r\n      postInitApplicationSQL:\r\n        - >-\r\n        - CREATE EXTENSION IF NOT EXISTS \"vectors\";\r\n        - ALTER DATABASE \"database\" SET SEARCH_PATH TO vectors;\r\n      secret:\r\n        name: testprd-owner\r\n  enablePDB: true\r\n  enableSuperuserAccess: false\r\n  failoverDelay: 0\r\n  imageName: '<images>'\r\n  instances: 3\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n      - key: queries\r\n        name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: 'on'\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_duration: 'on'\r\n      log_filename: postgres\r\n      log_min_duration_statement: '-1'\r\n      log_min_error_statement: error\r\n      log_min_messages: info\r\n      log_rotation_age: '0'\r\n      log_rotation_size: '0'\r\n      log_statement: mod\r\n      log_truncate_on_rotation: 'false'\r\n      logging_collector: 'on'\r\n      max_parallel_workers: '32'\r\n      max_replication_slots: '32'\r\n      max_worker_processes: '32'\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: ''\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_level: logical\r\n      wal_log_hints: 'on'\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    pg_hba:\r\n      - host replication replicatest all md5\r\n    shared_preload_libraries:\r\n      - vectors.so\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    synchronizeReplicas:\r\n      enabled: true\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      cpu: '8'\r\n      memory: 16Gi\r\n    requests:\r\n      cpu: '8'\r\n      memory: 16Gi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 30Gi\r\n    storageClass: trident-ontap-nas\r\n  switchoverDelay: 3600\r\n  tablespaces:\r\n    - name: data\r\n      owner:\r\n        name: owner\r\n      storage:\r\n        resizeInUseVolumes: true\r\n        size: 40Gi\r\n        storageClass: trident-ontap-nas\r\n      temporary: false\r\nstatus:\r\n  availableArchitectures:\r\n    - goArch: amd64\r\n      hash: <hash>\r\n    - goArch: arm64\r\n      hash: <hash>\r\n  certificates:\r\n    clientCASecret: testprd-ca\r\n    expirations:\r\n      testprd-ca: '2025-01-06 09:01:14 +0000 UTC'\r\n      testprd-replication: '2025-01-06 09:01:14 +0000 UTC'\r\n      testprd-server: '2025-01-06 09:01:14 +0000 UTC'\r\n    replicationTLSSecret: testprd-replication\r\n    serverAltDNSNames:\r\n      - testprd-rw\r\n      - testprd-rw.namespace\r\n      - testprd-rw.namespace.svc\r\n      - testprd-rw.namespace.svc.cluster.local\r\n      - testprd-r\r\n      - testprd-r.namespace\r\n      - testprd-r.namespace.svc\r\n      - testprd-r.namespace.svc.cluster.local\r\n      - testprd-ro\r\n      - testprd-ro.namespace\r\n      - testprd-ro.namespace.svc\r\n      - testprd-ro.namespace.svc.cluster.local\r\n    serverCASecret: testprd-ca\r\n    serverTLSSecret: testprd-server\r\n  cloudNativePGCommitHash: 5fe5bb6b\r\n  cloudNativePGOperatorHash: f20cbc18bb03eafc1b02c90a8872b8e7a199e63196e6ce546029ea2a503bb883\r\n  conditions:\r\n    - lastTransitionTime: '2024-10-08T09:12:03Z'\r\n      message: Cluster is Ready\r\n      reason: ClusterIsReady\r\n      status: 'True'\r\n      type: Ready\r\n    - lastTransitionTime: '2024-10-08T09:29:18Z'\r\n      message: Continuous archiving is working\r\n      reason: ContinuousArchivingSuccess\r\n      status: 'True'\r\n      type: ContinuousArchiving\r\n    - lastTransitionTime: '2024-10-09T03:00:09Z'\r\n      message: Backup was successful\r\n      reason: LastBackupSucceeded\r\n      status: 'True'\r\n      type: LastBackupSucceeded\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: '385609033'\r\n  currentPrimary: testprd-1\r\n  currentPrimaryTimestamp: '2024-10-08T09:08:01.239597Z'\r\n  firstRecoverabilityPoint: '2024-10-09T03:00:07Z'\r\n  firstRecoverabilityPointByMethod:\r\n    barmanObjectStore: '2024-10-09T03:00:07Z'\r\n  healthyPVC:\r\n    - testprd-1\r\n    - testprd-1-tbs-data\r\n    - testprd-2\r\n    - testprd-2-tbs-data\r\n    - testprd-3\r\n    - testprd-3-tbs-data\r\n  image: 'artifact.oocl.com/postgres-containers-15-modules-pgvector:15'\r\n  instanceNames:\r\n    - testprd-1\r\n    - testprd-2\r\n    - testprd-3\r\n  instances: 3\r\n  instancesReportedState:\r\n    testprd-1:\r\n      isPrimary: true\r\n      timeLineID: 1\r\n    testprd-2:\r\n      isPrimary: false\r\n      timeLineID: 1\r\n    testprd-3:\r\n      isPrimary: false\r\n      timeLineID: 1\r\n  instancesStatus:\r\n    healthy:\r\n      - testprd-1\r\n      - testprd-2\r\n      - testprd-3\r\n  lastSuccessfulBackup: '2024-10-09T03:00:07Z'\r\n  lastSuccessfulBackupByMethod:\r\n    barmanObjectStore: '2024-10-09T03:00:07Z'\r\n  latestGeneratedNode: 3\r\n  managedRolesStatus:\r\n    byStatus:\r\n      reconciled:\r\n        - owner\r\n      reserved:\r\n        - postgres\r\n        - streaming_replica\r\n    passwordStatus:\r\n      owner:\r\n        transactionID: 741\r\n  phase: Cluster in healthy state\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 6\r\n  readService: testprd-r\r\n  readyInstances: 3\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: '401300078'\r\n    barmanEndpointCA: '385669736'\r\n    clientCaSecretVersion: '401300089'\r\n    replicationSecretVersion: '401300092'\r\n    serverCaSecretVersion: '401300089'\r\n    serverSecretVersion: '401300091'\r\n  switchReplicaClusterStatus: {}\r\n  tablespacesStatus:\r\n    - name: testdata\r\n      owner: testowner\r\n      state: reconciled\r\n  targetPrimary: testprd-1\r\n  targetPrimaryTimestamp: '2024-10-08T09:06:14.308267Z'\r\n  timelineID: 1\r\n  topology:\r\n    instances:\r\n      testprd-1: {}\r\n      testprd-2: {}\r\n      testprd-3: {}\r\n    nodesUsed: 3\r\n    successfullyExtracted: true\r\n  writeService: testprd-rw\n```\n### Relevant log output\n```shell\nNo errors message related WAL archive.\r\n/var/lib/postgresql/data/pgdata/pg_wal\r\n-rw-------. 1 postgres 16777216 Sep 23 01:37 000000010000000000000001\r\n-rw-------. 1 postgres 16777216 Sep 23 01:39 000000010000000000000002\r\n-rw-------. 1 postgres 16777216 Sep 23 01:39 000000010000000000000003\r\n-rw-------. 1 postgres 16777216 Sep 23 01:41 000000010000000000000004\r\n-rw-------. 1 postgres 16777216 Sep 23 01:41 000000010000000000000005\r\n-rw-------. 1 postgres      338 Sep 23 01:41 000000010000000000000005.00000028.backup\r\n-rw-------. 1 postgres 16777216 Sep 23 01:46 000000010000000000000006\r\n-rw-------. 1 postgres 16777216 Sep 23 02:11 000000010000000000000007\r\n-rw-------. 1 postgres 16777216 Sep 23 03:01 000000010000000000000008\r\n-rw-------. 1 postgres 16777216 Sep 23 07:16 000000010000000000000009\r\n-rw-------. 1 postgres 16777216 Sep 23 07:16 00000001000000000000000A\r\n-rw-------. 1 postgres 16777216 Sep 23 07:21 00000001000000000000000B\r\n-rw-------. 1 postgres 16777216 Sep 23 07:26 00000001000000000000000C\r\n-rw-------. 1 postgres 16777216 Sep 23 07:31 00000001000000000000000D\r\n-rw-------. 1 postgres 16777216 Sep 23 07:56 00000001000000000000000E\r\n-rw-------. 1 postgres 16777216 Sep 23 08:01 00000001000000000000000F\r\n-rw-------. 1 postgres 16777216 Sep 24 03:02 000000010000000000000010\r\n-rw-------. 1 postgres 16777216 Sep 24 03:07 000000010000000000000011\r\ndrwx------. 2 postgres     4096 Sep 24 03:07 archive_status\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductTwo questions, before we confirm this is a bug:\r\n- are you consistently writing on the database?\r\n- can you send the content of the `pg_wal/archive_status` folder?\r\nThank you.\n---\nSame problem.\r\nMy cluster:\r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-example\r\nspec:\r\n  instances: 3\r\n  imageCatalogRef:\r\n    apiGroup: postgresql.cnpg.io\r\n    kind: ClusterImageCatalog\r\n    name: postgresql\r\n    major: 16\r\n  postgresql:\r\n    synchronous:\r\n      method: first\r\n      number: 1\r\n  resources:\r\n    requests:\r\n      cpu: 100m\r\n      memory: 128Mi\r\n    limits:\r\n      memory: 256Mi\r\n  storage:\r\n    size: 1Gi\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: \"s3://postgres-backup/\"\r\n      endpointURL: \"https://storage.corp.ltd\"\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: s3-creds\r\n          key: ACCESS_KEY_ID\r\n        secretAccessKey:\r\n          name: s3-creds\r\n          key: ACCESS_SECRET_KEY\r\n        region:\r\n          name: s3-creds\r\n          key: REGION\r\n      wal:\r\n        compression: gzip\r\n````\r\n```\r\n$ ls -lah /var/lib/postgresql/data/pgdata/pg_wal/\r\ntotal 65M\r\ndrwxrws---  3 postgres tape 4.0K Oct 18 16:39 .\r\ndrwx------ 20 postgres tape 4.0K Oct 18 16:34 ..\r\n-rw-------  1 postgres tape  16M Oct 18 16:29 00000001000000000000000C\r\n-rw-------  1 postgres tape  16M Oct 18 16:34 00000001000000000000000D\r\n-rw-------  1 postgres tape  16M Oct 18 16:39 00000001000000000000000E\r\n-rw-------  1 postgres tape  16M Oct 18 16:39 00000001000000000000000F\r\ndrwx--S---  2 postgres tape 4.0K Oct 18 16:39 archive_status\r\n$ ls -lah /var/lib/postgresql/data/pgdata/pg_wal/archive_status/\r\ntotal 8.0K\r\ndrwx--S--- 2 postgres tape 4.0K Oct 18 16:39 .\r\ndrwxrws--- 3 postgres tape 4.0K Oct 18 16:39 ..\r\n-rw------- 1 postgres tape    0 Oct 18 16:34 00000001000000000000000D.done\r\n-rw------- 1 postgres tape    0 Oct 18 16:39 00000001000000000000000E.done\r\n```\r\nFiles `00000001000000000000000D.gz` and `00000001000000000000000E.gz` are on S3 bucket. No errors message related WAL archive.\r\nIt works once if the cluster is restarted or promoted. It gets stuck after.\r\n@hei829 have you solved this problem?\n---\nI deleted the primary node (kubectl delete) and WAL archiving worked a few times but then stuck again:\r\n<img width=\"557\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b9ded26c-343b-4bbd-bb52-a6bf5ef8eb23\">\r\nI created a new cluster the same configuration.\r\n<img width=\"556\" alt=\"image\" src=\"https://github.com/user-attachments/assets/2019552d-4dc8-4c22-af95-6886de0ae1eb\">\r\n```\r\n\u276f k exec -t keycloak-cnpg-16-1 -- ls -lah /var/lib/postgresql/data/pgdata/pg_wal\r\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\r\ntotal 97M\r\ndrwxrws---  3 postgres tape 4.0K Oct 19 11:48 .\r\ndrwx------ 20 postgres tape 4.0K Oct 19 11:47 ..\r\n-rw-rw----  1 postgres tape  16M Oct 19 11:46 000000010000000000000001\r\n-rw-rw----  1 postgres tape  16M Oct 19 11:47 000000010000000000000002\r\n-rw-------  1 postgres tape  16M Oct 19 11:47 000000010000000000000003\r\n-rw-------  1 postgres tape  16M Oct 19 11:48 000000010000000000000004\r\n-rw-------  1 postgres tape  16M Oct 19 11:48 000000010000000000000005\r\n-rw-------  1 postgres tape  338 Oct 19 11:48 000000010000000000000005.00000028.backup\r\n-rw-------  1 postgres tape  16M Oct 19 11:53 000000010000000000000006\r\ndrwxrws---  2 postgres tape 4.0K Oct 19 11:53 archive_status\r\n\u276f k exec -t keycloak-cnpg-16-1 -- ls -lah /var/lib/postgresql/data/pgdata/pg_wal/archive_status\r\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\r\ntotal 8.0K\r\ndrwxrws--- 2 postgres tape 4.0K Oct 19 11:53 .\r\ndrwxrws--- 3 postgres tape 4.0K Oct 19 11:48 ..\r\n-rw------- 1 postgres tape    0 Oct 19 11:47 000000010000000000000002.done\r\n-rw------- 1 postgres tape    0 Oct 19 11:47 000000010000000000000003.done\r\n-rw------- 1 postgres tape    0 Oct 19 11:48 000000010000000000000004.done\r\n-rw------- 1 postgres tape    0 Oct 19 11:48 000000010000000000000005.00000028.backup.done\r\n-rw------- 1 postgres tape    0 Oct 19 11:48 000000010000000000000005.done\r\n-rw------- 1 postgres tape    0 Oct 19 11:53 000000010000000000000006.done\r\n```\r\non S3:\r\n<img width=\"779\" alt=\"image\" src=\"https://github.com/user-attachments/assets/7447e620-a2a9-402f-a389-58254f2fa12a\">\r\nlogs:\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.718227121Z\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"version\":\"1.24.1\",\"build\":{\"Version\":\"1.24.1\",\"Commit\":\"3f96930d\",\"Date\":\"2024-10-16\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.718486231Z\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.741470045Z\",\"msg\":\"starting tablespace manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.74155555Z\",\"msg\":\"starting external server manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.741680846Z\",\"msg\":\"starting controller-runtime manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.742003626Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.742067098Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.742640355Z\",\"msg\":\"Starting webserver\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"address\":\":8000\",\"hasTLS\":true}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.743081465Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.743115497Z\",\"logger\":\"roles_reconciler\",\"msg\":\"skipping the RoleSynchronizer in replicas\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.743154118Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.743185589Z\",\"msg\":\"Starting webserver\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"address\":\":9187\",\"hasTLS\":false}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.743251754Z\",\"msg\":\"Starting webserver\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"address\":\"localhost:8010\",\"hasTLS\":false}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.743615083Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.74383712Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.744099999Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.744229065Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.842430873Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.844523657Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.844660755Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.874247031Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"184f1b6f-3dc8-44dd-b073-f2a945d115f5\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"filename\":\"/controller/certificates/server.crt\",\"secret\":\"keycloak-cnpg-16-server\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.87732235Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"184f1b6f-3dc8-44dd-b073-f2a945d115f5\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"filename\":\"/controller/certificates/server.key\",\"secret\":\"keycloak-cnpg-16-server\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.883619476Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"184f1b6f-3dc8-44dd-b073-f2a945d115f5\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"filename\":\"/controller/certificates/streaming_replica.crt\",\"secret\":\"keycloak-cnpg-16-replication\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.891019249Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"184f1b6f-3dc8-44dd-b073-f2a945d115f5\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"filename\":\"/controller/certificates/streaming_replica.key\",\"secret\":\"keycloak-cnpg-16-replication\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.896587152Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"184f1b6f-3dc8-44dd-b073-f2a945d115f5\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"filename\":\"/controller/certificates/client-ca.crt\",\"secret\":\"keycloak-cnpg-16-ca\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.917778652Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"184f1b6f-3dc8-44dd-b073-f2a945d115f5\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"filename\":\"/controller/certificates/server-ca.crt\",\"secret\":\"keycloak-cnpg-16-ca\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.928258803Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"filename\":\"override.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.92982495Z\",\"msg\":\"Extracting pg_controldata information\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"reason\":\"postmaster start up\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.931745936Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7427451823224950804\\nDatabase cluster state:               in production\\npg_control last modified:             Sat 19 Oct 2024 11:48:42 AM UTC\\nLatest checkpoint location:           0/5000060\\nLatest checkpoint's REDO location:    0/5000028\\nLatest checkpoint's REDO WAL file:    000000010000000000000005\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:1231\\nLatest checkpoint's NextOID:          25750\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  1231\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Sat 19 Oct 2024 11:48:36 AM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            75e576f484f6b7cea6c003c2b63decebf744bdc85c0d8f450a071103c65778f0\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"keycloak-cnpg-16-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.93276523Z\",\"msg\":\"postmaster started\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"postMasterPID\":21}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:21.968157612Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"184f1b6f-3dc8-44dd-b073-f2a945d115f5\",\"logging_pod\":\"keycloak-cnpg-16-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.002476584Z\",\"logger\":\"postgres\",\"msg\":\"2024-10-19 11:49:22.002 UTC [21] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"keycloak-cnpg-16-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.002565877Z\",\"logger\":\"postgres\",\"msg\":\"2024-10-19 11:49:22.002 UTC [21] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"keycloak-cnpg-16-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.003140302Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:22.002 UTC\",\"process_id\":\"21\",\"session_id\":\"67139cc1.15\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:49:21 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.003436481Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:22.002 UTC\",\"process_id\":\"21\",\"session_id\":\"67139cc1.15\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-10-19 11:49:21 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 16.4 (Debian 16.4-1.pgdg110+2) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.003552263Z\",\"logger\":\"postgres\",\"msg\":\"2024-10-19 11:49:22.002 UTC [21] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"keycloak-cnpg-16-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.0035791Z\",\"logger\":\"postgres\",\"msg\":\"2024-10-19 11:49:22.002 UTC [21] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"keycloak-cnpg-16-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.003858257Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:22.003 UTC\",\"process_id\":\"21\",\"session_id\":\"67139cc1.15\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-10-19 11:49:21 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.003885746Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:22.003 UTC\",\"process_id\":\"21\",\"session_id\":\"67139cc1.15\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-10-19 11:49:21 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.020963302Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:22.020 UTC\",\"process_id\":\"21\",\"session_id\":\"67139cc1.15\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-10-19 11:49:21 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.045506188Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:22.045 UTC\",\"process_id\":\"25\",\"session_id\":\"67139cc2.19\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:49:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was interrupted; last known up at 2024-10-19 11:48:42 UTC\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.69815972Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:22.697 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"37\",\"connection_from\":\"[local]\",\"session_id\":\"67139cc2.25\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:49:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.699474792Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:22.699 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"38\",\"connection_from\":\"[local]\",\"session_id\":\"67139cc2.26\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:49:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.701273581Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:22.701 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"39\",\"connection_from\":\"[local]\",\"session_id\":\"67139cc2.27\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:49:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.701725775Z\",\"msg\":\"DB not available, will retry\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"bdeaf0c1-6630-4ccf-aa95-d8cc95bbca1b\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.714137358Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:22.713 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"40\",\"connection_from\":\"[local]\",\"session_id\":\"67139cc2.28\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:49:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.769799199Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"walName\":\"00000002.history\",\"options\":[\"--endpoint-url\",\"https://storage.corp.ltd\",\"--cloud-provider\",\"aws-s3\",\"s3://postgres-backup\",\"keycloak-cnpg-16\"],\"startTime\":\"2024-10-19T11:49:22.255961571Z\",\"endTime\":\"2024-10-19T11:49:22.769763581Z\",\"elapsedWalTime\":0.513802017}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.77208838Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:22.771 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"41\",\"connection_from\":\"[local]\",\"session_id\":\"67139cc2.29\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:49:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.874981394Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:22.874 UTC\",\"process_id\":\"25\",\"session_id\":\"67139cc2.19\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-10-19 11:49:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"entering standby mode\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:22.875392061Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:22.875 UTC\",\"process_id\":\"25\",\"session_id\":\"67139cc2.19\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-10-19 11:49:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting backup recovery with redo LSN 0/5000028, checkpoint LSN 0/5000060, on timeline ID 1\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:23.032171594Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:23.031 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"51\",\"connection_from\":\"[local]\",\"session_id\":\"67139cc3.33\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:49:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:23.034495381Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:23.034 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"52\",\"connection_from\":\"[local]\",\"session_id\":\"67139cc3.34\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:49:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:23.034517408Z\",\"msg\":\"DB not available, will retry\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"b2cd1193-8754-45bf-b4a5-fdaa67b2a193\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:23.046699394Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:23.046 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"53\",\"connection_from\":\"[local]\",\"session_id\":\"67139cc3.35\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:49:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:23.711742607Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"walName\":\"000000010000000000000005\",\"startTime\":\"2024-10-19T11:49:23.065691793Z\",\"endTime\":\"2024-10-19T11:49:23.711705115Z\",\"elapsedWalTime\":0.646013404}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:23.711944899Z\",\"logger\":\"wal-restore\",\"msg\":\"Set end-of-wal-stream flag as one of the WAL files to be prefetched was not found\",\"logging_pod\":\"keycloak-cnpg-16-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:23.712050765Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"walName\":\"000000010000000000000005\",\"maxParallel\":2,\"successfulWalRestore\":1,\"failedWalRestore\":1,\"endOfWALStream\":true,\"startTime\":\"2024-10-19T11:49:22.927099007Z\",\"downloadStartTime\":\"2024-10-19T11:49:23.065435454Z\",\"downloadTotalTime\":0.646604887,\"totalTime\":0.78494144}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:23.71755614Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:23.716 UTC\",\"process_id\":\"25\",\"session_id\":\"67139cc2.19\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-10-19 11:49:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"000000010000000000000005\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:23.774597826Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:23.774 UTC\",\"process_id\":\"25\",\"session_id\":\"67139cc2.19\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-10-19 11:49:22 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"redo starts at 0/5000028\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:23.944874913Z\",\"logger\":\"wal-restore\",\"msg\":\"end-of-wal-stream flag found.Exiting with error once to let Postgres try switching to streaming replication\",\"logging_pod\":\"keycloak-cnpg-16-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:23.961250494Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:23.960 UTC\",\"process_id\":\"25\",\"session_id\":\"67139cc2.19\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-10-19 11:49:22 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"completed backup recovery with redo LSN 0/5000028 and end LSN 0/5000100\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:23.961324948Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:23.960 UTC\",\"process_id\":\"25\",\"session_id\":\"67139cc2.19\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-10-19 11:49:22 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"consistent recovery state reached at 0/5000100\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:23.961338747Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:23.960 UTC\",\"process_id\":\"21\",\"session_id\":\"67139cc1.15\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-10-19 11:49:21 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is ready to accept read-only connections\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:24.743776461Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"walName\":\"000000010000000000000006\",\"options\":[\"--endpoint-url\",\"https://storage.corp.ltd\",\"--cloud-provider\",\"aws-s3\",\"s3://postgres-backup\",\"keycloak-cnpg-16\"],\"startTime\":\"2024-10-19T11:49:24.175925786Z\",\"endTime\":\"2024-10-19T11:49:24.743729911Z\",\"elapsedWalTime\":0.567804135}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:49:24.868136323Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:49:24.867 UTC\",\"process_id\":\"94\",\"session_id\":\"67139cc4.5e\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:49:24 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"started streaming WAL from primary at 0/6000000 on timeline 1\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:54:22.143939446Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:54:22.143 UTC\",\"process_id\":\"23\",\"session_id\":\"67139cc2.17\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:49:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restartpoint starting: time\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:54:22.22973607Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:54:22.229 UTC\",\"process_id\":\"23\",\"session_id\":\"67139cc2.17\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-10-19 11:49:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restartpoint complete: wrote 1 buffers (0.0%); 1 WAL file(s) added, 0 removed, 0 recycled; write=0.004 s, sync=0.001 s, total=0.086 s; sync files=0, longest=0.000 s, average=0.000 s; distance=16384 kB, estimate=16384 kB; lsn=0/6000098, redo lsn=0/6000060\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:54:22.229815063Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-3\",\"record\":{\"log_time\":\"2024-10-19 11:54:22.229 UTC\",\"process_id\":\"23\",\"session_id\":\"67139cc2.17\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-10-19 11:49:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"recovery restart point at 0/6000060\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.693417207Z\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"version\":\"1.24.1\",\"build\":{\"Version\":\"1.24.1\",\"Commit\":\"3f96930d\",\"Date\":\"2024-10-16\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.693591662Z\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.719167582Z\",\"msg\":\"starting tablespace manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.719212854Z\",\"msg\":\"starting external server manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.719247537Z\",\"msg\":\"starting controller-runtime manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.719488417Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.719670747Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.72036932Z\",\"msg\":\"Starting webserver\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"address\":\":8000\",\"hasTLS\":true}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.720494322Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.720645458Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.720681202Z\",\"msg\":\"Starting webserver\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"address\":\":9187\",\"hasTLS\":false}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.72070862Z\",\"msg\":\"Starting webserver\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"address\":\"localhost:8010\",\"hasTLS\":false}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.721983008Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.722142382Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.722595587Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.722699326Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.737451993Z\",\"msg\":\"Installed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"filename\":\"pg_ident.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.820633279Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.822712805Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.823259919Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.843028024Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"014aebdd-8722-4d67-b085-575224d0b086\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"filename\":\"/controller/certificates/server.crt\",\"secret\":\"keycloak-cnpg-16-server\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.852031513Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"014aebdd-8722-4d67-b085-575224d0b086\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"filename\":\"/controller/certificates/server.key\",\"secret\":\"keycloak-cnpg-16-server\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.85911936Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"014aebdd-8722-4d67-b085-575224d0b086\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"filename\":\"/controller/certificates/streaming_replica.crt\",\"secret\":\"keycloak-cnpg-16-replication\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.861913593Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"014aebdd-8722-4d67-b085-575224d0b086\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"filename\":\"/controller/certificates/streaming_replica.key\",\"secret\":\"keycloak-cnpg-16-replication\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.86671733Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"014aebdd-8722-4d67-b085-575224d0b086\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"filename\":\"/controller/certificates/client-ca.crt\",\"secret\":\"keycloak-cnpg-16-ca\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.877801706Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"014aebdd-8722-4d67-b085-575224d0b086\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"filename\":\"/controller/certificates/server-ca.crt\",\"secret\":\"keycloak-cnpg-16-ca\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.884069982Z\",\"msg\":\"Installed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"014aebdd-8722-4d67-b085-575224d0b086\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"filename\":\"pg_hba.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.892913712Z\",\"msg\":\"Installed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"014aebdd-8722-4d67-b085-575224d0b086\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"filename\":\"custom.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.898173998Z\",\"msg\":\"Cluster status\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"014aebdd-8722-4d67-b085-575224d0b086\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"currentPrimary\":\"\",\"targetPrimary\":\"keycloak-cnpg-16-1\",\"isReplicaCluster\":false}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.89820556Z\",\"msg\":\"First primary instance bootstrap, marking myself as primary\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"014aebdd-8722-4d67-b085-575224d0b086\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"currentPrimary\":\"\",\"targetPrimary\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.918891142Z\",\"msg\":\"Extracting pg_controldata information\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"reason\":\"postmaster start up\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.922612182Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7427451823224950804\\nDatabase cluster state:               shut down\\npg_control last modified:             Sat 19 Oct 2024 11:46:32 AM UTC\\nLatest checkpoint location:           0/2128690\\nLatest checkpoint's REDO location:    0/2128690\\nLatest checkpoint's REDO WAL file:    000000010000000000000002\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: off\\nLatest checkpoint's NextXID:          0:1218\\nLatest checkpoint's NextOID:          17558\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  0\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Sat 19 Oct 2024 11:46:32 AM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    minimal\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              0\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            75e576f484f6b7cea6c003c2b63decebf744bdc85c0d8f450a071103c65778f0\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.924472289Z\",\"msg\":\"postmaster started\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"postMasterPID\":22}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.934640316Z\",\"msg\":\"DB not available, will retry\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): dial error: dial unix /controller/run/.s.PGSQL.5432: connect: no such file or directory\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.965112777Z\",\"logger\":\"postgres\",\"msg\":\"2024-10-19 11:46:42.964 UTC [22] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.965146038Z\",\"logger\":\"postgres\",\"msg\":\"2024-10-19 11:46:42.964 UTC [22] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.965841448Z\",\"logger\":\"postgres\",\"msg\":\"2024-10-19 11:46:42.965 UTC [22] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.965890721Z\",\"logger\":\"postgres\",\"msg\":\"2024-10-19 11:46:42.965 UTC [22] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.96780207Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"014aebdd-8722-4d67-b085-575224d0b086\",\"logging_pod\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.970550916Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:46:42.965 UTC\",\"process_id\":\"22\",\"session_id\":\"67139c22.16\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.970732015Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:46:42.965 UTC\",\"process_id\":\"22\",\"session_id\":\"67139c22.16\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 16.4 (Debian 16.4-1.pgdg110+2) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.970773125Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:46:42.965 UTC\",\"process_id\":\"22\",\"session_id\":\"67139c22.16\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.970785658Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:46:42.965 UTC\",\"process_id\":\"22\",\"session_id\":\"67139c22.16\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.971381172Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:46:42.971 UTC\",\"process_id\":\"22\",\"session_id\":\"67139c22.16\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:42.987687981Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:46:42.987 UTC\",\"process_id\":\"28\",\"session_id\":\"67139c22.1c\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was shut down at 2024-10-19 11:46:32 UTC\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:43.016780395Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:46:43.016 UTC\",\"process_id\":\"22\",\"session_id\":\"67139c22.16\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is ready to accept connections\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:52.003335935Z\",\"msg\":\"Installed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"4e7904d1-207c-4cb2-b813-a6ec3f22445f\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"filename\":\"custom.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:52.052709894Z\",\"msg\":\"reloading the instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"4e7904d1-207c-4cb2-b813-a6ec3f22445f\",\"logging_pod\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:52.05274308Z\",\"msg\":\"Requesting configuration reload\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"4e7904d1-207c-4cb2-b813-a6ec3f22445f\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:52.055658087Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:46:52.055 UTC\",\"process_id\":\"22\",\"session_id\":\"67139c22.16\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"received SIGHUP, reloading configuration files\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:52.055859548Z\",\"logger\":\"pg_ctl\",\"msg\":\"server signaled\",\"pipe\":\"stdout\",\"logging_pod\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:52.058672904Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:46:52.056 UTC\",\"process_id\":\"22\",\"session_id\":\"67139c22.16\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"parameter \\\"synchronous_standby_names\\\" changed to \\\"FIRST 1 (\\\"keycloak-cnpg-16-2\\\",\\\"keycloak-cnpg-16-1\\\")\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:46:52.062808676Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:46:52.056 UTC\",\"process_id\":\"22\",\"session_id\":\"67139c22.16\",\"session_line_num\":\"9\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"parameter \\\"cnpg.config_sha256\\\" changed to \\\"a112d25e2cd05861d7e8e18ce9a5ed8d659243212140ee1320b7a7d7980055af\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"error\",\"ts\":\"2024-10-19T11:47:12.452501718Z\",\"msg\":\"unable to collect metrics\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"error\":\"not matching synchronous standby names regex: FIRST 1 (\\\"keycloak-cnpg-16-2\\\",\\\"keycloak-cnpg-16-1\\\")\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/machinery/pkg/log.Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectFromPrimarySynchronousStandbysNumber\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:522\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:393\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:304\\ngithub.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1\\n\\tpkg/mod/github.com/prometheus/client_golang@v1.20.4/prometheus/registry.go:456\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:15.725615485Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:47:15.725 UTC\",\"process_id\":\"26\",\"session_id\":\"67139c22.1a\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"checkpoint starting: force wait\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:15.950881689Z\",\"logger\":\"wal-archive\",\"msg\":\"barman-cloud-check-wal-archive checking the first wal\",\"logging_pod\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:16.389191276Z\",\"logger\":\"wal-archive\",\"msg\":\"Executing barman-cloud-wal-archive\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walName\":\"pg_wal/000000010000000000000002\",\"options\":[\"--gzip\",\"--endpoint-url\",\"https://storage.corp.ltd\",\"--cloud-provider\",\"aws-s3\",\"s3://postgres-backup\",\"keycloak-cnpg-16\",\"pg_wal/000000010000000000000002\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.556688359Z\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"version\":\"1.24.1\",\"build\":{\"Version\":\"1.24.1\",\"Commit\":\"3f96930d\",\"Date\":\"2024-10-16\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.556894333Z\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.578172869Z\",\"msg\":\"starting tablespace manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.578233384Z\",\"msg\":\"starting external server manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.578275972Z\",\"msg\":\"starting controller-runtime manager\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.578628366Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.578710196Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.579295069Z\",\"msg\":\"Starting webserver\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"address\":\":8000\",\"hasTLS\":true}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.579798227Z\",\"msg\":\"Starting webserver\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"address\":\":9187\",\"hasTLS\":false}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.579846081Z\",\"msg\":\"Starting webserver\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"address\":\"localhost:8010\",\"hasTLS\":false}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.580420472Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.580446127Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.580566966Z\",\"msg\":\"Starting EventSource\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.580576364Z\",\"msg\":\"Starting Controller\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.580667911Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.580697056Z\",\"logger\":\"roles_reconciler\",\"msg\":\"skipping the RoleSynchronizer in replicas\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.580728444Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.679403968Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:17.441885456Z\",\"logger\":\"wal-archive\",\"msg\":\"Archived WAL file\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walName\":\"pg_wal/000000010000000000000002\",\"startTime\":\"2024-10-19T11:47:16.389171524Z\",\"endTime\":\"2024-10-19T11:47:17.441852921Z\",\"elapsedWalTime\":1.052681501}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:21.762215022Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:47:21.762 UTC\",\"process_id\":\"26\",\"session_id\":\"67139c22.1a\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"checkpoint complete: wrote 61 buffers (0.4%); 0 WAL file(s) added, 0 removed, 0 recycled; write=5.859 s, sync=0.024 s, total=6.037 s; sync files=25, longest=0.013 s, average=0.001 s; distance=15198 kB, estimate=15198 kB; lsn=0/3000060, redo lsn=0/3000028\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:24.851036761Z\",\"logger\":\"wal-archive\",\"msg\":\"Executing barman-cloud-wal-archive\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walName\":\"pg_wal/000000010000000000000003.00000028.backup\",\"options\":[\"--gzip\",\"--endpoint-url\",\"https://storage.corp.ltd\",\"--cloud-provider\",\"aws-s3\",\"s3://postgres-backup\",\"keycloak-cnpg-16\",\"pg_wal/000000010000000000000003.00000028.backup\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:24.851080249Z\",\"logger\":\"wal-archive\",\"msg\":\"Executing barman-cloud-wal-archive\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walName\":\"pg_wal/000000010000000000000003\",\"options\":[\"--gzip\",\"--endpoint-url\",\"https://storage.corp.ltd\",\"--cloud-provider\",\"aws-s3\",\"s3://postgres-backup\",\"keycloak-cnpg-16\",\"pg_wal/000000010000000000000003\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:25.316755433Z\",\"logger\":\"wal-archive\",\"msg\":\"Archived WAL file\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walName\":\"pg_wal/000000010000000000000003.00000028.backup\",\"startTime\":\"2024-10-19T11:47:24.851013367Z\",\"endTime\":\"2024-10-19T11:47:25.316637439Z\",\"elapsedWalTime\":0.465624077}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:25.476747642Z\",\"logger\":\"wal-archive\",\"msg\":\"Archived WAL file\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walName\":\"pg_wal/000000010000000000000003\",\"startTime\":\"2024-10-19T11:47:24.851025865Z\",\"endTime\":\"2024-10-19T11:47:25.476712759Z\",\"elapsedWalTime\":0.625686903}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:25.476822422Z\",\"logger\":\"wal-archive\",\"msg\":\"Completed archive command (parallel)\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walsCount\":2,\"startTime\":\"2024-10-19T11:47:24.724886493Z\",\"uploadStartTime\":\"2024-10-19T11:47:24.850978376Z\",\"uploadTotalTime\":0.625839749,\"totalTime\":0.751931749}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:25.656892322Z\",\"logger\":\"wal-archive\",\"msg\":\"Archived WAL file (parallel)\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walName\":\"pg_wal/000000010000000000000003.00000028.backup\",\"currentPrimary\":\"keycloak-cnpg-16-1\",\"targetPrimary\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:40.743136992Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:47:40.742 UTC\",\"user_name\":\"streaming_replica\",\"process_id\":\"485\",\"connection_from\":\"10.114.146.53:46222\",\"session_id\":\"67139c5c.1e5\",\"session_line_num\":\"1\",\"command_tag\":\"streaming 0/40BA3E8\",\"session_start_time\":\"2024-10-19 11:47:40 UTC\",\"virtual_transaction_id\":\"3/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"standby \\\"keycloak-cnpg-16-2\\\" is now a synchronous standby with priority 1\",\"query\":\"START_REPLICATION SLOT \\\"_cnpg_keycloak_cnpg_16_2\\\" 0/4000000 TIMELINE 1\",\"application_name\":\"keycloak-cnpg-16-2\",\"backend_type\":\"walsender\",\"query_id\":\"0\"}}\r\n{\"level\":\"error\",\"ts\":\"2024-10-19T11:47:42.317340478Z\",\"msg\":\"unable to collect metrics\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"error\":\"not matching synchronous standby names regex: FIRST 1 (\\\"keycloak-cnpg-16-2\\\",\\\"keycloak-cnpg-16-1\\\")\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/machinery/pkg/log.Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectFromPrimarySynchronousStandbysNumber\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:522\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:393\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:304\\ngithub.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1\\n\\tpkg/mod/github.com/prometheus/client_golang@v1.20.4/prometheus/registry.go:456\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:46.451452381Z\",\"msg\":\"Installed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"41a7f557-8d39-4bb9-8aa1-d851d39e96c4\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"filename\":\"custom.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:46.493908656Z\",\"msg\":\"reloading the instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"41a7f557-8d39-4bb9-8aa1-d851d39e96c4\",\"logging_pod\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:46.493940787Z\",\"msg\":\"Requesting configuration reload\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"41a7f557-8d39-4bb9-8aa1-d851d39e96c4\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:46.495147635Z\",\"logger\":\"pg_ctl\",\"msg\":\"server signaled\",\"pipe\":\"stdout\",\"logging_pod\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:46.495270619Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:47:46.495 UTC\",\"process_id\":\"22\",\"session_id\":\"67139c22.16\",\"session_line_num\":\"10\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"received SIGHUP, reloading configuration files\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:46.49624293Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:47:46.496 UTC\",\"process_id\":\"22\",\"session_id\":\"67139c22.16\",\"session_line_num\":\"11\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"parameter \\\"synchronous_standby_names\\\" changed to \\\"FIRST 1 (\\\"keycloak-cnpg-16-2\\\",\\\"keycloak-cnpg-16-3\\\",\\\"keycloak-cnpg-16-1\\\")\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:46.496284145Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:47:46.496 UTC\",\"process_id\":\"22\",\"session_id\":\"67139c22.16\",\"session_line_num\":\"12\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"parameter \\\"cnpg.config_sha256\\\" changed to \\\"8bf7919287f1510f377bf41550981d9384650fe5b1999bb38d95e81915530874\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"error\",\"ts\":\"2024-10-19T11:48:12.357509619Z\",\"msg\":\"unable to collect metrics\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"error\":\"not matching synchronous standby names regex: FIRST 1 (\\\"keycloak-cnpg-16-2\\\",\\\"keycloak-cnpg-16-3\\\",\\\"keycloak-cnpg-16-1\\\")\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/machinery/pkg/log.Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectFromPrimarySynchronousStandbysNumber\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:522\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:393\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:304\\ngithub.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1\\n\\tpkg/mod/github.com/prometheus/client_golang@v1.20.4/prometheus/registry.go:456\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:48:36.806383631Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:48:36.805 UTC\",\"process_id\":\"26\",\"session_id\":\"67139c22.1a\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"checkpoint starting: force wait\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:48:36.995657545Z\",\"logger\":\"wal-archive\",\"msg\":\"Executing barman-cloud-wal-archive\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walName\":\"pg_wal/000000010000000000000004\",\"options\":[\"--gzip\",\"--endpoint-url\",\"https://storage.corp.ltd\",\"--cloud-provider\",\"aws-s3\",\"s3://postgres-backup\",\"keycloak-cnpg-16\",\"pg_wal/000000010000000000000004\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:48:37.903231325Z\",\"logger\":\"wal-archive\",\"msg\":\"Archived WAL file\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walName\":\"pg_wal/000000010000000000000004\",\"startTime\":\"2024-10-19T11:48:36.995634743Z\",\"endTime\":\"2024-10-19T11:48:37.903182236Z\",\"elapsedWalTime\":0.907547518}\r\n{\"level\":\"error\",\"ts\":\"2024-10-19T11:48:42.318159161Z\",\"msg\":\"unable to collect metrics\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"error\":\"not matching synchronous standby names regex: FIRST 1 (\\\"keycloak-cnpg-16-2\\\",\\\"keycloak-cnpg-16-3\\\",\\\"keycloak-cnpg-16-1\\\")\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/machinery/pkg/log.Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectFromPrimarySynchronousStandbysNumber\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:522\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:393\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:304\\ngithub.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1\\n\\tpkg/mod/github.com/prometheus/client_golang@v1.20.4/prometheus/registry.go:456\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:48:42.533248805Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:48:42.532 UTC\",\"process_id\":\"26\",\"session_id\":\"67139c22.1a\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"checkpoint complete: wrote 56 buffers (0.3%); 0 WAL file(s) added, 0 removed, 0 recycled; write=5.619 s, sync=0.015 s, total=5.728 s; sync files=15, longest=0.012 s, average=0.001 s; distance=32768 kB, estimate=32768 kB; lsn=0/5000060, redo lsn=0/5000028\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:48:43.102689569Z\",\"logger\":\"wal-archive\",\"msg\":\"Executing barman-cloud-wal-archive\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walName\":\"pg_wal/000000010000000000000005\",\"options\":[\"--gzip\",\"--endpoint-url\",\"https://storage.corp.ltd\",\"--cloud-provider\",\"aws-s3\",\"s3://postgres-backup\",\"keycloak-cnpg-16\",\"pg_wal/000000010000000000000005\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:48:43.102664807Z\",\"logger\":\"wal-archive\",\"msg\":\"Executing barman-cloud-wal-archive\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walName\":\"pg_wal/000000010000000000000005.00000028.backup\",\"options\":[\"--gzip\",\"--endpoint-url\",\"https://storage.corp.ltd\",\"--cloud-provider\",\"aws-s3\",\"s3://postgres-backup\",\"keycloak-cnpg-16\",\"pg_wal/000000010000000000000005.00000028.backup\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:48:43.720507946Z\",\"logger\":\"wal-archive\",\"msg\":\"Archived WAL file\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walName\":\"pg_wal/000000010000000000000005.00000028.backup\",\"startTime\":\"2024-10-19T11:48:43.10263701Z\",\"endTime\":\"2024-10-19T11:48:43.715982521Z\",\"elapsedWalTime\":0.613345513}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:48:43.898024409Z\",\"logger\":\"wal-archive\",\"msg\":\"Archived WAL file\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walName\":\"pg_wal/000000010000000000000005\",\"startTime\":\"2024-10-19T11:48:43.102641248Z\",\"endTime\":\"2024-10-19T11:48:43.897983273Z\",\"elapsedWalTime\":0.795342039}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:48:43.898112651Z\",\"logger\":\"wal-archive\",\"msg\":\"Completed archive command (parallel)\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walsCount\":2,\"startTime\":\"2024-10-19T11:48:42.94496484Z\",\"uploadStartTime\":\"2024-10-19T11:48:43.102585838Z\",\"uploadTotalTime\":0.795518578,\"totalTime\":0.953139704}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:48:44.099939402Z\",\"logger\":\"wal-archive\",\"msg\":\"Archived WAL file (parallel)\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walName\":\"pg_wal/000000010000000000000005.00000028.backup\",\"currentPrimary\":\"keycloak-cnpg-16-1\",\"targetPrimary\":\"keycloak-cnpg-16-1\"}\r\n{\"level\":\"error\",\"ts\":\"2024-10-19T11:49:12.316362274Z\",\"msg\":\"unable to collect metrics\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"error\":\"not matching synchronous standby names regex: FIRST 1 (\\\"keycloak-cnpg-16-2\\\",\\\"keycloak-cnpg-16-3\\\",\\\"keycloak-cnpg-16-1\\\")\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/machinery/pkg/log.Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectFromPrimarySynchronousStandbysNumber\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:522\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:393\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:304\\ngithub.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1\\n\\tpkg/mod/github.com/prometheus/client_golang@v1.20.4/prometheus/registry.go:456\"}\r\n{\"level\":\"error\",\"ts\":\"2024-10-19T11:53:12.358782149Z\",\"msg\":\"unable to collect metrics\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"error\":\"not matching synchronous standby names regex: FIRST 1 (\\\"keycloak-cnpg-16-2\\\",\\\"keycloak-cnpg-16-3\\\",\\\"keycloak-cnpg-16-1\\\")\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/machinery/pkg/log.Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectFromPrimarySynchronousStandbysNumber\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:522\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:393\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:304\\ngithub.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1\\n\\tpkg/mod/github.com/prometheus/client_golang@v1.20.4/prometheus/registry.go:456\"}\r\n..... the same\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:53:36.730643916Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:53:36.730 UTC\",\"process_id\":\"26\",\"session_id\":\"67139c22.1a\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"checkpoint starting: time\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:53:36.765249774Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"record\":{\"log_time\":\"2024-10-19 11:53:36.765 UTC\",\"process_id\":\"26\",\"session_id\":\"67139c22.1a\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-10-19 11:46:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"checkpoint complete: wrote 0 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.001 s, sync=0.001 s, total=0.035 s; sync files=0, longest=0.000 s, average=0.000 s; distance=16384 kB, estimate=31129 kB; lsn=0/6000098, redo lsn=0/6000060\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\n{\"level\":\"error\",\"ts\":\"2024-10-19T11:53:42.324375464Z\",\"msg\":\"unable to collect metrics\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"error\":\"not matching synchronous standby names regex: FIRST 1 (\\\"keycloak-cnpg-16-2\\\",\\\"keycloak-cnpg-16-3\\\",\\\"keycloak-cnpg-16-1\\\")\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/machinery/pkg/log.Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectFromPrimarySynchronousStandbysNumber\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:522\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:393\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:304\\ngithub.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1\\n\\tpkg/mod/github.com/prometheus/client_golang@v1.20.4/prometheus/registry.go:456\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:53:43.063524633Z\",\"logger\":\"wal-archive\",\"msg\":\"Executing barman-cloud-wal-archive\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walName\":\"pg_wal/000000010000000000000006\",\"options\":[\"--gzip\",\"--endpoint-url\",\"https://storage.corp.ltd\",\"--cloud-provider\",\"aws-s3\",\"s3://postgres-backup\",\"keycloak-cnpg-16\",\"pg_wal/000000010000000000000006\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:53:43.716502482Z\",\"logger\":\"wal-archive\",\"msg\":\"Archived WAL file\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"walName\":\"pg_wal/000000010000000000000006\",\"startTime\":\"2024-10-19T11:53:43.063506954Z\",\"endTime\":\"2024-10-19T11:53:43.716463566Z\",\"elapsedWalTime\":0.652956648}\r\n{\"level\":\"error\",\"ts\":\"2024-10-19T11:54:12.361142121Z\",\"msg\":\"unable to collect metrics\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"error\":\"not matching synchronous standby names regex: FIRST 1 (\\\"keycloak-cnpg-16-2\\\",\\\"keycloak-cnpg-16-3\\\",\\\"keycloak-cnpg-16-1\\\")\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/machinery/pkg/log.Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectFromPrimarySynchronousStandbysNumber\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:522\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:393\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:304\\ngithub.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1\\n\\tpkg/mod/github.com/prometheus/client_golang@v1.20.4/prometheus/registry.go:456\"}\r\n{\"level\":\"error\",\"ts\":\"2024-10-19T11:54:42.324293588Z\",\"msg\":\"unable to collect metrics\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"error\":\"not matching synchronous standby names regex: FIRST 1 (\\\"keycloak-cnpg-16-2\\\",\\\"keycloak-cnpg-16-3\\\",\\\"keycloak-cnpg-16-1\\\")\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/machinery/pkg/log.Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectFromPrimarySynchronousStandbysNumber\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:522\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:393\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:304\\ngithub.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1\\n\\tpkg/mod/github.com/prometheus/client_golang@v1.20.4/prometheus/registry.go:456\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.680699458Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-external-server\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.680855874Z\",\"msg\":\"Starting workers\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-tablespaces\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.707122945Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"54a24be8-9b43-4b5c-bc91-9a66dc778cf4\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"filename\":\"/controller/certificates/server.crt\",\"secret\":\"keycloak-cnpg-16-server\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.709391874Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"54a24be8-9b43-4b5c-bc91-9a66dc778cf4\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"filename\":\"/controller/certificates/server.key\",\"secret\":\"keycloak-cnpg-16-server\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.749951827Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"54a24be8-9b43-4b5c-bc91-9a66dc778cf4\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"filename\":\"/controller/certificates/streaming_replica.crt\",\"secret\":\"keycloak-cnpg-16-replication\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.773129655Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"54a24be8-9b43-4b5c-bc91-9a66dc778cf4\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"filename\":\"/controller/certificates/streaming_replica.key\",\"secret\":\"keycloak-cnpg-16-replication\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.78663499Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"54a24be8-9b43-4b5c-bc91-9a66dc778cf4\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"filename\":\"/controller/certificates/client-ca.crt\",\"secret\":\"keycloak-cnpg-16-ca\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.80103945Z\",\"msg\":\"Refreshed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"54a24be8-9b43-4b5c-bc91-9a66dc778cf4\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"filename\":\"/controller/certificates/server-ca.crt\",\"secret\":\"keycloak-cnpg-16-ca\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.810800709Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"filename\":\"override.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.812499441Z\",\"msg\":\"Extracting pg_controldata information\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"reason\":\"postmaster start up\"}\r\n{\"level\":\"error\",\"ts\":\"2024-10-19T11:55:12.37819657Z\",\"msg\":\"unable to collect metrics\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"error\":\"not matching synchronous standby names regex: FIRST 1 (\\\"keycloak-cnpg-16-2\\\",\\\"keycloak-cnpg-16-3\\\",\\\"keycloak-cnpg-16-1\\\")\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/machinery/pkg/log.Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectFromPrimarySynchronousStandbysNumber\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:522\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:393\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:304\\ngithub.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1\\n\\tpkg/mod/github.com/prometheus/client_golang@v1.20.4/prometheus/registry.go:456\"}\r\n..... the same\r\n{\"level\":\"error\",\"ts\":\"2024-10-19T12:06:42.329652686Z\",\"msg\":\"unable to collect metrics\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"error\":\"not matching synchronous standby names regex: FIRST 1 (\\\"keycloak-cnpg-16-2\\\",\\\"keycloak-cnpg-16-3\\\",\\\"keycloak-cnpg-16-1\\\")\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/machinery/pkg/log.Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectFromPrimarySynchronousStandbysNumber\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:522\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:393\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:304\\ngithub.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1\\n\\tpkg/mod/github.com/prometheus/client_golang@v1.20.4/prometheus/registry.go:456\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.814295027Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7427451823224950804\\nDatabase cluster state:               in production\\npg_control last modified:             Sat 19 Oct 2024 11:47:21 AM UTC\\nLatest checkpoint location:           0/3000060\\nLatest checkpoint's REDO location:    0/3000028\\nLatest checkpoint's REDO WAL file:    000000010000000000000003\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:1231\\nLatest checkpoint's NextOID:          25750\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  1231\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Sat 19 Oct 2024 11:47:15 AM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            75e576f484f6b7cea6c003c2b63decebf744bdc85c0d8f450a071103c65778f0\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"keycloak-cnpg-16-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.815095494Z\",\"msg\":\"postmaster started\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"postMasterPID\":23}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.850815255Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"54a24be8-9b43-4b5c-bc91-9a66dc778cf4\",\"logging_pod\":\"keycloak-cnpg-16-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.871997157Z\",\"logger\":\"postgres\",\"msg\":\"2024-10-19 11:47:37.871 UTC [23] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"keycloak-cnpg-16-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.872111074Z\",\"logger\":\"postgres\",\"msg\":\"2024-10-19 11:47:37.871 UTC [23] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"keycloak-cnpg-16-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.872693441Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:37.871 UTC\",\"process_id\":\"23\",\"session_id\":\"67139c59.17\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.87292376Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:37.871 UTC\",\"process_id\":\"23\",\"session_id\":\"67139c59.17\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 16.4 (Debian 16.4-1.pgdg110+2) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.872941704Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:37.872 UTC\",\"process_id\":\"23\",\"session_id\":\"67139c59.17\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.872960365Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:37.872 UTC\",\"process_id\":\"23\",\"session_id\":\"67139c59.17\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.872996403Z\",\"logger\":\"postgres\",\"msg\":\"2024-10-19 11:47:37.871 UTC [23] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"keycloak-cnpg-16-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.87301798Z\",\"logger\":\"postgres\",\"msg\":\"2024-10-19 11:47:37.871 UTC [23] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"keycloak-cnpg-16-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.902234279Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:37.902 UTC\",\"process_id\":\"23\",\"session_id\":\"67139c59.17\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:37.910301355Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:37.910 UTC\",\"process_id\":\"27\",\"session_id\":\"67139c59.1b\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was interrupted; last known up at 2024-10-19 11:47:21 UTC\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:38.53255728Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:38.532 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"38\",\"connection_from\":\"[local]\",\"session_id\":\"67139c5a.26\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:47:38 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:38.546740721Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:38.546 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"39\",\"connection_from\":\"[local]\",\"session_id\":\"67139c5a.27\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:47:38 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:38.580851032Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:38.580 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"40\",\"connection_from\":\"[local]\",\"session_id\":\"67139c5a.28\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:47:38 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:38.583589544Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:38.583 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"41\",\"connection_from\":\"[local]\",\"session_id\":\"67139c5a.29\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:47:38 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:38.583729806Z\",\"msg\":\"DB not available, will retry\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"f155262a-1b0f-4b33-81f0-e0da913522ba\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:38.613766818Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:38.613 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"42\",\"connection_from\":\"[local]\",\"session_id\":\"67139c5a.2a\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:47:38 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:38.650606751Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"walName\":\"00000002.history\",\"options\":[\"--endpoint-url\",\"https://storage.corp.ltd\",\"--cloud-provider\",\"aws-s3\",\"s3://postgres-backup\",\"keycloak-cnpg-16\"],\"startTime\":\"2024-10-19T11:47:38.104539556Z\",\"endTime\":\"2024-10-19T11:47:38.650574962Z\",\"elapsedWalTime\":0.546035401}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:38.754352Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:38.754 UTC\",\"process_id\":\"27\",\"session_id\":\"67139c59.1b\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"entering standby mode\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:38.755451074Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:38.755 UTC\",\"process_id\":\"27\",\"session_id\":\"67139c59.1b\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting backup recovery with redo LSN 0/3000028, checkpoint LSN 0/3000060, on timeline ID 1\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:38.885957218Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:38.885 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"53\",\"connection_from\":\"[local]\",\"session_id\":\"67139c5a.35\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:47:38 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:38.924255296Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:38.924 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"54\",\"connection_from\":\"[local]\",\"session_id\":\"67139c5a.36\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:47:38 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:38.926670575Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:38.926 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"55\",\"connection_from\":\"[local]\",\"session_id\":\"67139c5a.37\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:47:38 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:38.926728296Z\",\"msg\":\"DB not available, will retry\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"40325f14-7c32-4d3e-82c4-98d01127eedd\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:39.57679288Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"walName\":\"000000010000000000000003\",\"startTime\":\"2024-10-19T11:47:38.939291054Z\",\"endTime\":\"2024-10-19T11:47:39.576756096Z\",\"elapsedWalTime\":0.637465108}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:39.577015933Z\",\"logger\":\"wal-restore\",\"msg\":\"Set end-of-wal-stream flag as one of the WAL files to be prefetched was not found\",\"logging_pod\":\"keycloak-cnpg-16-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:39.577112823Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"walName\":\"000000010000000000000003\",\"maxParallel\":2,\"successfulWalRestore\":1,\"failedWalRestore\":1,\"endOfWALStream\":true,\"startTime\":\"2024-10-19T11:47:38.791506822Z\",\"downloadStartTime\":\"2024-10-19T11:47:38.939249754Z\",\"downloadTotalTime\":0.637855691,\"totalTime\":0.785598712}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:39.581353824Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:39.581 UTC\",\"process_id\":\"27\",\"session_id\":\"67139c59.1b\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"000000010000000000000003\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:39.639399492Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:39.639 UTC\",\"process_id\":\"27\",\"session_id\":\"67139c59.1b\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"redo starts at 0/3000028\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:39.807305631Z\",\"logger\":\"wal-restore\",\"msg\":\"end-of-wal-stream flag found.Exiting with error once to let Postgres try switching to streaming replication\",\"logging_pod\":\"keycloak-cnpg-16-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:39.814451766Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:39.814 UTC\",\"process_id\":\"27\",\"session_id\":\"67139c59.1b\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"completed backup recovery with redo LSN 0/3000028 and end LSN 0/3000100\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"error\",\"ts\":\"2024-10-19T12:07:12.322178594Z\",\"msg\":\"unable to collect metrics\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"error\":\"not matching synchronous standby names regex: FIRST 1 (\\\"keycloak-cnpg-16-2\\\",\\\"keycloak-cnpg-16-3\\\",\\\"keycloak-cnpg-16-1\\\")\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/machinery/pkg/log.Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectFromPrimarySynchronousStandbysNumber\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:522\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:393\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:304\\ngithub.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1\\n\\tpkg/mod/github.com/prometheus/client_golang@v1.20.4/prometheus/registry.go:456\"}\r\n..... the same\r\n{\"level\":\"error\",\"ts\":\"2024-10-19T12:13:12.324087491Z\",\"msg\":\"unable to collect metrics\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"error\":\"not matching synchronous standby names regex: FIRST 1 (\\\"keycloak-cnpg-16-2\\\",\\\"keycloak-cnpg-16-3\\\",\\\"keycloak-cnpg-16-1\\\")\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/machinery/pkg/log.Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectFromPrimarySynchronousStandbysNumber\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:522\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:393\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:304\\ngithub.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1\\n\\tpkg/mod/github.com/prometheus/client_golang@v1.20.4/prometheus/registry.go:456\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:39.814552303Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:39.814 UTC\",\"process_id\":\"27\",\"session_id\":\"67139c59.1b\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"consistent recovery state reached at 0/3000100\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:39.814567768Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:39.814 UTC\",\"process_id\":\"23\",\"session_id\":\"67139c59.17\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is ready to accept read-only connections\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:40.584004112Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"walName\":\"000000010000000000000004\",\"options\":[\"--endpoint-url\",\"https://storage.corp.ltd\",\"--cloud-provider\",\"aws-s3\",\"s3://postgres-backup\",\"keycloak-cnpg-16\"],\"startTime\":\"2024-10-19T11:47:39.985348757Z\",\"endTime\":\"2024-10-19T11:47:40.583957913Z\",\"elapsedWalTime\":0.598609082}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:40.706792236Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:40.706 UTC\",\"process_id\":\"95\",\"session_id\":\"67139c5c.5f\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:47:40 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"started streaming WAL from primary at 0/4000000 on timeline 1\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:46.296239039Z\",\"msg\":\"Installed configuration file\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"0eb2cca4-b518-4602-ac91-7c76888def49\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"filename\":\"custom.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:46.341173667Z\",\"msg\":\"reloading the instance\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"0eb2cca4-b518-4602-ac91-7c76888def49\",\"logging_pod\":\"keycloak-cnpg-16-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:46.341210616Z\",\"msg\":\"Requesting configuration reload\",\"logger\":\"instance-manager\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"keycloak-cnpg-16\",\"namespace\":\"keycloak\"},\"namespace\":\"keycloak\",\"name\":\"keycloak-cnpg-16\",\"reconcileID\":\"0eb2cca4-b518-4602-ac91-7c76888def49\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:46.342588864Z\",\"logger\":\"pg_ctl\",\"msg\":\"server signaled\",\"pipe\":\"stdout\",\"logging_pod\":\"keycloak-cnpg-16-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:46.342583201Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:46.342 UTC\",\"process_id\":\"23\",\"session_id\":\"67139c59.17\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"received SIGHUP, reloading configuration files\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:46.343779293Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:46.343 UTC\",\"process_id\":\"23\",\"session_id\":\"67139c59.17\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"parameter \\\"synchronous_standby_names\\\" changed to \\\"FIRST 1 (\\\"keycloak-cnpg-16-2\\\",\\\"keycloak-cnpg-16-3\\\",\\\"keycloak-cnpg-16-1\\\")\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:47:46.345048252Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:47:46.343 UTC\",\"process_id\":\"23\",\"session_id\":\"67139c59.17\",\"session_line_num\":\"9\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"parameter \\\"cnpg.config_sha256\\\" changed to \\\"8bf7919287f1510f377bf41550981d9384650fe5b1999bb38d95e81915530874\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:52:37.445260012Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:52:37.445 UTC\",\"process_id\":\"25\",\"session_id\":\"67139c59.19\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restartpoint starting: time\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:52:50.727898601Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:52:50.727 UTC\",\"process_id\":\"25\",\"session_id\":\"67139c59.19\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restartpoint complete: wrote 133 buffers (0.8%); 0 WAL file(s) added, 0 removed, 0 recycled; write=13.238 s, sync=0.005 s, total=13.283 s; sync files=15, longest=0.005 s, average=0.001 s; distance=32768 kB, estimate=32768 kB; lsn=0/5000060, redo lsn=0/5000028\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:52:50.727966141Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:52:50.727 UTC\",\"process_id\":\"25\",\"session_id\":\"67139c59.19\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"recovery restart point at 0/5000028\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:57:37.827030341Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:57:37.826 UTC\",\"process_id\":\"25\",\"session_id\":\"67139c59.19\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restartpoint starting: time\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:57:37.907969019Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:57:37.907 UTC\",\"process_id\":\"25\",\"session_id\":\"67139c59.19\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restartpoint complete: wrote 0 buffers (0.0%); 1 WAL file(s) added, 0 removed, 0 recycled; write=0.001 s, sync=0.001 s, total=0.081 s; sync files=0, longest=0.000 s, average=0.000 s; distance=16384 kB, estimate=31129 kB; lsn=0/6000098, redo lsn=0/6000060\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-10-19T11:57:37.908032408Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"keycloak-cnpg-16-2\",\"record\":{\"log_time\":\"2024-10-19 11:57:37.907 UTC\",\"process_id\":\"25\",\"session_id\":\"67139c59.19\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-10-19 11:47:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"recovery restart point at 0/6000060\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\n{\"level\":\"error\",\"ts\":\"2024-10-19T12:13:42.322157123Z\",\"msg\":\"unable to collect metrics\",\"logging_pod\":\"keycloak-cnpg-16-1\",\"error\":\"not matching synchronous standby names regex: FIRST 1 (\\\"keycloak-cnpg-16-2\\\",\\\"keycloak-cnpg-16-3\\\",\\\"keycloak-cnpg-16-1\\\")\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/machinery/pkg/log.Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectFromPrimarySynchronousStandbysNumber\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:522\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:393\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect\\n\\tpkg/management/postgres/webserver/metricserver/pg_collector.go:304\\ngithub.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1\\n\\tpkg/mod/github.com/prometheus/client_golang@v1.20.4/prometheus/registry.go:456\"}\r\n..... the same\r\n```\r\n```\r\nCluster Summary\r\nName                 keycloak/keycloak-cnpg-16\r\nSystem ID:           7427451823224950804\r\nPostgreSQL Image:    ghcr.io/cloudnative-pg/postgresql:16.4-40-bullseye@sha256:98be3d8f8fa936239aaa55fe5d62bb94d7eae82bf97c58eb26bae321186a8f85\r\nPrimary instance:    keycloak-cnpg-16-1\r\nPrimary start time:  2024-10-19 11:46:43 +0000 UTC (uptime 56m59s)\r\nStatus:              Cluster in healthy state \r\nInstances:           3\r\nReady instances:     3\r\nSize:                131M\r\nCurrent Write LSN:   0/7000000 (Timeline: 1 - WAL File: 000000010000000000000006)\r\nContinuous Backup status\r\nFirst Point of Recoverability:  Not Available\r\nWorking WAL archiving:          OK\r\nWALs waiting to be archived:    0\r\nLast Archived WAL:              000000010000000000000006   @   2024-10-19T11:53:43.720193Z\r\nLast Failed WAL:                -\r\nPhysical backups\r\nName  Phase  Started at  Total  Transferred  Progress  Tablespaces\r\n----  -----  ----------  -----  -----------  --------  -----------\r\nStreaming Replication status\r\nReplication Slots Enabled\r\nName                Sent LSN   Write LSN  Flush LSN  Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority  Replication Slot\r\n----                --------   ---------  ---------  ----------  ---------  ---------  ----------  -----      ----------  -------------  ----------------\r\nkeycloak-cnpg-16-3  0/7000000  0/7000000  0/7000000  0/7000000   00:00:00   00:00:00   00:00:00    streaming  potential   2              active\r\nkeycloak-cnpg-16-2  0/7000000  0/7000000  0/7000000  0/7000000   00:00:00   00:00:00   00:00:00    streaming  sync        1              active\r\nInstances status\r\nName                Current LSN  Replication role          Status  QoS        Manager Version  Node\r\n----                -----------  ----------------          ------  ---        ---------------  ----\r\nkeycloak-cnpg-16-1  0/7000000    Primary                   OK      Burstable  1.24.1           cl1p2b80sql1vabpq8dm-ulis\r\nkeycloak-cnpg-16-2  0/7000000    Standby (sync)            OK      Burstable  1.24.1           cl1lf5cvtp8varihtgn7-ezuk\r\nkeycloak-cnpg-16-3  0/7000000    Standby (potential sync)  OK      Burstable  1.24.1           cl1p2b80sql1vabpq8dm-afym\r\n```\n---\nFrom what I can see, everything looks OK. It seems like no write operations are happening on the primary and, as a result, Postgres is not archiving. This is normal behaviour for Postgres. What would you expect instead? That, even without write traffic, the WAL file is switched and archived as empty?\n---\nIt's my mistake. You are totally right!"
    },
    {
        "title": "[Feature]: Connection pooler as a sidecar instead of Deployment+Service",
        "id": 2570804889,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nHello. The current implementation of the connection pooler could be more optimal and efficient.\r\nCurrently, it's deployed using Deployment+Service:\r\n<img width=\"443\" alt=\"Screenshot 2024-10-07 at 18 23 30\" src=\"https://github.com/user-attachments/assets/f2bb6a58-4a92-4305-a31a-35c2d929097b\">\r\nBut there are problems:\r\n- The more pooler instances we have, the more postgres connections between the pooler and the postgres are used. We can't reuse these connections. The efficiency is even worse when multiple postgres users are used.\r\n- For clusters with read replicas, we need two pooler deployments in total: three instances of the pooler for the primary and three instances of the pooler for the read replicas.\r\n- There could be extra network hops between the pooler and postgres\r\n- There could be extra network hops between the applications and the pooler\r\nOf course, we can use `spec.internalTrafficPolicy: Local`, but CNI also has an unnecessary network overhead. Also, this could complicate debugging if there are connectivity issues between Postgres and the connection pooler.\r\n### Describe the solution you'd like\r\nWe could deploy connection poolers using sidecar containers as [it's implemented in the StackGres](https://stackgres.io/doc/1.6/administration/configuration/pool/). It's much more efficient in reusing pg connections and connectivity because it connects to postgres via localhost. Moreover, it also simplifies connecting to the read-only replicas. We just don't need extra deployments and services, just add an additional port to current r/ro/rw services for the connection pooler.\r\n<img width=\"401\" alt=\"Screenshot 2024-10-07 at 18 25 20\" src=\"https://github.com/user-attachments/assets/97e11e5f-317b-413a-9612-b549053e52b5\">\r\nIt's also worth noting that Yandex Odyssey is much more efficient than pgbouncer since it supports multi-threading, so we don't need to care about horizontal scaling of the connection pooler. We have used it in production for years, and it's rock solid.\r\nhttps://github.com/cloudnative-pg/cloudnative-pg/issues/718\r\n### Describe alternatives you've considered\r\nThere are no alternatives to do that.\r\nI thought I could implement it by myself, but CNPG doesn't have the ability to add sidecar containers: https://github.com/cloudnative-pg/cloudnative-pg/issues/4439\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nHello. The current implementation of the connection pooler could be more optimal and efficient.\r\nCurrently, it's deployed using Deployment+Service:\r\n<img width=\"443\" alt=\"Screenshot 2024-10-07 at 18 23 30\" src=\"https://github.com/user-attachments/assets/f2bb6a58-4a92-4305-a31a-35c2d929097b\">\r\nBut there are problems:\r\n- The more pooler instances we have, the more postgres connections between the pooler and the postgres are used. We can't reuse these connections. The efficiency is even worse when multiple postgres users are used.\r\n- For clusters with read replicas, we need two pooler deployments in total: three instances of the pooler for the primary and three instances of the pooler for the read replicas.\r\n- There could be extra network hops between the pooler and postgres\r\n- There could be extra network hops between the applications and the pooler\r\nOf course, we can use `spec.internalTrafficPolicy: Local`, but CNI also has an unnecessary network overhead. Also, this could complicate debugging if there are connectivity issues between Postgres and the connection pooler.\r\n### Describe the solution you'd like\r\nWe could deploy connection poolers using sidecar containers as [it's implemented in the StackGres](https://stackgres.io/doc/1.6/administration/configuration/pool/). It's much more efficient in reusing pg connections and connectivity because it connects to postgres via localhost. Moreover, it also simplifies connecting to the read-only replicas. We just don't need extra deployments and services, just add an additional port to current r/ro/rw services for the connection pooler.\r\n<img width=\"401\" alt=\"Screenshot 2024-10-07 at 18 25 20\" src=\"https://github.com/user-attachments/assets/97e11e5f-317b-413a-9612-b549053e52b5\">\r\nIt's also worth noting that Yandex Odyssey is much more efficient than pgbouncer since it supports multi-threading, so we don't need to care about horizontal scaling of the connection pooler. We have used it in production for years, and it's rock solid.\r\nhttps://github.com/cloudnative-pg/cloudnative-pg/issues/718\r\n### Describe alternatives you've considered\r\nThere are no alternatives to do that.\r\nI thought I could implement it by myself, but CNPG doesn't have the ability to add sidecar containers: https://github.com/cloudnative-pg/cloudnative-pg/issues/4439\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductFYI - We are working on a plugin's interface at the moment (CNPG-I). Although we are currently focusing on backups and recovery, this could be a use case. But ... we are now swamped on this task until completed."
    },
    {
        "title": "[Bug]: replica cluster does not ensure that override.conf is loaded",
        "id": 2570365514,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\nolder in 1.23.x\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhile setting up a replica cluster from an older version of cnpg (1.19), WAL file syncing fails after succesfully running pg_basebackup because the override.conf file is not loaded. `override.conf` contains the `primary_conninfo` option that specifies the connection to the primary database.\r\nThis is probably happening, because `pg_basebackup` syncs the `postgres.conf` file from the primary DB that does not include an `include override.conf` line.\r\nAdding the line `include override.conf` to `postgres.conf` fixes the problem and after running a `SELECT pg_reload_conf();` in the database, replication starts. But thsi will be lost whenever postgres.conf is rewritten.\r\nThis will also affect anyone who tries to replicate from a DB that is not managed by cnpg. This issue might be related: https://github.com/cloudnative-pg/cloudnative-pg/issues/1338\r\nThis problem was likely introduced with the move of configuration options from postgresql.auto.conf to override.conf: https://github.com/cloudnative-pg/cloudnative-pg/pull/2812\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  ...\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n  backup:\r\n    barmanObjectStore:\r\n      ...\r\n    target: prefer-standby\r\n  bootstrap:\r\n    pg_basebackup:\r\n      database: app\r\n      owner: app\r\n      source: mydb\r\n  ...\r\n  replica:\r\n    enabled: true\r\n    source: mydb\r\n  externalClusters:\r\n    - connectionParameters:\r\n        dbname: <REDACTED>\r\n        host: <REDACTED>\r\n        port: <REDACTED>\r\n        sslmode: verify-full\r\n        user: streaming_replica\r\n      name: mydb\r\n      sslCert:\r\n        key: tls.crt\r\n        name: mydb-replication-cert\r\n      sslKey:\r\n        key: tls.key\r\n        name: mydb-replication-cert\r\n      sslRootCert:\r\n        key: ca.crt\r\n        name: mydb-server-ca\r\n  failoverDelay: 0\r\n  imageName: 'ghcr.io/cloudnative-pg/postgresql:15.8'\r\n  instances: 3\r\n  logLevel: info\r\n  maxSyncReplicas: 2\r\n  minSyncReplicas: 1\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: always\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: '0'\r\n      log_rotation_size: '0'\r\n      log_truncate_on_rotation: 'false'\r\n      logging_collector: 'on'\r\n      max_connections: '700'\r\n      max_parallel_workers: '32'\r\n      max_replication_slots: '32'\r\n      max_slot_wal_keep_size: 10GB\r\n      max_worker_processes: '32'\r\n      shared_buffers: 256MB\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: ''\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      superuser_reserved_connections: '10'\r\n      wal_keep_size: 512MB\r\n      wal_level: logical\r\n      wal_log_hints: 'on'\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: supervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    synchronizeReplicas:\r\n      enabled: true\r\n    updateInterval: 30\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  superuserSecret:\r\n    name: mydb-cluster-superuser\r\n  switchoverDelay: 3600\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-10-07T12:28:14Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"webdb-1\",\"record\":{\"log_time\":\"2024-10-07 12:28:14.316 UTC\",\"process_id\":\"33\",\"session_id\":\"6703bdd0.21\",\"session_line_num\":\"1136\",\"session_start_time\":\"2024-10-07 10:54:08 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"waiting for WAL to become available at 293/66000018\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\nolder in 1.23.x\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhile setting up a replica cluster from an older version of cnpg (1.19), WAL file syncing fails after succesfully running pg_basebackup because the override.conf file is not loaded. `override.conf` contains the `primary_conninfo` option that specifies the connection to the primary database.\r\nThis is probably happening, because `pg_basebackup` syncs the `postgres.conf` file from the primary DB that does not include an `include override.conf` line.\r\nAdding the line `include override.conf` to `postgres.conf` fixes the problem and after running a `SELECT pg_reload_conf();` in the database, replication starts. But thsi will be lost whenever postgres.conf is rewritten.\r\nThis will also affect anyone who tries to replicate from a DB that is not managed by cnpg. This issue might be related: https://github.com/cloudnative-pg/cloudnative-pg/issues/1338\r\nThis problem was likely introduced with the move of configuration options from postgresql.auto.conf to override.conf: https://github.com/cloudnative-pg/cloudnative-pg/pull/2812\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  ...\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n  backup:\r\n    barmanObjectStore:\r\n      ...\r\n    target: prefer-standby\r\n  bootstrap:\r\n    pg_basebackup:\r\n      database: app\r\n      owner: app\r\n      source: mydb\r\n  ...\r\n  replica:\r\n    enabled: true\r\n    source: mydb\r\n  externalClusters:\r\n    - connectionParameters:\r\n        dbname: <REDACTED>\r\n        host: <REDACTED>\r\n        port: <REDACTED>\r\n        sslmode: verify-full\r\n        user: streaming_replica\r\n      name: mydb\r\n      sslCert:\r\n        key: tls.crt\r\n        name: mydb-replication-cert\r\n      sslKey:\r\n        key: tls.key\r\n        name: mydb-replication-cert\r\n      sslRootCert:\r\n        key: ca.crt\r\n        name: mydb-server-ca\r\n  failoverDelay: 0\r\n  imageName: 'ghcr.io/cloudnative-pg/postgresql:15.8'\r\n  instances: 3\r\n  logLevel: info\r\n  maxSyncReplicas: 2\r\n  minSyncReplicas: 1\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: always\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: '0'\r\n      log_rotation_size: '0'\r\n      log_truncate_on_rotation: 'false'\r\n      logging_collector: 'on'\r\n      max_connections: '700'\r\n      max_parallel_workers: '32'\r\n      max_replication_slots: '32'\r\n      max_slot_wal_keep_size: 10GB\r\n      max_worker_processes: '32'\r\n      shared_buffers: 256MB\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: ''\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      superuser_reserved_connections: '10'\r\n      wal_keep_size: 512MB\r\n      wal_level: logical\r\n      wal_log_hints: 'on'\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: supervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    synchronizeReplicas:\r\n      enabled: true\r\n    updateInterval: 30\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  superuserSecret:\r\n    name: mydb-cluster-superuser\r\n  switchoverDelay: 3600\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-10-07T12:28:14Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"webdb-1\",\"record\":{\"log_time\":\"2024-10-07 12:28:14.316 UTC\",\"process_id\":\"33\",\"session_id\":\"6703bdd0.21\",\"session_line_num\":\"1136\",\"session_start_time\":\"2024-10-07 10:54:08 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"waiting for WAL to become available at 293/66000018\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: Pooler spec.serviceTemplate: field not declared in schema",
        "id": 2568487547,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\napiVersion: postgresql.cnpg.io/v1\r\nkind: Pooler\r\nmetadata:\r\n  name: pooler-example-rw\r\nspec:\r\n  cluster:\r\n    name: pg-prod-01\r\n  instances: 3\r\n  type: rw\r\n  serviceTemplate:\r\n    metadata:\r\n      labels:\r\n        app: pooler\r\n    spec:\r\n      type: LoadBalancer\r\n  pgbouncer:\r\n    poolMode: session\r\n    parameters:\r\n      max_client_conn: \"1000\"\r\n      default_pool_size: \"10\"\r\n  spec.serviceTemplate: field not declared in schema\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\napiVersion: postgresql.cnpg.io/v1\r\nkind: Pooler\r\nmetadata:\r\n  name: pooler-example-rw\r\nspec:\r\n  cluster:\r\n    name: pg-prod-01\r\n  instances: 3\r\n  type: rw\r\n  serviceTemplate:\r\n    metadata:\r\n      labels:\r\n        app: pooler\r\n    spec:\r\n      type: LoadBalancer\r\n  pgbouncer:\r\n    poolMode: session\r\n    parameters:\r\n      max_client_conn: \"1000\"\r\n      default_pool_size: \"10\"\r\n  spec.serviceTemplate: field not declared in schema\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: separate file for CRD installation",
        "id": 2558381400,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nGood day! I want to manage cnpg installation by FluxCD. Many operators have CRDs as a separate file. Examples:\r\n- cert-manager\r\n- prometheus operator\r\n- sealed secrets \r\netc.\r\nIt allow me to install CRDs as a separate Kustomization before installation of all other components. It is even more important as management of cyclic dependencies is nightmare: let's say - I need to install nginx, I enable service monitor, but there are no service monitors in the cluster, because they are installed with k8s prometheus stack chart and this chart is dependent on nginx. So it is natural choice to separate CRDs and operators and install all CRDs as a separate step before all other components. Also as CRDs are changed very rarely (not like operators) such a separation is not an issue.\r\nAnother application of such a file could be smoke testing or validation of manifests from GitOps repo without actual installation of cnpg operator.\n### Describe the solution you'd like\nprovide a separate file with all CRDs of cnpg, publish it like an artifact on release pages with proper tagging.\n### Describe alternatives you've considered\nno alternatives\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nGood day! I want to manage cnpg installation by FluxCD. Many operators have CRDs as a separate file. Examples:\r\n- cert-manager\r\n- prometheus operator\r\n- sealed secrets \r\netc.\r\nIt allow me to install CRDs as a separate Kustomization before installation of all other components. It is even more important as management of cyclic dependencies is nightmare: let's say - I need to install nginx, I enable service monitor, but there are no service monitors in the cluster, because they are installed with k8s prometheus stack chart and this chart is dependent on nginx. So it is natural choice to separate CRDs and operators and install all CRDs as a separate step before all other components. Also as CRDs are changed very rarely (not like operators) such a separation is not an issue.\r\nAnother application of such a file could be smoke testing or validation of manifests from GitOps repo without actual installation of cnpg operator.\n### Describe the solution you'd like\nprovide a separate file with all CRDs of cnpg, publish it like an artifact on release pages with proper tagging.\n### Describe alternatives you've considered\nno alternatives\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductHello @gecube \r\nhow you suggest to do this while using kubebuilder? AFAIU that's not possible using kubebuilder\r\nRegards,\n---\n@sxd sorry, I don't understand how it is related. kubebuilder - ok. Does it create some restrictions? \r\nSeveral examples:\r\n- cert-manager https://github.com/cert-manager/cert-manager/releases/tag/v1.15.3\r\n- https://github.com/prometheus-operator/prometheus-operator/releases\r\netc.\r\nSo it is not impossible to provide a convenient way to get the CRDs only.\n---\nHello @gecube \r\nThe operator CRD and deployment, basically the yaml file to deploy the operator is created using Kubebuilder, which is the framework used to create the operator.\r\nRegards,\n---\n@gecube you can deploy just the CRDs via the kustomization.yaml that's present.\r\nFWIW I maintain a fork without the `varReference`: https://github.com/james-callahan/cloudnative-pg-kustomize/tree/main/crds\n---\nA separate file for the CRDs only, as provided by other operators, would be really helpful! This would allow easier integration with tools that ingest CRDs to generate code, for example Java Fabric8."
    },
    {
        "title": "[Chore]: use builtin multi namespace watcher ",
        "id": 2556885680,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe operator uses a custom-made code to watch multiple namespaces\n### Describe the solution you'd like\nThe operator should use the multi namespace watcher offered by controller-runtime.\n### Describe alternatives you've considered\nNA\n### Additional context\nWe should carefully test this to avoid introducing new bugs\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe operator uses a custom-made code to watch multiple namespaces\n### Describe the solution you'd like\nThe operator should use the multi namespace watcher offered by controller-runtime.\n### Describe alternatives you've considered\nNA\n### Additional context\nWe should carefully test this to avoid introducing new bugs\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Change client_tls_sslmode in pooler",
        "id": 2556254881,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nPooler: only prefer mode for client \r\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/25281f568abe7eed3f3f161e479bdfdf391ffad2/pkg/management/pgbouncer/config/config.go#L126\r\n### Describe the solution you'd like\nUsers can change tls client mode to other\n### Describe alternatives you've considered\n-\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nPooler: only prefer mode for client \r\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/25281f568abe7eed3f3f161e479bdfdf391ffad2/pkg/management/pgbouncer/config/config.go#L126\r\n### Describe the solution you'd like\nUsers can change tls client mode to other\n### Describe alternatives you've considered\n-\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI just hit on this which makes it kind of impossible to adjust this at first glance.\r\n```\r\nInvalid value: \"client_tls_sslmode\": Invalid or reserved parameter\r\nInvalid value: \"server_tls_sslmode\": Invalid or reserved parameter\r\nInvalid value: \"dns_max_ttl\": Invalid or reserved parameter\r\n```"
    },
    {
        "title": "[Feature]: Support management of PostgreSQL modules in pg_available_extensions on the base CNPG image",
        "id": 2552934570,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nHello, I am looking into enabling the extension `pgstattuple` on the PostgreSQL instances I manage through CNPG.\r\nI added them to the `shared_preload_libraries` parameter of the Cluster yaml definition to manage them the same way as extensions such as pgaudit.\r\nThe extension however had not been registered yet, because it is not supported by CNPG.\r\nWhile I understand not trying to support every possible extension, I was wondering if it would be possible to add to this list the extensions already provided by PostgreSQL, so that they can be managed the same way as the currently supported extensions.\r\nSo all extensions returned by `SELECT * FROM pg_available_extensions;` run on the base CNPG image.\r\n### Describe the solution you'd like\r\nIdeally, I would like to be able to add extensions such as `pgstattuple` in `shared_preload_libraries` in the Cluster yaml definition and that CNPG also runs the `CREATE EXTENSIONS` query for it, just like it does for pgaudit for example.\r\nTo do so, I would like CNPG to extend the list of supported extensions where it will run `CREATE EXTENSIONS` if set in `shared_preload_libraries` from: auto_explain, pg_stat_statements, pgaudit and pg_failover_slots to these 4 + all extensions provided by default on PostgreSQL built with the world target.\r\nList of modules and extensions provided by PostgreSQL: https://www.postgresql.org/docs/16/contrib.html\r\n### Describe alternatives you've considered\r\nRun `CREATE EXTENSIONS` query for the extensions that are not handled by CNPG operator using the postInitSQL feature.\r\n### Additional context\r\nI don't have much experience with managing Postgres extensions apart from the ones I'm actively using, so while the solution I'm describing seems straightforward in terms of implementations (adding the extensions to `ManagedExtensions` defined in https://github.com/cloudnative-pg/cloudnative-pg/blob/e0b36fce824aeb3c040843ea735330311b0ccd5f/pkg/postgres/configuration.go#L368), I may be missing some intricacies that makes it more difficult to implement.\r\n### Backport?\r\nNo\r\n### Are you willing to actively contribute to this feature?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nHello, I am looking into enabling the extension `pgstattuple` on the PostgreSQL instances I manage through CNPG.\r\nI added them to the `shared_preload_libraries` parameter of the Cluster yaml definition to manage them the same way as extensions such as pgaudit.\r\nThe extension however had not been registered yet, because it is not supported by CNPG.\r\nWhile I understand not trying to support every possible extension, I was wondering if it would be possible to add to this list the extensions already provided by PostgreSQL, so that they can be managed the same way as the currently supported extensions.\r\nSo all extensions returned by `SELECT * FROM pg_available_extensions;` run on the base CNPG image.\r\n### Describe the solution you'd like\r\nIdeally, I would like to be able to add extensions such as `pgstattuple` in `shared_preload_libraries` in the Cluster yaml definition and that CNPG also runs the `CREATE EXTENSIONS` query for it, just like it does for pgaudit for example.\r\nTo do so, I would like CNPG to extend the list of supported extensions where it will run `CREATE EXTENSIONS` if set in `shared_preload_libraries` from: auto_explain, pg_stat_statements, pgaudit and pg_failover_slots to these 4 + all extensions provided by default on PostgreSQL built with the world target.\r\nList of modules and extensions provided by PostgreSQL: https://www.postgresql.org/docs/16/contrib.html\r\n### Describe alternatives you've considered\r\nRun `CREATE EXTENSIONS` query for the extensions that are not handled by CNPG operator using the postInitSQL feature.\r\n### Additional context\r\nI don't have much experience with managing Postgres extensions apart from the ones I'm actively using, so while the solution I'm describing seems straightforward in terms of implementations (adding the extensions to `ManagedExtensions` defined in https://github.com/cloudnative-pg/cloudnative-pg/blob/e0b36fce824aeb3c040843ea735330311b0ccd5f/pkg/postgres/configuration.go#L368), I may be missing some intricacies that makes it more difficult to implement.\r\n### Backport?\r\nNo\r\n### Are you willing to actively contribute to this feature?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductWhat are you trying to achieve? `pgstattuple` is already there."
    },
    {
        "title": "[Bug]: With monitoring I can't access specifics schemas with custom queries",
        "id": 2552713689,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ngregoire.bellon-gervais@docaposte.fr\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nHello,\nFirst, my cluster is a self-managed RKE2.\nI have tried to add custom queries to collect tables, indexes and schema size.\nI have added the following block to add my custom ConfigMap:\n```yaml\n...\nspec:\n  monitoring:\n    customQueriesConfigMap:\n      - key: easi-pg-queries\n        name: easi-pg-queries\n      - key: queries\n        name: cnpg-default-monitoring\n...\n```\nAnd the ConfigMap to add my custom queries:\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: easi-pg-queries\n  namespace: my-namespace\n  labels:\n    cnpg.io/reload: ''\ndata:\n  easi-pg-queries: |\n    easi_pg_tables:\n      target_databases:\n        - my-database\n      query: |\n        SELECT current_database() as datname, table_schema, table_name,\n          pg_table_size(\n            quote_ident(table_schema) || '.' || quote_ident(table_name)\n          ),\n          pg_indexes_size(\n            quote_ident(table_schema) || '.' || quote_ident(table_name)\n          )\n        FROM information_schema.tables\n        ORDER BY 4\n      metrics:\n        - datname:\n            usage: 'LABEL'\n            description: 'Name of the database'\n        - table_schema:\n            usage: 'LABEL'\n            description: 'Name of the schema'\n        - table_name:\n            usage: 'LABEL'\n            description: 'Name of the table'\n        - pg_table_size:\n            usage: 'GAUGE'\n            description: 'Computes the disk space used by the specified table, excluding indexes (but including its TOAST table if any, free space map, and visibility map) in bytes'\n        - pg_indexes_size:\n            usage: 'GAUGE'\n            description: 'Computes the total disk space used by indexes attached to the specified table in bytes'\n    easi_pg_schemas:\n      target_databases:\n        - my-database\n      query: |\n        SELECT current_database() as datname, table_schema,\n          sum(\n            pg_total_relation_size(\n              quote_ident(table_schema) || '.' || quote_ident(table_name)\n            )\n          )::bigint AS schema_size\n        FROM information_schema.tables\n        GROUP BY table_catalog, table_schema\n        ORDER BY table_catalog, schema_size\n      metrics:\n        - datname:\n            usage: 'LABEL'\n            description: 'Name of the database'\n        - table_schema:\n            usage: 'LABEL'\n            description: 'Name of the schema'\n        - schema_size:\n            usage: 'GAUGE'\n            description: 'Size of the schema in bytes'\n```\nIt works when I connect inside the Pod to the database using `postgres` user as defined in [doc](https://cloudnative-pg.io/documentation/1.24/monitoring/#monitoring-instances).\nHere the output:\n```bash\npostgres@my-database-1:/$ psql -d my-database\npsql (16.4 (Debian 16.4-1.pgdg110+1))\nType \"help\" for help.\nmy-database=# SELECT current_database() as datname, table_schema,\n          sum(\n            pg_total_relation_size(\n              quote_ident(table_schema) || '.' || quote_ident(table_name)\n            )\n          )::bigint AS schema_size\n        FROM information_schema.tables\n        GROUP BY table_catalog, table_schema\n        ORDER BY table_catalog, schema_size;\n datname |           table_schema            | schema_size \n---------+-----------------------------------+-------------\n my-database     | information_schema                |      253952\n my-database     | my-custom-schema-1                |     2531328\n my-database     | pg_catalog                        |    12591104\n my-database     | another-custom-schema-1           |    13811712\n my-database     | and-another-custom-schema-1       |    37052416\n(5 rows)\nmy-database=# \n```\nBut my metrics in prometheus, I just have `information_schema` and `pg_catalog`.\nNothing related to my others schemas `my-custom-schema-1 `, `another-custom-schema-1` and `and-another-custom-schema-1`.\nI have also checked using a direct curl call to `:9187/metrics` and it contains also contains just `information_schema` and `pg_catalog`.\nDo you have an idea of what I have forgot ?\nThanks a lot for your help\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  annotations:\n    argocd.argoproj.io/sync-wave: '1'\n  labels:\n    app: my-cluster\n    app.kubernetes.io/component: database\n    app.kubernetes.io/instance: my-cluster\n    app.kubernetes.io/managed-by: Kustomize\n    app.kubernetes.io/name: my-cluster\n  name: my-cluster\n  namespace: my-namespace\nspec:\n  affinity:\n    podAntiAffinityType: preferred\n  backup:\n    barmanObjectStore:\n      data:\n        compression: snappy\n        encryption: AES256\n        jobs: 2\n      destinationPath: s3://cloudnative-pg-my-cluster-dev\n      endpointURL: https://oos.my-s3.outscale.com\n      s3Credentials:\n        accessKeyId:\n          key: ACCESS_KEY_ID\n          name: cloudnative-pg-s3\n        secretAccessKey:\n          key: ACCESS_SECRET_KEY\n          name: cloudnative-pg-s3\n      serverName: my-database\n      wal:\n        compression: snappy\n        encryption: AES256\n        maxParallel: 8\n    retentionPolicy: 90d\n  bootstrap:\n    initdb:\n      dataChecksums: true\n      database: my-database\n      encoding: UTF8\n      import:\n        databases:\n          - my-database\n        source:\n          externalCluster: cluster-crunchy\n        type: microservice\n      localeCType: C\n      localeCollate: C\n      owner: my-cluster\n  enableSuperuserAccess: false\n  externalClusters:\n    - connectionParameters:\n        dbname: my-database\n        host: my-cluster-primary.my-cluster-dev.svc\n        user: my-cluster\n      name: cluster-crunchy\n      password:\n        key: password\n        name: my-cluster-pguser-my-cluster\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.4\n  instances: 1\n  monitoring:\n    customQueriesConfigMap:\n      - key: easi-pg-queries\n        name: easi-pg-queries\n      - key: queries\n        name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: true\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: 'on'\n      archive_timeout: 5min\n      dynamic_shared_memory_type: posix\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: '0'\n      log_rotation_size: '0'\n      log_truncate_on_rotation: 'false'\n      logging_collector: 'on'\n      max_parallel_workers: '32'\n      max_replication_slots: '32'\n      max_worker_processes: '32'\n      pgaudit.log: all, -misc\n      pgaudit.log_catalog: 'off'\n      pgaudit.log_parameter: 'on'\n      pgaudit.log_relation: 'on'\n      shared_memory_type: mmap\n      shared_preload_libraries: ''\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      wal_keep_size: 512MB\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateStrategy: unsupervised\n  resources:\n    limits:\n      cpu: '2'\n      ephemeral-storage: 1Gi\n      memory: 8Gi\n    requests:\n      cpu: 100m\n      ephemeral-storage: '0'\n      memory: 128Mi\n  storage:\n    pvcTemplate:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 20Gi\n      storageClassName: gp2-retain-immediate\n      volumeMode: Filesystem\n  walStorage:\n    pvcTemplate:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 20Gi\n      storageClassName: gp2-retain-immediate\n      volumeMode: Filesystem\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ngregoire.bellon-gervais@docaposte.fr\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nHello,\nFirst, my cluster is a self-managed RKE2.\nI have tried to add custom queries to collect tables, indexes and schema size.\nI have added the following block to add my custom ConfigMap:\n```yaml\n...\nspec:\n  monitoring:\n    customQueriesConfigMap:\n      - key: easi-pg-queries\n        name: easi-pg-queries\n      - key: queries\n        name: cnpg-default-monitoring\n...\n```\nAnd the ConfigMap to add my custom queries:\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: easi-pg-queries\n  namespace: my-namespace\n  labels:\n    cnpg.io/reload: ''\ndata:\n  easi-pg-queries: |\n    easi_pg_tables:\n      target_databases:\n        - my-database\n      query: |\n        SELECT current_database() as datname, table_schema, table_name,\n          pg_table_size(\n            quote_ident(table_schema) || '.' || quote_ident(table_name)\n          ),\n          pg_indexes_size(\n            quote_ident(table_schema) || '.' || quote_ident(table_name)\n          )\n        FROM information_schema.tables\n        ORDER BY 4\n      metrics:\n        - datname:\n            usage: 'LABEL'\n            description: 'Name of the database'\n        - table_schema:\n            usage: 'LABEL'\n            description: 'Name of the schema'\n        - table_name:\n            usage: 'LABEL'\n            description: 'Name of the table'\n        - pg_table_size:\n            usage: 'GAUGE'\n            description: 'Computes the disk space used by the specified table, excluding indexes (but including its TOAST table if any, free space map, and visibility map) in bytes'\n        - pg_indexes_size:\n            usage: 'GAUGE'\n            description: 'Computes the total disk space used by indexes attached to the specified table in bytes'\n    easi_pg_schemas:\n      target_databases:\n        - my-database\n      query: |\n        SELECT current_database() as datname, table_schema,\n          sum(\n            pg_total_relation_size(\n              quote_ident(table_schema) || '.' || quote_ident(table_name)\n            )\n          )::bigint AS schema_size\n        FROM information_schema.tables\n        GROUP BY table_catalog, table_schema\n        ORDER BY table_catalog, schema_size\n      metrics:\n        - datname:\n            usage: 'LABEL'\n            description: 'Name of the database'\n        - table_schema:\n            usage: 'LABEL'\n            description: 'Name of the schema'\n        - schema_size:\n            usage: 'GAUGE'\n            description: 'Size of the schema in bytes'\n```\nIt works when I connect inside the Pod to the database using `postgres` user as defined in [doc](https://cloudnative-pg.io/documentation/1.24/monitoring/#monitoring-instances).\nHere the output:\n```bash\npostgres@my-database-1:/$ psql -d my-database\npsql (16.4 (Debian 16.4-1.pgdg110+1))\nType \"help\" for help.\nmy-database=# SELECT current_database() as datname, table_schema,\n          sum(\n            pg_total_relation_size(\n              quote_ident(table_schema) || '.' || quote_ident(table_name)\n            )\n          )::bigint AS schema_size\n        FROM information_schema.tables\n        GROUP BY table_catalog, table_schema\n        ORDER BY table_catalog, schema_size;\n datname |           table_schema            | schema_size \n---------+-----------------------------------+-------------\n my-database     | information_schema                |      253952\n my-database     | my-custom-schema-1                |     2531328\n my-database     | pg_catalog                        |    12591104\n my-database     | another-custom-schema-1           |    13811712\n my-database     | and-another-custom-schema-1       |    37052416\n(5 rows)\nmy-database=# \n```\nBut my metrics in prometheus, I just have `information_schema` and `pg_catalog`.\nNothing related to my others schemas `my-custom-schema-1 `, `another-custom-schema-1` and `and-another-custom-schema-1`.\nI have also checked using a direct curl call to `:9187/metrics` and it contains also contains just `information_schema` and `pg_catalog`.\nDo you have an idea of what I have forgot ?\nThanks a lot for your help\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  annotations:\n    argocd.argoproj.io/sync-wave: '1'\n  labels:\n    app: my-cluster\n    app.kubernetes.io/component: database\n    app.kubernetes.io/instance: my-cluster\n    app.kubernetes.io/managed-by: Kustomize\n    app.kubernetes.io/name: my-cluster\n  name: my-cluster\n  namespace: my-namespace\nspec:\n  affinity:\n    podAntiAffinityType: preferred\n  backup:\n    barmanObjectStore:\n      data:\n        compression: snappy\n        encryption: AES256\n        jobs: 2\n      destinationPath: s3://cloudnative-pg-my-cluster-dev\n      endpointURL: https://oos.my-s3.outscale.com\n      s3Credentials:\n        accessKeyId:\n          key: ACCESS_KEY_ID\n          name: cloudnative-pg-s3\n        secretAccessKey:\n          key: ACCESS_SECRET_KEY\n          name: cloudnative-pg-s3\n      serverName: my-database\n      wal:\n        compression: snappy\n        encryption: AES256\n        maxParallel: 8\n    retentionPolicy: 90d\n  bootstrap:\n    initdb:\n      dataChecksums: true\n      database: my-database\n      encoding: UTF8\n      import:\n        databases:\n          - my-database\n        source:\n          externalCluster: cluster-crunchy\n        type: microservice\n      localeCType: C\n      localeCollate: C\n      owner: my-cluster\n  enableSuperuserAccess: false\n  externalClusters:\n    - connectionParameters:\n        dbname: my-database\n        host: my-cluster-primary.my-cluster-dev.svc\n        user: my-cluster\n      name: cluster-crunchy\n      password:\n        key: password\n        name: my-cluster-pguser-my-cluster\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.4\n  instances: 1\n  monitoring:\n    customQueriesConfigMap:\n      - key: easi-pg-queries\n        name: easi-pg-queries\n      - key: queries\n        name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: true\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: 'on'\n      archive_timeout: 5min\n      dynamic_shared_memory_type: posix\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: '0'\n      log_rotation_size: '0'\n      log_truncate_on_rotation: 'false'\n      logging_collector: 'on'\n      max_parallel_workers: '32'\n      max_replication_slots: '32'\n      max_worker_processes: '32'\n      pgaudit.log: all, -misc\n      pgaudit.log_catalog: 'off'\n      pgaudit.log_parameter: 'on'\n      pgaudit.log_relation: 'on'\n      shared_memory_type: mmap\n      shared_preload_libraries: ''\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      wal_keep_size: 512MB\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateStrategy: unsupervised\n  resources:\n    limits:\n      cpu: '2'\n      ephemeral-storage: 1Gi\n      memory: 8Gi\n    requests:\n      cpu: 100m\n      ephemeral-storage: '0'\n      memory: 128Mi\n  storage:\n    pvcTemplate:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 20Gi\n      storageClassName: gp2-retain-immediate\n      volumeMode: Filesystem\n  walStorage:\n    pvcTemplate:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 20Gi\n      storageClassName: gp2-retain-immediate\n      volumeMode: Filesystem\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductIs there a way to have DEBUG level log for monitoring ?\r\nI'd like to understand why my others schemas are not displayed output.\r\nMaybe also, I could provide you more logs that could help to fix this problem.\r\nThanks a lot :)\n---\nHello,\nno one to help me ?\nMaybe as I say in my previous post, some information to be able to enable debug ?\n---\n@albundy83 I have the same issue. It seems to be due from a lack of permission for the `pg_monitor` role.\nYou can replicate this way:\n- Connect as `postgres`\n```sh\npsql -U postgres mydb\n```\n- Test the query\n```sql\nSELECT current_database() as datname, table_schema,\n          sum(\n            pg_total_relation_size(\n              quote_ident(table_schema) || '.' || quote_ident(table_name)\n            )\n          )::bigint AS schema_size\n        FROM information_schema.tables\n        GROUP BY table_catalog, table_schema\n        ORDER BY table_catalog, schema_size;\n```\n- Now switch to the `pg_monitor` role\n```sql\nSET ROLE pg_monitor;\n```\n- Repeat the monitoring query --> tables are non longer listed\nThis may be related to https://github.com/prometheus-community/postgres_exporter/issues/1032\n---\n@albundy83 Interestingly the following query work even when using `pg_monitor` role\n```sql\nSELECT current_database() AS datname, * \n  FROM (\n  SELECT *, table_total_size_bytes-indexes_size_bytes-COALESCE(table_toast_size_bytes,0) AS table_size_bytes \n    FROM (\n      SELECT nspname AS schemaname, relname AS tablename \n          , c.reltuples AS table_row_estimate\n          , pg_total_relation_size(c.oid) AS table_total_size_bytes\n          , pg_indexes_size(c.oid) AS indexes_size_bytes\n          , COALESCE(pg_total_relation_size(reltoastrelid), 0) AS table_toast_size_bytes\n      FROM pg_class c\n      LEFT JOIN pg_namespace n ON n.oid = c.relnamespace\n      WHERE relkind = 'r' ORDER BY table_total_size_bytes DESC\n  ) a) a;\n```\nref: https://www.sql-easy.com/learn/how-to-check-database-size-in-postgresql/\nSo I guess something is wrong when we pass the table name and it needs to be converted to an OID, while if we provide the OID directly, we have no issues\n---\nAh my hero :-), it has perfectly worked using your query:\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: easi-pg-queries\n  namespace: mfs-pprod\n  labels:\n    cnpg.io/reload: ''\ndata:\n  easi-pg-queries: |\n    easi_pg_tables:\n      primary: true\n      query: |\n        SELECT current_database() AS database, * \n        FROM (\n        SELECT *, table_total_size_bytes-table_indexes_size_bytes-COALESCE(table_toast_size_bytes,0) AS table_size_bytes \n          FROM (\n            SELECT nspname AS schema, relname AS table,\n                c.reltuples AS table_rows_count_estimate,\n                pg_total_relation_size(c.oid) AS table_total_size_bytes,\n                pg_indexes_size(c.oid) AS table_indexes_size_bytes,\n                COALESCE(pg_total_relation_size(reltoastrelid), 0) AS table_toast_size_bytes\n            FROM pg_class c \n            LEFT JOIN pg_namespace n ON n.oid = c.relnamespace \n            WHERE relkind = 'r' ORDER BY table_total_size_bytes DESC\n        ) a) a\n      metrics:\n        - database:\n            usage: 'LABEL'\n            description: 'Name of the database'\n        - schema:\n            usage: 'LABEL'\n            description: 'Name of the schema'\n        - table:\n            usage: 'LABEL'\n            description: 'Name of the table'\n        - table_rows_count_estimate:\n            usage: 'GAUGE'\n            description: 'An estimate of the number of rows in the table'\n        - table_total_size_bytes:\n            usage: 'GAUGE'\n            description: 'The total size of the table, including all associated components: the main table data, indexes, and TOAST (if applicable) in bytes'\n        - table_indexes_size_bytes:\n            usage: 'GAUGE'\n            description: 'The total size of all indexes on the table in bytes'\n        - table_toast_size_bytes:\n            usage: 'GAUGE'\n            description: 'The size of the TOAST table associated with the main table in bytes'\n        - table_size_bytes:\n            usage: 'GAUGE'\n            description: 'The size of the table excluding indexes and TOAST data in bytes'\n    easi_pg_schemas:\n      primary: true\n      query: |\n        SELECT \n          current_database() AS database,\n          schemaname AS schema,\n          SUM(table_total_size_bytes)::bigint AS schema_size_bytes \n        FROM (\n          SELECT \n            nspname AS schemaname,\n            relname AS tablename,\n            pg_total_relation_size(c.oid) AS table_total_size_bytes \n          FROM pg_class c \n          LEFT JOIN pg_namespace n ON n.oid = c.relnamespace \n          WHERE relkind = 'r'\n        ) a \n        GROUP BY schemaname \n        ORDER BY schema_size_bytes DESC\n      metrics:\n        - database:\n            usage: 'LABEL'\n            description: 'Name of the database'\n        - schema:\n            usage: 'LABEL'\n            description: 'Name of the schema'\n        - schema_size_bytes:\n            usage: 'GAUGE'\n            description: 'Size of the schema in bytes'\n```"
    },
    {
        "title": "[Bug]: Clusters restart at random times every few hours / days",
        "id": 2551850016,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nNo restart. Sometimes, the pods can stay alive for a few days.\n### Cluster resource\n_No response_\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-09-27T01:13:33Z\",\"msg\":\"Admission controllers (webhooks) appear to have been disabled. Please enable them for this object/namespace\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"...\"},\"namespace\":\"...\",\"name\":\"postgres\",\"reconcileID\":\"...\"}\r\n```\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-09-27T01:49:56Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"...\"},\"namespace\":\"...\",\"name\":\"postgres\",\"reconcileID\":\"...\",\"name\":\"postgres-1\",\"error\":\"error status code: 500, body: failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is shutting down (SQLSTATE 57P03)\\n\"}\r\n```\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nNo restart. Sometimes, the pods can stay alive for a few days.\n### Cluster resource\n_No response_\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-09-27T01:13:33Z\",\"msg\":\"Admission controllers (webhooks) appear to have been disabled. Please enable them for this object/namespace\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"...\"},\"namespace\":\"...\",\"name\":\"postgres\",\"reconcileID\":\"...\"}\r\n```\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-09-27T01:49:56Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"...\"},\"namespace\":\"...\",\"name\":\"postgres\",\"reconcileID\":\"...\",\"name\":\"postgres-1\",\"error\":\"error status code: 500, body: failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is shutting down (SQLSTATE 57P03)\\n\"}\r\n```\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "Update troubleshooting documentation for Linkerd integration",
        "id": 2551249102,
        "state": "open",
        "first": "",
        "messages": "Can someone with Linkerd experience review this?"
    },
    {
        "title": "[Feature]: Add option to clear S3 bucket on cluster creation for WAL archiving reuse",
        "id": 2545142712,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe current design of CloudNativePG (CNPG) doesn't allow reusing an S3 bucket that was previously used for WAL archiving. This limitation causes issues in scenarios such as CI/CD processes or when recreating the same cluster is necessary. In these cases, WAL archiving fails to function properly because the S3 bucket contains data from the previous cluster.\r\n### Describe the solution you'd like\nWe propose adding a new parameter to the CNPG Cluster custom resource called `clearBucketOnCreation` (or similar). When set to `true`, this parameter would instruct the operator to empty the specified S3 bucket before initializing WAL archiving for the new cluster.\r\nObviously, this setting should be turned off by default.\n### Describe alternatives you've considered\nWe need to clear manually the S3 bucket before recreating the CNPG cluster.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe current design of CloudNativePG (CNPG) doesn't allow reusing an S3 bucket that was previously used for WAL archiving. This limitation causes issues in scenarios such as CI/CD processes or when recreating the same cluster is necessary. In these cases, WAL archiving fails to function properly because the S3 bucket contains data from the previous cluster.\r\n### Describe the solution you'd like\nWe propose adding a new parameter to the CNPG Cluster custom resource called `clearBucketOnCreation` (or similar). When set to `true`, this parameter would instruct the operator to empty the specified S3 bucket before initializing WAL archiving for the new cluster.\r\nObviously, this setting should be turned off by default.\n### Describe alternatives you've considered\nWe need to clear manually the S3 bucket before recreating the CNPG cluster.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductSo I found in documentation that there's an option to disable the check, by setting an annotation on the cluster:\r\n`cnpg.io/skipEmptyWalArchiveCheck=enabled` (bottom of https://cloudnative-pg.io/documentation/1.24/recovery/#restoring-into-a-cluster-with-a-backup-section)\r\nI was wondering if that could solve your issue, but also have question to others, whether this option is safe to use on cluster that was recovered from the archive (i.e. it can continue writing new wal logs and nothing will break?). The documentation is not as clear on that.\n---\nMore useful for me perhaps would be a finalizer on the Cluster that deletes the backups when the cluster is deleted.\nFor throwaway clusters/testing you would turn it on so that deleting cluster/namespace cleans up the bucket."
    },
    {
        "title": "chore: use builtin multi namespace watcher",
        "id": 2543097713,
        "state": "open",
        "first": "Closes #5688",
        "messages": "Closes #5688/test limit=local\n---\nI think this should be thoroughly tested on OpenShift, installing the operator in multiple namespaces but not in the whole cluster.\n---\n@fcanovai @sxd made the requested changes"
    },
    {
        "title": "[Bug]: Unable to restore from backup using targetTime",
        "id": 2541604874,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nTake a backup on S3  and don't add anything to database.Try to restore the postgres cluster by give targetTime (Stopped  time of backup)..Restore will fails.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name:  test-restore\r\n  namespace: restore-new\r\nspec:\r\n  instances: 1\r\n  storage:\r\n    size: 3Gi\r\n  enableSuperuserAccess: true\r\n  superuserSecret:\r\n    name: superuser-secret\r\n  bootstrap:\r\n    recovery:\r\n      backup:\r\n       name: backup-2\r\n      recoveryTarget:\r\n        backupID: 20240813T115029\r\n        targetTime: \"2024-08-13 11:50:30.00000+00\"\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:15.3\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nTake a backup on S3  and don't add anything to database.Try to restore the postgres cluster by give targetTime (Stopped  time of backup)..Restore will fails.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name:  test-restore\r\n  namespace: restore-new\r\nspec:\r\n  instances: 1\r\n  storage:\r\n    size: 3Gi\r\n  enableSuperuserAccess: true\r\n  superuserSecret:\r\n    name: superuser-secret\r\n  bootstrap:\r\n    recovery:\r\n      backup:\r\n       name: backup-2\r\n      recoveryTarget:\r\n        backupID: 20240813T115029\r\n        targetTime: \"2024-08-13 11:50:30.00000+00\"\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:15.3\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conductany updates ?\n---\nFollowing this issue. Thanks for the work!"
    },
    {
        "title": "[Docs]: Can't find how to completely disable SSL",
        "id": 2541036076,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nI want to use Istio for strict mutual TLS, but I can't figure out how to completely disable SSL from operator.\n### Describe what additions need to be done to the documentation\n_No response_\n### Describe what pages need to change in the documentation, if any\n_No response_\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nN/A\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [x] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nI want to use Istio for strict mutual TLS, but I can't figure out how to completely disable SSL from operator.\n### Describe what additions need to be done to the documentation\n_No response_\n### Describe what pages need to change in the documentation, if any\n_No response_\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nN/A\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conducthey @framework20 , did you able to disable the ssl? i'm facing the same issue while migrating the zalando postgres cluster to cloudnative-pg.\n---\nHi @anaskhantpl! Not yet, I'm trying to figure it out in my free time. If you find a solution or understand which way to look, write here pls, I will be glad and we will think together."
    },
    {
        "title": "[Feature]: Tablespace creation should still work with PrimaryUpdateMethodSwitchover",
        "id": 2538838814,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe initial implementation of Tablespaces requires a forced update of the primary without a switchover.\r\nThe reason is that the tablespace reconciliation is done entirely in the primary instance, without knowledge of the status of the replicas.\r\nIf the `CREATE TABLESPACE` command is issued in the primary, the tablespace mounts/PVCs need to be present for both the primary and the replicas. Otherwise the postmaster will crash.\r\nIf we did a switchover + upgrade, the old primary will still be present a while, without the tablespace PVC's mounted. The new primary would verify it has the mounts, and would issue the `CREATE TABLESPACE`, and this would break the cluster.\r\nIt is for this reason that there is an argument `forceRecreate` in the `updatePrimaryPod` method in `internal/controller/cluster_upgrade.go` (see below).\r\nWith this argument set to true, the primary will be updated without switchover, no matter the value of `PrimaryUpdateStrategy`.\r\n``` go\r\nfunc (r *ClusterReconciler) updatePrimaryPod(\r\n\tctx context.Context,\r\n\tcluster *apiv1.Cluster,\r\n\tpodList *postgres.PostgresqlStatusList,\r\n\tprimaryPod corev1.Pod,\r\n\tinPlacePossible bool,\r\n\tforceRecreate bool,\r\n```\n### Describe the solution you'd like\nThe tablespace controller should verify that ALL the instances have the mounts for a given tablespace (MyTbs), before issuing the `CREATE TABLESPACE`.\r\nThe necessary information is in the cluster status, which keeps a list of initializing and healthy PVCs:\r\n``` yaml\r\n  initializingPVC:\r\n  - cluster-example-with-tablespaces-3\r\n  - cluster-example-with-tablespaces-3-tbs-another-tablespace\r\n  - cluster-example-with-tablespaces-3-tbs-atablespace\r\n  - cluster-example-with-tablespaces-3-tbs-tablespacea1\r\n```\r\n``` yaml\r\nhealthyPVC:\r\n  - cluster-example-with-tablespaces-1\r\n  - cluster-example-with-tablespaces-1-tbs-another-tablespace\r\n  - cluster-example-with-tablespaces-1-tbs-atablespace\r\n  - cluster-example-with-tablespaces-1-tbs-tablespacea1\r\n```\r\nIf the controller can verify that the mounts for MyTbs are there for all the instances before issuing the `CREATE TABLESPACE`, then the extra logic in `func (r *ClusterReconciler) updatePrimaryPod(` could be eliminated.\n### Describe alternatives you've considered\nThere is a viable alternative in doing nothing. The feature is working, though the instance upgrade logic is cumbersome.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe initial implementation of Tablespaces requires a forced update of the primary without a switchover.\r\nThe reason is that the tablespace reconciliation is done entirely in the primary instance, without knowledge of the status of the replicas.\r\nIf the `CREATE TABLESPACE` command is issued in the primary, the tablespace mounts/PVCs need to be present for both the primary and the replicas. Otherwise the postmaster will crash.\r\nIf we did a switchover + upgrade, the old primary will still be present a while, without the tablespace PVC's mounted. The new primary would verify it has the mounts, and would issue the `CREATE TABLESPACE`, and this would break the cluster.\r\nIt is for this reason that there is an argument `forceRecreate` in the `updatePrimaryPod` method in `internal/controller/cluster_upgrade.go` (see below).\r\nWith this argument set to true, the primary will be updated without switchover, no matter the value of `PrimaryUpdateStrategy`.\r\n``` go\r\nfunc (r *ClusterReconciler) updatePrimaryPod(\r\n\tctx context.Context,\r\n\tcluster *apiv1.Cluster,\r\n\tpodList *postgres.PostgresqlStatusList,\r\n\tprimaryPod corev1.Pod,\r\n\tinPlacePossible bool,\r\n\tforceRecreate bool,\r\n```\n### Describe the solution you'd like\nThe tablespace controller should verify that ALL the instances have the mounts for a given tablespace (MyTbs), before issuing the `CREATE TABLESPACE`.\r\nThe necessary information is in the cluster status, which keeps a list of initializing and healthy PVCs:\r\n``` yaml\r\n  initializingPVC:\r\n  - cluster-example-with-tablespaces-3\r\n  - cluster-example-with-tablespaces-3-tbs-another-tablespace\r\n  - cluster-example-with-tablespaces-3-tbs-atablespace\r\n  - cluster-example-with-tablespaces-3-tbs-tablespacea1\r\n```\r\n``` yaml\r\nhealthyPVC:\r\n  - cluster-example-with-tablespaces-1\r\n  - cluster-example-with-tablespaces-1-tbs-another-tablespace\r\n  - cluster-example-with-tablespaces-1-tbs-atablespace\r\n  - cluster-example-with-tablespaces-1-tbs-tablespacea1\r\n```\r\nIf the controller can verify that the mounts for MyTbs are there for all the instances before issuing the `CREATE TABLESPACE`, then the extra logic in `func (r *ClusterReconciler) updatePrimaryPod(` could be eliminated.\n### Describe alternatives you've considered\nThere is a viable alternative in doing nothing. The feature is working, though the instance upgrade logic is cumbersome.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: Header being added to my backup files",
        "id": 2535698018,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nAll of my backup files had these lines, which cause the error, can not extracted backup ID\r\n![image](https://github.com/user-attachments/assets/a17545b9-94a0-4a6b-ad90-4595b4ba43eb)\r\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nAll of my backup files had these lines, which cause the error, can not extracted backup ID\r\n![image](https://github.com/user-attachments/assets/a17545b9-94a0-4a6b-ad90-4595b4ba43eb)\r\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Specifying a CatalogImage or ClusterCatalogImage in Cluster should care of imagePullSecrets configuration",
        "id": 2534731375,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI'm maintaining a private container registry where I'm publishing customized postgresql-containers images (following https://github.com/cloudnative-pg/postgres-containers). Pod can't retrieve the image because it's not using the pull secret.\n### Describe the solution you'd like\nI created a CatalogImage with several images referring to that private registry.\r\nWhen I'm using it in the Cluster, the pod then fails to get the image. The pull secret is not referenced in the spec.\r\nI think it would be appreciated the pod will spawn with the pull secret in it in order to be able to download the image from private registry.\n### Describe alternatives you've considered\nI'm using the Helm chart. Cluster specs disallows using a complex imageName probably because I'm receiving \"invalid version tag\". That's why I use CatalogImage which I think is the cleanest way to achieve this.\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI'm maintaining a private container registry where I'm publishing customized postgresql-containers images (following https://github.com/cloudnative-pg/postgres-containers). Pod can't retrieve the image because it's not using the pull secret.\n### Describe the solution you'd like\nI created a CatalogImage with several images referring to that private registry.\r\nWhen I'm using it in the Cluster, the pod then fails to get the image. The pull secret is not referenced in the spec.\r\nI think it would be appreciated the pod will spawn with the pull secret in it in order to be able to download the image from private registry.\n### Describe alternatives you've considered\nI'm using the Helm chart. Cluster specs disallows using a complex imageName probably because I'm receiving \"invalid version tag\". That's why I use CatalogImage which I think is the cleanest way to achieve this.\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: could not receive data from client: Connection reset by peer",
        "id": 2533346783,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nWe are having a problem with closed connections and can not make sense of it.\r\nThe connections are closed with \"could not receive data from client: Connection reset by peer\", but the application / application pooler is not closing the connections.\r\nWhat seems strange to me is that the connections are closed in batches of 10-20 connections at exactly the same time\r\n![image](https://github.com/user-attachments/assets/a8de134d-966a-404b-84dd-6c397742d7db)\r\nWhat they have in common is that all of the dropped connections came through a firewall.\r\nOn the Server side, tcp_keepalives parameters are set, so they should not be killed by some idle timeout.\n### Cluster resource\n```shell\n...\r\n  postgresql:\r\n    parameters:\r\n      tcp_keepalives_idle : \"60\"\r\n      tcp_keepalives_interval: \"30\"\r\n      tcp_keepalives_count: \"3\"  \r\n...\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nWe are having a problem with closed connections and can not make sense of it.\r\nThe connections are closed with \"could not receive data from client: Connection reset by peer\", but the application / application pooler is not closing the connections.\r\nWhat seems strange to me is that the connections are closed in batches of 10-20 connections at exactly the same time\r\n![image](https://github.com/user-attachments/assets/a8de134d-966a-404b-84dd-6c397742d7db)\r\nWhat they have in common is that all of the dropped connections came through a firewall.\r\nOn the Server side, tcp_keepalives parameters are set, so they should not be killed by some idle timeout.\n### Cluster resource\n```shell\n...\r\n  postgresql:\r\n    parameters:\r\n      tcp_keepalives_idle : \"60\"\r\n      tcp_keepalives_interval: \"30\"\r\n      tcp_keepalives_count: \"3\"  \r\n...\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI have the same issue\n---\nHello @5h4k4r \nDo you have a way to reproduce your issue? This will be very helpful \nRegards!\n---\nHello @sxd I don't remember how I fixed this. Thank you for the follow up."
    },
    {
        "title": "fix: set pgpass host to * to match all hostnames a client might use",
        "id": 2531830399,
        "state": "open",
        "first": "In the app secret we only set the `-rw` endpoint, resulting in no match if any of the other endpoints is used, I think we can safely switch to `*` instead.",
        "messages": "In the app secret we only set the `-rw` endpoint, resulting in no match if any of the other endpoints is used, I think we can safely switch to `*` instead."
    },
    {
        "title": "[Feature]: Disable non-SSL connections by default",
        "id": 2531708176,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nAt the moment, CloudNativePG allows insecure connections by default, using this fallback entry in `pg_hba.conf`:\r\n```\r\nhost all all all <default-authentication-method>\r\n```\r\nTo continue with our secure-by-default approach, we should consider a way to disable non-SSL connections altogether.\n### Describe the solution you'd like\nAdd `.spec.postgresql.enableNonSSLConnections` (better names are welcome) that, if set to off, changes the fallback rule to:\r\n```\r\nhostssl all all all <default-authentication-method>\r\n```\r\nI would consider making this the default for 1.25, ensuring that it is properly documented.\r\nI encourage feedback from users before we start working on this.\n### Describe alternatives you've considered\nWe could reverse the meaning, and have `disableNonSSLConnections` or have the user specify the default fallback host rule, such as `defaultFallbackHostRule` with values: host or hostssl.\n### Additional context\nFor PostgreSQL information on pg_hba, see: https://www.postgresql.org/docs/current/auth-pg-hba-conf.html\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nAt the moment, CloudNativePG allows insecure connections by default, using this fallback entry in `pg_hba.conf`:\r\n```\r\nhost all all all <default-authentication-method>\r\n```\r\nTo continue with our secure-by-default approach, we should consider a way to disable non-SSL connections altogether.\n### Describe the solution you'd like\nAdd `.spec.postgresql.enableNonSSLConnections` (better names are welcome) that, if set to off, changes the fallback rule to:\r\n```\r\nhostssl all all all <default-authentication-method>\r\n```\r\nI would consider making this the default for 1.25, ensuring that it is properly documented.\r\nI encourage feedback from users before we start working on this.\n### Describe alternatives you've considered\nWe could reverse the meaning, and have `disableNonSSLConnections` or have the user specify the default fallback host rule, such as `defaultFallbackHostRule` with values: host or hostssl.\n### Additional context\nFor PostgreSQL information on pg_hba, see: https://www.postgresql.org/docs/current/auth-pg-hba-conf.html\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductHi, AFAIK it is currently not possible to remove the `host all all all <default-authentication-method>` line to disable non SSL connections so this ticket could be split in 2:\n1. Add feature to disable SSL connections\n2. Change default behavior (breaking)"
    },
    {
        "title": "[Feature]: Add support for kubernetes.io/tls in clientCASecret and serverCASecret",
        "id": 2530427490,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently serverCASecret and clientCASecret uses secrets with fields ca.crt and ca.key which is incompatible with secrets of type kubernetes.io/tls\r\n### Describe the solution you'd like\nSupport for fields tls.key and tls.crt in serverCASecret and clientCASecret with kubernetes.io/tls secret type. Either by checking fields or by checking secret type\n### Describe alternatives you've considered\nKyverno mutation hooks or other solutions to transform objects\n### Additional context\nCertificates, generated with cert-manager are consist of fields tls.crt, tls.key and ca.crt, even if CA certificate is created. The issue is that secret must somehow be transformed with added field ca.key = tls.key and ca.crt = tls.crt.\r\nOnly related issue with similar context i found was #2841\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently serverCASecret and clientCASecret uses secrets with fields ca.crt and ca.key which is incompatible with secrets of type kubernetes.io/tls\r\n### Describe the solution you'd like\nSupport for fields tls.key and tls.crt in serverCASecret and clientCASecret with kubernetes.io/tls secret type. Either by checking fields or by checking secret type\n### Describe alternatives you've considered\nKyverno mutation hooks or other solutions to transform objects\n### Additional context\nCertificates, generated with cert-manager are consist of fields tls.crt, tls.key and ca.crt, even if CA certificate is created. The issue is that secret must somehow be transformed with added field ca.key = tls.key and ca.crt = tls.crt.\r\nOnly related issue with similar context i found was #2841\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: CNPG Kubectl Plugin Fails to Correctly Report Cluster Status",
        "id": 2526902971,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nthegeek@me.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nWhen querying a cluster status using the kubectl plugin it fails to communicate with any of the nodes. It incorrectly reports no primary and is unable to display any WAL information.\n### Cluster resource\n```shell\nName:         grafana-pg-cluster\r\nNamespace:    database\r\nLabels:       kustomize.toolkit.fluxcd.io/name=flux-system\r\n              kustomize.toolkit.fluxcd.io/namespace=flux-system\r\nAnnotations:  <none>\r\nAPI Version:  postgresql.cnpg.io/v1\r\nKind:         Cluster\r\nMetadata:\r\n  Creation Timestamp:  2024-08-15T17:15:42Z\r\n  Generation:          3\r\n  Resource Version:    121883948\r\n  UID:                 a3162484-82e2-4475-99d3-06feeb593f87\r\nSpec:\r\n  Affinity:\r\n    Pod Anti Affinity Type:  preferred\r\n  Backup:\r\n    Barman Object Store:\r\n      Destination Path:  s3://postgres-backups.macbytes.io/\r\n      s3Credentials:\r\n        Access Key Id:\r\n          Key:   ACCESS_KEY_ID\r\n          Name:  cloudnative-pg-aws-credentials\r\n        Secret Access Key:\r\n          Key:   ACCESS_SECRET_KEY\r\n          Name:  cloudnative-pg-aws-credentials\r\n      Wal:\r\n        Compression:   gzip\r\n    Retention Policy:  7d\r\n    Target:            prefer-standby\r\n  Bootstrap:\r\n    Initdb:\r\n      Database:             grafana\r\n      Encoding:             UTF8\r\n      Locale C Type:        C\r\n      Locale Collate:       C\r\n      Owner:                grafana\r\n  Enable PDB:               true\r\n  Enable Superuser Access:  true\r\n  Failover Delay:           0\r\n  Image Name:               ghcr.io/cloudnative-pg/postgresql:16.3\r\n  Instances:                3\r\n  Log Level:                info\r\n  Max Sync Replicas:        0\r\n  Min Sync Replicas:        0\r\n  Monitoring:\r\n    Custom Queries Config Map:\r\n      Key:                    queries\r\n      Name:                   cnpg-default-monitoring\r\n    Disable Default Queries:  false\r\n    Enable Pod Monitor:       true\r\n  Postgres GID:               26\r\n  Postgres UID:               26\r\n  Postgresql:\r\n    Parameters:\r\n      archive_mode:                on\r\n      archive_timeout:             5min\r\n      dynamic_shared_memory_type:  posix\r\n      log_destination:             csvlog\r\n      log_directory:               /controller/log\r\n      log_filename:                postgres\r\n      log_rotation_age:            0\r\n      log_rotation_size:           0\r\n      log_truncate_on_rotation:    false\r\n      logging_collector:           on\r\n      max_parallel_workers:        32\r\n      max_replication_slots:       32\r\n      max_worker_processes:        32\r\n      shared_memory_type:          mmap\r\n      shared_preload_libraries:\r\n      ssl_max_protocol_version:    TLSv1.3\r\n      ssl_min_protocol_version:    TLSv1.3\r\n      wal_keep_size:               512MB\r\n      wal_level:                   logical\r\n      wal_log_hints:               on\r\n      wal_receiver_timeout:        5s\r\n      wal_sender_timeout:          5s\r\n    Sync Replica Election Constraint:\r\n      Enabled:              false\r\n  Primary Update Method:    restart\r\n  Primary Update Strategy:  unsupervised\r\n  Replication Slots:\r\n    High Availability:\r\n      Enabled:      true\r\n      Slot Prefix:  _cnpg_\r\n    Synchronize Replicas:\r\n      Enabled:        true\r\n    Update Interval:  30\r\n  Resources:\r\n  Smart Shutdown Timeout:  180\r\n  Start Delay:             3600\r\n  Stop Delay:              1800\r\n  Storage:\r\n    Resize In Use Volumes:  true\r\n    Size:                   10Gi\r\n    Storage Class:          longhorn\r\n  Switchover Delay:         3600\r\nStatus:\r\n  Available Architectures:\r\n    Go Arch:  amd64\r\n    Hash:     f20cbc18bb03eafc1b02c90a8872b8e7a199e63196e6ce546029ea2a503bb883\r\n    Go Arch:  arm64\r\n    Hash:     e28f9109055832cc1fa1ece7866547f267b43e16c800d7b4335cab24ad5420d4\r\n  Certificates:\r\n    Client CA Secret:  grafana-pg-cluster-ca\r\n    Expirations:\r\n      Grafana - Pg - Cluster - Ca:           2024-11-13 17:10:43 +0000 UTC\r\n      Grafana - Pg - Cluster - Replication:  2024-11-13 17:10:43 +0000 UTC\r\n      Grafana - Pg - Cluster - Server:       2024-11-20 15:51:27 +0000 UTC\r\n    Replication TLS Secret:                  grafana-pg-cluster-replication\r\n    Server Alt DNS Names:\r\n      grafana-pg-cluster-rw\r\n      grafana-pg-cluster-rw.database\r\n      grafana-pg-cluster-rw.database.svc\r\n      grafana-pg-cluster-rw.database.svc.cluster.local\r\n      grafana-pg-cluster-r\r\n      grafana-pg-cluster-r.database\r\n      grafana-pg-cluster-r.database.svc\r\n      grafana-pg-cluster-r.database.svc.cluster.local\r\n      grafana-pg-cluster-ro\r\n      grafana-pg-cluster-ro.database\r\n      grafana-pg-cluster-ro.database.svc\r\n      grafana-pg-cluster-ro.database.svc.cluster.local\r\n    Server CA Secret:             grafana-pg-cluster-ca\r\n    Server TLS Secret:            grafana-pg-cluster-server\r\n  Cloud Native PG Commit Hash:    5fe5bb6b\r\n  Cloud Native PG Operator Hash:  f20cbc18bb03eafc1b02c90a8872b8e7a199e63196e6ce546029ea2a503bb883\r\n  Conditions:\r\n    Last Transition Time:  2024-09-14T20:24:26Z\r\n    Message:               Cluster is Ready\r\n    Reason:                ClusterIsReady\r\n    Status:                True\r\n    Type:                  Ready\r\n    Last Transition Time:  2024-09-14T20:23:25Z\r\n    Message:               Continuous archiving is working\r\n    Reason:                ContinuousArchivingSuccess\r\n    Status:                True\r\n    Type:                  ContinuousArchiving\r\n    Last Transition Time:  2024-09-15T00:00:38Z\r\n    Message:               Backup was successful\r\n    Reason:                LastBackupSucceeded\r\n    Status:                True\r\n    Type:                  LastBackupSucceeded\r\n  Config Map Resource Version:\r\n    Metrics:\r\n      Cnpg - Default - Monitoring:  102827884\r\n  Current Primary:                  grafana-pg-cluster-2\r\n  Current Primary Timestamp:        2024-09-14T20:23:21.729277Z\r\n  First Recoverability Point:       2024-09-08T00:00:05Z\r\n  First Recoverability Point By Method:\r\n    Barman Object Store:  2024-09-08T00:00:05Z\r\n  Healthy PVC:\r\n    grafana-pg-cluster-1\r\n    grafana-pg-cluster-2\r\n    grafana-pg-cluster-3\r\n  Image:  ghcr.io/cloudnative-pg/postgresql:16.3\r\n  Instance Names:\r\n    grafana-pg-cluster-1\r\n    grafana-pg-cluster-2\r\n    grafana-pg-cluster-3\r\n  Instances:  3\r\n  Instances Reported State:\r\n    grafana-pg-cluster-1:\r\n      Is Primary:    false\r\n      Time Line ID:  2\r\n    grafana-pg-cluster-2:\r\n      Is Primary:    true\r\n      Time Line ID:  2\r\n    grafana-pg-cluster-3:\r\n      Is Primary:    false\r\n      Time Line ID:  2\r\n  Instances Status:\r\n    Healthy:\r\n      grafana-pg-cluster-1\r\n      grafana-pg-cluster-2\r\n      grafana-pg-cluster-3\r\n  Last Successful Backup:  2024-09-15T00:00:32Z\r\n  Last Successful Backup By Method:\r\n    Barman Object Store:  2024-09-15T00:00:32Z\r\n  Latest Generated Node:  3\r\n  Managed Roles Status:\r\n  Phase:  Cluster in healthy state\r\n  Pooler Integrations:\r\n    Pg Bouncer Integration:\r\n  Pvc Count:        3\r\n  Read Service:     grafana-pg-cluster-r\r\n  Ready Instances:  3\r\n  Secrets Resource Version:\r\n    Application Secret Version:  102828249\r\n    Client Ca Secret Version:    97389628\r\n    Replication Secret Version:  97389630\r\n    Server Ca Secret Version:    97389628\r\n    Server Secret Version:       102828239\r\n    Superuser Secret Version:    102828241\r\n  Switch Replica Cluster Status:\r\n  Target Primary:            grafana-pg-cluster-2\r\n  Target Primary Timestamp:  2024-09-14T20:23:13.949561Z\r\n  Timeline ID:               2\r\n  Topology:\r\n    Instances:\r\n      grafana-pg-cluster-1:\r\n      grafana-pg-cluster-2:\r\n      grafana-pg-cluster-3:\r\n    Nodes Used:              3\r\n    Successfully Extracted:  true\r\n  Write Service:             grafana-pg-cluster-rw\r\nEvents:                      <none>\n```\n### Relevant log output\n```shell\nkubectl cnpg status grafana-pg-cluster -n database --verbose\r\nCluster Summary\r\nName:                grafana-pg-cluster\r\nNamespace:           database\r\nPostgreSQL Image:    ghcr.io/cloudnative-pg/postgresql:16.3\r\nPrimary instance:    grafana-pg-cluster-2\r\nPrimary start time:  2024-09-14 20:23:22 +0000 UTC (uptime 16h33m17s)\r\nStatus:              Cluster in healthy state\r\nInstances:           3\r\nReady instances:     3\r\nPostgreSQL Configuration\r\narchive_command = '/controller/manager wal-archive --log-destination /controller/log/postgres.json %p'\r\narchive_mode = 'on'\r\narchive_timeout = '5min'\r\ncluster_name = 'grafana-pg-cluster'\r\ndynamic_shared_memory_type = 'posix'\r\nfull_page_writes = 'on'\r\nhot_standby = 'true'\r\nlisten_addresses = '*'\r\nlog_destination = 'csvlog'\r\nlog_directory = '/controller/log'\r\nlog_filename = 'postgres'\r\nlog_rotation_age = '0'\r\nlog_rotation_size = '0'\r\nlog_truncate_on_rotation = 'false'\r\nlogging_collector = 'on'\r\nmax_parallel_workers = '32'\r\nmax_replication_slots = '32'\r\nmax_worker_processes = '32'\r\nport = '5432'\r\nrestart_after_crash = 'false'\r\nshared_memory_type = 'mmap'\r\nshared_preload_libraries = ''\r\nssl = 'on'\r\nssl_ca_file = '/controller/certificates/client-ca.crt'\r\nssl_cert_file = '/controller/certificates/server.crt'\r\nssl_key_file = '/controller/certificates/server.key'\r\nssl_max_protocol_version = 'TLSv1.3'\r\nssl_min_protocol_version = 'TLSv1.3'\r\nunix_socket_directories = '/controller/run'\r\nwal_keep_size = '512MB'\r\nwal_level = 'logical'\r\nwal_log_hints = 'on'\r\nwal_receiver_timeout = '5s'\r\nwal_sender_timeout = '5s'\r\ncnpg.config_sha256 = '130bd169023b987f03a9f20d8aa0989a7ef6397473f6d9f3c31915f83aa3690d'\r\nPostgreSQL HBA Rules\r\n#\r\n# FIXED RULES\r\n#\r\n# Grant local access ('local' user map)\r\nlocal all all peer map=local\r\n# Require client certificate authentication for the streaming_replica user\r\nhostssl postgres streaming_replica all cert\r\nhostssl replication streaming_replica all cert\r\nhostssl all cnpg_pooler_pgbouncer all cert\r\n#\r\n# USER-DEFINED RULES\r\n#\r\n#\r\n# DEFAULT RULES\r\n#\r\nhost all all all scram-sha-256\r\nCertificates Status\r\nCertificate Name                Expiration Date                Days Left Until Expiration\r\n----------------                ---------------                --------------------------\r\ngrafana-pg-cluster-ca           2024-11-13 17:10:43 +0000 UTC  59.18\r\ngrafana-pg-cluster-replication  2024-11-13 17:10:43 +0000 UTC  59.18\r\ngrafana-pg-cluster-server       2024-11-20 15:51:27 +0000 UTC  66.12\r\nContinuous Backup status\r\nFirst Point of Recoverability:  2024-09-08T00:00:05Z\r\nNo Primary instance found\r\nPhysical backups\r\nPrimary instance not found\r\nStreaming Replication status\r\nPrimary instance not found\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nManaged roles status\r\nNo roles managed\r\nTablespaces status\r\nNo managed tablespaces\r\nPod Disruption Budgets status\r\nName                        Role  Expected Pods  Current Healthy  Minimum Desired Healthy  Disruptions Allowed\r\n----                        ----  -------------  ---------------  -----------------------  -------------------\r\ngrafana-pg-cluster                2              2                1                        1\r\ngrafana-pg-cluster-primary        1              1                1                        0\r\nInstances status\r\nName                  Database Size  Current LSN  Replication role  Status      QoS         Manager Version  Node\r\n----                  -------------  -----------  ----------------  ------      ---         ---------------  ----\r\ngrafana-pg-cluster-1  -              -            -                 BadRequest  BestEffort  -                k3s-node-01\r\ngrafana-pg-cluster-2  -              -            -                 BadRequest  BestEffort  -                k3s-node-03\r\ngrafana-pg-cluster-3  -              -            -                 BadRequest  BestEffort  -                k3s-node-02\r\nError(s) extracting status\r\n-----------------------------------\r\nthe server rejected our request for an unknown reason (get pods http:grafana-pg-cluster-1:8000)\r\nthe server rejected our request for an unknown reason (get pods http:grafana-pg-cluster-2:8000)\r\nthe server rejected our request for an unknown reason (get pods http:grafana-pg-cluster-3:8000)\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nthegeek@me.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nWhen querying a cluster status using the kubectl plugin it fails to communicate with any of the nodes. It incorrectly reports no primary and is unable to display any WAL information.\n### Cluster resource\n```shell\nName:         grafana-pg-cluster\r\nNamespace:    database\r\nLabels:       kustomize.toolkit.fluxcd.io/name=flux-system\r\n              kustomize.toolkit.fluxcd.io/namespace=flux-system\r\nAnnotations:  <none>\r\nAPI Version:  postgresql.cnpg.io/v1\r\nKind:         Cluster\r\nMetadata:\r\n  Creation Timestamp:  2024-08-15T17:15:42Z\r\n  Generation:          3\r\n  Resource Version:    121883948\r\n  UID:                 a3162484-82e2-4475-99d3-06feeb593f87\r\nSpec:\r\n  Affinity:\r\n    Pod Anti Affinity Type:  preferred\r\n  Backup:\r\n    Barman Object Store:\r\n      Destination Path:  s3://postgres-backups.macbytes.io/\r\n      s3Credentials:\r\n        Access Key Id:\r\n          Key:   ACCESS_KEY_ID\r\n          Name:  cloudnative-pg-aws-credentials\r\n        Secret Access Key:\r\n          Key:   ACCESS_SECRET_KEY\r\n          Name:  cloudnative-pg-aws-credentials\r\n      Wal:\r\n        Compression:   gzip\r\n    Retention Policy:  7d\r\n    Target:            prefer-standby\r\n  Bootstrap:\r\n    Initdb:\r\n      Database:             grafana\r\n      Encoding:             UTF8\r\n      Locale C Type:        C\r\n      Locale Collate:       C\r\n      Owner:                grafana\r\n  Enable PDB:               true\r\n  Enable Superuser Access:  true\r\n  Failover Delay:           0\r\n  Image Name:               ghcr.io/cloudnative-pg/postgresql:16.3\r\n  Instances:                3\r\n  Log Level:                info\r\n  Max Sync Replicas:        0\r\n  Min Sync Replicas:        0\r\n  Monitoring:\r\n    Custom Queries Config Map:\r\n      Key:                    queries\r\n      Name:                   cnpg-default-monitoring\r\n    Disable Default Queries:  false\r\n    Enable Pod Monitor:       true\r\n  Postgres GID:               26\r\n  Postgres UID:               26\r\n  Postgresql:\r\n    Parameters:\r\n      archive_mode:                on\r\n      archive_timeout:             5min\r\n      dynamic_shared_memory_type:  posix\r\n      log_destination:             csvlog\r\n      log_directory:               /controller/log\r\n      log_filename:                postgres\r\n      log_rotation_age:            0\r\n      log_rotation_size:           0\r\n      log_truncate_on_rotation:    false\r\n      logging_collector:           on\r\n      max_parallel_workers:        32\r\n      max_replication_slots:       32\r\n      max_worker_processes:        32\r\n      shared_memory_type:          mmap\r\n      shared_preload_libraries:\r\n      ssl_max_protocol_version:    TLSv1.3\r\n      ssl_min_protocol_version:    TLSv1.3\r\n      wal_keep_size:               512MB\r\n      wal_level:                   logical\r\n      wal_log_hints:               on\r\n      wal_receiver_timeout:        5s\r\n      wal_sender_timeout:          5s\r\n    Sync Replica Election Constraint:\r\n      Enabled:              false\r\n  Primary Update Method:    restart\r\n  Primary Update Strategy:  unsupervised\r\n  Replication Slots:\r\n    High Availability:\r\n      Enabled:      true\r\n      Slot Prefix:  _cnpg_\r\n    Synchronize Replicas:\r\n      Enabled:        true\r\n    Update Interval:  30\r\n  Resources:\r\n  Smart Shutdown Timeout:  180\r\n  Start Delay:             3600\r\n  Stop Delay:              1800\r\n  Storage:\r\n    Resize In Use Volumes:  true\r\n    Size:                   10Gi\r\n    Storage Class:          longhorn\r\n  Switchover Delay:         3600\r\nStatus:\r\n  Available Architectures:\r\n    Go Arch:  amd64\r\n    Hash:     f20cbc18bb03eafc1b02c90a8872b8e7a199e63196e6ce546029ea2a503bb883\r\n    Go Arch:  arm64\r\n    Hash:     e28f9109055832cc1fa1ece7866547f267b43e16c800d7b4335cab24ad5420d4\r\n  Certificates:\r\n    Client CA Secret:  grafana-pg-cluster-ca\r\n    Expirations:\r\n      Grafana - Pg - Cluster - Ca:           2024-11-13 17:10:43 +0000 UTC\r\n      Grafana - Pg - Cluster - Replication:  2024-11-13 17:10:43 +0000 UTC\r\n      Grafana - Pg - Cluster - Server:       2024-11-20 15:51:27 +0000 UTC\r\n    Replication TLS Secret:                  grafana-pg-cluster-replication\r\n    Server Alt DNS Names:\r\n      grafana-pg-cluster-rw\r\n      grafana-pg-cluster-rw.database\r\n      grafana-pg-cluster-rw.database.svc\r\n      grafana-pg-cluster-rw.database.svc.cluster.local\r\n      grafana-pg-cluster-r\r\n      grafana-pg-cluster-r.database\r\n      grafana-pg-cluster-r.database.svc\r\n      grafana-pg-cluster-r.database.svc.cluster.local\r\n      grafana-pg-cluster-ro\r\n      grafana-pg-cluster-ro.database\r\n      grafana-pg-cluster-ro.database.svc\r\n      grafana-pg-cluster-ro.database.svc.cluster.local\r\n    Server CA Secret:             grafana-pg-cluster-ca\r\n    Server TLS Secret:            grafana-pg-cluster-server\r\n  Cloud Native PG Commit Hash:    5fe5bb6b\r\n  Cloud Native PG Operator Hash:  f20cbc18bb03eafc1b02c90a8872b8e7a199e63196e6ce546029ea2a503bb883\r\n  Conditions:\r\n    Last Transition Time:  2024-09-14T20:24:26Z\r\n    Message:               Cluster is Ready\r\n    Reason:                ClusterIsReady\r\n    Status:                True\r\n    Type:                  Ready\r\n    Last Transition Time:  2024-09-14T20:23:25Z\r\n    Message:               Continuous archiving is working\r\n    Reason:                ContinuousArchivingSuccess\r\n    Status:                True\r\n    Type:                  ContinuousArchiving\r\n    Last Transition Time:  2024-09-15T00:00:38Z\r\n    Message:               Backup was successful\r\n    Reason:                LastBackupSucceeded\r\n    Status:                True\r\n    Type:                  LastBackupSucceeded\r\n  Config Map Resource Version:\r\n    Metrics:\r\n      Cnpg - Default - Monitoring:  102827884\r\n  Current Primary:                  grafana-pg-cluster-2\r\n  Current Primary Timestamp:        2024-09-14T20:23:21.729277Z\r\n  First Recoverability Point:       2024-09-08T00:00:05Z\r\n  First Recoverability Point By Method:\r\n    Barman Object Store:  2024-09-08T00:00:05Z\r\n  Healthy PVC:\r\n    grafana-pg-cluster-1\r\n    grafana-pg-cluster-2\r\n    grafana-pg-cluster-3\r\n  Image:  ghcr.io/cloudnative-pg/postgresql:16.3\r\n  Instance Names:\r\n    grafana-pg-cluster-1\r\n    grafana-pg-cluster-2\r\n    grafana-pg-cluster-3\r\n  Instances:  3\r\n  Instances Reported State:\r\n    grafana-pg-cluster-1:\r\n      Is Primary:    false\r\n      Time Line ID:  2\r\n    grafana-pg-cluster-2:\r\n      Is Primary:    true\r\n      Time Line ID:  2\r\n    grafana-pg-cluster-3:\r\n      Is Primary:    false\r\n      Time Line ID:  2\r\n  Instances Status:\r\n    Healthy:\r\n      grafana-pg-cluster-1\r\n      grafana-pg-cluster-2\r\n      grafana-pg-cluster-3\r\n  Last Successful Backup:  2024-09-15T00:00:32Z\r\n  Last Successful Backup By Method:\r\n    Barman Object Store:  2024-09-15T00:00:32Z\r\n  Latest Generated Node:  3\r\n  Managed Roles Status:\r\n  Phase:  Cluster in healthy state\r\n  Pooler Integrations:\r\n    Pg Bouncer Integration:\r\n  Pvc Count:        3\r\n  Read Service:     grafana-pg-cluster-r\r\n  Ready Instances:  3\r\n  Secrets Resource Version:\r\n    Application Secret Version:  102828249\r\n    Client Ca Secret Version:    97389628\r\n    Replication Secret Version:  97389630\r\n    Server Ca Secret Version:    97389628\r\n    Server Secret Version:       102828239\r\n    Superuser Secret Version:    102828241\r\n  Switch Replica Cluster Status:\r\n  Target Primary:            grafana-pg-cluster-2\r\n  Target Primary Timestamp:  2024-09-14T20:23:13.949561Z\r\n  Timeline ID:               2\r\n  Topology:\r\n    Instances:\r\n      grafana-pg-cluster-1:\r\n      grafana-pg-cluster-2:\r\n      grafana-pg-cluster-3:\r\n    Nodes Used:              3\r\n    Successfully Extracted:  true\r\n  Write Service:             grafana-pg-cluster-rw\r\nEvents:                      <none>\n```\n### Relevant log output\n```shell\nkubectl cnpg status grafana-pg-cluster -n database --verbose\r\nCluster Summary\r\nName:                grafana-pg-cluster\r\nNamespace:           database\r\nPostgreSQL Image:    ghcr.io/cloudnative-pg/postgresql:16.3\r\nPrimary instance:    grafana-pg-cluster-2\r\nPrimary start time:  2024-09-14 20:23:22 +0000 UTC (uptime 16h33m17s)\r\nStatus:              Cluster in healthy state\r\nInstances:           3\r\nReady instances:     3\r\nPostgreSQL Configuration\r\narchive_command = '/controller/manager wal-archive --log-destination /controller/log/postgres.json %p'\r\narchive_mode = 'on'\r\narchive_timeout = '5min'\r\ncluster_name = 'grafana-pg-cluster'\r\ndynamic_shared_memory_type = 'posix'\r\nfull_page_writes = 'on'\r\nhot_standby = 'true'\r\nlisten_addresses = '*'\r\nlog_destination = 'csvlog'\r\nlog_directory = '/controller/log'\r\nlog_filename = 'postgres'\r\nlog_rotation_age = '0'\r\nlog_rotation_size = '0'\r\nlog_truncate_on_rotation = 'false'\r\nlogging_collector = 'on'\r\nmax_parallel_workers = '32'\r\nmax_replication_slots = '32'\r\nmax_worker_processes = '32'\r\nport = '5432'\r\nrestart_after_crash = 'false'\r\nshared_memory_type = 'mmap'\r\nshared_preload_libraries = ''\r\nssl = 'on'\r\nssl_ca_file = '/controller/certificates/client-ca.crt'\r\nssl_cert_file = '/controller/certificates/server.crt'\r\nssl_key_file = '/controller/certificates/server.key'\r\nssl_max_protocol_version = 'TLSv1.3'\r\nssl_min_protocol_version = 'TLSv1.3'\r\nunix_socket_directories = '/controller/run'\r\nwal_keep_size = '512MB'\r\nwal_level = 'logical'\r\nwal_log_hints = 'on'\r\nwal_receiver_timeout = '5s'\r\nwal_sender_timeout = '5s'\r\ncnpg.config_sha256 = '130bd169023b987f03a9f20d8aa0989a7ef6397473f6d9f3c31915f83aa3690d'\r\nPostgreSQL HBA Rules\r\n#\r\n# FIXED RULES\r\n#\r\n# Grant local access ('local' user map)\r\nlocal all all peer map=local\r\n# Require client certificate authentication for the streaming_replica user\r\nhostssl postgres streaming_replica all cert\r\nhostssl replication streaming_replica all cert\r\nhostssl all cnpg_pooler_pgbouncer all cert\r\n#\r\n# USER-DEFINED RULES\r\n#\r\n#\r\n# DEFAULT RULES\r\n#\r\nhost all all all scram-sha-256\r\nCertificates Status\r\nCertificate Name                Expiration Date                Days Left Until Expiration\r\n----------------                ---------------                --------------------------\r\ngrafana-pg-cluster-ca           2024-11-13 17:10:43 +0000 UTC  59.18\r\ngrafana-pg-cluster-replication  2024-11-13 17:10:43 +0000 UTC  59.18\r\ngrafana-pg-cluster-server       2024-11-20 15:51:27 +0000 UTC  66.12\r\nContinuous Backup status\r\nFirst Point of Recoverability:  2024-09-08T00:00:05Z\r\nNo Primary instance found\r\nPhysical backups\r\nPrimary instance not found\r\nStreaming Replication status\r\nPrimary instance not found\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nManaged roles status\r\nNo roles managed\r\nTablespaces status\r\nNo managed tablespaces\r\nPod Disruption Budgets status\r\nName                        Role  Expected Pods  Current Healthy  Minimum Desired Healthy  Disruptions Allowed\r\n----                        ----  -------------  ---------------  -----------------------  -------------------\r\ngrafana-pg-cluster                2              2                1                        1\r\ngrafana-pg-cluster-primary        1              1                1                        0\r\nInstances status\r\nName                  Database Size  Current LSN  Replication role  Status      QoS         Manager Version  Node\r\n----                  -------------  -----------  ----------------  ------      ---         ---------------  ----\r\ngrafana-pg-cluster-1  -              -            -                 BadRequest  BestEffort  -                k3s-node-01\r\ngrafana-pg-cluster-2  -              -            -                 BadRequest  BestEffort  -                k3s-node-03\r\ngrafana-pg-cluster-3  -              -            -                 BadRequest  BestEffort  -                k3s-node-02\r\nError(s) extracting status\r\n-----------------------------------\r\nthe server rejected our request for an unknown reason (get pods http:grafana-pg-cluster-1:8000)\r\nthe server rejected our request for an unknown reason (get pods http:grafana-pg-cluster-2:8000)\r\nthe server rejected our request for an unknown reason (get pods http:grafana-pg-cluster-3:8000)\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductSame problem here, after upgrading to `1.24` the `status` command seems to fail retrieving the instances status, outputting the same useless information. \r\nIt works properly **up to** `v1.23.2`, higher versions don't work\n---\nSame issue using 1.24 with minikube\r\n$ kubectl-cnpg status cluster-pruebas-cnpg -n demo                  \r\nCluster Summary\r\nName:                cluster-pruebas-cnpg\r\nNamespace:           demo\r\nPostgreSQL Image:    [ghcr.io/cloudnative-pg/postgresql:15.6](http://ghcr.io/cloudnative-pg/postgresql:15.6)\r\nPrimary instance:    cluster-pruebas-cnpg-1\r\nPrimary start time:  2024-10-18 09:26:51 +0000 UTC (uptime 38m25s)\r\nStatus:              Cluster in healthy state  \r\nInstances:           3\r\nReady instances:     3\r\nCertificates Status\r\nCertificate Name                  Expiration Date                Days Left Until Expiration\r\n----------------                  ---------------                --------------------------\r\ncluster-pruebas-cnpg-ca           2025-01-16 09:21:44 +0000 UTC  89.97\r\ncluster-pruebas-cnpg-replication  2025-01-16 09:21:44 +0000 UTC  89.97\r\ncluster-pruebas-cnpg-server       2025-01-16 09:21:44 +0000 UTC  89.97\r\nContinuous Backup status\r\nNot configured\r\nPhysical backups\r\nPrimary instance not found\r\nStreaming Replication status\r\nPrimary instance not found\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nManaged roles status\r\nNo roles managed\r\nTablespaces status\r\nNo managed tablespaces\r\nPod Disruption Budgets status\r\nName                          Role  Expected Pods  Current Healthy  Minimum Desired Healthy  Disruptions Allowed\r\n----                          ----  -------------  ---------------  -----------------------  -------------------\r\ncluster-pruebas-cnpg                2              2                1                        1\r\ncluster-pruebas-cnpg-primary        1              1                1                        0\r\nInstances status\r\nName                    Database Size  Current LSN  Replication role  Status      QoS        Manager Version  Node\r\n----                    -------------  -----------  ----------------  ------      ---        ---------------  ----\r\ncluster-pruebas-cnpg-1  -              -            -                 BadRequest  Burstable  -                minikube\r\ncluster-pruebas-cnpg-2  -              -            -                 BadRequest  Burstable  -                minikube\r\ncluster-pruebas-cnpg-3  -              -            -                 BadRequest  Burstable  -                minikube\r\nError(s) extracting status\r\n-----------------------------------\r\nthe server rejected our request for an unknown reason (get pods http:cluster-pruebas-cnpg-1:8000)\r\nthe server rejected our request for an unknown reason (get pods http:cluster-pruebas-cnpg-2:8000)\r\nthe server rejected our request for an unknown reason (get pods http:cluster-pruebas-cnpg-3:8000)\r\nif we downgrade to 1.23.4 , the get of status works properly.\n---\nJust a sanity check for this issue - have you upgraded the cnpg plugin?\nEg. via `kubectl krew upgrade`\nI saw some bad request output before I upgraded the plugin to match the version running in my cluster.\n---\nHello,\nI confirm that we had the same issue starting 1.23.3 until latest (1.24.1) version.\nFrom a cluster far far away it's taking close to 5min to report the status and it's not able to get the pod state.\n---\nHi,\nHaving same issue, latest working version 1.23.2 for us"
    },
    {
        "title": "[Bug]: Postgres unavailability caused by scheduled backup failures",
        "id": 2524513407,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nHelm\n### What happened?\nWe have a few Postgres clusters managed by the CloudNativePG operator. These are configured with a nightly scheduled backup using volume snapshots.\r\nWe had a misconfiguration where the volume snapshot class used for the scheduled backups was not available on the cluster, and it caused an intermittent problem where memory usage of Postgres pods appeared to leak, eventually causing unavailability. \r\nExample showing how this issue presents:\r\n![image](https://github.com/user-attachments/assets/afae78b1-1c24-49f1-ad9f-92d5f75f7dca)\r\nRestarting the pod or fixing the misconfiguration by using a valid volume snapshot class solves this issue but it is not desirable behaviour for failed backups to cause unavailability; Postgres should continue running normally when backups fail.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: openfga-postgres\r\n  namespace: monitoring\r\n  annotations:\r\n    argocd.argoproj.io/sync-wave: \"-2\"\r\nspec:\r\n  backup:\r\n    volumeSnapshot:\r\n      className: missing-snapshot-class\r\n  imageCatalogRef:\r\n    apiGroup: postgresql.cnpg.io\r\n    kind: ClusterImageCatalog\r\n    major: 16\r\n    name: postgresql\r\n  imagePullSecrets:\r\n  - name: image-pull-secret\r\n  instances: 3\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  resources:\r\n    limits:\r\n      memory: 256Mi\r\n    requests:\r\n      cpu: 150m\r\n      memory: 256Mi\r\n  storage:\r\n    size: 1Gi\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nHelm\n### What happened?\nWe have a few Postgres clusters managed by the CloudNativePG operator. These are configured with a nightly scheduled backup using volume snapshots.\r\nWe had a misconfiguration where the volume snapshot class used for the scheduled backups was not available on the cluster, and it caused an intermittent problem where memory usage of Postgres pods appeared to leak, eventually causing unavailability. \r\nExample showing how this issue presents:\r\n![image](https://github.com/user-attachments/assets/afae78b1-1c24-49f1-ad9f-92d5f75f7dca)\r\nRestarting the pod or fixing the misconfiguration by using a valid volume snapshot class solves this issue but it is not desirable behaviour for failed backups to cause unavailability; Postgres should continue running normally when backups fail.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: openfga-postgres\r\n  namespace: monitoring\r\n  annotations:\r\n    argocd.argoproj.io/sync-wave: \"-2\"\r\nspec:\r\n  backup:\r\n    volumeSnapshot:\r\n      className: missing-snapshot-class\r\n  imageCatalogRef:\r\n    apiGroup: postgresql.cnpg.io\r\n    kind: ClusterImageCatalog\r\n    major: 16\r\n    name: postgresql\r\n  imagePullSecrets:\r\n  - name: image-pull-secret\r\n  instances: 3\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  resources:\r\n    limits:\r\n      memory: 256Mi\r\n    requests:\r\n      cpu: 150m\r\n      memory: 256Mi\r\n  storage:\r\n    size: 1Gi\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: Timescaledb image can't be parsed",
        "id": 2520428832,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\napavarnitsyn@gmail.com\n### Version\nolder in 1.23.x\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nHelm\n### What happened?\nI've tried to setup cnpg-operator and cluster with the latest helm charts on Flux.\r\nI tried all default values and I've got an error: \r\n`Helm upgrade failed: execution error at (cluster/templates/cluster.yaml:16:16): You need to provide your own cluster.imageName as an official timescaledb image doesn't exist yet.`\r\nSo I tried to setup different `cluster.imageName` values to deploy the cluster but it failed all the time. I used these [docker images](https://hub.docker.com/r/timescale/timescaledb-ha/tags?page=1&page_size=&name=&ordering=) with different tags:\r\n- timescale/timescaledb-ha:pg16.4-ts2.16.1\r\n- timescale/timescaledb-ha:pg16\r\nAnd helm-controller failed with the error: `Helm install failed: admission webhook \\\"vcluster.cnpg.io\\\" denied the request: Cluster.postgresql.cnpg.io \\\"timescaledb-cluster\\\" is invalid: spec.imageName: Invalid value: \\\"timescale/timescaledb-ha:pg16.4-ts2.16.1\\\": invalid version tag\"`\r\nI've also tried another  [docker images](https://hub.docker.com/r/timescale/timescaledb/tags?page=1&page_size=&name=&ordering=) with another tag:\r\n- timescale/timescaledb:2.16.1-pg16\r\nBut the configuration validation also failed with the error: `\"Helm install failed: admission webhook \\\"vcluster.cnpg.io\\\" denied the request: Cluster.postgresql.cnpg.io \\\"timescaledb-cluster\\\" is invalid: [spec.imageName: Invalid value: \\\"timescale/timescaledb:2.16.1-pg16\\\": Unsupported PostgreSQL version. Versions 11 or newer are supported, spec.replicationSlots.highAvailability.enabled: Invalid value: true: Cannot enable HA replication slots synchronization. PostgreSQL 11 or above required]\"`\r\nAs far as I can see there is a **validateImageName()** function in `cloudnative-pg/api/v1/cluster_webhook.go` validates the image name but it's not working with timescaleDB images. Probably we can add a new validation for tags like `pg16` to get major PostgreSQL version from it. \r\n### Cluster resource\n```shell\napiVersion: helm.toolkit.fluxcd.io/v2beta1\r\nkind: HelmRelease\r\nmetadata:\r\n  name: timescaledb\r\n  namespace: timescaledb\r\nspec:\r\n  releaseName: timescaledb\r\n  chart:\r\n    spec:\r\n      chart: cluster\r\n      sourceRef:\r\n        kind: HelmRepository\r\n        name: cnpg\r\n        namespace: flux-system\r\n      version: \"0.0.11\"\r\n  interval: 10m0s\r\n  values:\r\n    type: timescaledb\n```\n### Relevant log output\n```shell\nHelm install failed: admission webhook \\\"vcluster.cnpg.io\\\" denied the request: Cluster.postgresql.cnpg.io \\\"timescaledb-cluster\\\" is invalid: spec.imageName: Invalid value: \\\"timescale/timescaledb-ha:pg16.4-ts2.16.1\\\": invalid version tag\r\nHelm install failed: admission webhook \\\"vcluster.cnpg.io\\\" denied the request: Cluster.postgresql.cnpg.io \\\"timescaledb-cluster\\\" is invalid: [spec.imageName: Invalid value: \\\"timescale/timescaledb:2.16.1-pg16\\\": Unsupported PostgreSQL version. Versions 11 or newer are supported, spec.replicationSlots.highAvailability.enabled: Invalid value: true: Cannot enable HA replication slots synchronization. PostgreSQL 11 or above required]\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\napavarnitsyn@gmail.com\n### Version\nolder in 1.23.x\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nHelm\n### What happened?\nI've tried to setup cnpg-operator and cluster with the latest helm charts on Flux.\r\nI tried all default values and I've got an error: \r\n`Helm upgrade failed: execution error at (cluster/templates/cluster.yaml:16:16): You need to provide your own cluster.imageName as an official timescaledb image doesn't exist yet.`\r\nSo I tried to setup different `cluster.imageName` values to deploy the cluster but it failed all the time. I used these [docker images](https://hub.docker.com/r/timescale/timescaledb-ha/tags?page=1&page_size=&name=&ordering=) with different tags:\r\n- timescale/timescaledb-ha:pg16.4-ts2.16.1\r\n- timescale/timescaledb-ha:pg16\r\nAnd helm-controller failed with the error: `Helm install failed: admission webhook \\\"vcluster.cnpg.io\\\" denied the request: Cluster.postgresql.cnpg.io \\\"timescaledb-cluster\\\" is invalid: spec.imageName: Invalid value: \\\"timescale/timescaledb-ha:pg16.4-ts2.16.1\\\": invalid version tag\"`\r\nI've also tried another  [docker images](https://hub.docker.com/r/timescale/timescaledb/tags?page=1&page_size=&name=&ordering=) with another tag:\r\n- timescale/timescaledb:2.16.1-pg16\r\nBut the configuration validation also failed with the error: `\"Helm install failed: admission webhook \\\"vcluster.cnpg.io\\\" denied the request: Cluster.postgresql.cnpg.io \\\"timescaledb-cluster\\\" is invalid: [spec.imageName: Invalid value: \\\"timescale/timescaledb:2.16.1-pg16\\\": Unsupported PostgreSQL version. Versions 11 or newer are supported, spec.replicationSlots.highAvailability.enabled: Invalid value: true: Cannot enable HA replication slots synchronization. PostgreSQL 11 or above required]\"`\r\nAs far as I can see there is a **validateImageName()** function in `cloudnative-pg/api/v1/cluster_webhook.go` validates the image name but it's not working with timescaleDB images. Probably we can add a new validation for tags like `pg16` to get major PostgreSQL version from it. \r\n### Cluster resource\n```shell\napiVersion: helm.toolkit.fluxcd.io/v2beta1\r\nkind: HelmRelease\r\nmetadata:\r\n  name: timescaledb\r\n  namespace: timescaledb\r\nspec:\r\n  releaseName: timescaledb\r\n  chart:\r\n    spec:\r\n      chart: cluster\r\n      sourceRef:\r\n        kind: HelmRepository\r\n        name: cnpg\r\n        namespace: flux-system\r\n      version: \"0.0.11\"\r\n  interval: 10m0s\r\n  values:\r\n    type: timescaledb\n```\n### Relevant log output\n```shell\nHelm install failed: admission webhook \\\"vcluster.cnpg.io\\\" denied the request: Cluster.postgresql.cnpg.io \\\"timescaledb-cluster\\\" is invalid: spec.imageName: Invalid value: \\\"timescale/timescaledb-ha:pg16.4-ts2.16.1\\\": invalid version tag\r\nHelm install failed: admission webhook \\\"vcluster.cnpg.io\\\" denied the request: Cluster.postgresql.cnpg.io \\\"timescaledb-cluster\\\" is invalid: [spec.imageName: Invalid value: \\\"timescale/timescaledb:2.16.1-pg16\\\": Unsupported PostgreSQL version. Versions 11 or newer are supported, spec.replicationSlots.highAvailability.enabled: Invalid value: true: Cannot enable HA replication slots synchronization. PostgreSQL 11 or above required]\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductThat's expected, because the tag of timescaledb images doesn't comply with https://cloudnative-pg.io/documentation/current/container_images/#image-tag-requirements.\r\nHowever you can use https://cloudnative-pg.io/documentation/current/image_catalog/, which allows you to use any custom image tag.\n---\nThis issue is resolved in the Cluster Helm chart by specifying an `ImageCatalog`. You can take a look at the implementation there, or you can simply use it instead.\nhttps://github.com/cloudnative-pg/charts/blob/main/charts/cluster/templates/image-catalog-timescaledb-ha.yaml"
    },
    {
        "title": "fix: primary is not switchover when adding wal storage",
        "id": 2516672782,
        "state": "open",
        "first": "Under `primaryUpdateMethod=switchover` and add wal volume in cluster, \r\nafter standby is restarted, the old primary pod is not doing a switchover but \r\nrestart itself with primary role not changed.  The old primary should switchover first\r\nthen let podSpec decide if a recreate pod is needed. \r\ncloses: #5484",
        "messages": "Under `primaryUpdateMethod=switchover` and add wal volume in cluster, \r\nafter standby is restarted, the old primary pod is not doing a switchover but \r\nrestart itself with primary role not changed.  The old primary should switchover first\r\nthen let podSpec decide if a recreate pod is needed. \r\ncloses: #5484/test limit=local\n---\nThe logic for the upgrade is more complicated than it needs to be. Not the fault of this PR, but this PR adds more complexity. Had a chat with @mnencia  about the special logic for upgrades, given the Tablespaces feature.\r\nAnd his idea was that the tablespace controller should simply not do the `CREATE TABLESPACE` until all the instance pods have the required mount.\r\nIf we had that, the tablespaces could follow the same logic as the WAL vol, and the upgrade method would not matter.\r\nI've opened https://github.com/cloudnative-pg/cloudnative-pg/issues/5564\r\nfor that.\r\nTao, can the present PR wait? Or better to fix, and eventually simplify once #5564 is done?\n---\n> The logic for the upgrade is more complicated than it needs to be. Not the fault of this PR, but this PR adds more complexity. Had a chat with @mnencia about the special logic for upgrades, given the Tablespaces feature. And his idea was that the tablespace controller should simply not do the `CREATE TABLESPACE` until all the instance pods have the required mount. If we had that, the tablespaces could follow the same logic as the WAL vol, and the upgrade method would not matter.\r\n> \r\n> I've opened #5564 for that. Tao, can the present PR wait? Or better to fix, and eventually simplify once #5564 is done?\r\nHello @jsilvela \r\nMount first is a good idea. I think this PR can wait, it is a issue found internal and we just need an explaination why expected switchover is not happened. Let use the alternate way.\n---\nI just hit this today. Curious what the correct behavior should be. I wasn't expecting downtime."
    },
    {
        "title": "[Bug]: Pooler doesn't support managed services",
        "id": 2516644027,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nThe pooler doesn't support the managed services feature, causing the pooler deployment to fail in specific scenarios. For example:\n- pooler is of type ro\n- the cluster has ro default service disabled\n- the cluster uses a managed service ro implementation\n- the pooler wrongly points to the default ro service. \nThe root cause is the template config of the pooler that assumes that the default services are used.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nThe pooler doesn't support the managed services feature, causing the pooler deployment to fail in specific scenarios. For example:\n- pooler is of type ro\n- the cluster has ro default service disabled\n- the cluster uses a managed service ro implementation\n- the pooler wrongly points to the default ro service. \nThe root cause is the template config of the pooler that assumes that the default services are used.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: primary pod is not switchover when add new wal volume",
        "id": 2516606605,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nwhen primaryUpdateMethod=switchover is set, update the cluster yaml file and add a wal storage, noticed that primary pod is doing restart instead of switchover \n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nwhen primaryUpdateMethod=switchover is set, update the cluster yaml file and add a wal storage, noticed that primary pod is doing restart instead of switchover \n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Support \"-r\" backend for Pooler (in addition to \"-ro\" and \"-rw\" currently supported)",
        "id": 2513799027,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIdeally PgBouncer should support all available database backend configurations. Including directing traffic to any of the read capable databases (identified with \"-r\" prefix). Currently user is presented with the following error:\r\n```\r\nspec.type: Unsupported value: \"r\": supported values: \"rw\", \"ro\"\r\n```\n### Describe the solution you'd like\nAll database services can be selected as backend for pooler: **rw**, **ro** and **r**\n### Describe alternatives you've considered\none can always run and configure PgBouncer manually but ideally we don't have to\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIdeally PgBouncer should support all available database backend configurations. Including directing traffic to any of the read capable databases (identified with \"-r\" prefix). Currently user is presented with the following error:\r\n```\r\nspec.type: Unsupported value: \"r\": supported values: \"rw\", \"ro\"\r\n```\n### Describe the solution you'd like\nAll database services can be selected as backend for pooler: **rw**, **ro** and **r**\n### Describe alternatives you've considered\none can always run and configure PgBouncer manually but ideally we don't have to\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Support \"statement\" mode in Pooler",
        "id": 2513787548,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently one can only create pooler in session or transaction mode. Anything else will give the following error:\r\n```\r\nspec.pgbouncer.poolMode: Unsupported value: \"statement\": supported values: \"session\", \"transaction\"\r\n```\r\n\"statement\" pool mode is missing\n### Describe the solution you'd like\nAll PgBouncer pool modes are supported as in https://www.pgbouncer.org/config.html\n### Describe alternatives you've considered\none can always run and configure PgBouncer manually but ideally we don't have to\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently one can only create pooler in session or transaction mode. Anything else will give the following error:\r\n```\r\nspec.pgbouncer.poolMode: Unsupported value: \"statement\": supported values: \"session\", \"transaction\"\r\n```\r\n\"statement\" pool mode is missing\n### Describe the solution you'd like\nAll PgBouncer pool modes are supported as in https://www.pgbouncer.org/config.html\n### Describe alternatives you've considered\none can always run and configure PgBouncer manually but ideally we don't have to\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: shouldn't require access to cluster-wide secrets ",
        "id": 2506616733,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently the manager requires a role that grants cluster-wide secret access. \r\nIf you don't grant the operator `list` on all secrets, you get the error:\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-09-05T02:02:16Z\",\"msg\":\"pkg/mod/k8s.io/client-go@v0.30.3/tools/cache/reflector.go:232: failed to list *v1.Secret: secrets is forbidden: User \\\"system:serviceaccount:cloudnative-pg:manager\\\" cannot list resource \\\"secrets\\\" in API group \\\"\\\" at the cluster scope\"}\r\n{\"level\":\"error\",\"ts\":\"2024-09-05T02:02:16Z\",\"msg\":\"pkg/mod/k8s.io/client-go@v0.30.3/tools/cache/reflector.go:232: Failed to watch *v1.Secret: failed to list *v1.Secret: secrets is forbidden: User \\\"system:serviceaccount:cloudnative-pg:manager\\\" cannot list resource \\\"secrets\\\" in API group \\\"\\\" at the cluster scope\",\"stacktrace\":\"k8s.io/client-go/tools/cache.DefaultWatchErrorHandler\\n\\tpkg/mod/k8s.io/client-go@v0.30.3/tools/cache/reflector.go:150\\nk8s.io/client-go/tools/cache.(*Reflector).Run.func1\\n\\tpkg/mod/k8s.io/client-go@v0.30.3/tools/cache/reflector.go:299\\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/backoff.go:226\\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/backoff.go:227\\nk8s.io/client-go/tools/cache.(*Reflector).Run\\n\\tpkg/mod/k8s.io/client-go@v0.30.3/tools/cache/reflector.go:297\\nk8s.io/client-go/tools/cache.(*controller).Run.(*Group).StartWithChannel.func2\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/wait.go:55\\nk8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/wait.go:72\"}\r\n```\r\nIn `kube-system` (and other namespaces) I have some very sensitive secrets that I would *not* like the cloudnative-pg operator to be able to access.\r\nNote that `list` on secrets exposes the contents of the secret.\n### Describe the solution you'd like\nI should only have to grant access to secrets in the namespaces where I'm going to run clusters.\r\n### Describe alternatives you've considered\nN/A\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently the manager requires a role that grants cluster-wide secret access. \r\nIf you don't grant the operator `list` on all secrets, you get the error:\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-09-05T02:02:16Z\",\"msg\":\"pkg/mod/k8s.io/client-go@v0.30.3/tools/cache/reflector.go:232: failed to list *v1.Secret: secrets is forbidden: User \\\"system:serviceaccount:cloudnative-pg:manager\\\" cannot list resource \\\"secrets\\\" in API group \\\"\\\" at the cluster scope\"}\r\n{\"level\":\"error\",\"ts\":\"2024-09-05T02:02:16Z\",\"msg\":\"pkg/mod/k8s.io/client-go@v0.30.3/tools/cache/reflector.go:232: Failed to watch *v1.Secret: failed to list *v1.Secret: secrets is forbidden: User \\\"system:serviceaccount:cloudnative-pg:manager\\\" cannot list resource \\\"secrets\\\" in API group \\\"\\\" at the cluster scope\",\"stacktrace\":\"k8s.io/client-go/tools/cache.DefaultWatchErrorHandler\\n\\tpkg/mod/k8s.io/client-go@v0.30.3/tools/cache/reflector.go:150\\nk8s.io/client-go/tools/cache.(*Reflector).Run.func1\\n\\tpkg/mod/k8s.io/client-go@v0.30.3/tools/cache/reflector.go:299\\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/backoff.go:226\\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/backoff.go:227\\nk8s.io/client-go/tools/cache.(*Reflector).Run\\n\\tpkg/mod/k8s.io/client-go@v0.30.3/tools/cache/reflector.go:297\\nk8s.io/client-go/tools/cache.(*controller).Run.(*Group).StartWithChannel.func2\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/wait.go:55\\nk8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/wait.go:72\"}\r\n```\r\nIn `kube-system` (and other namespaces) I have some very sensitive secrets that I would *not* like the cloudnative-pg operator to be able to access.\r\nNote that `list` on secrets exposes the contents of the secret.\n### Describe the solution you'd like\nI should only have to grant access to secrets in the namespaces where I'm going to run clusters.\r\n### Describe alternatives you've considered\nN/A\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductHello @james-callahan \r\nHave you look into the documentation here https://cloudnative-pg.io/documentation/current/security/#role-based-access-control-rbac that may solve your questions, this is tight on the way the operator is deployed, can you let us know if that clarifies your question?\r\nRegards,\n---\n> Have you look into the documentation here https://cloudnative-pg.io/documentation/current/security/#role-based-access-control-rbac that may solve your questions, this is tight on the way the operator is deployed, can you let us know if that clarifies your question?\r\nNo it doesn't answer the question/address the issue.\n---\nBumping this thread, can we review this design? This is very important for a zero-trust, shared kubernetes cluster.\nIt does not make sense for a DB operator to require access to secrets in kube-system or any namespaces which does not use postgresql.\nFrom this doc: https://cloudnative-pg.io/documentation/current/security/#role-based-access-control-rbac\n>secrets\nUnless you provide certificates and passwords to your Cluster objects, the operator adopts the \"convention over configuration\" paradigm by self-provisioning random generated passwords and TLS certificates, and by storing them in secrets.\nIt implies \"if I provide certs and passwords to my Cluster objects, then the operator does not need access to Secrets\".\nIn that sense, why does cnpg requires cluster-wide access to secrets?"
    },
    {
        "title": "[Bug]: Replica fail to start with status CrashLoopBackOff",
        "id": 2505471268,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\nolder in 1.23.x\n### What version of Kubernetes are you using?\n1.27 (unsupported)\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nThe replica won't able to start. It's showing CrashLoopBackOff\n### Cluster resource\n```shell\nNAME              AGE    INSTANCES   READY   STATUS                                       PRIMARY\r\nsidm-db-cluster   181d   2           1       Waiting for the instances to become active   sidm-db-cluster-2\r\nNAME                                 READY   STATUS             RESTARTS       AGE\r\nsidm-db-cluster-2                    1/1     Running            0              173d\r\nsidm-db-cluster-3                    0/1     CrashLoopBackOff   17 (45s ago)   63m\r\nkubectl describe cluster sidm-db-cluster -n db-operator\r\nName:         sidm-db-cluster\r\nNamespace:    db-operator\r\nLabels:       app.kubernetes.io/managed-by=Helm\r\nAnnotations:  meta.helm.sh/release-name: db-operator\r\n              meta.helm.sh/release-namespace: default\r\nAPI Version:  postgresql.cnpg.io/v1\r\nKind:         Cluster\r\nMetadata:\r\n  Creation Timestamp:  2024-03-07T08:43:25Z\r\n  Generation:          6\r\n  Managed Fields:\r\n    API Version:  postgresql.cnpg.io/v1\r\n    Fields Type:  FieldsV1\r\n    fieldsV1:\r\n      f:metadata:\r\n        f:annotations:\r\n          .:\r\n          f:meta.helm.sh/release-name:\r\n          f:meta.helm.sh/release-namespace:\r\n        f:labels:\r\n          .:\r\n          f:app.kubernetes.io/managed-by:\r\n      f:spec:\r\n        .:\r\n        f:affinity:\r\n          .:\r\n          f:enablePodAntiAffinity:\r\n          f:topologyKey:\r\n        f:backup:\r\n          .:\r\n          f:barmanObjectStore:\r\n            .:\r\n            f:data:\r\n              .:\r\n              f:compression:\r\n              f:encryption:\r\n              f:immediateCheckpoint:\r\n              f:jobs:\r\n            f:destinationPath:\r\n            f:endpointURL:\r\n            f:s3Credentials:\r\n              .:\r\n              f:accessKeyId:\r\n                .:\r\n                f:key:\r\n                f:name:\r\n              f:region:\r\n                .:\r\n                f:key:\r\n                f:name:\r\n              f:secretAccessKey:\r\n                .:\r\n                f:key:\r\n                f:name:\r\n            f:wal:\r\n              .:\r\n              f:compression:\r\n              f:encryption:\r\n              f:maxParallel:\r\n          f:retentionPolicy:\r\n          f:target:\r\n        f:bootstrap:\r\n          .:\r\n          f:pg_basebackup:\r\n            .:\r\n            f:source:\r\n        f:description:\r\n        f:enableSuperuserAccess:\r\n        f:externalClusters:\r\n        f:failoverDelay:\r\n        f:imageName:\r\n        f:imagePullSecrets:\r\n        f:instances:\r\n        f:logLevel:\r\n        f:maxSyncReplicas:\r\n        f:minSyncReplicas:\r\n        f:nodeMaintenanceWindow:\r\n          .:\r\n          f:inProgress:\r\n          f:reusePVC:\r\n        f:postgresGID:\r\n        f:postgresUID:\r\n        f:postgresql:\r\n          .:\r\n          f:parameters:\r\n            .:\r\n            f:auto_explain.log_min_duration:\r\n            f:pg_stat_statements.max:\r\n            f:pg_stat_statements.track:\r\n            f:shared_buffers:\r\n          f:pg_hba:\r\n        f:primaryUpdateMethod:\r\n        f:primaryUpdateStrategy:\r\n        f:replica:\r\n          .:\r\n          f:enabled:\r\n          f:source:\r\n        f:replicationSlots:\r\n          .:\r\n          f:highAvailability:\r\n            .:\r\n            f:enabled:\r\n            f:slotPrefix:\r\n          f:updateInterval:\r\n        f:resources:\r\n          .:\r\n          f:limits:\r\n            .:\r\n            f:cpu:\r\n            f:memory:\r\n          f:requests:\r\n            .:\r\n            f:cpu:\r\n            f:memory:\r\n        f:smartShutdownTimeout:\r\n        f:startDelay:\r\n        f:stopDelay:\r\n        f:storage:\r\n          .:\r\n          f:resizeInUseVolumes:\r\n          f:size:\r\n          f:storageClass:\r\n        f:superuserSecret:\r\n          .:\r\n          f:name:\r\n        f:switchoverDelay:\r\n        f:walStorage:\r\n          .:\r\n          f:resizeInUseVolumes:\r\n          f:size:\r\n          f:storageClass:\r\n    Manager:      helm\r\n    Operation:    Update\r\n    Time:         2024-03-25T15:43:00Z\r\n    API Version:  postgresql.cnpg.io/v1\r\n    Fields Type:  FieldsV1\r\n    fieldsV1:\r\n      f:spec:\r\n        f:postgresql:\r\n          f:parameters:\r\n            f:wal_level:\r\n            f:wal_log_hints:\r\n        f:replicationSlots:\r\n          f:synchronizeReplicas:\r\n            .:\r\n            f:enabled:\r\n    Manager:      manager\r\n    Operation:    Update\r\n    Time:         2024-05-22T12:48:53Z\r\n    API Version:  postgresql.cnpg.io/v1\r\n    Fields Type:  FieldsV1\r\n    fieldsV1:\r\n      f:status:\r\n        .:\r\n        f:availableArchitectures:\r\n        f:certificates:\r\n          .:\r\n          f:clientCASecret:\r\n          f:expirations:\r\n            .:\r\n            f:sidm-db-cluster-ca:\r\n            f:sidm-db-cluster-replication:\r\n            f:sidm-db-cluster-server:\r\n          f:replicationTLSSecret:\r\n          f:serverAltDNSNames:\r\n          f:serverCASecret:\r\n          f:serverTLSSecret:\r\n        f:cloudNativePGCommitHash:\r\n        f:cloudNativePGOperatorHash:\r\n        f:conditions:\r\n        f:configMapResourceVersion:\r\n          .:\r\n          f:metrics:\r\n            .:\r\n            f:cnpg-default-monitoring:\r\n        f:currentPrimary:\r\n        f:currentPrimaryTimestamp:\r\n        f:firstRecoverabilityPoint:\r\n        f:firstRecoverabilityPointByMethod:\r\n          .:\r\n          f:barmanObjectStore:\r\n        f:healthyPVC:\r\n        f:image:\r\n        f:instanceNames:\r\n        f:instances:\r\n        f:instancesReportedState:\r\n          .:\r\n          f:sidm-db-cluster-2:\r\n            .:\r\n            f:isPrimary:\r\n            f:timeLineID:\r\n          f:sidm-db-cluster-3:\r\n            .:\r\n            f:isPrimary:\r\n        f:instancesStatus:\r\n          .:\r\n          f:healthy:\r\n          f:replicating:\r\n        f:jobCount:\r\n        f:lastSuccessfulBackup:\r\n        f:lastSuccessfulBackupByMethod:\r\n          .:\r\n          f:barmanObjectStore:\r\n        f:latestGeneratedNode:\r\n        f:managedRolesStatus:\r\n        f:phase:\r\n        f:phaseReason:\r\n        f:poolerIntegrations:\r\n          .:\r\n          f:pgBouncerIntegration:\r\n            .:\r\n            f:secrets:\r\n        f:pvcCount:\r\n        f:readService:\r\n        f:readyInstances:\r\n        f:secretsResourceVersion:\r\n          .:\r\n          f:clientCaSecretVersion:\r\n          f:externalClusterSecretVersion:\r\n            .:\r\n            f:cluster-sidm-superuser:\r\n          f:replicationSecretVersion:\r\n          f:serverCaSecretVersion:\r\n          f:serverSecretVersion:\r\n          f:superuserSecretVersion:\r\n        f:switchReplicaClusterStatus:\r\n        f:targetPrimary:\r\n        f:targetPrimaryTimestamp:\r\n        f:topology:\r\n          .:\r\n          f:instances:\r\n            .:\r\n            f:sidm-db-cluster-2:\r\n            f:sidm-db-cluster-3:\r\n          f:nodesUsed:\r\n          f:successfullyExtracted:\r\n        f:writeService:\r\n    Manager:         manager\r\n    Operation:       Update\r\n    Subresource:     status\r\n    Time:            2024-09-04T12:55:00Z\r\n  Resource Version:  186977545\r\n  UID:               b583fcb2-288b-4ac4-a08a-ba3f8054f271\r\nSpec:\r\n  Affinity:\r\n    Enable Pod Anti Affinity:  true\r\n    Pod Anti Affinity Type:    preferred\r\n    Topology Key:              topology.kubernetes.io/zone\r\n  Backup:\r\n    Barman Object Store:\r\n      Data:\r\n        Compression:     gzip\r\n        Encryption:      AES256\r\n        Jobs:            2\r\n      Destination Path:  s3://sidm-se-stage/se-backups\r\n      Endpoint URL:      https://s3.se.teliacompany.net\r\n      s3Credentials:\r\n        Access Key Id:\r\n          Key:   ACCESS_KEY_ID\r\n          Name:  backup-creds\r\n        Region:\r\n          Key:   REGION_KEY\r\n          Name:  backup-creds\r\n        Secret Access Key:\r\n          Key:   ACCESS_SECRET_KEY\r\n          Name:  backup-creds\r\n      Wal:\r\n        Compression:   gzip\r\n        Encryption:    AES256\r\n        Max Parallel:  2\r\n    Retention Policy:  30d\r\n    Target:            prefer-standby\r\n  Bootstrap:\r\n    pg_basebackup:\r\n      Database:             app\r\n      Owner:                app\r\n      Source:               sidm-db-cluster\r\n  Description:              SIDM DB cluster\r\n  Enable PDB:               true\r\n  Enable Superuser Access:  true\r\n  External Clusters:\r\n    Connection Parameters:\r\n      Dbname:  sidm-db-se\r\n      Host:    10.30.177.115\r\n      Port:    9998\r\n      User:    postgres\r\n    Name:      sidm-db-cluster\r\n    Password:\r\n      Key:         password\r\n      Name:        cluster-sidm-superuser\r\n  Failover Delay:  0\r\n  Image Name:      pcoc-remotes-virtual.jfrog.teliacompany.io/cloudnative-pg/postgresql:14.5\r\n  Image Pull Secrets:\r\n    Name:             private-registry-creds\r\n  Instances:          2\r\n  Log Level:          info\r\n  Max Sync Replicas:  1\r\n  Min Sync Replicas:  1\r\n  Monitoring:\r\n    Custom Queries Config Map:\r\n      Key:                    queries\r\n      Name:                   cnpg-default-monitoring\r\n    Disable Default Queries:  false\r\n    Enable Pod Monitor:       false\r\n  Node Maintenance Window:\r\n    In Progress:  false\r\n    Reuse PVC:    false\r\n  Postgres GID:   26\r\n  Postgres UID:   26\r\n  Postgresql:\r\n    Parameters:\r\n      archive_mode:                   always\r\n      archive_timeout:                5min\r\n      auto_explain.log_min_duration:  10s\r\n      dynamic_shared_memory_type:     posix\r\n      log_destination:                csvlog\r\n      log_directory:                  /controller/log\r\n      log_filename:                   postgres\r\n      log_rotation_age:               0\r\n      log_rotation_size:              0\r\n      log_truncate_on_rotation:       false\r\n      logging_collector:              on\r\n      max_parallel_workers:           32\r\n      max_replication_slots:          32\r\n      max_worker_processes:           32\r\n      pg_stat_statements.max:         10000\r\n      pg_stat_statements.track:       all\r\n      shared_buffers:                 256MB\r\n      shared_memory_type:             mmap\r\n      shared_preload_libraries:       \r\n      ssl_max_protocol_version:       TLSv1.3\r\n      ssl_min_protocol_version:       TLSv1.3\r\n      wal_keep_size:                  512MB\r\n      wal_level:                      logical\r\n      wal_log_hints:                  on\r\n      wal_receiver_timeout:           5s\r\n      wal_sender_timeout:             5s\r\n    pg_hba:\r\n      host all all all md5\r\n      local all all trust\r\n      host all all 127.0.0.1/32 md5\r\n      host all all ::1/128 md5\r\n      host all all 10.244.0.0/16 md5\r\n      host replication postgres 0.0.0.0/0 md5\r\n    Sync Replica Election Constraint:\r\n      Enabled:              false\r\n  Primary Update Method:    restart\r\n  Primary Update Strategy:  unsupervised\r\n  Replica:\r\n    Enabled:  true\r\n    Source:   sidm-db-cluster\r\n  Replication Slots:\r\n    High Availability:\r\n      Enabled:      true\r\n      Slot Prefix:  _cnpg_\r\n    Synchronize Replicas:\r\n      Enabled:        true\r\n    Update Interval:  30\r\n  Resources:\r\n    Limits:\r\n      Cpu:     3\r\n      Memory:  2Gi\r\n    Requests:\r\n      Cpu:                 500m\r\n      Memory:              512Mi\r\n  Smart Shutdown Timeout:  180\r\n  Start Delay:             300\r\n  Stop Delay:              300\r\n  Storage:\r\n    Resize In Use Volumes:  true\r\n    Size:                   30Gi\r\n    Storage Class:          sidm-sc\r\n  Superuser Secret:\r\n    Name:            cluster-sidm-superuser\r\n  Switchover Delay:  3600\r\n  Wal Storage:\r\n    Resize In Use Volumes:  true\r\n    Size:                   20Gi\r\n    Storage Class:          sidm-sc\r\nStatus:\r\n  Available Architectures:\r\n    Go Arch:  amd64\r\n    Hash:     144e71b00bdcfc5edafa10055fb0cc4a6efa9f467a8e66826d5e7bb2b254b706\r\n    Go Arch:  arm64\r\n    Hash:     0027f50a9d35e24040cfc2f27cea04cbdf4375c226ac7b42764b5bb91f9beca4\r\n  Certificates:\r\n    Client CA Secret:  sidm-db-cluster-ca\r\n    Expirations:\r\n      Sidm - Db - Cluster - Ca:           2024-11-18 08:28:26 +0000 UTC\r\n      Sidm - Db - Cluster - Replication:  2024-11-18 08:28:26 +0000 UTC\r\n      Sidm - Db - Cluster - Server:       2024-11-18 08:28:26 +0000 UTC\r\n    Replication TLS Secret:               sidm-db-cluster-replication\r\n    Server Alt DNS Names:\r\n      sidm-db-cluster-rw\r\n      sidm-db-cluster-rw.db-operator\r\n      sidm-db-cluster-rw.db-operator.svc\r\n      sidm-db-cluster-r\r\n      sidm-db-cluster-r.db-operator\r\n      sidm-db-cluster-r.db-operator.svc\r\n      sidm-db-cluster-ro\r\n      sidm-db-cluster-ro.db-operator\r\n      sidm-db-cluster-ro.db-operator.svc\r\n    Server CA Secret:             sidm-db-cluster-ca\r\n    Server TLS Secret:            sidm-db-cluster-server\r\n  Cloud Native PG Commit Hash:    2b489ad6\r\n  Cloud Native PG Operator Hash:  144e71b00bdcfc5edafa10055fb0cc4a6efa9f467a8e66826d5e7bb2b254b706\r\n  Conditions:\r\n    Last Transition Time:  2024-09-04T10:14:11Z\r\n    Message:               Cluster Is Not Ready\r\n    Reason:                ClusterIsNotReady\r\n    Status:                False\r\n    Type:                  Ready\r\n    Last Transition Time:  2024-03-25T13:55:28Z\r\n    Message:               Continuous archiving is working\r\n    Reason:                ContinuousArchivingSuccess\r\n    Status:                True\r\n    Type:                  ContinuousArchiving\r\n    Last Transition Time:  2024-03-27T00:00:00Z\r\n    Message:               New Backup starting up\r\n    Reason:                BackupStarted\r\n    Status:                False\r\n    Type:                  LastBackupSucceeded\r\n  Config Map Resource Version:\r\n    Metrics:\r\n      Cnpg - Default - Monitoring:  170510018\r\n  Current Primary:                  sidm-db-cluster-2\r\n  Current Primary Timestamp:        2024-09-04T10:14:11.214135Z\r\n  First Recoverability Point:       2024-03-08T13:49:34Z\r\n  First Recoverability Point By Method:\r\n    Barman Object Store:  2024-03-08T13:49:34Z\r\n  Healthy PVC:\r\n    sidm-db-cluster-2\r\n    sidm-db-cluster-2-wal\r\n    sidm-db-cluster-3\r\n    sidm-db-cluster-3-wal\r\n  Image:  pcoc-remotes-virtual.jfrog.teliacompany.io/cloudnative-pg/postgresql:14.5\r\n  Instance Names:\r\n    sidm-db-cluster-2\r\n    sidm-db-cluster-3\r\n  Instances:  2\r\n  Instances Reported State:\r\n    sidm-db-cluster-2:\r\n      Is Primary:    false\r\n      Time Line ID:  3\r\n    sidm-db-cluster-3:\r\n      Is Primary:  false\r\n  Instances Status:\r\n    Healthy:\r\n      sidm-db-cluster-2\r\n    Replicating:\r\n      sidm-db-cluster-3\r\n  Job Count:               1\r\n  Last Successful Backup:  2024-03-26T00:00:09Z\r\n  Last Successful Backup By Method:\r\n    Barman Object Store:  2024-03-26T00:00:09Z\r\n  Latest Generated Node:  3\r\n  Managed Roles Status:\r\n  Phase:         Waiting for the instances to become active\r\n  Phase Reason:  Some instances are not yet active. Please wait.\r\n  Pooler Integrations:\r\n    Pg Bouncer Integration:\r\n      Secrets:\r\n        sidm-db-cluster-pooler\r\n  Pvc Count:        4\r\n  Read Service:     sidm-db-cluster-r\r\n  Ready Instances:  1\r\n  Secrets Resource Version:\r\n    Client Ca Secret Version:  174529907\r\n    External Cluster Secret Version:\r\n      Cluster - Sidm - Superuser:  38599818\r\n    Replication Secret Version:    174529909\r\n    Server Ca Secret Version:      174529907\r\n    Server Secret Version:         174529908\r\n    Superuser Secret Version:      38599818\r\n  Switch Replica Cluster Status:\r\n  Target Primary:            sidm-db-cluster-2\r\n  Target Primary Timestamp:  2024-09-04T10:14:11.147441Z\r\n  Topology:\r\n    Instances:\r\n      sidm-db-cluster-2:\r\n      sidm-db-cluster-3:\r\n    Nodes Used:              2\r\n    Successfully Extracted:  true\r\n  Write Service:             sidm-db-cluster-rw\r\nEvents:                      <none>\n```\n### Relevant log output\n```shell\nkubectl logs sidm-db-cluster-2 -n db-operator\r\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:52:36Z\",\"logger\":\"Replicator\",\"msg\":\"synchronizing replication slots\",\"logging_pod\":\"sidm-db-cluster-2\",\"err\":\"getting replication slot status from primary: failed to connect to `host=sidm-db-cluster-rw user=streaming_replica database=postgres`: dial error (timeout: dial tcp 172.16.7.0:5432: connect: connection timed out)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:10Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-2\",\"record\":{\"log_time\":\"2024-09-03 06:54:10.757 UTC\",\"process_id\":\"1997795\",\"session_id\":\"66d6b210.1e7be3\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-03 06:52:00 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"08006\",\"message\":\"could not connect to the primary server: connection to server at \\\"sidm-db-cluster-rw\\\" (172.16.7.0), port 5432 failed: Connection timed out\\n\\tIs the server running on that host and accepting TCP/IP connections?\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:11Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000004.history\",\"startTime\":\"2024-09-03T06:54:10Z\",\"endTime\":\"2024-09-03T06:54:11Z\",\"elapsedWalTime\":0.65855262}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:11Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000004.history\",\"maxParallel\":2,\"successfulWalRestore\":1,\"failedWalRestore\":1,\"endOfWALStream\":false,\"startTime\":\"2024-09-03T06:54:10Z\",\"downloadStartTime\":\"2024-09-03T06:54:10Z\",\"downloadTotalTime\":0.658772761,\"totalTime\":0.786042716}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:11Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-2\",\"record\":{\"log_time\":\"2024-09-03 06:54:11.572 UTC\",\"process_id\":\"24\",\"session_id\":\"65f33a43.18\",\"session_line_num\":\"272848\",\"session_start_time\":\"2024-03-14 17:56:19 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000004.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:12Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000005.history\",\"options\":[\"--endpoint-url\",\"https://s3.se.teliacompany.net\",\"--cloud-provider\",\"aws-s3\",\"s3://sidm-se-stage/se-backups\",\"sidm-db-cluster\"],\"startTime\":\"2024-09-03T06:54:11Z\",\"endTime\":\"2024-09-03T06:54:12Z\",\"elapsedWalTime\":0.538944636}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:12Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000004.history\",\"startTime\":\"2024-09-03T06:54:12Z\",\"endTime\":\"2024-09-03T06:54:12Z\",\"elapsedWalTime\":0.506157}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:12Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000004.history\",\"maxParallel\":2,\"successfulWalRestore\":1,\"failedWalRestore\":1,\"endOfWALStream\":false,\"startTime\":\"2024-09-03T06:54:12Z\",\"downloadStartTime\":\"2024-09-03T06:54:12Z\",\"downloadTotalTime\":0.506379513,\"totalTime\":0.601262193}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:12Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-2\",\"record\":{\"log_time\":\"2024-09-03 06:54:12.972 UTC\",\"process_id\":\"24\",\"session_id\":\"65f33a43.18\",\"session_line_num\":\"272849\",\"session_start_time\":\"2024-03-14 17:56:19 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000004.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:12Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-2\",\"record\":{\"log_time\":\"2024-09-03 06:54:12.978 UTC\",\"process_id\":\"24\",\"session_id\":\"65f33a43.18\",\"session_line_num\":\"272850\",\"session_start_time\":\"2024-03-14 17:56:19 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"new timeline 4 forked off current database system timeline 3 before current recovery point 4/650000A0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:13Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"000000030000000400000065\",\"options\":[\"--endpoint-url\",\"https://s3.se.teliacompany.net\",\"--cloud-provider\",\"aws-s3\",\"s3://sidm-se-stage/se-backups\",\"sidm-db-cluster\"],\"startTime\":\"2024-09-03T06:54:13Z\",\"endTime\":\"2024-09-03T06:54:13Z\",\"elapsedWalTime\":0.3590011}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:47Z\",\"logger\":\"Replicator\",\"msg\":\"synchronizing replication slots\",\"logging_pod\":\"sidm-db-cluster-2\",\"err\":\"getting replication slot status from primary: failed to connect to `host=sidm-db-cluster-rw user=streaming_replica database=postgres`: dial error (timeout: dial tcp 172.16.7.0:5432: connect: connection timed out)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-2\",\"record\":{\"log_time\":\"2024-09-03 06:56:23.877 UTC\",\"process_id\":\"1998265\",\"session_id\":\"66d6b295.1e7db9\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-03 06:54:13 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"08006\",\"message\":\"could not connect to the primary server: connection to server at \\\"sidm-db-cluster-rw\\\" (172.16.7.0), port 5432 failed: Connection timed out\\n\\tIs the server running on that host and accepting TCP/IP connections?\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:24Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000004.history\",\"startTime\":\"2024-09-03T06:56:24Z\",\"endTime\":\"2024-09-03T06:56:24Z\",\"elapsedWalTime\":0.61121078}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:24Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000004.history\",\"maxParallel\":2,\"successfulWalRestore\":1,\"failedWalRestore\":1,\"endOfWALStream\":false,\"startTime\":\"2024-09-03T06:56:23Z\",\"downloadStartTime\":\"2024-09-03T06:56:24Z\",\"downloadTotalTime\":0.61142247,\"totalTime\":0.713546688}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:24Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-2\",\"record\":{\"log_time\":\"2024-09-03 06:56:24.616 UTC\",\"process_id\":\"24\",\"session_id\":\"65f33a43.18\",\"session_line_num\":\"272851\",\"session_start_time\":\"2024-03-14 17:56:19 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000004.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:25Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000005.history\",\"options\":[\"--endpoint-url\",\"https://s3.se.teliacompany.net\",\"--cloud-provider\",\"aws-s3\",\"s3://sidm-se-stage/se-backups\",\"sidm-db-cluster\"],\"startTime\":\"2024-09-03T06:56:24Z\",\"endTime\":\"2024-09-03T06:56:25Z\",\"elapsedWalTime\":0.518540386}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:25Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000004.history\",\"startTime\":\"2024-09-03T06:56:25Z\",\"endTime\":\"2024-09-03T06:56:25Z\",\"elapsedWalTime\":0.516822395}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:25Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000004.history\",\"maxParallel\":2,\"successfulWalRestore\":1,\"failedWalRestore\":1,\"endOfWALStream\":false,\"startTime\":\"2024-09-03T06:56:25Z\",\"downloadStartTime\":\"2024-09-03T06:56:25Z\",\"downloadTotalTime\":0.517076456,\"totalTime\":0.612954641}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:25Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-2\",\"record\":{\"log_time\":\"2024-09-03 06:56:25.998 UTC\",\"process_id\":\"24\",\"session_id\":\"65f33a43.18\",\"session_line_num\":\"272852\",\"session_start_time\":\"2024-03-14 17:56:19 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000004.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:26Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-2\",\"record\":{\"log_time\":\"2024-09-03 06:56:26.000 UTC\",\"process_id\":\"24\",\"session_id\":\"65f33a43.18\",\"session_line_num\":\"272853\",\"session_start_time\":\"2024-03-14 17:56:19 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"new timeline 4 forked off current database system timeline 3 before current recovery point 4/650000A0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:26Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"000000030000000400000065\",\"options\":[\"--endpoint-url\",\"https://s3.se.teliacompany.net\",\"--cloud-provider\",\"aws-s3\",\"s3://sidm-se-stage/se-backups\",\"sidm-db-cluster\"],\"startTime\":\"2024-09-03T06:56:26Z\",\"endTime\":\"2024-09-03T06:56:26Z\",\"elapsedWalTime\":0.36545608}\r\nkubectl logs sidm-db-cluster-3 -n db-operator\r\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"sidm-db-cluster-3\",\"version\":\"1.23.3\",\"build\":{\"Version\":\"1.23.3\",\"Commit\":\"2b489ad6\",\"Date\":\"2024-07-29\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"logger\":\"setup\",\"msg\":\"starting tablespace manager\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"logger\":\"setup\",\"msg\":\"starting external server manager\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"sidm-db-cluster-3\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"logger\":\"roles_reconciler\",\"msg\":\"skipping the RoleSynchronizer in replicas\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"sidm-db-cluster-3\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"sidm-db-cluster-3\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"sidm-db-cluster-3\",\"syncReplicas\":0,\"minSyncReplicas\":1,\"maxSyncReplicas\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"sidm-db-cluster-3\",\"syncReplicas\":0,\"minSyncReplicas\":1,\"maxSyncReplicas\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Found previous run flag\",\"logging_pod\":\"sidm-db-cluster-3\",\"filename\":\"/var/lib/postgresql/data/pgdata/cnpg_initialized-sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"sidm-db-cluster-3\",\"reason\":\"postmaster start up\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202107181\\nDatabase system identifier:           7343539008069017621\\nDatabase cluster state:               in archive recovery\\npg_control last modified:             Tue 26 Mar 2024 09:53:08 AM UTC\\nLatest checkpoint location:           4/65000028\\nLatest checkpoint's REDO location:    4/65000028\\nLatest checkpoint's REDO WAL file:    000000030000000400000065\\nLatest checkpoint's TimeLineID:       3\\nLatest checkpoint's PrevTimeLineID:   3\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:212089\\nLatest checkpoint's NextOID:          25326\\nLatest checkpoint's NextMultiXactId:  3038\\nLatest checkpoint's NextMultiOffset:  12544\\nLatest checkpoint's oldestXID:        726\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  0\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Tue 26 Mar 2024 09:51:30 AM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     4/650000A0\\nMin recovery ending loc's timeline:   3\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           1\\nMock authentication nonce:            d701fb124777ddc4d8246b5159a574b85b0b5c243dfdf78945d9731f08807970\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"postmaster started\",\"logging_pod\":\"sidm-db-cluster-3\",\"postMasterPID\":21}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"sidm-db-cluster\",\"namespace\":\"db-operator\"},\"namespace\":\"db-operator\",\"name\":\"sidm-db-cluster\",\"reconcileID\":\"59c81daf-3d8a-488b-919f-8d3d2cef24cd\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"2024-09-04 13:57:57.014 UTC [21] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"2024-09-04 13:57:57.014 UTC [21] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"2024-09-04 13:57:57.014 UTC [21] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"2024-09-04 13:57:57.014 UTC [21] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.014 UTC\",\"process_id\":\"21\",\"session_id\":\"66d86764.15\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.014 UTC\",\"process_id\":\"21\",\"session_id\":\"66d86764.15\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-09-04 13:57:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 14.5 (Debian 14.5-2.pgdg110+2) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.015 UTC\",\"process_id\":\"21\",\"session_id\":\"66d86764.15\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-09-04 13:57:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.015 UTC\",\"process_id\":\"21\",\"session_id\":\"66d86764.15\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-09-04 13:57:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.017 UTC\",\"process_id\":\"21\",\"session_id\":\"66d86764.15\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-09-04 13:57:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.021 UTC\",\"process_id\":\"23\",\"session_id\":\"66d86765.17\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was interrupted while in recovery at log time 2024-03-26 09:51:30 UTC\",\"hint\":\"If this has occurred more than once some data might be corrupted and you might need to choose an earlier recovery target.\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.692 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"33\",\"connection_from\":\"[local]\",\"session_id\":\"66d86765.21\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.771 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"34\",\"connection_from\":\"[local]\",\"session_id\":\"66d86765.22\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.785 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"35\",\"connection_from\":\"[local]\",\"session_id\":\"66d86765.23\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"sidm-db-cluster-3\",\"walName\":\"00000004.history\",\"startTime\":\"2024-09-04T13:57:57Z\",\"endTime\":\"2024-09-04T13:57:57Z\",\"elapsedWalTime\":0.612906878}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"sidm-db-cluster-3\",\"walName\":\"00000004.history\",\"maxParallel\":2,\"successfulWalRestore\":1,\"failedWalRestore\":1,\"endOfWALStream\":false,\"startTime\":\"2024-09-04T13:57:57Z\",\"downloadStartTime\":\"2024-09-04T13:57:57Z\",\"downloadTotalTime\":0.613087985,\"totalTime\":0.721095011}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.791 UTC\",\"process_id\":\"23\",\"session_id\":\"66d86765.17\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000004.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.842 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"44\",\"connection_from\":\"[local]\",\"session_id\":\"66d86765.2c\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:58Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"sidm-db-cluster-3\",\"syncReplicas\":0,\"minSyncReplicas\":1,\"maxSyncReplicas\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:58Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"sidm-db-cluster-3\",\"syncReplicas\":0,\"minSyncReplicas\":1,\"maxSyncReplicas\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:58Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:58.071 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"47\",\"connection_from\":\"[local]\",\"session_id\":\"66d86766.2f\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:58 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:58Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"sidm-db-cluster\",\"namespace\":\"db-operator\"},\"namespace\":\"db-operator\",\"name\":\"sidm-db-cluster\",\"reconcileID\":\"f432eed1-9af9-47ae-a220-4704f6d982f8\",\"logging_pod\":\"sidm-db-cluster-3\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:58Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:58.072 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"48\",\"connection_from\":\"[local]\",\"session_id\":\"66d86766.30\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:58 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:58Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:58.108 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"49\",\"connection_from\":\"[local]\",\"session_id\":\"66d86766.31\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:58 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:58Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"sidm-db-cluster-3\",\"walName\":\"00000005.history\",\"options\":[\"--endpoint-url\",\"https://s3.se.teliacompany.net\",\"--cloud-provider\",\"aws-s3\",\"s3://sidm-se-stage/se-backups\",\"sidm-db-cluster\"],\"startTime\":\"2024-09-04T13:57:57Z\",\"endTime\":\"2024-09-04T13:57:58Z\",\"elapsedWalTime\":0.59445327}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:58Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:58.613 UTC\",\"process_id\":\"23\",\"session_id\":\"66d86765.17\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"entering standby mode\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"sidm-db-cluster-3\",\"syncReplicas\":0,\"minSyncReplicas\":1,\"maxSyncReplicas\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"sidm-db-cluster-3\",\"syncReplicas\":0,\"minSyncReplicas\":1,\"maxSyncReplicas\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:59.126 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"60\",\"connection_from\":\"[local]\",\"session_id\":\"66d86767.3c\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:59 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:59.127 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"61\",\"connection_from\":\"[local]\",\"session_id\":\"66d86767.3d\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:59 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"sidm-db-cluster\",\"namespace\":\"db-operator\"},\"namespace\":\"db-operator\",\"name\":\"sidm-db-cluster\",\"reconcileID\":\"24d2bace-989c-4d6b-a9b0-2e9fd9611450\",\"logging_pod\":\"sidm-db-cluster-3\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"sidm-db-cluster-3\",\"walName\":\"00000004.history\",\"startTime\":\"2024-09-04T13:57:58Z\",\"endTime\":\"2024-09-04T13:57:59Z\",\"elapsedWalTime\":0.536345575}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"sidm-db-cluster-3\",\"walName\":\"00000004.history\",\"maxParallel\":2,\"successfulWalRestore\":1,\"failedWalRestore\":1,\"endOfWALStream\":false,\"startTime\":\"2024-09-04T13:57:58Z\",\"downloadStartTime\":\"2024-09-04T13:57:58Z\",\"downloadTotalTime\":0.536557887,\"totalTime\":0.637531543}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:59.277 UTC\",\"process_id\":\"23\",\"session_id\":\"66d86765.17\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000004.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:59.486 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"72\",\"connection_from\":\"[local]\",\"session_id\":\"66d86767.48\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:59 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:59.586 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"73\",\"connection_from\":\"[local]\",\"session_id\":\"66d86767.49\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:59 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:59.599 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"74\",\"connection_from\":\"[local]\",\"session_id\":\"66d86767.4a\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:59 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:59.656 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"75\",\"connection_from\":\"[local]\",\"session_id\":\"66d86767.4b\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:59 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"sidm-db-cluster-3\",\"walName\":\"000000040000000400000065\",\"options\":[\"--endpoint-url\",\"https://s3.se.teliacompany.net\",\"--cloud-provider\",\"aws-s3\",\"s3://sidm-se-stage/se-backups\",\"sidm-db-cluster\"],\"startTime\":\"2024-09-04T13:57:59Z\",\"endTime\":\"2024-09-04T13:57:59Z\",\"elapsedWalTime\":0.368525491}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:59.924 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"84\",\"connection_from\":\"[local]\",\"session_id\":\"66d86767.54\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:59 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"sidm-db-cluster-3\",\"syncReplicas\":0,\"minSyncReplicas\":1,\"maxSyncReplicas\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"sidm-db-cluster-3\",\"syncReplicas\":0,\"minSyncReplicas\":1,\"maxSyncReplicas\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:58:00.180 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"88\",\"connection_from\":\"[local]\",\"session_id\":\"66d86768.58\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:58:00 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"sidm-db-cluster\",\"namespace\":\"db-operator\"},\"namespace\":\"db-operator\",\"name\":\"sidm-db-cluster\",\"reconcileID\":\"bd7d9ee2-6c1b-4b2e-885e-f81e91410e9e\",\"logging_pod\":\"sidm-db-cluster-3\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:58:00.182 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"89\",\"connection_from\":\"[local]\",\"session_id\":\"66d86768.59\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:58:00 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"sidm-db-cluster-3\",\"walName\":\"000000030000000400000065\",\"options\":[\"--endpoint-url\",\"https://s3.se.teliacompany.net\",\"--cloud-provider\",\"aws-s3\",\"s3://sidm-se-stage/se-backups\",\"sidm-db-cluster\"],\"startTime\":\"2024-09-04T13:57:59Z\",\"endTime\":\"2024-09-04T13:58:00Z\",\"elapsedWalTime\":0.366740558}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:58:00.473 UTC\",\"process_id\":\"23\",\"session_id\":\"66d86765.17\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"XX000\",\"message\":\"requested timeline 4 is not a child of this server's history\",\"detail\":\"Latest checkpoint is at 4/65000028 on timeline 3, but in the history of the requested timeline, the server forked off from that timeline at 4/630189D8.\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:58:00.475 UTC\",\"process_id\":\"21\",\"session_id\":\"66d86764.15\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-09-04 13:57:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 23) exited with exit code 1\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:58:00.475 UTC\",\"process_id\":\"21\",\"session_id\":\"66d86764.15\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-09-04 13:57:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"aborting startup due to startup process failure\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:58:00.478 UTC\",\"process_id\":\"21\",\"session_id\":\"66d86764.15\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-09-04 13:57:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"postmaster exited\",\"logging_pod\":\"sidm-db-cluster-3\",\"postmasterExitStatus\":\"exit status 1\",\"postMasterPID\":21}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"sidm-db-cluster-3\",\"reason\":\"postmaster has exited\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202107181\\nDatabase system identifier:           7343539008069017621\\nDatabase cluster state:               in archive recovery\\npg_control last modified:             Tue 26 Mar 2024 09:53:08 AM UTC\\nLatest checkpoint location:           4/65000028\\nLatest checkpoint's REDO location:    4/65000028\\nLatest checkpoint's REDO WAL file:    000000030000000400000065\\nLatest checkpoint's TimeLineID:       3\\nLatest checkpoint's PrevTimeLineID:   3\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:212089\\nLatest checkpoint's NextOID:          25326\\nLatest checkpoint's NextMultiXactId:  3038\\nLatest checkpoint's NextMultiOffset:  12544\\nLatest checkpoint's oldestXID:        726\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  0\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Tue 26 Mar 2024 09:51:30 AM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     4/650000A0\\nMin recovery ending loc's timeline:   3\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           1\\nMock authentication nonce:            d701fb124777ddc4d8246b5159a574b85b0b5c243dfdf78945d9731f08807970\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"error\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"PostgreSQL process exited with errors\",\"logging_pod\":\"sidm-db-cluster-3\",\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start.func1\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:104\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:112\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/manager/runnable_group.go:226\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\r\n{\"level\":\"error\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"error received after stop sequence was engaged\",\"error\":\"exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).engageStopProcedure.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/manager/internal.go:499\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Stopping and waiting for leader election runnables\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"sidm-db-cluster-3\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"sidm-db-cluster-3\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"sidm-db-cluster-3\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Stopping and waiting for caches\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Stopping and waiting for webhooks\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Stopping and waiting for HTTP servers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\nolder in 1.23.x\n### What version of Kubernetes are you using?\n1.27 (unsupported)\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nThe replica won't able to start. It's showing CrashLoopBackOff\n### Cluster resource\n```shell\nNAME              AGE    INSTANCES   READY   STATUS                                       PRIMARY\r\nsidm-db-cluster   181d   2           1       Waiting for the instances to become active   sidm-db-cluster-2\r\nNAME                                 READY   STATUS             RESTARTS       AGE\r\nsidm-db-cluster-2                    1/1     Running            0              173d\r\nsidm-db-cluster-3                    0/1     CrashLoopBackOff   17 (45s ago)   63m\r\nkubectl describe cluster sidm-db-cluster -n db-operator\r\nName:         sidm-db-cluster\r\nNamespace:    db-operator\r\nLabels:       app.kubernetes.io/managed-by=Helm\r\nAnnotations:  meta.helm.sh/release-name: db-operator\r\n              meta.helm.sh/release-namespace: default\r\nAPI Version:  postgresql.cnpg.io/v1\r\nKind:         Cluster\r\nMetadata:\r\n  Creation Timestamp:  2024-03-07T08:43:25Z\r\n  Generation:          6\r\n  Managed Fields:\r\n    API Version:  postgresql.cnpg.io/v1\r\n    Fields Type:  FieldsV1\r\n    fieldsV1:\r\n      f:metadata:\r\n        f:annotations:\r\n          .:\r\n          f:meta.helm.sh/release-name:\r\n          f:meta.helm.sh/release-namespace:\r\n        f:labels:\r\n          .:\r\n          f:app.kubernetes.io/managed-by:\r\n      f:spec:\r\n        .:\r\n        f:affinity:\r\n          .:\r\n          f:enablePodAntiAffinity:\r\n          f:topologyKey:\r\n        f:backup:\r\n          .:\r\n          f:barmanObjectStore:\r\n            .:\r\n            f:data:\r\n              .:\r\n              f:compression:\r\n              f:encryption:\r\n              f:immediateCheckpoint:\r\n              f:jobs:\r\n            f:destinationPath:\r\n            f:endpointURL:\r\n            f:s3Credentials:\r\n              .:\r\n              f:accessKeyId:\r\n                .:\r\n                f:key:\r\n                f:name:\r\n              f:region:\r\n                .:\r\n                f:key:\r\n                f:name:\r\n              f:secretAccessKey:\r\n                .:\r\n                f:key:\r\n                f:name:\r\n            f:wal:\r\n              .:\r\n              f:compression:\r\n              f:encryption:\r\n              f:maxParallel:\r\n          f:retentionPolicy:\r\n          f:target:\r\n        f:bootstrap:\r\n          .:\r\n          f:pg_basebackup:\r\n            .:\r\n            f:source:\r\n        f:description:\r\n        f:enableSuperuserAccess:\r\n        f:externalClusters:\r\n        f:failoverDelay:\r\n        f:imageName:\r\n        f:imagePullSecrets:\r\n        f:instances:\r\n        f:logLevel:\r\n        f:maxSyncReplicas:\r\n        f:minSyncReplicas:\r\n        f:nodeMaintenanceWindow:\r\n          .:\r\n          f:inProgress:\r\n          f:reusePVC:\r\n        f:postgresGID:\r\n        f:postgresUID:\r\n        f:postgresql:\r\n          .:\r\n          f:parameters:\r\n            .:\r\n            f:auto_explain.log_min_duration:\r\n            f:pg_stat_statements.max:\r\n            f:pg_stat_statements.track:\r\n            f:shared_buffers:\r\n          f:pg_hba:\r\n        f:primaryUpdateMethod:\r\n        f:primaryUpdateStrategy:\r\n        f:replica:\r\n          .:\r\n          f:enabled:\r\n          f:source:\r\n        f:replicationSlots:\r\n          .:\r\n          f:highAvailability:\r\n            .:\r\n            f:enabled:\r\n            f:slotPrefix:\r\n          f:updateInterval:\r\n        f:resources:\r\n          .:\r\n          f:limits:\r\n            .:\r\n            f:cpu:\r\n            f:memory:\r\n          f:requests:\r\n            .:\r\n            f:cpu:\r\n            f:memory:\r\n        f:smartShutdownTimeout:\r\n        f:startDelay:\r\n        f:stopDelay:\r\n        f:storage:\r\n          .:\r\n          f:resizeInUseVolumes:\r\n          f:size:\r\n          f:storageClass:\r\n        f:superuserSecret:\r\n          .:\r\n          f:name:\r\n        f:switchoverDelay:\r\n        f:walStorage:\r\n          .:\r\n          f:resizeInUseVolumes:\r\n          f:size:\r\n          f:storageClass:\r\n    Manager:      helm\r\n    Operation:    Update\r\n    Time:         2024-03-25T15:43:00Z\r\n    API Version:  postgresql.cnpg.io/v1\r\n    Fields Type:  FieldsV1\r\n    fieldsV1:\r\n      f:spec:\r\n        f:postgresql:\r\n          f:parameters:\r\n            f:wal_level:\r\n            f:wal_log_hints:\r\n        f:replicationSlots:\r\n          f:synchronizeReplicas:\r\n            .:\r\n            f:enabled:\r\n    Manager:      manager\r\n    Operation:    Update\r\n    Time:         2024-05-22T12:48:53Z\r\n    API Version:  postgresql.cnpg.io/v1\r\n    Fields Type:  FieldsV1\r\n    fieldsV1:\r\n      f:status:\r\n        .:\r\n        f:availableArchitectures:\r\n        f:certificates:\r\n          .:\r\n          f:clientCASecret:\r\n          f:expirations:\r\n            .:\r\n            f:sidm-db-cluster-ca:\r\n            f:sidm-db-cluster-replication:\r\n            f:sidm-db-cluster-server:\r\n          f:replicationTLSSecret:\r\n          f:serverAltDNSNames:\r\n          f:serverCASecret:\r\n          f:serverTLSSecret:\r\n        f:cloudNativePGCommitHash:\r\n        f:cloudNativePGOperatorHash:\r\n        f:conditions:\r\n        f:configMapResourceVersion:\r\n          .:\r\n          f:metrics:\r\n            .:\r\n            f:cnpg-default-monitoring:\r\n        f:currentPrimary:\r\n        f:currentPrimaryTimestamp:\r\n        f:firstRecoverabilityPoint:\r\n        f:firstRecoverabilityPointByMethod:\r\n          .:\r\n          f:barmanObjectStore:\r\n        f:healthyPVC:\r\n        f:image:\r\n        f:instanceNames:\r\n        f:instances:\r\n        f:instancesReportedState:\r\n          .:\r\n          f:sidm-db-cluster-2:\r\n            .:\r\n            f:isPrimary:\r\n            f:timeLineID:\r\n          f:sidm-db-cluster-3:\r\n            .:\r\n            f:isPrimary:\r\n        f:instancesStatus:\r\n          .:\r\n          f:healthy:\r\n          f:replicating:\r\n        f:jobCount:\r\n        f:lastSuccessfulBackup:\r\n        f:lastSuccessfulBackupByMethod:\r\n          .:\r\n          f:barmanObjectStore:\r\n        f:latestGeneratedNode:\r\n        f:managedRolesStatus:\r\n        f:phase:\r\n        f:phaseReason:\r\n        f:poolerIntegrations:\r\n          .:\r\n          f:pgBouncerIntegration:\r\n            .:\r\n            f:secrets:\r\n        f:pvcCount:\r\n        f:readService:\r\n        f:readyInstances:\r\n        f:secretsResourceVersion:\r\n          .:\r\n          f:clientCaSecretVersion:\r\n          f:externalClusterSecretVersion:\r\n            .:\r\n            f:cluster-sidm-superuser:\r\n          f:replicationSecretVersion:\r\n          f:serverCaSecretVersion:\r\n          f:serverSecretVersion:\r\n          f:superuserSecretVersion:\r\n        f:switchReplicaClusterStatus:\r\n        f:targetPrimary:\r\n        f:targetPrimaryTimestamp:\r\n        f:topology:\r\n          .:\r\n          f:instances:\r\n            .:\r\n            f:sidm-db-cluster-2:\r\n            f:sidm-db-cluster-3:\r\n          f:nodesUsed:\r\n          f:successfullyExtracted:\r\n        f:writeService:\r\n    Manager:         manager\r\n    Operation:       Update\r\n    Subresource:     status\r\n    Time:            2024-09-04T12:55:00Z\r\n  Resource Version:  186977545\r\n  UID:               b583fcb2-288b-4ac4-a08a-ba3f8054f271\r\nSpec:\r\n  Affinity:\r\n    Enable Pod Anti Affinity:  true\r\n    Pod Anti Affinity Type:    preferred\r\n    Topology Key:              topology.kubernetes.io/zone\r\n  Backup:\r\n    Barman Object Store:\r\n      Data:\r\n        Compression:     gzip\r\n        Encryption:      AES256\r\n        Jobs:            2\r\n      Destination Path:  s3://sidm-se-stage/se-backups\r\n      Endpoint URL:      https://s3.se.teliacompany.net\r\n      s3Credentials:\r\n        Access Key Id:\r\n          Key:   ACCESS_KEY_ID\r\n          Name:  backup-creds\r\n        Region:\r\n          Key:   REGION_KEY\r\n          Name:  backup-creds\r\n        Secret Access Key:\r\n          Key:   ACCESS_SECRET_KEY\r\n          Name:  backup-creds\r\n      Wal:\r\n        Compression:   gzip\r\n        Encryption:    AES256\r\n        Max Parallel:  2\r\n    Retention Policy:  30d\r\n    Target:            prefer-standby\r\n  Bootstrap:\r\n    pg_basebackup:\r\n      Database:             app\r\n      Owner:                app\r\n      Source:               sidm-db-cluster\r\n  Description:              SIDM DB cluster\r\n  Enable PDB:               true\r\n  Enable Superuser Access:  true\r\n  External Clusters:\r\n    Connection Parameters:\r\n      Dbname:  sidm-db-se\r\n      Host:    10.30.177.115\r\n      Port:    9998\r\n      User:    postgres\r\n    Name:      sidm-db-cluster\r\n    Password:\r\n      Key:         password\r\n      Name:        cluster-sidm-superuser\r\n  Failover Delay:  0\r\n  Image Name:      pcoc-remotes-virtual.jfrog.teliacompany.io/cloudnative-pg/postgresql:14.5\r\n  Image Pull Secrets:\r\n    Name:             private-registry-creds\r\n  Instances:          2\r\n  Log Level:          info\r\n  Max Sync Replicas:  1\r\n  Min Sync Replicas:  1\r\n  Monitoring:\r\n    Custom Queries Config Map:\r\n      Key:                    queries\r\n      Name:                   cnpg-default-monitoring\r\n    Disable Default Queries:  false\r\n    Enable Pod Monitor:       false\r\n  Node Maintenance Window:\r\n    In Progress:  false\r\n    Reuse PVC:    false\r\n  Postgres GID:   26\r\n  Postgres UID:   26\r\n  Postgresql:\r\n    Parameters:\r\n      archive_mode:                   always\r\n      archive_timeout:                5min\r\n      auto_explain.log_min_duration:  10s\r\n      dynamic_shared_memory_type:     posix\r\n      log_destination:                csvlog\r\n      log_directory:                  /controller/log\r\n      log_filename:                   postgres\r\n      log_rotation_age:               0\r\n      log_rotation_size:              0\r\n      log_truncate_on_rotation:       false\r\n      logging_collector:              on\r\n      max_parallel_workers:           32\r\n      max_replication_slots:          32\r\n      max_worker_processes:           32\r\n      pg_stat_statements.max:         10000\r\n      pg_stat_statements.track:       all\r\n      shared_buffers:                 256MB\r\n      shared_memory_type:             mmap\r\n      shared_preload_libraries:       \r\n      ssl_max_protocol_version:       TLSv1.3\r\n      ssl_min_protocol_version:       TLSv1.3\r\n      wal_keep_size:                  512MB\r\n      wal_level:                      logical\r\n      wal_log_hints:                  on\r\n      wal_receiver_timeout:           5s\r\n      wal_sender_timeout:             5s\r\n    pg_hba:\r\n      host all all all md5\r\n      local all all trust\r\n      host all all 127.0.0.1/32 md5\r\n      host all all ::1/128 md5\r\n      host all all 10.244.0.0/16 md5\r\n      host replication postgres 0.0.0.0/0 md5\r\n    Sync Replica Election Constraint:\r\n      Enabled:              false\r\n  Primary Update Method:    restart\r\n  Primary Update Strategy:  unsupervised\r\n  Replica:\r\n    Enabled:  true\r\n    Source:   sidm-db-cluster\r\n  Replication Slots:\r\n    High Availability:\r\n      Enabled:      true\r\n      Slot Prefix:  _cnpg_\r\n    Synchronize Replicas:\r\n      Enabled:        true\r\n    Update Interval:  30\r\n  Resources:\r\n    Limits:\r\n      Cpu:     3\r\n      Memory:  2Gi\r\n    Requests:\r\n      Cpu:                 500m\r\n      Memory:              512Mi\r\n  Smart Shutdown Timeout:  180\r\n  Start Delay:             300\r\n  Stop Delay:              300\r\n  Storage:\r\n    Resize In Use Volumes:  true\r\n    Size:                   30Gi\r\n    Storage Class:          sidm-sc\r\n  Superuser Secret:\r\n    Name:            cluster-sidm-superuser\r\n  Switchover Delay:  3600\r\n  Wal Storage:\r\n    Resize In Use Volumes:  true\r\n    Size:                   20Gi\r\n    Storage Class:          sidm-sc\r\nStatus:\r\n  Available Architectures:\r\n    Go Arch:  amd64\r\n    Hash:     144e71b00bdcfc5edafa10055fb0cc4a6efa9f467a8e66826d5e7bb2b254b706\r\n    Go Arch:  arm64\r\n    Hash:     0027f50a9d35e24040cfc2f27cea04cbdf4375c226ac7b42764b5bb91f9beca4\r\n  Certificates:\r\n    Client CA Secret:  sidm-db-cluster-ca\r\n    Expirations:\r\n      Sidm - Db - Cluster - Ca:           2024-11-18 08:28:26 +0000 UTC\r\n      Sidm - Db - Cluster - Replication:  2024-11-18 08:28:26 +0000 UTC\r\n      Sidm - Db - Cluster - Server:       2024-11-18 08:28:26 +0000 UTC\r\n    Replication TLS Secret:               sidm-db-cluster-replication\r\n    Server Alt DNS Names:\r\n      sidm-db-cluster-rw\r\n      sidm-db-cluster-rw.db-operator\r\n      sidm-db-cluster-rw.db-operator.svc\r\n      sidm-db-cluster-r\r\n      sidm-db-cluster-r.db-operator\r\n      sidm-db-cluster-r.db-operator.svc\r\n      sidm-db-cluster-ro\r\n      sidm-db-cluster-ro.db-operator\r\n      sidm-db-cluster-ro.db-operator.svc\r\n    Server CA Secret:             sidm-db-cluster-ca\r\n    Server TLS Secret:            sidm-db-cluster-server\r\n  Cloud Native PG Commit Hash:    2b489ad6\r\n  Cloud Native PG Operator Hash:  144e71b00bdcfc5edafa10055fb0cc4a6efa9f467a8e66826d5e7bb2b254b706\r\n  Conditions:\r\n    Last Transition Time:  2024-09-04T10:14:11Z\r\n    Message:               Cluster Is Not Ready\r\n    Reason:                ClusterIsNotReady\r\n    Status:                False\r\n    Type:                  Ready\r\n    Last Transition Time:  2024-03-25T13:55:28Z\r\n    Message:               Continuous archiving is working\r\n    Reason:                ContinuousArchivingSuccess\r\n    Status:                True\r\n    Type:                  ContinuousArchiving\r\n    Last Transition Time:  2024-03-27T00:00:00Z\r\n    Message:               New Backup starting up\r\n    Reason:                BackupStarted\r\n    Status:                False\r\n    Type:                  LastBackupSucceeded\r\n  Config Map Resource Version:\r\n    Metrics:\r\n      Cnpg - Default - Monitoring:  170510018\r\n  Current Primary:                  sidm-db-cluster-2\r\n  Current Primary Timestamp:        2024-09-04T10:14:11.214135Z\r\n  First Recoverability Point:       2024-03-08T13:49:34Z\r\n  First Recoverability Point By Method:\r\n    Barman Object Store:  2024-03-08T13:49:34Z\r\n  Healthy PVC:\r\n    sidm-db-cluster-2\r\n    sidm-db-cluster-2-wal\r\n    sidm-db-cluster-3\r\n    sidm-db-cluster-3-wal\r\n  Image:  pcoc-remotes-virtual.jfrog.teliacompany.io/cloudnative-pg/postgresql:14.5\r\n  Instance Names:\r\n    sidm-db-cluster-2\r\n    sidm-db-cluster-3\r\n  Instances:  2\r\n  Instances Reported State:\r\n    sidm-db-cluster-2:\r\n      Is Primary:    false\r\n      Time Line ID:  3\r\n    sidm-db-cluster-3:\r\n      Is Primary:  false\r\n  Instances Status:\r\n    Healthy:\r\n      sidm-db-cluster-2\r\n    Replicating:\r\n      sidm-db-cluster-3\r\n  Job Count:               1\r\n  Last Successful Backup:  2024-03-26T00:00:09Z\r\n  Last Successful Backup By Method:\r\n    Barman Object Store:  2024-03-26T00:00:09Z\r\n  Latest Generated Node:  3\r\n  Managed Roles Status:\r\n  Phase:         Waiting for the instances to become active\r\n  Phase Reason:  Some instances are not yet active. Please wait.\r\n  Pooler Integrations:\r\n    Pg Bouncer Integration:\r\n      Secrets:\r\n        sidm-db-cluster-pooler\r\n  Pvc Count:        4\r\n  Read Service:     sidm-db-cluster-r\r\n  Ready Instances:  1\r\n  Secrets Resource Version:\r\n    Client Ca Secret Version:  174529907\r\n    External Cluster Secret Version:\r\n      Cluster - Sidm - Superuser:  38599818\r\n    Replication Secret Version:    174529909\r\n    Server Ca Secret Version:      174529907\r\n    Server Secret Version:         174529908\r\n    Superuser Secret Version:      38599818\r\n  Switch Replica Cluster Status:\r\n  Target Primary:            sidm-db-cluster-2\r\n  Target Primary Timestamp:  2024-09-04T10:14:11.147441Z\r\n  Topology:\r\n    Instances:\r\n      sidm-db-cluster-2:\r\n      sidm-db-cluster-3:\r\n    Nodes Used:              2\r\n    Successfully Extracted:  true\r\n  Write Service:             sidm-db-cluster-rw\r\nEvents:                      <none>\n```\n### Relevant log output\n```shell\nkubectl logs sidm-db-cluster-2 -n db-operator\r\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:52:36Z\",\"logger\":\"Replicator\",\"msg\":\"synchronizing replication slots\",\"logging_pod\":\"sidm-db-cluster-2\",\"err\":\"getting replication slot status from primary: failed to connect to `host=sidm-db-cluster-rw user=streaming_replica database=postgres`: dial error (timeout: dial tcp 172.16.7.0:5432: connect: connection timed out)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:10Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-2\",\"record\":{\"log_time\":\"2024-09-03 06:54:10.757 UTC\",\"process_id\":\"1997795\",\"session_id\":\"66d6b210.1e7be3\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-03 06:52:00 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"08006\",\"message\":\"could not connect to the primary server: connection to server at \\\"sidm-db-cluster-rw\\\" (172.16.7.0), port 5432 failed: Connection timed out\\n\\tIs the server running on that host and accepting TCP/IP connections?\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:11Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000004.history\",\"startTime\":\"2024-09-03T06:54:10Z\",\"endTime\":\"2024-09-03T06:54:11Z\",\"elapsedWalTime\":0.65855262}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:11Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000004.history\",\"maxParallel\":2,\"successfulWalRestore\":1,\"failedWalRestore\":1,\"endOfWALStream\":false,\"startTime\":\"2024-09-03T06:54:10Z\",\"downloadStartTime\":\"2024-09-03T06:54:10Z\",\"downloadTotalTime\":0.658772761,\"totalTime\":0.786042716}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:11Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-2\",\"record\":{\"log_time\":\"2024-09-03 06:54:11.572 UTC\",\"process_id\":\"24\",\"session_id\":\"65f33a43.18\",\"session_line_num\":\"272848\",\"session_start_time\":\"2024-03-14 17:56:19 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000004.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:12Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000005.history\",\"options\":[\"--endpoint-url\",\"https://s3.se.teliacompany.net\",\"--cloud-provider\",\"aws-s3\",\"s3://sidm-se-stage/se-backups\",\"sidm-db-cluster\"],\"startTime\":\"2024-09-03T06:54:11Z\",\"endTime\":\"2024-09-03T06:54:12Z\",\"elapsedWalTime\":0.538944636}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:12Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000004.history\",\"startTime\":\"2024-09-03T06:54:12Z\",\"endTime\":\"2024-09-03T06:54:12Z\",\"elapsedWalTime\":0.506157}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:12Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000004.history\",\"maxParallel\":2,\"successfulWalRestore\":1,\"failedWalRestore\":1,\"endOfWALStream\":false,\"startTime\":\"2024-09-03T06:54:12Z\",\"downloadStartTime\":\"2024-09-03T06:54:12Z\",\"downloadTotalTime\":0.506379513,\"totalTime\":0.601262193}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:12Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-2\",\"record\":{\"log_time\":\"2024-09-03 06:54:12.972 UTC\",\"process_id\":\"24\",\"session_id\":\"65f33a43.18\",\"session_line_num\":\"272849\",\"session_start_time\":\"2024-03-14 17:56:19 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000004.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:12Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-2\",\"record\":{\"log_time\":\"2024-09-03 06:54:12.978 UTC\",\"process_id\":\"24\",\"session_id\":\"65f33a43.18\",\"session_line_num\":\"272850\",\"session_start_time\":\"2024-03-14 17:56:19 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"new timeline 4 forked off current database system timeline 3 before current recovery point 4/650000A0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:13Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"000000030000000400000065\",\"options\":[\"--endpoint-url\",\"https://s3.se.teliacompany.net\",\"--cloud-provider\",\"aws-s3\",\"s3://sidm-se-stage/se-backups\",\"sidm-db-cluster\"],\"startTime\":\"2024-09-03T06:54:13Z\",\"endTime\":\"2024-09-03T06:54:13Z\",\"elapsedWalTime\":0.3590011}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:54:47Z\",\"logger\":\"Replicator\",\"msg\":\"synchronizing replication slots\",\"logging_pod\":\"sidm-db-cluster-2\",\"err\":\"getting replication slot status from primary: failed to connect to `host=sidm-db-cluster-rw user=streaming_replica database=postgres`: dial error (timeout: dial tcp 172.16.7.0:5432: connect: connection timed out)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-2\",\"record\":{\"log_time\":\"2024-09-03 06:56:23.877 UTC\",\"process_id\":\"1998265\",\"session_id\":\"66d6b295.1e7db9\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-03 06:54:13 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"08006\",\"message\":\"could not connect to the primary server: connection to server at \\\"sidm-db-cluster-rw\\\" (172.16.7.0), port 5432 failed: Connection timed out\\n\\tIs the server running on that host and accepting TCP/IP connections?\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:24Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000004.history\",\"startTime\":\"2024-09-03T06:56:24Z\",\"endTime\":\"2024-09-03T06:56:24Z\",\"elapsedWalTime\":0.61121078}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:24Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000004.history\",\"maxParallel\":2,\"successfulWalRestore\":1,\"failedWalRestore\":1,\"endOfWALStream\":false,\"startTime\":\"2024-09-03T06:56:23Z\",\"downloadStartTime\":\"2024-09-03T06:56:24Z\",\"downloadTotalTime\":0.61142247,\"totalTime\":0.713546688}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:24Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-2\",\"record\":{\"log_time\":\"2024-09-03 06:56:24.616 UTC\",\"process_id\":\"24\",\"session_id\":\"65f33a43.18\",\"session_line_num\":\"272851\",\"session_start_time\":\"2024-03-14 17:56:19 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000004.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:25Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000005.history\",\"options\":[\"--endpoint-url\",\"https://s3.se.teliacompany.net\",\"--cloud-provider\",\"aws-s3\",\"s3://sidm-se-stage/se-backups\",\"sidm-db-cluster\"],\"startTime\":\"2024-09-03T06:56:24Z\",\"endTime\":\"2024-09-03T06:56:25Z\",\"elapsedWalTime\":0.518540386}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:25Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000004.history\",\"startTime\":\"2024-09-03T06:56:25Z\",\"endTime\":\"2024-09-03T06:56:25Z\",\"elapsedWalTime\":0.516822395}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:25Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"00000004.history\",\"maxParallel\":2,\"successfulWalRestore\":1,\"failedWalRestore\":1,\"endOfWALStream\":false,\"startTime\":\"2024-09-03T06:56:25Z\",\"downloadStartTime\":\"2024-09-03T06:56:25Z\",\"downloadTotalTime\":0.517076456,\"totalTime\":0.612954641}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:25Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-2\",\"record\":{\"log_time\":\"2024-09-03 06:56:25.998 UTC\",\"process_id\":\"24\",\"session_id\":\"65f33a43.18\",\"session_line_num\":\"272852\",\"session_start_time\":\"2024-03-14 17:56:19 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000004.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:26Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-2\",\"record\":{\"log_time\":\"2024-09-03 06:56:26.000 UTC\",\"process_id\":\"24\",\"session_id\":\"65f33a43.18\",\"session_line_num\":\"272853\",\"session_start_time\":\"2024-03-14 17:56:19 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"new timeline 4 forked off current database system timeline 3 before current recovery point 4/650000A0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-03T06:56:26Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"sidm-db-cluster-2\",\"walName\":\"000000030000000400000065\",\"options\":[\"--endpoint-url\",\"https://s3.se.teliacompany.net\",\"--cloud-provider\",\"aws-s3\",\"s3://sidm-se-stage/se-backups\",\"sidm-db-cluster\"],\"startTime\":\"2024-09-03T06:56:26Z\",\"endTime\":\"2024-09-03T06:56:26Z\",\"elapsedWalTime\":0.36545608}\r\nkubectl logs sidm-db-cluster-3 -n db-operator\r\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"sidm-db-cluster-3\",\"version\":\"1.23.3\",\"build\":{\"Version\":\"1.23.3\",\"Commit\":\"2b489ad6\",\"Date\":\"2024-07-29\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"logger\":\"setup\",\"msg\":\"starting tablespace manager\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"logger\":\"setup\",\"msg\":\"starting external server manager\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"sidm-db-cluster-3\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"logger\":\"roles_reconciler\",\"msg\":\"skipping the RoleSynchronizer in replicas\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"sidm-db-cluster-3\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"sidm-db-cluster-3\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"sidm-db-cluster-3\",\"syncReplicas\":0,\"minSyncReplicas\":1,\"maxSyncReplicas\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"sidm-db-cluster-3\",\"syncReplicas\":0,\"minSyncReplicas\":1,\"maxSyncReplicas\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Found previous run flag\",\"logging_pod\":\"sidm-db-cluster-3\",\"filename\":\"/var/lib/postgresql/data/pgdata/cnpg_initialized-sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"sidm-db-cluster-3\",\"reason\":\"postmaster start up\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202107181\\nDatabase system identifier:           7343539008069017621\\nDatabase cluster state:               in archive recovery\\npg_control last modified:             Tue 26 Mar 2024 09:53:08 AM UTC\\nLatest checkpoint location:           4/65000028\\nLatest checkpoint's REDO location:    4/65000028\\nLatest checkpoint's REDO WAL file:    000000030000000400000065\\nLatest checkpoint's TimeLineID:       3\\nLatest checkpoint's PrevTimeLineID:   3\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:212089\\nLatest checkpoint's NextOID:          25326\\nLatest checkpoint's NextMultiXactId:  3038\\nLatest checkpoint's NextMultiOffset:  12544\\nLatest checkpoint's oldestXID:        726\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  0\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Tue 26 Mar 2024 09:51:30 AM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     4/650000A0\\nMin recovery ending loc's timeline:   3\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           1\\nMock authentication nonce:            d701fb124777ddc4d8246b5159a574b85b0b5c243dfdf78945d9731f08807970\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:56Z\",\"msg\":\"postmaster started\",\"logging_pod\":\"sidm-db-cluster-3\",\"postMasterPID\":21}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"sidm-db-cluster\",\"namespace\":\"db-operator\"},\"namespace\":\"db-operator\",\"name\":\"sidm-db-cluster\",\"reconcileID\":\"59c81daf-3d8a-488b-919f-8d3d2cef24cd\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"2024-09-04 13:57:57.014 UTC [21] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"2024-09-04 13:57:57.014 UTC [21] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"2024-09-04 13:57:57.014 UTC [21] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"2024-09-04 13:57:57.014 UTC [21] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.014 UTC\",\"process_id\":\"21\",\"session_id\":\"66d86764.15\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.014 UTC\",\"process_id\":\"21\",\"session_id\":\"66d86764.15\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-09-04 13:57:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 14.5 (Debian 14.5-2.pgdg110+2) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.015 UTC\",\"process_id\":\"21\",\"session_id\":\"66d86764.15\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-09-04 13:57:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.015 UTC\",\"process_id\":\"21\",\"session_id\":\"66d86764.15\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-09-04 13:57:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.017 UTC\",\"process_id\":\"21\",\"session_id\":\"66d86764.15\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-09-04 13:57:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.021 UTC\",\"process_id\":\"23\",\"session_id\":\"66d86765.17\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was interrupted while in recovery at log time 2024-03-26 09:51:30 UTC\",\"hint\":\"If this has occurred more than once some data might be corrupted and you might need to choose an earlier recovery target.\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.692 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"33\",\"connection_from\":\"[local]\",\"session_id\":\"66d86765.21\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.771 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"34\",\"connection_from\":\"[local]\",\"session_id\":\"66d86765.22\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.785 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"35\",\"connection_from\":\"[local]\",\"session_id\":\"66d86765.23\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"sidm-db-cluster-3\",\"walName\":\"00000004.history\",\"startTime\":\"2024-09-04T13:57:57Z\",\"endTime\":\"2024-09-04T13:57:57Z\",\"elapsedWalTime\":0.612906878}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"sidm-db-cluster-3\",\"walName\":\"00000004.history\",\"maxParallel\":2,\"successfulWalRestore\":1,\"failedWalRestore\":1,\"endOfWALStream\":false,\"startTime\":\"2024-09-04T13:57:57Z\",\"downloadStartTime\":\"2024-09-04T13:57:57Z\",\"downloadTotalTime\":0.613087985,\"totalTime\":0.721095011}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.791 UTC\",\"process_id\":\"23\",\"session_id\":\"66d86765.17\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000004.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:57Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:57.842 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"44\",\"connection_from\":\"[local]\",\"session_id\":\"66d86765.2c\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:58Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"sidm-db-cluster-3\",\"syncReplicas\":0,\"minSyncReplicas\":1,\"maxSyncReplicas\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:58Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"sidm-db-cluster-3\",\"syncReplicas\":0,\"minSyncReplicas\":1,\"maxSyncReplicas\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:58Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:58.071 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"47\",\"connection_from\":\"[local]\",\"session_id\":\"66d86766.2f\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:58 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:58Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"sidm-db-cluster\",\"namespace\":\"db-operator\"},\"namespace\":\"db-operator\",\"name\":\"sidm-db-cluster\",\"reconcileID\":\"f432eed1-9af9-47ae-a220-4704f6d982f8\",\"logging_pod\":\"sidm-db-cluster-3\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:58Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:58.072 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"48\",\"connection_from\":\"[local]\",\"session_id\":\"66d86766.30\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:58 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:58Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:58.108 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"49\",\"connection_from\":\"[local]\",\"session_id\":\"66d86766.31\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:58 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:58Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"sidm-db-cluster-3\",\"walName\":\"00000005.history\",\"options\":[\"--endpoint-url\",\"https://s3.se.teliacompany.net\",\"--cloud-provider\",\"aws-s3\",\"s3://sidm-se-stage/se-backups\",\"sidm-db-cluster\"],\"startTime\":\"2024-09-04T13:57:57Z\",\"endTime\":\"2024-09-04T13:57:58Z\",\"elapsedWalTime\":0.59445327}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:58Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:58.613 UTC\",\"process_id\":\"23\",\"session_id\":\"66d86765.17\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"entering standby mode\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"sidm-db-cluster-3\",\"syncReplicas\":0,\"minSyncReplicas\":1,\"maxSyncReplicas\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"sidm-db-cluster-3\",\"syncReplicas\":0,\"minSyncReplicas\":1,\"maxSyncReplicas\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:59.126 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"60\",\"connection_from\":\"[local]\",\"session_id\":\"66d86767.3c\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:59 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:59.127 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"61\",\"connection_from\":\"[local]\",\"session_id\":\"66d86767.3d\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:59 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"sidm-db-cluster\",\"namespace\":\"db-operator\"},\"namespace\":\"db-operator\",\"name\":\"sidm-db-cluster\",\"reconcileID\":\"24d2bace-989c-4d6b-a9b0-2e9fd9611450\",\"logging_pod\":\"sidm-db-cluster-3\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"sidm-db-cluster-3\",\"walName\":\"00000004.history\",\"startTime\":\"2024-09-04T13:57:58Z\",\"endTime\":\"2024-09-04T13:57:59Z\",\"elapsedWalTime\":0.536345575}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"sidm-db-cluster-3\",\"walName\":\"00000004.history\",\"maxParallel\":2,\"successfulWalRestore\":1,\"failedWalRestore\":1,\"endOfWALStream\":false,\"startTime\":\"2024-09-04T13:57:58Z\",\"downloadStartTime\":\"2024-09-04T13:57:58Z\",\"downloadTotalTime\":0.536557887,\"totalTime\":0.637531543}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:59.277 UTC\",\"process_id\":\"23\",\"session_id\":\"66d86765.17\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000004.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:59.486 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"72\",\"connection_from\":\"[local]\",\"session_id\":\"66d86767.48\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:59 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:59.586 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"73\",\"connection_from\":\"[local]\",\"session_id\":\"66d86767.49\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:59 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:59.599 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"74\",\"connection_from\":\"[local]\",\"session_id\":\"66d86767.4a\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:59 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:59.656 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"75\",\"connection_from\":\"[local]\",\"session_id\":\"66d86767.4b\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:59 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"sidm-db-cluster-3\",\"walName\":\"000000040000000400000065\",\"options\":[\"--endpoint-url\",\"https://s3.se.teliacompany.net\",\"--cloud-provider\",\"aws-s3\",\"s3://sidm-se-stage/se-backups\",\"sidm-db-cluster\"],\"startTime\":\"2024-09-04T13:57:59Z\",\"endTime\":\"2024-09-04T13:57:59Z\",\"elapsedWalTime\":0.368525491}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:57:59Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:57:59.924 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"84\",\"connection_from\":\"[local]\",\"session_id\":\"66d86767.54\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:57:59 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"sidm-db-cluster-3\",\"syncReplicas\":0,\"minSyncReplicas\":1,\"maxSyncReplicas\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"sidm-db-cluster-3\",\"syncReplicas\":0,\"minSyncReplicas\":1,\"maxSyncReplicas\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:58:00.180 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"88\",\"connection_from\":\"[local]\",\"session_id\":\"66d86768.58\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:58:00 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"sidm-db-cluster\",\"namespace\":\"db-operator\"},\"namespace\":\"db-operator\",\"name\":\"sidm-db-cluster\",\"reconcileID\":\"bd7d9ee2-6c1b-4b2e-885e-f81e91410e9e\",\"logging_pod\":\"sidm-db-cluster-3\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:58:00.182 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"89\",\"connection_from\":\"[local]\",\"session_id\":\"66d86768.59\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-09-04 13:58:00 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"sidm-db-cluster-3\",\"walName\":\"000000030000000400000065\",\"options\":[\"--endpoint-url\",\"https://s3.se.teliacompany.net\",\"--cloud-provider\",\"aws-s3\",\"s3://sidm-se-stage/se-backups\",\"sidm-db-cluster\"],\"startTime\":\"2024-09-04T13:57:59Z\",\"endTime\":\"2024-09-04T13:58:00Z\",\"elapsedWalTime\":0.366740558}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:58:00.473 UTC\",\"process_id\":\"23\",\"session_id\":\"66d86765.17\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-09-04 13:57:57 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"XX000\",\"message\":\"requested timeline 4 is not a child of this server's history\",\"detail\":\"Latest checkpoint is at 4/65000028 on timeline 3, but in the history of the requested timeline, the server forked off from that timeline at 4/630189D8.\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:58:00.475 UTC\",\"process_id\":\"21\",\"session_id\":\"66d86764.15\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-09-04 13:57:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 23) exited with exit code 1\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:58:00.475 UTC\",\"process_id\":\"21\",\"session_id\":\"66d86764.15\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-09-04 13:57:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"aborting startup due to startup process failure\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"sidm-db-cluster-3\",\"record\":{\"log_time\":\"2024-09-04 13:58:00.478 UTC\",\"process_id\":\"21\",\"session_id\":\"66d86764.15\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-09-04 13:57:56 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"postmaster exited\",\"logging_pod\":\"sidm-db-cluster-3\",\"postmasterExitStatus\":\"exit status 1\",\"postMasterPID\":21}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"sidm-db-cluster-3\",\"reason\":\"postmaster has exited\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202107181\\nDatabase system identifier:           7343539008069017621\\nDatabase cluster state:               in archive recovery\\npg_control last modified:             Tue 26 Mar 2024 09:53:08 AM UTC\\nLatest checkpoint location:           4/65000028\\nLatest checkpoint's REDO location:    4/65000028\\nLatest checkpoint's REDO WAL file:    000000030000000400000065\\nLatest checkpoint's TimeLineID:       3\\nLatest checkpoint's PrevTimeLineID:   3\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:212089\\nLatest checkpoint's NextOID:          25326\\nLatest checkpoint's NextMultiXactId:  3038\\nLatest checkpoint's NextMultiOffset:  12544\\nLatest checkpoint's oldestXID:        726\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  0\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Tue 26 Mar 2024 09:51:30 AM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     4/650000A0\\nMin recovery ending loc's timeline:   3\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           1\\nMock authentication nonce:            d701fb124777ddc4d8246b5159a574b85b0b5c243dfdf78945d9731f08807970\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"error\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"PostgreSQL process exited with errors\",\"logging_pod\":\"sidm-db-cluster-3\",\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start.func1\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:104\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:112\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/manager/runnable_group.go:226\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\r\n{\"level\":\"error\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"error received after stop sequence was engaged\",\"error\":\"exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).engageStopProcedure.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/manager/internal.go:499\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Stopping and waiting for leader election runnables\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"sidm-db-cluster-3\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"sidm-db-cluster-3\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"sidm-db-cluster-3\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"sidm-db-cluster-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Stopping and waiting for caches\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Stopping and waiting for webhooks\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Stopping and waiting for HTTP servers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-04T13:58:00Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: ability to set default pooler image",
        "id": 2504252303,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently, the pg bouncer image when not specified in the `Pooler` defaults to \r\n```go\r\n    DefaultPgbouncerImage = \"ghcr.io/cloudnative-pg/pgbouncer:1.23.0\"\r\n```\n### Describe the solution you'd like\nI want to be able to select a different default pg bouncer image via environment variable, similar to `POSTGRES_IMAGE_NAME`.\n### Describe alternatives you've considered\nConfiguration via CRD; there isn't really a good place in the `ImageCatalog` for pooler images; but it could be added with enough consideration. However that's probably a good thing to do *in addition* to the env var.\n### Additional context\nI have clusters where all images are hashlocked (i.e. must have a digest in the image field in the pod spec). It would be nice to allow `Pooler` resources to work out-of-the-box without patching in the `containers[name=pgbouncer].image` field at each usage site.\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently, the pg bouncer image when not specified in the `Pooler` defaults to \r\n```go\r\n    DefaultPgbouncerImage = \"ghcr.io/cloudnative-pg/pgbouncer:1.23.0\"\r\n```\n### Describe the solution you'd like\nI want to be able to select a different default pg bouncer image via environment variable, similar to `POSTGRES_IMAGE_NAME`.\n### Describe alternatives you've considered\nConfiguration via CRD; there isn't really a good place in the `ImageCatalog` for pooler images; but it could be added with enough consideration. However that's probably a good thing to do *in addition* to the env var.\n### Additional context\nI have clusters where all images are hashlocked (i.e. must have a digest in the image field in the pod spec). It would be nice to allow `Pooler` resources to work out-of-the-box without patching in the `containers[name=pgbouncer].image` field at each usage site.\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductHi @james-callahan \r\nThis use case is covered in the documentation, can you check here https://cloudnative-pg.io/documentation/current/connection_pooling/ and let us know if that solves your issue?\r\nRegards!\n---\n> This use case is covered in the documentation, can you check here https://cloudnative-pg.io/documentation/current/connection_pooling/ and let us know if that solves your issue?\r\nThis feature request is about changing the default in the operator (which is currently hard-coded); not changing it on a per-pooler basis.\n---\nI see a similar issue with other hardcoded images:\n```\n\t// DefaultImageName is the default image used by the operator to create pods\n\tDefaultImageName = \"ghcr.io/cloudnative-pg/postgresql:17.2\"\n\t// DefaultOperatorImageName is the default operator image used by the controller in the pods running PostgreSQL\n\tDefaultOperatorImageName = \"ghcr.io/cloudnative-pg/cloudnative-pg:1.24.1\"\n```\nIt would be great if this could be configured via env var or similar."
    },
    {
        "title": "[Bug]: Unable to recover backup data from s3 storage",
        "id": 2504106197,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nodevswe@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nRecently i deployed cnpg into my self hosted k3s cluster with the following values.yml\r\n```\r\napiVersion: v1\r\ndata:\r\n  password: b21zb21zMTIz\r\n  username: b21z\r\n  ACCESS_KEY_ID: xxxx\r\n  ACCESS_SECRET_KEY: xxxx\r\n  AWS_REGION: xxxx\r\nkind: Secret\r\nmetadata:\r\n  name: cnpg-cluster-app-user\r\n  namespace: cnpg\r\ntype: kubernetes.io/basic-auth\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cnpg-cluster\r\n  namespace: cnpg\r\nspec:\r\n  # backup:\r\n  #   barmanObjectStore:\r\n  #     serverName: \"cnpg-cluster\"\r\n  #     destinationPath: \"s3://odev-pg\"\r\n  #     s3Credentials:\r\n  #       accessKeyId:\r\n  #         key: ACCESS_KEY_ID\r\n  #         name: cnpg-cluster-app-user\r\n  #       secretAccessKey:\r\n  #         key: ACCESS_SECRET_KEY\r\n  #         name: cnpg-cluster-app-user\r\n  #     wal:\r\n  #       compression: \"bzip2\"\r\n  #       encryption: \"AES256\"\r\n  #     data:\r\n  #       compression: \"bzip2\"\r\n  #       encryption: \"AES256\"\r\n  #   retentionPolicy: \"30d\"\r\n  #   target: \"prefer-standby\"\r\n  externalClusters:\r\n    - name: cluster-backup\r\n      barmanObjectStore:\r\n        serverName: \"cnpg-cluster\"\r\n        destinationPath: \"s3://odev-pg\"\r\n        wal:\r\n          compression: \"bzip2\"\r\n          encryption: \"AES256\"\r\n        data:\r\n          compression: \"bzip2\"\r\n          encryption: \"AES256\"\r\n        s3Credentials:\r\n          region:\r\n            key: AWS_REGION\r\n            name: cnpg-cluster-app-user\r\n          accessKeyId:\r\n            key: ACCESS_KEY_ID\r\n            name: cnpg-cluster-app-user\r\n          secretAccessKey:\r\n            key: ACCESS_SECRET_KEY\r\n            name: cnpg-cluster-app-user\r\n  instances: 3\r\n  storage:\r\n    size: 1Gi\r\n  postgresql:\r\n    parameters:\r\n      shared_buffers: 256MB\r\n      pg_stat_statements.max: '10000'\r\n      pg_stat_statements.track: all\r\n      log_statement: all\r\n      auto_explain.log_min_duration: '10s'\r\n    pg_hba:\r\n      - host all all 10.244.0.0/16 md5\r\n  bootstrap:\r\n    # initdb:\r\n    #   database: oms-db\r\n    #   owner: oms\r\n    #   secret:\r\n    #     name: cnpg-cluster-app-user\r\n    recovery:\r\n      source: cluster-backup\r\n```\r\nbut unfortunely it unable to recover due to some error: \"error\":\"no target backup found\"\n### Cluster resource\n_No response_\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-09-04T01:26:09Z\",\"msg\":\"Recovering from external cluster\",\"logging_pod\":\"cnpg-cluster-1-full-recovery\",\"sourceName\":\"cluster-backup\"}\r\n{\"level\":\"error\",\"ts\":\"2024-09-04T01:26:10Z\",\"msg\":\"Error while restoring a backup\",\"logging_pod\":\"cnpg-cluster-1-full-recovery\",\"error\":\"no target backup found\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/cmd.go:92\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.NewCmd.func2\\n\\tinternal/cmd/manager/instance/restore/cmd.go:63\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.22.6/x64/src/runtime/proc.go:271\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nodevswe@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nRecently i deployed cnpg into my self hosted k3s cluster with the following values.yml\r\n```\r\napiVersion: v1\r\ndata:\r\n  password: b21zb21zMTIz\r\n  username: b21z\r\n  ACCESS_KEY_ID: xxxx\r\n  ACCESS_SECRET_KEY: xxxx\r\n  AWS_REGION: xxxx\r\nkind: Secret\r\nmetadata:\r\n  name: cnpg-cluster-app-user\r\n  namespace: cnpg\r\ntype: kubernetes.io/basic-auth\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cnpg-cluster\r\n  namespace: cnpg\r\nspec:\r\n  # backup:\r\n  #   barmanObjectStore:\r\n  #     serverName: \"cnpg-cluster\"\r\n  #     destinationPath: \"s3://odev-pg\"\r\n  #     s3Credentials:\r\n  #       accessKeyId:\r\n  #         key: ACCESS_KEY_ID\r\n  #         name: cnpg-cluster-app-user\r\n  #       secretAccessKey:\r\n  #         key: ACCESS_SECRET_KEY\r\n  #         name: cnpg-cluster-app-user\r\n  #     wal:\r\n  #       compression: \"bzip2\"\r\n  #       encryption: \"AES256\"\r\n  #     data:\r\n  #       compression: \"bzip2\"\r\n  #       encryption: \"AES256\"\r\n  #   retentionPolicy: \"30d\"\r\n  #   target: \"prefer-standby\"\r\n  externalClusters:\r\n    - name: cluster-backup\r\n      barmanObjectStore:\r\n        serverName: \"cnpg-cluster\"\r\n        destinationPath: \"s3://odev-pg\"\r\n        wal:\r\n          compression: \"bzip2\"\r\n          encryption: \"AES256\"\r\n        data:\r\n          compression: \"bzip2\"\r\n          encryption: \"AES256\"\r\n        s3Credentials:\r\n          region:\r\n            key: AWS_REGION\r\n            name: cnpg-cluster-app-user\r\n          accessKeyId:\r\n            key: ACCESS_KEY_ID\r\n            name: cnpg-cluster-app-user\r\n          secretAccessKey:\r\n            key: ACCESS_SECRET_KEY\r\n            name: cnpg-cluster-app-user\r\n  instances: 3\r\n  storage:\r\n    size: 1Gi\r\n  postgresql:\r\n    parameters:\r\n      shared_buffers: 256MB\r\n      pg_stat_statements.max: '10000'\r\n      pg_stat_statements.track: all\r\n      log_statement: all\r\n      auto_explain.log_min_duration: '10s'\r\n    pg_hba:\r\n      - host all all 10.244.0.0/16 md5\r\n  bootstrap:\r\n    # initdb:\r\n    #   database: oms-db\r\n    #   owner: oms\r\n    #   secret:\r\n    #     name: cnpg-cluster-app-user\r\n    recovery:\r\n      source: cluster-backup\r\n```\r\nbut unfortunely it unable to recover due to some error: \"error\":\"no target backup found\"\n### Cluster resource\n_No response_\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-09-04T01:26:09Z\",\"msg\":\"Recovering from external cluster\",\"logging_pod\":\"cnpg-cluster-1-full-recovery\",\"sourceName\":\"cluster-backup\"}\r\n{\"level\":\"error\",\"ts\":\"2024-09-04T01:26:10Z\",\"msg\":\"Error while restoring a backup\",\"logging_pod\":\"cnpg-cluster-1-full-recovery\",\"error\":\"no target backup found\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/cmd.go:92\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.NewCmd.func2\\n\\tinternal/cmd/manager/instance/restore/cmd.go:63\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.22.6/x64/src/runtime/proc.go:271\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI'm observing a similar issue as well; the CNPG version is `1.23.3` in my case.\n---\n@alvin319 yup i was trying to figure whole day but still none of the solutions works yet...\n---\nI faced the same issue today while getting my head around backups/recoveries. In my case, the issue was that the `externalClusters.name` was not matching the backup name  (the folder name on my S3 storage). Looking at your manifest, is there any chance the backup is called `cnpg-cluster` and by setting the `externalClusters.name` field to that it works?\r\nI'm not sure if that's the expected value, I just came across this issue while troubleshooting and felt like leaving my findings here to try to help.\n---\n@odev-swe, this issue is resolved for me because I need to run an ad-hoc / scheduled backup to generate the base files and recover the DB along with the WAL files. See [this](https://cloudnative-pg.io/documentation/1.16/backup_recovery/#on-demand-backups).\n---\nI have the same problem on 1.24.1\n```\n{\"level\":\"info\",\"ts\":\"2024-10-29T09:31:01.963055589Z\",\"msg\":\"Recovering from external cluster\",\"logging_pod\":\"boostrap-demo-cnpg-1-full-recovery\",\"sourceName\":\"postgresql-uat01\"}\n{\"level\":\"error\",\"ts\":\"2024-10-29T09:31:02.25994965Z\",\"msg\":\"Error while restoring a backup\",\"logging_pod\":\"boostrap-demo-cnpg-1-full-recovery\",\"error\":\"no target backup found\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/cmd.go:93\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.NewCmd.func2\\n\\tinternal/cmd/manager/instance/restore/cmd.go:63\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\n```\n```\n---\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: boostrap-demo-cnpg\n  namespace: uat\nspec:\n  instances: 3\n  storage:\n    storageClass: local-path\n    size: 10Gi\n  bootstrap:\n    recovery:\n     source: postgresql-uat01\n  externalClusters:\n    - name: postgresql-uat01\n      barmanObjectStore:\n        endpointURL: \"http://minio.storage.svc\"\n        destinationPath: \"s3://postgresql-uat/postgresql-uat01\" #s3://bucket/last-server-name-to-restore\n        s3Credentials:\n          accessKeyId:\n            name: cnpg-barman-s3\n            key: CONSOLE_ACCESS_KEY\n          secretAccessKey:\n            name: cnpg-barman-s3\n            key: CONSOLE_SECRET_KEY\n        wal:\n          maxParallel: 8\n```\n![Image](https://github.com/user-attachments/assets/c22efd1a-1772-4c97-a25c-dfba80452bb6)\nUPDATE:\nSo after further readind the [docs](https://cloudnative-pg.io/documentation/1.20/bootstrap/#bootstrap-from-another-cluster) it was \"users error\" in the end, where destination should just point to the bucket and externalClusters.name will be appeneded.\n![Image](https://github.com/user-attachments/assets/6cf067a8-45a9-4f44-904f-a82a24a0a0c0)\nworking yaml\n```\n---\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: boostrap-demo-cnpg\n  namespace: uat\nspec:\n  instances: 3\n  storage:\n    storageClass: local-path\n    size: 10Gi\n  bootstrap:\n    recovery:\n     source: postgresql-uat01\n  externalClusters:\n    # By default the recovery method strictly uses the name of the cluster in the externalClusters section\n    # to locate the main folder of the backup data within the object store, which is normally reserved for\n    # the name of the server. You can specify a different one with the barmanObjectStore.serverName property\n    # (by default assigned to the value of name in the external cluster definition).\n    - name: postgresql-uat01\n      barmanObjectStore:\n        endpointURL: \"http://minio.storage.svc\"\n        destinationPath: \"s3://postgresql-uat\" #s3://bucket, NOTE: externalClusters.name will be appended here\n        s3Credentials:\n          accessKeyId:\n            name: cnpg-barman-s3\n            key: CONSOLE_ACCESS_KEY\n          secretAccessKey:\n            name: cnpg-barman-s3\n            key: CONSOLE_SECRET_KEY\n        wal:\n          maxParallel: 8\n```\n---\nFor anyone encountering this on Azure blob storage, you need to use the full path upto the container name under :\n```\n  externalClusters:\n      - name: <blob-prefix-on-azure-storage>\n        barmanObjectStore:\n          destinationPath: https://${PG_PRIMARY_STORAGE_ACCOUNT_NAME}.blob.core.windows.net/backups # This is the path of the storage account including container name\n```\n``\n---\nThis logic with `extrnalclusters[].name` is over complicated. As an operator, I'd expect the configuration of the restore block to take the same input as the backup block. \nWith it, no headeachs when we recover, we \"just\" have to copy/paste the block. \nThank you to everyone in this thread for solution and explanation.\n---\nCan you, guys, at any chance PRINT in the logs WHAT exactly operator tries to fetch? It's a nightmare to debug Go logs that doesn't make any sense. \nThanks\n---\n> I faced the same issue today while getting my head around backups/recoveries. In my case, the issue was that the `externalClusters.name` was not matching the backup name (the folder name on my S3 storage). Looking at your manifest, is there any chance the backup is called `cnpg-cluster` and by setting the `externalClusters.name` field to that it works?\n> \n> I'm not sure if that's the expected value, I just came across this issue while troubleshooting and felt like leaving my findings here to try to help.\n\u8001\u54e5,\u4f60\u8fd9\u4e2a\u65b9\u6cd5\u597d\u7528,\u5c31\u662f\u8fd9\u4e2a\u95ee\u9898. \n  externalClusters:\n    - name: pg-instance \u8981\u5199\u6210\u5b9e\u9645\u7684\u6587\u4ef6\u5939\u7684\u540d\u79f0\u624d\u53ef\u4ee5"
    },
    {
        "title": "[Chore]: migrate Renovate config to new syntax",
        "id": 2503157821,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nCheck any recent [continuous-integration/renovate-linter](https://github.com/cloudnative-pg/cloudnative-pg/blob/main/.github/workflows/continuous-integration.yml#L170) run. The linter suggests migrating to a newer config syntax.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.31\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nCheck any recent [continuous-integration/renovate-linter](https://github.com/cloudnative-pg/cloudnative-pg/blob/main/.github/workflows/continuous-integration.yml#L170) run. The linter suggests migrating to a newer config syntax.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Docs]: Update E2E tests page",
        "id": 2502195132,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThe [E2E test page](https://cloudnative-pg.io/documentation/current/e2e/) is not up-to-date\n### Describe what additions need to be done to the documentation\nAdd the tests that have been introduced and update K8s and Postgres versions.\n### Describe what pages need to change in the documentation, if any\nhttps://cloudnative-pg.io/documentation/current/e2e/\n### Describe what pages need to be removed from the documentation, if any\n-\n### Additional context\n_No response_\n### Backport?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThe [E2E test page](https://cloudnative-pg.io/documentation/current/e2e/) is not up-to-date\n### Describe what additions need to be done to the documentation\nAdd the tests that have been introduced and update K8s and Postgres versions.\n### Describe what pages need to change in the documentation, if any\nhttps://cloudnative-pg.io/documentation/current/e2e/\n### Describe what pages need to be removed from the documentation, if any\n-\n### Additional context\n_No response_\n### Backport?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: support rewind on cluster demotion without token if target primary is available",
        "id": 2497787583,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nBootstraping from storage snapshots will meet many use cases for quickly re-establishing regional redundancy after an unplanned failover, particularly in cases where the original cluster is no longer available or cannot be trusted.\r\nNonetheless, there are some cases - for example a temporary DC-level network outage [eg. canonical backhoe hits a fiber line?] - where a primary cluster is temporarily unavailable, the business triggers a regional cross cluster failover, and the original cluster becomes available again a short time later.\r\n### Describe the solution you'd like\r\nIn cases like these, we could reduce the RTO on full cross-region redundancy by adding support for in-place conversion of the existing cluster into a replica, avoiding the need to replay WAL from the time of the last storage snapshot.\r\n### Describe alternatives you've considered\r\nThis will also reduce the need for very frequent snapshots, which some CNPG users might attempt as a means to mitigate concerns over too much log replay in this scenario.\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nNo\r\n### Are you willing to actively contribute to this feature?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nBootstraping from storage snapshots will meet many use cases for quickly re-establishing regional redundancy after an unplanned failover, particularly in cases where the original cluster is no longer available or cannot be trusted.\r\nNonetheless, there are some cases - for example a temporary DC-level network outage [eg. canonical backhoe hits a fiber line?] - where a primary cluster is temporarily unavailable, the business triggers a regional cross cluster failover, and the original cluster becomes available again a short time later.\r\n### Describe the solution you'd like\r\nIn cases like these, we could reduce the RTO on full cross-region redundancy by adding support for in-place conversion of the existing cluster into a replica, avoiding the need to replay WAL from the time of the last storage snapshot.\r\n### Describe alternatives you've considered\r\nThis will also reduce the need for very frequent snapshots, which some CNPG users might attempt as a means to mitigate concerns over too much log replay in this scenario.\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nNo\r\n### Are you willing to actively contribute to this feature?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductSorry if this is not the right place to ask, but does this address a way to do unplanned failures via manual processes compared to the Promotion Tokens that was recently released? It's something I'm trying to figure out how to do in a reliable way. Currently, I have a keepalived script that does a pg_rewind that works pretty well but have been looking at moving to a k8s deployment.\n---\nI am testing some DR scenarios and I believe that this feature would be useful for the disaster scenarios I am testrunning (basically, the unplanned falure where a new primary is started with recovery from object store). However, a question:\n> avoiding the need to replay WAL from the time of the last storage snapshot.\nCan this be done without recreating the cluster? I tried to demote an existing cluster without token and did not succeed, was I doing something wrong? It was stuck into some postgres errors and did not recover (maybe I was not patient enough, I have yet to replay it again and analyze logs)"
    },
    {
        "title": "[Chore]: Automate the refresh of licenses directory",
        "id": 2491881129,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe licenses directory is not updated automatically, but there is a Makefile target that a developer needs to run to update the files on disk. Then, they need to manually create a PR. This process should be automated.\n### Describe the solution you'd like\n We should periodically run it and automatically create a pull request if there is something to update.\n### Describe alternatives you've considered\nN/A\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe licenses directory is not updated automatically, but there is a Makefile target that a developer needs to run to update the files on disk. Then, they need to manually create a PR. This process should be automated.\n### Describe the solution you'd like\n We should periodically run it and automatically create a pull request if there is something to update.\n### Describe alternatives you've considered\nN/A\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Docs]: Role Based Access Control (RBAC) is outdated/wrong",
        "id": 2486564373,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThe problem is that the current documentation states something about the cluster role cnpg-manager. (https://cloudnative-pg.io/documentation/1.24/security/#role-based-access-control-rbac). But this role is not deployed with the current helm chart (https://github.com/cloudnative-pg/charts/blob/main/charts/cloudnative-pg/templates/rbac.yaml).\r\nIt would be good to update the section of the documentation to give administrators the chance to have correct arguments when talking to their security department to get approval to run the operator.\n### Describe what additions need to be done to the documentation\n_No response_\n### Describe what pages need to change in the documentation, if any\n_No response_\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\nOur security department reviews all new operators which require additional permissions which are not limited to its own namespace. Giving wrong information to them makes a bad first impression and reduces the chance of getting the operator deployed. Also, reading through the complete source code is not an option.\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThe problem is that the current documentation states something about the cluster role cnpg-manager. (https://cloudnative-pg.io/documentation/1.24/security/#role-based-access-control-rbac). But this role is not deployed with the current helm chart (https://github.com/cloudnative-pg/charts/blob/main/charts/cloudnative-pg/templates/rbac.yaml).\r\nIt would be good to update the section of the documentation to give administrators the chance to have correct arguments when talking to their security department to get approval to run the operator.\n### Describe what additions need to be done to the documentation\n_No response_\n### Describe what pages need to change in the documentation, if any\n_No response_\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\nOur security department reviews all new operators which require additional permissions which are not limited to its own namespace. Giving wrong information to them makes a bad first impression and reduces the chance of getting the operator deployed. Also, reading through the complete source code is not an option.\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductHello @mk-webfleet \r\nCan you clarify which role is the one that is not deployed?\r\nThank you!\n---\nHi @sxd,\r\ncurrently, the role cnpg-manager is not deployed. But the following roles are deployed:\r\n* cnpg-cloudnative-pg\r\n* cnpg-cloudnative-pg-edit\r\n* cnpg-cloudnative-pg-view\r\nSee https://github.com/cloudnative-pg/charts/blob/main/charts/cloudnative-pg/templates/rbac.yaml#L21, https://github.com/cloudnative-pg/charts/blob/main/charts/cloudnative-pg/templates/rbac.yaml#L393 and https://github.com/cloudnative-pg/charts/blob/main/charts/cloudnative-pg/templates/rbac.yaml#L417\r\nThx for investigating.\n---\n@mk-webfleet \r\nThe documentation states  \"The specific name and type of this role also depend on the deployment method\", probably here is missing documentation for the chart but not the operator, probably we can ask the creator of that patch to document the roles in the helm chart?\r\nOn the documentation side I don't see a problem, since we cannot document every possible way to name a role or type of it\r\nRegards,\n---\n@sxd: I agree with you that the name can be different. But what puzzled me was the fact that instead of one role, now 3 roles are deployed. And since the helm chart is part of this project I thought I mention it. But if you think that this is out of scope, then please forgive me and close the ticket with \"wont do\". Fine with me. Thx anyway for your time.\n---\n@mk-webfleet the helm chart it's another project, same organization but different project, so it should have it's own documentation I think, probably we should add this documentation there too with the specific modification for that, what do you think about it? \r\nWhat do you think about adding documentation to the helm chart @phisco ? \r\nThanks for bringing this @mk-webfleet it's really useful to have the perspective of users !\n---\n@sxd: I think my confusion came from the fact that https://cloudnative-pg.io/documentation/1.24/ is the primary site for documentation. Also, in a sub-page for installation (https://cloudnative-pg.io/documentation/1.24/installation_upgrade/#using-the-helm-chart) one method is Helm. Nowhere is stated that depending on the installation method, there are differences for the operator.\r\nFrom the documentation page of view, the installation method is just a tiny part of the whole page and as a user I would expect that regardless how I get the operator running, everything else in the documentation applies."
    },
    {
        "title": "Made barman integration to gs easier to set up by improving docs",
        "id": 2485518509,
        "state": "open",
        "first": "by providing an easier to understand gcp service account value pattern and relevant docs",
        "messages": "by providing an easier to understand gcp service account value pattern and relevant docsIn addition to what @valeriodelsarto wrote, please fix the DCO issue reported here https://github.com/cloudnative-pg/cloudnative-pg/pull/5370/checks?check_run_id=29227595276\n---\nCiao ragazzi!\r\nI appreciate your comments!\r\nI wish I could be more helpful but literally fresh-off-the-boat when it comes to GKE.\r\nPlease, take the lead on this.\r\nMy feedback is: setting up backup to GCP GS is way underdocumented. It took me like 2 or 3 days to get the barman thing going. BTW I can see WALs coming into GS but not the data. Is that gonna come in once every 24 hours or is more config needed for the non-wal backup on barman thing?\r\nThank you for this wonderful thing and for letting me know I should create a custom service account and specify that instead.\r\nFixed the DCO thing.\n---\nI followed the tutorial you [linked](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#migrate_applications_to)  - thanks. \r\nI set it up and confirmed it's set up correctly by running commands in https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#verify chapter.\r\nIt doesn't work in this thing.\r\n```yaml\r\n# i tried creating a sa in both namespaces on separate occasions\r\n# try with default namespace\r\niam.gke.io/gcp-service-account: postgres-barman-gs@PROJECT-ID.iam.gserviceaccount.com\r\n# try with custom namesapce\r\niam.gke.io/gcp-service-account: db/postgres-barman-gs@PROJECT-ID.iam.gserviceaccount.com\r\n```\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-08-27T00:38:18Z\",\"logger\":\"barman-cloud-check-wal-archive\",\"msg\":\"2024-08-27 00:38:18,854 [51] ERROR: Barman cloud WAL archive check exception: (\\\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/db/postgres-barman-gs@PROJECT-ID.iam.gserviceaccount.com/token?scopes=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control%2Chttps%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.read_only%2Chttps%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.read_write from the Google Compute Engine metadata service. Status: 404 Response:\\\\nb'Not Found\\\\\\\\n'\\\", <google.auth.transport.requests._Response object at 0x7ec5cfb37e50>)\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-1\"}\r\n```\r\nI tried both string and numerical PROJECT-ID, but I'm guessing it's the string one.\r\nI tried both `default` namespace service account and a `db` namespace service account.\n---\n> I tried both string and numerical PROJECT-ID, but I'm guessing it's the string one.\r\n> I tried both `default` namespace service account and a `db` namespace service account.\r\nHi Nino, for the PROJECT-ID you should use the ID and not the NAME, e.g. what you get by doing `gcloud config get-value project` with the CLI or looking at the ID column in the UI when selecting your project.\r\nBefore the @ you should put the GCP Service Account Name, without any namespace reference. The namespace of the GKE Service Account is being set in the GCP Service Account permissions, e.g. step 5 of [this guide](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity?hl=it#kubernetes-sa-to-iam) when doing\r\n```\r\ngcloud iam service-accounts add-iam-policy-binding IAM_SA_NAME@IAM_SA_PROJECT_ID.iam.gserviceaccount.com \\\r\n    --role roles/iam.workloadIdentityUser \\\r\n    --member \"serviceAccount:PROJECT_ID.svc.id.goog[NAMESPACE/KSA_NAME]\"\r\n```\r\nLet me know if it still doesn't work!\n---\nDear @valeriodelsarto,\r\nI was trying various things, but I did do what you described as well, except I was linking a GKE ServiceAccount I created, not GCP SA.\r\nWhich GCP SA should I link? In GCP -> IAM -> Service Accounts I can only see the \"Compute Engine default service account\".\r\nhmmm I'll go over it again and share all my steps in case it still doesn't work.\r\nThank you Valerio.\n---\n> Which GCP SA should I link?\r\nNo worries!\r\nYou are supposed to create a new GCP Service Account, usually we create a single one per PG cluster to give more granular permissions to each one, and use it in the CNPG cluster annotation.\r\nFeel free to reach out if you need!\n---\nOkay I created a new service account in GCP web console. Its email column is `postgres@PROJECT-ID.iam.gserviceaccount.com` so I put that into the `serviceAccountTemplate.metadata.annotation.iam.gke.io/gcp-service-account`.\r\nThen I applied your last piece of code, which I understand is binding service account to an iam policy:\r\n```bash\r\ngcloud iam service-accounts add-iam-policy-binding postgres@PROJECT-ID.iam.gserviceaccount.com \\\r\n    --role roles/iam.workloadIdentityUser \\\r\n    --member \"serviceAccount:PROJECT-ID.svc.id.goog[db/postgres-barman-gs]\"\r\n```\r\ndb is namespace and postgres-barman-gs is service account.\r\nThe error is:\r\n```\r\n'Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/postgres@PROJECT-ID.iam.gserviceaccount.com/token?scopes=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control%2Chttps%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.read_only%2Chttps%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.read_write from the Google Compute Engine metadata service. Status: 403 Response:\\\\nb\\\"Unable to generate access token; IAM returned 403 Forbidden: Permission \\\\'iam.serviceAccounts.getAccessToken\\\\' denied on resource (or it may not exist).\\\\\\\\nThis error could be caused by a missing IAM policy binding on the target IAM service account.\\\\\\\\nFor more information, refer to the Workload Identity documentation:\\\\\\\\n\\\\\\\\thttps://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#authenticating_to\\\\\\\\n\\\\\\\\n\\\"', <google.auth.transport.requests._Response object at 0x78c43c92ad30>)\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-1\"}\r\n```\r\nI did try giving the IAM SA full admin access to cloud storage.\n---\nThat's weird because the error you got is saying that the Kubernetes service account is missing rights to get a token from the GCP service account, which is clearly what you should have gave by running the command you reported\r\n```\r\ngcloud iam service-accounts add-iam-policy-binding postgres@PROJECT-ID.iam.gserviceaccount.com \\\r\n    --role roles/iam.workloadIdentityUser \\\r\n    --member \"serviceAccount:PROJECT-ID.svc.id.goog[db/postgres-barman-gs]\"\r\n```\r\nI suppose what you can try checking is:\r\n1. Ensure that the IAM policy binding for the `roles/iam.workloadIdentityUser` role is correctly set up for the GCP service account: `gcloud iam service-accounts get-iam-policy postgres@PROJECT-ID.iam.gserviceaccount.com` (should report the role `roles/iam.workloadIdentityUser` assigned to the K8s service account previously set);\r\n2. Verify that the Kubernetes service account is annotated with the correct GCP service account: `kubectl get serviceaccount postgres-barman-gs -n db -o yaml` checking for the annotation. Unfortunately you can't check PG pod for injected ENV variables like you could on Azure or AWS because they both use a mutating webhook to inject the workload identity token but GCP does not work in the same way for GKE;\r\n3. Ensure that Workload Identity is correctly configured for your GKE cluster: `gcloud container clusters describe CLUSTER_NAME --region REGION` (should report something like workloadIdentityConfig: workloadPool: PROJECT-ID.svc.id.goog);\r\nIf all of them are correct, I suppose you can try doing a PG restart like using the kubectl CNPG plugin with `kubectl cnpg restart cnpg-cluster-name` and see if something change...\n---\nThanks so much for this Valerio, I really appreciate it. I'll go through it again and report back.\n---\nJust trying to see if this can be added to the backlog for 1.24.2. Any news @NinoSkopac and @valeriodelsarto ? Thanks\n---\nI haven't got around doing it yet\n---\nI think if we apply my suggestions we can already go ahead and merge, at least the doc will be more clear"
    },
    {
        "title": "[Bug]: ",
        "id": 2485197990,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ng.colangiuli@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.3\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nI'm selfhosting a k3s cluster with 3 server nodes in high avaiability using etcd.\r\n2 nodes are intel and 1 nodes is arm (a raspberry pi 5)\r\nI deployed the operator with helm chart:\r\nhelm upgrade --install cnpg --namespace cnpg-system --create-namespace cnpg/cloudnative-pg --values op-values.yaml\r\nwhere in op-values.yaml I only changed the replica from 1 to 3, all the other value are default.\r\nThen I deployed a database with helm chart:\r\nhelm upgrade --install database --namespace authentik-ha --create-namespace --values values.yaml cnpg/cluster\r\nin values.yaml I only changed the initdb part to init a new database with my user&password:\r\n initdb:\r\n    database: authentik\r\n    owner: user\r\n    secret:\r\n      name: cluster-example-app-user\r\nAfter the deployment I have an SVC called:\r\ndatabase-cluster-rw \r\nthat point to the master pod, in this moment is:\r\ndatabase-cluster-2  \r\nIf I kill the pod database-cluster-2  it remains in terminating status for minutes and the SVC still point it for minutes instead of \"rapidly switching\" to another master.\r\n**My impression is that if the pod of the master node is not terminated the SVC still point to it**\r\nIn this way I didn't have the High Avaiability because killing the master node need minutes before the terminating process ends. And if for some reason the pod stay in terminating status the risk is to don't have the switch to the new master.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ng.colangiuli@gmail.com\n### Version\n1.24.0\n### What version of Kubernetes are you using?\n1.3\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nI'm selfhosting a k3s cluster with 3 server nodes in high avaiability using etcd.\r\n2 nodes are intel and 1 nodes is arm (a raspberry pi 5)\r\nI deployed the operator with helm chart:\r\nhelm upgrade --install cnpg --namespace cnpg-system --create-namespace cnpg/cloudnative-pg --values op-values.yaml\r\nwhere in op-values.yaml I only changed the replica from 1 to 3, all the other value are default.\r\nThen I deployed a database with helm chart:\r\nhelm upgrade --install database --namespace authentik-ha --create-namespace --values values.yaml cnpg/cluster\r\nin values.yaml I only changed the initdb part to init a new database with my user&password:\r\n initdb:\r\n    database: authentik\r\n    owner: user\r\n    secret:\r\n      name: cluster-example-app-user\r\nAfter the deployment I have an SVC called:\r\ndatabase-cluster-rw \r\nthat point to the master pod, in this moment is:\r\ndatabase-cluster-2  \r\nIf I kill the pod database-cluster-2  it remains in terminating status for minutes and the SVC still point it for minutes instead of \"rapidly switching\" to another master.\r\n**My impression is that if the pod of the master node is not terminated the SVC still point to it**\r\nIn this way I didn't have the High Avaiability because killing the master node need minutes before the terminating process ends. And if for some reason the pod stay in terminating status the risk is to don't have the switch to the new master.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductCan you please confirm that your Kubernetes cluster only has 3 nodes in total with two different architectures and you are asking to run a Postgres cluster on different architectures?\n---\nYes I confirm, 2 node are intel 64 , 1 node is ARMv8.\r\nAll the 3 node are K3S server node to have the HA witch etcd.\r\nThe cluster starts, I'm able to install successfully an application working with it (Authentik) but when I test killing one pod of the cluster or power off one entire node, the entire application working on it stop working. \r\nWhat I look is that the node of the databaese killed stay in terminating state for minutes and the RW service take minutes before pointing to the new master.\r\nMy expectations is that this switch take less to avoid that that the application connected go down."
    },
    {
        "title": "[Feature]: Allow to set replication slot on recovery (standby) cluster",
        "id": 2483361201,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIf I want to create replication (standby) cluster based on another cluster I not have option to utilize replication slots while doing sync from origin.\n### Describe the solution you'd like\nExpand existing `externalClusters[*].connectionParameters` to support `primary_slot_name` that will be rendered to `/var/lib/postgresql/data/pgdata/override.conf` near `primary_conninfo=...`\n### Describe alternatives you've considered\nNot see any\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIf I want to create replication (standby) cluster based on another cluster I not have option to utilize replication slots while doing sync from origin.\n### Describe the solution you'd like\nExpand existing `externalClusters[*].connectionParameters` to support `primary_slot_name` that will be rendered to `/var/lib/postgresql/data/pgdata/override.conf` near `primary_conninfo=...`\n### Describe alternatives you've considered\nNot see any\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductThe request is to allow a replica cluster to specify a replication slot that must exist on its source cluster as part of the `externalClusters` definition. Only the *designated primary* will set `primary_slot_name` when connecting to the source, while the replicas in the cluster will continue to use the default replication slot for high availability (if enabled).\r\nTherefore, we need to understand how to incorporate this into a distributed topology scenario to facilitate the coordination of resetting the replication slot after a successful demotion and promotion of a new primary.\n---\nHi, @gbartolini, first of all thank you for taking attention to my FR.\r\nAbout your comment, yes only `Primary standby server` will use `primary_slot_name` while replicating data from `externalClusters`, when new `Primary standby server` will be elected his configuration of `primary_conninfo` should be changed from `Primary standby server` to `externalCluster` one, and `primary_slot_name` adjusted to correct one. The same opposite action should be done while we downgrade `Primary standby` to `Replica` - change `primary_conninfo` and `primary_slot_name` to listen `Primary standby server` and use `slot` managed by Cloudnative-PG.\r\nAFAIK such changes requires `postgres` process to be restarted to pick up new settings: https://postgresqlco.nf/doc/en/param/primary_conninfo/\n---\nThis deserves more discussion as we need to integrate this in the demotion and promotion phases. Moving to 1.26."
    },
    {
        "title": "[Feature]: Add namespace to object storage backup path",
        "id": 2480973687,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nWhen uploading backups to object storage the destination defaults to value of: `[.spec.backup.barmanObjectStore.destinationPath]/[.metadata.name]`. This makes it awkward to handle multiple deployments of multiple clusters **with the same name** in different namespaces since, by default, they will conflict when trying to send backups to the same path.\r\nI'm aware that you can override the path by changing the value of `.spec.backup.barmanObjectStore.destinationPath` or `.spec.backup.barmanObjectStore.serverName` to prevent conflicting backup paths.\r\nCurrently we get around this by templating the namespace in a flux kustomization's `postBuild` and set the path to something like: `destinationPath: s3://pg-backups/${namespace}`.\r\n### Describe the solution you'd like\r\nI would like the operator to handle adding the namespace to the object storage path. Given that its valid to deploy clusters with the same name in different namespaces, object storage backups should also be compatible.\r\nPerhaps a bool to control whether namespaces further scope the backups like this:\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: example-pg\r\nspec:\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3://pg-backups\r\n      namespacedPath: true # <-- NEW BOOL\r\n```\r\nWhen `namespacedPath` is `true` it results in a final path of `s3://pg-backups/[namespace]/[cluster name]`\r\nInitially, `namespacedPath` can also default to `false` to keep with current backup behavior, but I think eventually `true` should become the default.\r\n### Describe alternatives you've considered\r\nWe already try to work around this by templating the namespace in a kustomization.\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nWhen uploading backups to object storage the destination defaults to value of: `[.spec.backup.barmanObjectStore.destinationPath]/[.metadata.name]`. This makes it awkward to handle multiple deployments of multiple clusters **with the same name** in different namespaces since, by default, they will conflict when trying to send backups to the same path.\r\nI'm aware that you can override the path by changing the value of `.spec.backup.barmanObjectStore.destinationPath` or `.spec.backup.barmanObjectStore.serverName` to prevent conflicting backup paths.\r\nCurrently we get around this by templating the namespace in a flux kustomization's `postBuild` and set the path to something like: `destinationPath: s3://pg-backups/${namespace}`.\r\n### Describe the solution you'd like\r\nI would like the operator to handle adding the namespace to the object storage path. Given that its valid to deploy clusters with the same name in different namespaces, object storage backups should also be compatible.\r\nPerhaps a bool to control whether namespaces further scope the backups like this:\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: example-pg\r\nspec:\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3://pg-backups\r\n      namespacedPath: true # <-- NEW BOOL\r\n```\r\nWhen `namespacedPath` is `true` it results in a final path of `s3://pg-backups/[namespace]/[cluster name]`\r\nInitially, `namespacedPath` can also default to `false` to keep with current backup behavior, but I think eventually `true` should become the default.\r\n### Describe alternatives you've considered\r\nWe already try to work around this by templating the namespace in a kustomization.\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conductthat would be really nice to have \u2795"
    },
    {
        "title": "Declarative role management (`Role` CRD)",
        "id": 2479044714,
        "state": "open",
        "first": "With the implementation of database management via the `Database` CRD, it\u2019s important to offer an alternative approach to role management, currently handled through the `.spec.managed.roles` stanza. We propose introducing a new CRD called `Role`, which will eventually replace the existing implementation. For now, we will maintain both interfaces, ensuring that they are mutually exclusive if coexistence is not feasible.",
        "messages": "With the implementation of database management via the `Database` CRD, it\u2019s important to offer an alternative approach to role management, currently handled through the `.spec.managed.roles` stanza. We propose introducing a new CRD called `Role`, which will eventually replace the existing implementation. For now, we will maintain both interfaces, ensuring that they are mutually exclusive if coexistence is not feasible.Given the current timeline and the need to release version 1.25 before the end of the year, there is not enough time to complete this feature within the 1.25 release cycle. I\u2019m moving this to version 1.26 for better planning and execution."
    },
    {
        "title": "[Feature]: Allow statements before initialization in BootstrapInitDB",
        "id": 2478071370,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nFor some extensions it is required to run statements before executing `pg_restore` (for TimescaleDB it is `timescaledb_pre_restore()`, see [docs](https://docs.timescale.com/api/latest/administration/#timescaledb_pre_restore)).\n### Describe the solution you'd like\nIt would be helpful to have `preInitSQL` next to the existing `postInitSQL` parameter to support this use case.\n### Describe alternatives you've considered\nNone \n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nFor some extensions it is required to run statements before executing `pg_restore` (for TimescaleDB it is `timescaledb_pre_restore()`, see [docs](https://docs.timescale.com/api/latest/administration/#timescaledb_pre_restore)).\n### Describe the solution you'd like\nIt would be helpful to have `preInitSQL` next to the existing `postInitSQL` parameter to support this use case.\n### Describe alternatives you've considered\nNone \n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct+1 \nI'm also looking for a similar solution"
    },
    {
        "title": "[Feature]: Labels inherited from pooler to the pods",
        "id": 2473049514,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njarmd@vestas.com\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhen deploying Pooler components using the cluster helm template, the INHERITED_LABELS from the operator is never deployed to the following components of the pooler:\r\n- Pooler\r\n- Deployment\r\n- Pods\r\nWe are using the following Operator deployment:\r\n```yaml\r\nreplicaCount: 3\r\ncrds:\r\n  create: true\r\npodLabels:\r\n  vks.local/tenant: \"o11y\"\r\n  vks.local/finance-id: \"CF_UID_0012\"\r\nconfig:\r\n  data:\r\n    INHERITED_LABELS: environment, workload, app, vks.local/tenant, vks.local/finance-id\r\nresources:\r\n  limits:\r\n    memory: 200Mi\r\n  requests:\r\n    cpu: 100m\r\n    memory: 200Mi\r\nmonitoring:\r\n  podMonitorEnabled: true\r\n  grafanaDashboard:\r\n    create: false\r\n    namespace: \"insights-ui\"\r\n```\r\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/last-applied-configuration: |\r\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"insights-ui\",\"app.kubernetes.io/managed-by\":\"Helm\",\"app.kubernetes.io/name\":\"cnpg-postgresql-cluster\",\"app.kubernetes.io/part-of\":\"cloudnative-pg\",\"argocd.argoproj.io/instance\":\"o11y-azweu-stg-insights-ui\",\"helm.sh/chart\":\"cluster-0.0.9\",\"vks.local/finance-id\":\"CF_UID_0012\",\"vks.local/tenant\":\"o11y\"},\"name\":\"cnpg-postgresql-cluster\",\"namespace\":\"insights-ui\"},\"spec\":{\"affinity\":{\"topologyKey\":\"topology.kubernetes.io/zone\"},\"bootstrap\":{\"initdb\":{\"database\":\"grafana\",\"owner\":\"grafana\",\"postInitApplicationSQL\":null,\"secret\":{\"name\":\"insights-ui-cnpg-app-credentials-grafana\"}}},\"enableSuperuserAccess\":true,\"imageName\":\"ghcr.io/cloudnative-pg/postgresql:15.2\",\"imagePullPolicy\":\"IfNotPresent\",\"instances\":3,\"logLevel\":\"info\",\"managed\":null,\"monitoring\":{\"enablePodMonitor\":true},\"postgresGID\":26,\"postgresUID\":26,\"postgresql\":{\"shared_preload_libraries\":null},\"primaryUpdateMethod\":\"switchover\",\"primaryUpdateStrategy\":\"unsupervised\",\"priorityClassName\":null,\"resources\":{\"limits\":{\"memory\":\"4Gi\"},\"requests\":{\"cpu\":\"600m\",\"memory\":\"4Gi\"}},\"storage\":{\"size\":\"5Gi\",\"storageClass\":\"managed-csi-premium\"},\"superuserSecret\":{\"name\":\"insights-ui-cnpg-superuser\"}}}\r\n  creationTimestamp: \"2024-08-19T07:06:44Z\"\r\n  generation: 1\r\n  labels:\r\n    app.kubernetes.io/instance: insights-ui\r\n    app.kubernetes.io/managed-by: Helm\r\n    app.kubernetes.io/name: cnpg-postgresql-cluster\r\n    app.kubernetes.io/part-of: cloudnative-pg\r\n    argocd.argoproj.io/instance: o11y-azweu-stg-insights-ui\r\n    helm.sh/chart: cluster-0.0.9\r\n    vks.local/finance-id: CF_UID_0012\r\n    vks.local/tenant: o11y\r\n  name: cnpg-postgresql-cluster\r\n  namespace: insights-ui\r\n  resourceVersion: \"479713565\"\r\n  uid: e62d306c-e5be-42ea-b484-2c34d4758657\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n    topologyKey: topology.kubernetes.io/zone\r\n  bootstrap:\r\n    initdb:\r\n      database: grafana\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: grafana\r\n      secret:\r\n        name: insights-ui-cnpg-app-credentials-grafana\r\n  enablePDB: true\r\n  enableSuperuserAccess: true\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:15.2\r\n  imagePullPolicy: IfNotPresent\r\n  instances: 3\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_level: logical\r\n      wal_log_hints: \"on\"\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: switchover\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    synchronizeReplicas:\r\n      enabled: true\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      memory: 4Gi\r\n    requests:\r\n      cpu: 600m\r\n      memory: 4Gi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 5Gi\r\n    storageClass: managed-csi-premium\r\n  superuserSecret:\r\n    name: insights-ui-cnpg-superuser\r\n  switchoverDelay: 3600\r\nstatus:\r\n  availableArchitectures:\r\n  - goArch: amd64\r\n    hash: 144e71b00bdcfc5edafa10055fb0cc4a6efa9f467a8e66826d5e7bb2b254b706\r\n  - goArch: arm64\r\n    hash: 0027f50a9d35e24040cfc2f27cea04cbdf4375c226ac7b42764b5bb91f9beca4\r\n  certificates:\r\n    clientCASecret: cnpg-postgresql-cluster-ca\r\n    expirations:\r\n      cnpg-postgresql-cluster-ca: 2024-11-17 07:01:44 +0000 UTC\r\n      cnpg-postgresql-cluster-replication: 2024-11-17 07:01:44 +0000 UTC\r\n      cnpg-postgresql-cluster-server: 2024-11-17 07:01:44 +0000 UTC\r\n    replicationTLSSecret: cnpg-postgresql-cluster-replication\r\n    serverAltDNSNames:\r\n    - cnpg-postgresql-cluster-rw\r\n    - cnpg-postgresql-cluster-rw.insights-ui\r\n    - cnpg-postgresql-cluster-rw.insights-ui.svc\r\n    - cnpg-postgresql-cluster-r\r\n    - cnpg-postgresql-cluster-r.insights-ui\r\n    - cnpg-postgresql-cluster-r.insights-ui.svc\r\n    - cnpg-postgresql-cluster-ro\r\n    - cnpg-postgresql-cluster-ro.insights-ui\r\n    - cnpg-postgresql-cluster-ro.insights-ui.svc\r\n    serverCASecret: cnpg-postgresql-cluster-ca\r\n    serverTLSSecret: cnpg-postgresql-cluster-server\r\n  cloudNativePGCommitHash: 2b489ad6\r\n  cloudNativePGOperatorHash: 144e71b00bdcfc5edafa10055fb0cc4a6efa9f467a8e66826d5e7bb2b254b706\r\n  conditions:\r\n  - lastTransitionTime: \"2024-08-19T07:26:36Z\"\r\n    message: Cluster is Ready\r\n    reason: ClusterIsReady\r\n    status: \"True\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-08-19T07:10:43Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"479605130\"\r\n  currentPrimary: cnpg-postgresql-cluster-1\r\n  currentPrimaryTimestamp: \"2024-08-19T07:10:43.374717Z\"\r\n  healthyPVC:\r\n  - cnpg-postgresql-cluster-1\r\n  - cnpg-postgresql-cluster-2\r\n  - cnpg-postgresql-cluster-3\r\n  image: ghcr.io/cloudnative-pg/postgresql:15.2\r\n  instanceNames:\r\n  - cnpg-postgresql-cluster-1\r\n  - cnpg-postgresql-cluster-2\r\n  - cnpg-postgresql-cluster-3\r\n  instances: 3\r\n  instancesReportedState:\r\n    cnpg-postgresql-cluster-1:\r\n      isPrimary: true\r\n      timeLineID: 1\r\n    cnpg-postgresql-cluster-2:\r\n      isPrimary: false\r\n      timeLineID: 1\r\n    cnpg-postgresql-cluster-3:\r\n      isPrimary: false\r\n      timeLineID: 1\r\n  instancesStatus:\r\n    healthy:\r\n    - cnpg-postgresql-cluster-1\r\n    - cnpg-postgresql-cluster-2\r\n    - cnpg-postgresql-cluster-3\r\n  latestGeneratedNode: 3\r\n  managedRolesStatus: {}\r\n  phase: Cluster in healthy state\r\n  poolerIntegrations:\r\n    pgBouncerIntegration:\r\n      secrets:\r\n      - cnpg-postgresql-cluster-pooler\r\n  pvcCount: 3\r\n  readService: cnpg-postgresql-cluster-r\r\n  readyInstances: 3\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"479713560\"\r\n    clientCaSecretVersion: \"479605102\"\r\n    replicationSecretVersion: \"479605104\"\r\n    serverCaSecretVersion: \"479605102\"\r\n    serverSecretVersion: \"479605103\"\r\n    superuserSecretVersion: \"479713561\"\r\n  switchReplicaClusterStatus: {}\r\n  targetPrimary: cnpg-postgresql-cluster-1\r\n  targetPrimaryTimestamp: \"2024-08-19T07:10:17.321994Z\"\r\n  timelineID: 1\r\n  topology:\r\n    instances:\r\n      cnpg-postgresql-cluster-1: {}\r\n      cnpg-postgresql-cluster-2: {}\r\n      cnpg-postgresql-cluster-3: {}\r\n    nodesUsed: 3\r\n    successfullyExtracted: true\r\n  writeService: cnpg-postgresql-cluster-rw\r\nThe pooler resource:\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Pooler\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/last-applied-configuration: |\r\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Pooler\",\"metadata\":{\"annotations\":{},\"labels\":{\"argocd.argoproj.io/instance\":\"o11y-azweu-stg-insights-ui\"},\"name\":\"cnpg-postgresql-cluster-pooler-rw\",\"namespace\":\"insights-ui\"},\"spec\":{\"cluster\":{\"name\":\"cnpg-postgresql-cluster\"},\"instances\":3,\"monitoring\":{\"enablePodMonitor\":true},\"pgbouncer\":{\"parameters\":{\"default_pool_size\":\"25\",\"max_client_conn\":\"1000\"},\"poolMode\":\"session\"},\"type\":\"rw\"}}\r\n  creationTimestamp: \"2024-08-19T09:02:16Z\"\r\n  generation: 1\r\n  labels:\r\n    argocd.argoproj.io/instance: o11y-azweu-stg-insights-ui\r\n  name: cnpg-postgresql-cluster-pooler-rw\r\n  namespace: insights-ui\r\n  resourceVersion: \"479714049\"\r\n  uid: abfa604d-54e2-430a-a65d-e59af4b9ba11\r\nspec:\r\n  cluster:\r\n    name: cnpg-postgresql-cluster\r\n  instances: 3\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  pgbouncer:\r\n    parameters:\r\n      default_pool_size: \"25\"\r\n      max_client_conn: \"1000\"\r\n    paused: false\r\n    poolMode: session\r\n  type: rw\r\nstatus:\r\n  instances: 3\r\n  secrets:\r\n    clientCA:\r\n      name: cnpg-postgresql-cluster-ca\r\n      version: \"479605102\"\r\n    pgBouncerSecrets:\r\n      authQuery:\r\n        name: cnpg-postgresql-cluster-pooler\r\n        version: \"479605141\"\r\n    serverCA:\r\n      name: cnpg-postgresql-cluster-ca\r\n      version: \"479605102\"\r\n    serverTLS:\r\n      name: cnpg-postgresql-cluster-server\r\n      version: \"479605103\"\n```\n### Relevant log output\n```shell\nN/A\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njarmd@vestas.com\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhen deploying Pooler components using the cluster helm template, the INHERITED_LABELS from the operator is never deployed to the following components of the pooler:\r\n- Pooler\r\n- Deployment\r\n- Pods\r\nWe are using the following Operator deployment:\r\n```yaml\r\nreplicaCount: 3\r\ncrds:\r\n  create: true\r\npodLabels:\r\n  vks.local/tenant: \"o11y\"\r\n  vks.local/finance-id: \"CF_UID_0012\"\r\nconfig:\r\n  data:\r\n    INHERITED_LABELS: environment, workload, app, vks.local/tenant, vks.local/finance-id\r\nresources:\r\n  limits:\r\n    memory: 200Mi\r\n  requests:\r\n    cpu: 100m\r\n    memory: 200Mi\r\nmonitoring:\r\n  podMonitorEnabled: true\r\n  grafanaDashboard:\r\n    create: false\r\n    namespace: \"insights-ui\"\r\n```\r\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/last-applied-configuration: |\r\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"insights-ui\",\"app.kubernetes.io/managed-by\":\"Helm\",\"app.kubernetes.io/name\":\"cnpg-postgresql-cluster\",\"app.kubernetes.io/part-of\":\"cloudnative-pg\",\"argocd.argoproj.io/instance\":\"o11y-azweu-stg-insights-ui\",\"helm.sh/chart\":\"cluster-0.0.9\",\"vks.local/finance-id\":\"CF_UID_0012\",\"vks.local/tenant\":\"o11y\"},\"name\":\"cnpg-postgresql-cluster\",\"namespace\":\"insights-ui\"},\"spec\":{\"affinity\":{\"topologyKey\":\"topology.kubernetes.io/zone\"},\"bootstrap\":{\"initdb\":{\"database\":\"grafana\",\"owner\":\"grafana\",\"postInitApplicationSQL\":null,\"secret\":{\"name\":\"insights-ui-cnpg-app-credentials-grafana\"}}},\"enableSuperuserAccess\":true,\"imageName\":\"ghcr.io/cloudnative-pg/postgresql:15.2\",\"imagePullPolicy\":\"IfNotPresent\",\"instances\":3,\"logLevel\":\"info\",\"managed\":null,\"monitoring\":{\"enablePodMonitor\":true},\"postgresGID\":26,\"postgresUID\":26,\"postgresql\":{\"shared_preload_libraries\":null},\"primaryUpdateMethod\":\"switchover\",\"primaryUpdateStrategy\":\"unsupervised\",\"priorityClassName\":null,\"resources\":{\"limits\":{\"memory\":\"4Gi\"},\"requests\":{\"cpu\":\"600m\",\"memory\":\"4Gi\"}},\"storage\":{\"size\":\"5Gi\",\"storageClass\":\"managed-csi-premium\"},\"superuserSecret\":{\"name\":\"insights-ui-cnpg-superuser\"}}}\r\n  creationTimestamp: \"2024-08-19T07:06:44Z\"\r\n  generation: 1\r\n  labels:\r\n    app.kubernetes.io/instance: insights-ui\r\n    app.kubernetes.io/managed-by: Helm\r\n    app.kubernetes.io/name: cnpg-postgresql-cluster\r\n    app.kubernetes.io/part-of: cloudnative-pg\r\n    argocd.argoproj.io/instance: o11y-azweu-stg-insights-ui\r\n    helm.sh/chart: cluster-0.0.9\r\n    vks.local/finance-id: CF_UID_0012\r\n    vks.local/tenant: o11y\r\n  name: cnpg-postgresql-cluster\r\n  namespace: insights-ui\r\n  resourceVersion: \"479713565\"\r\n  uid: e62d306c-e5be-42ea-b484-2c34d4758657\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n    topologyKey: topology.kubernetes.io/zone\r\n  bootstrap:\r\n    initdb:\r\n      database: grafana\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: grafana\r\n      secret:\r\n        name: insights-ui-cnpg-app-credentials-grafana\r\n  enablePDB: true\r\n  enableSuperuserAccess: true\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:15.2\r\n  imagePullPolicy: IfNotPresent\r\n  instances: 3\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_level: logical\r\n      wal_log_hints: \"on\"\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: switchover\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    synchronizeReplicas:\r\n      enabled: true\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      memory: 4Gi\r\n    requests:\r\n      cpu: 600m\r\n      memory: 4Gi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 5Gi\r\n    storageClass: managed-csi-premium\r\n  superuserSecret:\r\n    name: insights-ui-cnpg-superuser\r\n  switchoverDelay: 3600\r\nstatus:\r\n  availableArchitectures:\r\n  - goArch: amd64\r\n    hash: 144e71b00bdcfc5edafa10055fb0cc4a6efa9f467a8e66826d5e7bb2b254b706\r\n  - goArch: arm64\r\n    hash: 0027f50a9d35e24040cfc2f27cea04cbdf4375c226ac7b42764b5bb91f9beca4\r\n  certificates:\r\n    clientCASecret: cnpg-postgresql-cluster-ca\r\n    expirations:\r\n      cnpg-postgresql-cluster-ca: 2024-11-17 07:01:44 +0000 UTC\r\n      cnpg-postgresql-cluster-replication: 2024-11-17 07:01:44 +0000 UTC\r\n      cnpg-postgresql-cluster-server: 2024-11-17 07:01:44 +0000 UTC\r\n    replicationTLSSecret: cnpg-postgresql-cluster-replication\r\n    serverAltDNSNames:\r\n    - cnpg-postgresql-cluster-rw\r\n    - cnpg-postgresql-cluster-rw.insights-ui\r\n    - cnpg-postgresql-cluster-rw.insights-ui.svc\r\n    - cnpg-postgresql-cluster-r\r\n    - cnpg-postgresql-cluster-r.insights-ui\r\n    - cnpg-postgresql-cluster-r.insights-ui.svc\r\n    - cnpg-postgresql-cluster-ro\r\n    - cnpg-postgresql-cluster-ro.insights-ui\r\n    - cnpg-postgresql-cluster-ro.insights-ui.svc\r\n    serverCASecret: cnpg-postgresql-cluster-ca\r\n    serverTLSSecret: cnpg-postgresql-cluster-server\r\n  cloudNativePGCommitHash: 2b489ad6\r\n  cloudNativePGOperatorHash: 144e71b00bdcfc5edafa10055fb0cc4a6efa9f467a8e66826d5e7bb2b254b706\r\n  conditions:\r\n  - lastTransitionTime: \"2024-08-19T07:26:36Z\"\r\n    message: Cluster is Ready\r\n    reason: ClusterIsReady\r\n    status: \"True\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-08-19T07:10:43Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"479605130\"\r\n  currentPrimary: cnpg-postgresql-cluster-1\r\n  currentPrimaryTimestamp: \"2024-08-19T07:10:43.374717Z\"\r\n  healthyPVC:\r\n  - cnpg-postgresql-cluster-1\r\n  - cnpg-postgresql-cluster-2\r\n  - cnpg-postgresql-cluster-3\r\n  image: ghcr.io/cloudnative-pg/postgresql:15.2\r\n  instanceNames:\r\n  - cnpg-postgresql-cluster-1\r\n  - cnpg-postgresql-cluster-2\r\n  - cnpg-postgresql-cluster-3\r\n  instances: 3\r\n  instancesReportedState:\r\n    cnpg-postgresql-cluster-1:\r\n      isPrimary: true\r\n      timeLineID: 1\r\n    cnpg-postgresql-cluster-2:\r\n      isPrimary: false\r\n      timeLineID: 1\r\n    cnpg-postgresql-cluster-3:\r\n      isPrimary: false\r\n      timeLineID: 1\r\n  instancesStatus:\r\n    healthy:\r\n    - cnpg-postgresql-cluster-1\r\n    - cnpg-postgresql-cluster-2\r\n    - cnpg-postgresql-cluster-3\r\n  latestGeneratedNode: 3\r\n  managedRolesStatus: {}\r\n  phase: Cluster in healthy state\r\n  poolerIntegrations:\r\n    pgBouncerIntegration:\r\n      secrets:\r\n      - cnpg-postgresql-cluster-pooler\r\n  pvcCount: 3\r\n  readService: cnpg-postgresql-cluster-r\r\n  readyInstances: 3\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"479713560\"\r\n    clientCaSecretVersion: \"479605102\"\r\n    replicationSecretVersion: \"479605104\"\r\n    serverCaSecretVersion: \"479605102\"\r\n    serverSecretVersion: \"479605103\"\r\n    superuserSecretVersion: \"479713561\"\r\n  switchReplicaClusterStatus: {}\r\n  targetPrimary: cnpg-postgresql-cluster-1\r\n  targetPrimaryTimestamp: \"2024-08-19T07:10:17.321994Z\"\r\n  timelineID: 1\r\n  topology:\r\n    instances:\r\n      cnpg-postgresql-cluster-1: {}\r\n      cnpg-postgresql-cluster-2: {}\r\n      cnpg-postgresql-cluster-3: {}\r\n    nodesUsed: 3\r\n    successfullyExtracted: true\r\n  writeService: cnpg-postgresql-cluster-rw\r\nThe pooler resource:\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Pooler\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/last-applied-configuration: |\r\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Pooler\",\"metadata\":{\"annotations\":{},\"labels\":{\"argocd.argoproj.io/instance\":\"o11y-azweu-stg-insights-ui\"},\"name\":\"cnpg-postgresql-cluster-pooler-rw\",\"namespace\":\"insights-ui\"},\"spec\":{\"cluster\":{\"name\":\"cnpg-postgresql-cluster\"},\"instances\":3,\"monitoring\":{\"enablePodMonitor\":true},\"pgbouncer\":{\"parameters\":{\"default_pool_size\":\"25\",\"max_client_conn\":\"1000\"},\"poolMode\":\"session\"},\"type\":\"rw\"}}\r\n  creationTimestamp: \"2024-08-19T09:02:16Z\"\r\n  generation: 1\r\n  labels:\r\n    argocd.argoproj.io/instance: o11y-azweu-stg-insights-ui\r\n  name: cnpg-postgresql-cluster-pooler-rw\r\n  namespace: insights-ui\r\n  resourceVersion: \"479714049\"\r\n  uid: abfa604d-54e2-430a-a65d-e59af4b9ba11\r\nspec:\r\n  cluster:\r\n    name: cnpg-postgresql-cluster\r\n  instances: 3\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  pgbouncer:\r\n    parameters:\r\n      default_pool_size: \"25\"\r\n      max_client_conn: \"1000\"\r\n    paused: false\r\n    poolMode: session\r\n  type: rw\r\nstatus:\r\n  instances: 3\r\n  secrets:\r\n    clientCA:\r\n      name: cnpg-postgresql-cluster-ca\r\n      version: \"479605102\"\r\n    pgBouncerSecrets:\r\n      authQuery:\r\n        name: cnpg-postgresql-cluster-pooler\r\n        version: \"479605141\"\r\n    serverCA:\r\n      name: cnpg-postgresql-cluster-ca\r\n      version: \"479605102\"\r\n    serverTLS:\r\n      name: cnpg-postgresql-cluster-server\r\n      version: \"479605103\"\n```\n### Relevant log output\n```shell\nN/A\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductCan you please clarify the expected behaviour in this situation?\n---\nThe expected behavior is the same as with the Cluster\r\nWhen configuring the INHERITED_LABELS: environment, workload, app, vks.local/tenant, vks.local/finance-id on the operator it should also add these to the pooler components: \r\n- Pooler\r\n- Deployment\r\n- Pods\r\nThe Cluster components has the following labels attached to them:\r\n```bash\r\nkubectl get cluster cnpg-postgresql-cluster --show-labels\r\nNAME                      AGE   INSTANCES   READY   STATUS                     PRIMARY                     LABELS\r\ncnpg-postgresql-cluster   24h   3           3       Cluster in healthy state   cnpg-postgresql-cluster-1   app.kubernetes.io/instance=insights-ui,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=cnpg-postgresql-cluster,app.kubernetes.io/part-of=cloudnative-pg,argocd.argoproj.io/instance=o11y-azweu-stg-insights-ui,helm.sh/chart=cluster-0.0.9,vks.vestas.com/finance-id=CF_UID_0012,vks.vestas.com/tenant=o11y\r\nkubectl get pods cnpg-postgresql-cluster-1 --show-labels\r\nNAME                        READY   STATUS    RESTARTS   AGE   LABELS\r\ncnpg-postgresql-cluster-1   1/1     Running   0          24h   cnpg.io/cluster=cnpg-postgresql-cluster,cnpg.io/instanceName=cnpg-postgresql-cluster-1,cnpg.io/instanceRole=primary,cnpg.io/podRole=instance,role=primary,vks.vestas.com/finance-id=CF_UID_0012,vks.vestas.com/tenant=o11y\r\n```\r\nCurrently none is deployed:\r\n```bash\r\nkubectl get pooler cnpg-postgresql-cluster-pooler-rw --show-labels\r\nNAME                                AGE   CLUSTER                   TYPE   LABELS\r\ncnpg-postgresql-cluster-pooler-rw   22h   cnpg-postgresql-cluster   rw     argocd.argoproj.io/instance=o11y-azweu-stg-insights-ui\r\nkubectl get deployment cnpg-postgresql-cluster-pooler-rw --show-labels\r\nNAME                                READY   UP-TO-DATE   AVAILABLE   AGE   LABELS\r\ncnpg-postgresql-cluster-pooler-rw   3/3     3            3           22h   cnpg.io/cluster=cnpg-postgresql-cluster,cnpg.io/podRole=pooler,cnpg.io/poolerName=cnpg-postgresql-cluster-pooler-rw\r\nkubectl get pods cnpg-postgresql-cluster-pooler-rw-655f6b9554-9wgz6 --show-labels\r\nNAME                                                 READY   STATUS    RESTARTS   AGE   LABELS\r\ncnpg-postgresql-cluster-pooler-rw-655f6b9554-9wgz6   1/1     Running   0          22h   cnpg.io/cluster=cnpg-postgresql-cluster,cnpg.io/podRole=pooler,cnpg.io/poolerName=cnpg-postgresql-cluster-pooler-rw,pod-template-hash=655f6b9554\r\n```\r\nBoth the Cluster object and Pooler Object are both deployed using the Helm Chart `cluster-0.0.9`\r\nThe Cluster component supports adding extra labels by configuring \r\n```yaml\r\nadditionalLabels:\r\n      vks.local/tenant: \"o11y\"\r\n      vks.local/finance-id: \"CF_UID_0012\"\r\n```\r\nSame kind of option should be available for the Pooler configuration in the Helm chart.\r\nFor the Objects deployed by the operator for the pooler: \r\n- Deployment \r\n- Pods\r\nThe labels should be deployed via the INHERITED_LABELS option on the operator :)\r\nBest regards\r\nJan P. Madsen\n---\nOk, so this is a feature request.\r\nJust a clarification: the pooler object is not owned by the `Cluster` resource, so the `Pooler` should have its labels that, in the case of matching the `INHERITED_LABELS` of the operator, are replicated to the owned objects (deployment, replicaset, pods, and service). Is this what you would expect?\r\nSame thing for annotations.\n---\nYes that's all correct. And yes same thing for annotations.\r\nI have changed the headline to be Feature :)\n---\nShall we also match the behaviour of `.spec.inheritedMetadata`?\n---\nYes it should also match the behavior of `.spec.inheritedMetadata`\r\nBut currently that is not supported in the cluster-0.0.9 helm chart at the moment as I can see.\r\nThis is why we are using the INHERITED_LABELS from the operator for now."
    },
    {
        "title": "[Bug]: Replication stuck on \"Standby (file based)\"",
        "id": 2470194481,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.23.2\r\n### What version of Kubernetes are you using?\r\n1.28\r\n### What is your Kubernetes environment?\r\nSelf-managed: kind (evaluation)\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\nTried to delete pod / pvc,  promote, scale down and up the cluster but nothing helped. \r\nI initially set up backups and then removed them from the cluster CR and deleted all backups.  \r\nI checked postgresql.conf inside the container, all the replication settings are commented out. \r\nAlso there's no entry in `select * from pg_stat_wal_receiver;` on the replica.\r\nThanks for your help.\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    meta.helm.sh/release-name: testdb\r\n    meta.helm.sh/release-namespace: testns\r\n  creationTimestamp: \"2023-11-17T09:34:00Z\"\r\n  generation: 14\r\n  labels:\r\n    app.kubernetes.io/managed-by: Helm\r\n  name: testdb\r\n  namespace: testns\r\n  resourceVersion: \"259706240\"\r\n  uid: 2e326d14-10a3-424d-b9d4-7b8fd6dcd5c3\r\nspec:\r\n  affinity:\r\n    enablePodAntiAffinity: false\r\n    podAntiAffinityType: preferred\r\n    topologyKey: failure-domain.beta.kubernetes.io/zone\r\n  bootstrap:\r\n    initdb:\r\n      database: testdb\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: testdb\r\n      secret:\r\n        name: testdb-appuser\r\n  description: Testdb Postgres Cluster\r\n  enablePDB: true\r\n  enableSuperuserAccess: true\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:15\r\n  instances: 2\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: false\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: false\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      auto_explain.log_min_duration: 10s\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      pg_stat_statements.max: \"1024\"\r\n      pg_stat_statements.track: all\r\n      shared_buffers: 256MB\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_level: logical\r\n      wal_log_hints: \"on\"\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    pg_hba:\r\n    - host all all 192.168.0.0/16 md5\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    synchronizeReplicas:\r\n      enabled: true\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      cpu: \"4\"\r\n      memory: 4Gi\r\n    requests:\r\n      cpu: \"1\"\r\n      memory: 512Mi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 300\r\n  stopDelay: 300\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 20Gi\r\n    storageClass: ceph-block\r\n  superuserSecret:\r\n    name: testdb-superuser\r\n  switchoverDelay: 40000000\r\nstatus:\r\n  availableArchitectures:\r\n  - goArch: amd64\r\n    hash: 144e71b00bdcfc5edafa10055fb0cc4a6efa9f467a8e66826d5e7bb2b254b706\r\n  - goArch: arm64\r\n    hash: 0027f50a9d35e24040cfc2f27cea04cbdf4375c226ac7b42764b5bb91f9beca4\r\n  certificates:\r\n    clientCASecret: testdb-ca\r\n    expirations:\r\n      testdb-ca: 2024-10-21 15:24:25 +0000 UTC\r\n      testdb-replication: 2024-10-21 15:24:25 +0000 UTC\r\n      testdb-server: 2024-10-21 15:24:25 +0000 UTC\r\n    replicationTLSSecret: testdb-replication\r\n    serverAltDNSNames:\r\n    - testdb-rw\r\n    - testdb-rw.testns\r\n    - testdb-rw.testns.svc\r\n    - testdb-r\r\n    - testdb-r.testns\r\n    - testdb-r.testns.svc\r\n    - testdb-ro\r\n    - testdb-ro.testns\r\n    - testdb-ro.testns.svc\r\n    serverCASecret: testdb-ca\r\n    serverTLSSecret: testdb-server\r\n  cloudNativePGCommitHash: 2b489ad6\r\n  cloudNativePGOperatorHash: 144e71b00bdcfc5edafa10055fb0cc4a6efa9f467a8e66826d5e7bb2b254b706\r\n  conditions:\r\n  - lastTransitionTime: \"2024-08-16T12:22:03Z\"\r\n    message: Cluster is Ready\r\n    reason: ClusterIsReady\r\n    status: \"True\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-08-16T12:19:15Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  - lastTransitionTime: \"2024-01-04T17:57:27Z\"\r\n    message: \"cmd: [/controller/manager backup testdb-backup-1704240000]\\nerror: unable\r\n      to upgrade connection: container not found (\\\"postgres\\\")\\nstdErr: \"\r\n    reason: LastBackupFailed\r\n    status: \"False\"\r\n    type: LastBackupSucceeded\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"259682625\"\r\n  currentPrimary: testdb-7\r\n  currentPrimaryTimestamp: \"2024-08-16T12:16:28.064882Z\"\r\n  healthyPVC:\r\n  - testdb-7\r\n  - testdb-9\r\n  image: ghcr.io/cloudnative-pg/postgresql:15\r\n  instanceNames:\r\n  - testdb-7\r\n  - testdb-9\r\n  instances: 2\r\n  instancesReportedState:\r\n    testdb-7:\r\n      isPrimary: true\r\n      timeLineID: 8\r\n    testdb-9:\r\n      isPrimary: false\r\n      timeLineID: 8\r\n  instancesStatus:\r\n    healthy:\r\n    - testdb-7\r\n    - testdb-9\r\n  latestGeneratedNode: 9\r\n  managedRolesStatus: {}\r\n  phase: Cluster in healthy state\r\n  poolerIntegrations:\r\n    pgBouncerIntegration:\r\n      secrets:\r\n      - testdb-pooler\r\n  pvcCount: 2\r\n  readService: testdb-r\r\n  readyInstances: 2\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"7352697\"\r\n    clientCaSecretVersion: \"242704022\"\r\n    replicationSecretVersion: \"242704024\"\r\n    serverCaSecretVersion: \"242704022\"\r\n    serverSecretVersion: \"242704023\"\r\n    superuserSecretVersion: \"7352695\"\r\n  switchReplicaClusterStatus: {}\r\n  targetPrimary: testdb-7\r\n  targetPrimaryTimestamp: \"2024-08-16T13:16:27.636702+01:00\"\r\n  timelineID: 8\r\n  topology:\r\n    instances:\r\n      testdb-7: {}\r\n      testdb-9: {}\r\n    nodesUsed: 2\r\n    successfullyExtracted: true\r\n  writeService: testdb-rw\r\n```\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.23.2\r\n### What version of Kubernetes are you using?\r\n1.28\r\n### What is your Kubernetes environment?\r\nSelf-managed: kind (evaluation)\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\nTried to delete pod / pvc,  promote, scale down and up the cluster but nothing helped. \r\nI initially set up backups and then removed them from the cluster CR and deleted all backups.  \r\nI checked postgresql.conf inside the container, all the replication settings are commented out. \r\nAlso there's no entry in `select * from pg_stat_wal_receiver;` on the replica.\r\nThanks for your help.\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    meta.helm.sh/release-name: testdb\r\n    meta.helm.sh/release-namespace: testns\r\n  creationTimestamp: \"2023-11-17T09:34:00Z\"\r\n  generation: 14\r\n  labels:\r\n    app.kubernetes.io/managed-by: Helm\r\n  name: testdb\r\n  namespace: testns\r\n  resourceVersion: \"259706240\"\r\n  uid: 2e326d14-10a3-424d-b9d4-7b8fd6dcd5c3\r\nspec:\r\n  affinity:\r\n    enablePodAntiAffinity: false\r\n    podAntiAffinityType: preferred\r\n    topologyKey: failure-domain.beta.kubernetes.io/zone\r\n  bootstrap:\r\n    initdb:\r\n      database: testdb\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: testdb\r\n      secret:\r\n        name: testdb-appuser\r\n  description: Testdb Postgres Cluster\r\n  enablePDB: true\r\n  enableSuperuserAccess: true\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:15\r\n  instances: 2\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: false\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: false\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      auto_explain.log_min_duration: 10s\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      pg_stat_statements.max: \"1024\"\r\n      pg_stat_statements.track: all\r\n      shared_buffers: 256MB\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_level: logical\r\n      wal_log_hints: \"on\"\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    pg_hba:\r\n    - host all all 192.168.0.0/16 md5\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    synchronizeReplicas:\r\n      enabled: true\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      cpu: \"4\"\r\n      memory: 4Gi\r\n    requests:\r\n      cpu: \"1\"\r\n      memory: 512Mi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 300\r\n  stopDelay: 300\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 20Gi\r\n    storageClass: ceph-block\r\n  superuserSecret:\r\n    name: testdb-superuser\r\n  switchoverDelay: 40000000\r\nstatus:\r\n  availableArchitectures:\r\n  - goArch: amd64\r\n    hash: 144e71b00bdcfc5edafa10055fb0cc4a6efa9f467a8e66826d5e7bb2b254b706\r\n  - goArch: arm64\r\n    hash: 0027f50a9d35e24040cfc2f27cea04cbdf4375c226ac7b42764b5bb91f9beca4\r\n  certificates:\r\n    clientCASecret: testdb-ca\r\n    expirations:\r\n      testdb-ca: 2024-10-21 15:24:25 +0000 UTC\r\n      testdb-replication: 2024-10-21 15:24:25 +0000 UTC\r\n      testdb-server: 2024-10-21 15:24:25 +0000 UTC\r\n    replicationTLSSecret: testdb-replication\r\n    serverAltDNSNames:\r\n    - testdb-rw\r\n    - testdb-rw.testns\r\n    - testdb-rw.testns.svc\r\n    - testdb-r\r\n    - testdb-r.testns\r\n    - testdb-r.testns.svc\r\n    - testdb-ro\r\n    - testdb-ro.testns\r\n    - testdb-ro.testns.svc\r\n    serverCASecret: testdb-ca\r\n    serverTLSSecret: testdb-server\r\n  cloudNativePGCommitHash: 2b489ad6\r\n  cloudNativePGOperatorHash: 144e71b00bdcfc5edafa10055fb0cc4a6efa9f467a8e66826d5e7bb2b254b706\r\n  conditions:\r\n  - lastTransitionTime: \"2024-08-16T12:22:03Z\"\r\n    message: Cluster is Ready\r\n    reason: ClusterIsReady\r\n    status: \"True\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-08-16T12:19:15Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  - lastTransitionTime: \"2024-01-04T17:57:27Z\"\r\n    message: \"cmd: [/controller/manager backup testdb-backup-1704240000]\\nerror: unable\r\n      to upgrade connection: container not found (\\\"postgres\\\")\\nstdErr: \"\r\n    reason: LastBackupFailed\r\n    status: \"False\"\r\n    type: LastBackupSucceeded\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"259682625\"\r\n  currentPrimary: testdb-7\r\n  currentPrimaryTimestamp: \"2024-08-16T12:16:28.064882Z\"\r\n  healthyPVC:\r\n  - testdb-7\r\n  - testdb-9\r\n  image: ghcr.io/cloudnative-pg/postgresql:15\r\n  instanceNames:\r\n  - testdb-7\r\n  - testdb-9\r\n  instances: 2\r\n  instancesReportedState:\r\n    testdb-7:\r\n      isPrimary: true\r\n      timeLineID: 8\r\n    testdb-9:\r\n      isPrimary: false\r\n      timeLineID: 8\r\n  instancesStatus:\r\n    healthy:\r\n    - testdb-7\r\n    - testdb-9\r\n  latestGeneratedNode: 9\r\n  managedRolesStatus: {}\r\n  phase: Cluster in healthy state\r\n  poolerIntegrations:\r\n    pgBouncerIntegration:\r\n      secrets:\r\n      - testdb-pooler\r\n  pvcCount: 2\r\n  readService: testdb-r\r\n  readyInstances: 2\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"7352697\"\r\n    clientCaSecretVersion: \"242704022\"\r\n    replicationSecretVersion: \"242704024\"\r\n    serverCaSecretVersion: \"242704022\"\r\n    serverSecretVersion: \"242704023\"\r\n    superuserSecretVersion: \"7352695\"\r\n  switchReplicaClusterStatus: {}\r\n  targetPrimary: testdb-7\r\n  targetPrimaryTimestamp: \"2024-08-16T13:16:27.636702+01:00\"\r\n  timelineID: 8\r\n  topology:\r\n    instances:\r\n      testdb-7: {}\r\n      testdb-9: {}\r\n    nodesUsed: 2\r\n    successfullyExtracted: true\r\n  writeService: testdb-rw\r\n```\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductI believe that I'm seeing a similar behaviour sometimes as well. We have an existing Postgres cluster with approximately 15GiB of data running Postgres 15 and deployed with CNPG, and we're trying to migrate it to a new cluster running Postgres 17 within the same Kubernetes environment. I've set up a `bootstrap:initdb` section on the new cluster to bootstrap from the existing cluster, and the connection to the source cluster works fine.\r\nWhat I've observed is that if I bring up the new cluster with three replicas, the primary instance works fine, but then the subsequent two replicas will virtually always start up in the `Standby (file based)` replication state. However, if I modify the cluster definition to have a silly number of replicas (e.g. 10), replicas numbered `4` and above will virtually always enter the correct `Standby (async)` state. It _feels_ like the success chances of the replicas are a function of the size of the database being created, as I have not been able to observe this behaviour on trivial or empty databases.  As @sle78 mentioned the replicas that are \"stuck\" in the `Standby (file based)` state return no data for the `select * from pg_stat_wal_receiver;` query. \r\nThe logs on the currently active primary have many repeating logs like this:\r\n```json\r\n{\"level\":\"info\",\"ts\":\"2024-10-17T15:32:41.434366466Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"destination-migration-example-db-1\",\"record\":{\"log_time\":\"2024-10-17 15:32:41.434 UTC\",\"user_name\":\"streaming_replica\",\"process_id\":\"15712\",\"connection_from\":\"10.42.253.162:43130\",\"session_id\":\"67112e19.3d60\",\"session_line_num\":\"1\",\"command_tag\":\"START_REPLICATION\",\"session_start_time\":\"2024-10-17 15:32:41 UTC\",\"virtual_transaction_id\":\"139/0\",\"transaction_id\":\"0\",\"error_severity\":\"ERROR\",\"sql_state_code\":\"58P01\",\"message\":\"requested WAL segment 000000010000000300000092 has already been removed\", \"query\":\"START_REPLICATION SLOT \\\"_cnpg_destination_migration_example_db_3\\\" 3/92000000 TIMELINE 1\",\"application_name\":\"destination-migration-example-db-3\",\"backend_type\":\"walsender\",\"query_id\":\"0\"}} \r\n```\r\n`destination-migration-example-db-3` is one of the replicas that's stuck in the `Standby (file based)` state; it seems like it's looking for a WAL file that no longer exists for some reason?\r\nFor now, my migration strategy has been to spin up the new cluster as a a brand-new empty database, `pg_dump` the source database to so temporary location, and then use `psql` to load the data into the new database. This has had a 100% success rate so far but it is certainly more inconvenient.\r\nEnvironment:\r\n- Kubernetes `1.30.4` (RKE2 `v1.30.4+rke2r1`) self-hosted on vSphere `7.x`\r\n- CNPG `1.24.1`"
    },
    {
        "title": "[Feature]: Check for nodes cordoned by karpenter",
        "id": 2468162261,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nAt the moment, when a primary runs on EKS nodes managed by karpenter, you will have to manually failover if a node is replaced or removed. This interrupts the automatic provisioning/deprovisioning of nodes.\r\nKubernetes uses the \"node.kubernetes.io/unschedulable\" taint and the \"spec.unschedulable\" field on a node to mark it as cordoned. The operator checks nodes for this information.\r\nIf a primary database is running on a node which was cordoned the operator will promote a secondary on another node.\r\nThis allows to evict the cordoned node without blocking the process due to the PDB of the primary.\r\nKarpenter does not rely on \"node.kubernetes.io/unschedulable\" taint and/or the \"spec.unschedulable\" field.\r\nNext to other things, the project basically needs to know that karpenter was the one that initiated the tainting.\r\nhttps://github.com/kubernetes-sigs/karpenter/issues/1152#issuecomment-2032555856\r\n### Describe the solution you'd like\r\nKarpenter taints nodes with: \"karpenter.sh/disruption:NoSchedule\"\r\nhttps://karpenter.sh/docs/concepts/disruption/#disruption-controller\r\nIt would be nice if the operator could also check for an additional/alternative taint like this.\r\n### Describe alternatives you've considered\r\nYou can set \"enablePDB: false\" - but this is officially not recommended for production environments.\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nN/A\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nAt the moment, when a primary runs on EKS nodes managed by karpenter, you will have to manually failover if a node is replaced or removed. This interrupts the automatic provisioning/deprovisioning of nodes.\r\nKubernetes uses the \"node.kubernetes.io/unschedulable\" taint and the \"spec.unschedulable\" field on a node to mark it as cordoned. The operator checks nodes for this information.\r\nIf a primary database is running on a node which was cordoned the operator will promote a secondary on another node.\r\nThis allows to evict the cordoned node without blocking the process due to the PDB of the primary.\r\nKarpenter does not rely on \"node.kubernetes.io/unschedulable\" taint and/or the \"spec.unschedulable\" field.\r\nNext to other things, the project basically needs to know that karpenter was the one that initiated the tainting.\r\nhttps://github.com/kubernetes-sigs/karpenter/issues/1152#issuecomment-2032555856\r\n### Describe the solution you'd like\r\nKarpenter taints nodes with: \"karpenter.sh/disruption:NoSchedule\"\r\nhttps://karpenter.sh/docs/concepts/disruption/#disruption-controller\r\nIt would be nice if the operator could also check for an additional/alternative taint like this.\r\n### Describe alternatives you've considered\r\nYou can set \"enablePDB: false\" - but this is officially not recommended for production environments.\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nN/A\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductAhhh, so _that's_ what's going on! Would be great to implement this as a custom field.\n---\nI too found this after working out why my maximum age on karpenter issued nodes wasn't cycling the nodes out and instead blocked by postgres instances. Thanks OP for working this out.\n---\nRan into the same problem, thanks for working it out!\nKarpenter community is also arguing about this, but they seem to prefer to stick to their current taint and not leverage the k8s ones:\nhttps://github.com/kubernetes-sigs/karpenter/issues/1152\n---\nAny movement on this? It's making clusters managed by Karpenter unable to reliably consolidate or roll nodes without user interaction.\nEDIT: If anyone gets to it before I have a chance to, this is how the AWS EBS CSI Driver handles it https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/7bacf2d36f397bd098b3388403e8759c480be7e5/cmd/hooks/prestop.go#L47"
    },
    {
        "title": "[Feature]: schedule next update timestamp of a `Cluster`",
        "id": 2463606269,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nAn upgrade of the operator triggers a rolling update on any Postgres cluster managed by the operator. While this is not a problem in the case of a small number of clusters, it might create problems for larger fleets.\n### Describe the solution you'd like\nWhile waiting for maintenance windows (#14) we should introduce two new fields to the status:\r\n- `lastUpdateTimestamp`: storing the timestamp of the last successful update of the cluster object (if null, no previous update has been done)\r\n- `nextUpdateTimestamp`: storing the timestamp of the next scheduled update of the cluster object (if null, no update is scheduled)\r\nThe idea is that the first reconciliation loop after `nextUpdateTimestamp` triggers an update, following the same rules as the supervised/unsupervised method and strategy.\r\nIn terms of usability, we should clearly highlight that the next update of the cluster will happen at `nextUpdateTimeStamp` (if not null). This includes the `cnpg status` command.\r\nAs part of this ticket we should also define a default update window of 10 minutes (we can choose a different value, but make sure it is a global constant for now - we can address customisation in a follow-up ticket which also considers #14). The idea is that, following an operator upgrade process or a change of the image catalog, instead of immediately triggering the update, we randomly throw a value between the current timestamp and the above window, and set the `nextUpdateTimestamp`.\r\nThis should randomly distribute in the space of 10 minutes all the cluster updates.\r\n### Describe alternatives you've considered\nThis path had already been decided a couple of years ago, when we introduced the concept of current/target primary and primary timestamp fields, but the implementation had been postponed.\n### Additional context\nThis work is preliminary for maintenance windows in #14 \r\nWe should also consider another important aspect: interaction of this with any rolling update process. Ideally, all updates should be gated by this feature.\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nAn upgrade of the operator triggers a rolling update on any Postgres cluster managed by the operator. While this is not a problem in the case of a small number of clusters, it might create problems for larger fleets.\n### Describe the solution you'd like\nWhile waiting for maintenance windows (#14) we should introduce two new fields to the status:\r\n- `lastUpdateTimestamp`: storing the timestamp of the last successful update of the cluster object (if null, no previous update has been done)\r\n- `nextUpdateTimestamp`: storing the timestamp of the next scheduled update of the cluster object (if null, no update is scheduled)\r\nThe idea is that the first reconciliation loop after `nextUpdateTimestamp` triggers an update, following the same rules as the supervised/unsupervised method and strategy.\r\nIn terms of usability, we should clearly highlight that the next update of the cluster will happen at `nextUpdateTimeStamp` (if not null). This includes the `cnpg status` command.\r\nAs part of this ticket we should also define a default update window of 10 minutes (we can choose a different value, but make sure it is a global constant for now - we can address customisation in a follow-up ticket which also considers #14). The idea is that, following an operator upgrade process or a change of the image catalog, instead of immediately triggering the update, we randomly throw a value between the current timestamp and the above window, and set the `nextUpdateTimestamp`.\r\nThis should randomly distribute in the space of 10 minutes all the cluster updates.\r\n### Describe alternatives you've considered\nThis path had already been decided a couple of years ago, when we introduced the concept of current/target primary and primary timestamp fields, but the implementation had been postponed.\n### Additional context\nThis work is preliminary for maintenance windows in #14 \r\nWe should also consider another important aspect: interaction of this with any rolling update process. Ideally, all updates should be gated by this feature.\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: SQL insert / update / delete statements hang endlessly when a node with the lead DB is terminated.",
        "id": 2456213924,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ndavid_sisson@dell.com\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nHelm\n### What happened?\nOn a 3+ node cluster when a node with the lead DB pod is terminated via power outage.\r\nThe remaining DB pods will failover to a new lead but SQL insert / update / delete commands run against the new primary will hang  endlessly.\r\nThis is caused by synchronous_standby_names not being updated after CNPG has performed the failover.\r\n synchronous_standby_names              |   ANY 2 (\"postgres-ha-cnpg-1\",\"postgres-ha-cnpg-3\")\r\n The only way we have seen to fix this is to forcefully delete the old lead pod.\r\n Once the old lead pod goes into \"Pending\" state the hung transactions complete and this is updated\r\n synchronous_standby_names              | ANY 1 (\"postgres-ha-cnpg-3\")\r\n We also notice the timeLineID in the instancesReportedState is typically inaccurate.\r\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\n...\r\n  instanceNames:\r\n  - postgres-ha-cnpg-1\r\n  - postgres-ha-cnpg-2\r\n  - postgres-ha-cnpg-3\r\n  instances: 3\r\n...\r\n  failoverDelay: 0\r\n...\r\n  smartShutdownTimeout: 15\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n...\r\n  maxSyncReplicas: 2\r\n  minSyncReplicas: 1\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ndavid_sisson@dell.com\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nHelm\n### What happened?\nOn a 3+ node cluster when a node with the lead DB pod is terminated via power outage.\r\nThe remaining DB pods will failover to a new lead but SQL insert / update / delete commands run against the new primary will hang  endlessly.\r\nThis is caused by synchronous_standby_names not being updated after CNPG has performed the failover.\r\n synchronous_standby_names              |   ANY 2 (\"postgres-ha-cnpg-1\",\"postgres-ha-cnpg-3\")\r\n The only way we have seen to fix this is to forcefully delete the old lead pod.\r\n Once the old lead pod goes into \"Pending\" state the hung transactions complete and this is updated\r\n synchronous_standby_names              | ANY 1 (\"postgres-ha-cnpg-3\")\r\n We also notice the timeLineID in the instancesReportedState is typically inaccurate.\r\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\n...\r\n  instanceNames:\r\n  - postgres-ha-cnpg-1\r\n  - postgres-ha-cnpg-2\r\n  - postgres-ha-cnpg-3\r\n  instances: 3\r\n...\r\n  failoverDelay: 0\r\n...\r\n  smartShutdownTimeout: 15\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n...\r\n  maxSyncReplicas: 2\r\n  minSyncReplicas: 1\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: refusing to create the primary instance while the latest generated serial is not zero",
        "id": 2451952206,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nmousetraphun@gmail.com\n### Version\nolder in 1.23.x\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nWhile doing my initial deployment I notice that it always got stuck after completing the \"cnpg-cluster-1-initdb\", checking the logs of it I see that it start but then it got shutdown. From the operator all that I see is the error of the title of this issue.\n### Cluster resource\n```shell\nName:             cnpg-cluster-1-initdb-z9clj\r\nNamespace:        cnpg-database\r\nPriority:         0\r\nService Account:  cnpg-cluster\r\nNode:             node2/192.168.88.202\r\nStart Time:       Wed, 07 Aug 2024 00:42:16 +0000\r\nLabels:           batch.kubernetes.io/controller-uid=afc0e4dd-5485-48d4-a468-c9e5859df8c4\r\n                  batch.kubernetes.io/job-name=cnpg-cluster-1-initdb\r\n                  cnpg.io/cluster=cnpg-cluster\r\n                  cnpg.io/instanceName=cnpg-cluster-1\r\n                  cnpg.io/jobRole=initdb\r\n                  controller-uid=afc0e4dd-5485-48d4-a468-c9e5859df8c4\r\n                  job-name=cnpg-cluster-1-initdb\r\nAnnotations:      cni.projectcalico.org/containerID: ada0bee1e7733875ba10ad29ace85779c3b39b9ce1d4ee19e65a63970d1e9e01\r\n                  cni.projectcalico.org/podIP:\r\n                  cni.projectcalico.org/podIPs:\r\nStatus:           Succeeded\r\nSeccompProfile:   RuntimeDefault\r\nIP:               172.168.104.3\r\nIPs:\r\n  IP:           172.168.104.3\r\nControlled By:  Job/cnpg-cluster-1-initdb\r\nInit Containers:\r\n  bootstrap-controller:\r\n    Container ID:    containerd://6254af8b43c4c395038b9da674b484f79ee8b3667455639b03e633a691f406b8\r\n    Image:           ghcr.io/cloudnative-pg/cloudnative-pg:1.23.3\r\n    Image ID:        ghcr.io/cloudnative-pg/cloudnative-pg@sha256:09e65ad4891b4a45052d1334171d86e9ede1625c4cb04d372e3c8699cb4529ae\r\n    Port:            <none>\r\n    Host Port:       <none>\r\n    SeccompProfile:  RuntimeDefault\r\n    Command:\r\n      /manager\r\n      bootstrap\r\n      /controller/manager\r\n      --log-level=info\r\n    State:          Terminated\r\n      Reason:       Completed\r\n      Exit Code:    0\r\n      Started:      Wed, 07 Aug 2024 00:42:16 +0000\r\n      Finished:     Wed, 07 Aug 2024 00:42:16 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Environment:    <none>\r\n    Mounts:\r\n      /controller from scratch-data (rw)\r\n      /dev/shm from shm (rw)\r\n      /etc/app-secret from app-secret (rw)\r\n      /etc/superuser-secret from superuser-secret (rw)\r\n      /run from scratch-data (rw)\r\n      /var/lib/postgresql/data from pgdata (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nrtsn (ro)\r\nContainers:\r\n  initdb:\r\n    Container ID:    containerd://ceb1c64fbb50448c0c436e09b1a2bbfcdf9e018d8406eafb0ac9264c1067ca2a\r\n    Image:           ghcr.io/cloudnative-pg/postgresql:15.2\r\n    Image ID:        ghcr.io/cloudnative-pg/postgresql@sha256:4c5c7266afcaffb736e6181d9bb71f05c67cba971afd9aab76207e6be312c546\r\n    Port:            <none>\r\n    Host Port:       <none>\r\n    SeccompProfile:  RuntimeDefault\r\n    Command:\r\n      /controller/manager\r\n      instance\r\n      init\r\n      --initdb-flags\r\n      --encoding=UTF8 --lc-collate=C --lc-ctype=C\r\n      --app-db-name\r\n      app\r\n      --app-user\r\n      app\r\n      --log-level=info\r\n    State:          Terminated\r\n      Reason:       Completed\r\n      Exit Code:    0\r\n      Started:      Wed, 07 Aug 2024 00:43:37 +0000\r\n      Finished:     Wed, 07 Aug 2024 00:43:38 +0000\r\n    Ready:          False\r\n    Restart Count:  0\r\n    Environment:\r\n      PGDATA:        /var/lib/postgresql/data/pgdata\r\n      POD_NAME:      cnpg-cluster-1-initdb\r\n      NAMESPACE:     cnpg-database\r\n      CLUSTER_NAME:  cnpg-cluster\r\n      PGPORT:        5432\r\n      PGHOST:        /controller/run\r\n    Mounts:\r\n      /controller from scratch-data (rw)\r\n      /dev/shm from shm (rw)\r\n      /etc/app-secret from app-secret (rw)\r\n      /etc/superuser-secret from superuser-secret (rw)\r\n      /run from scratch-data (rw)\r\n      /var/lib/postgresql/data from pgdata (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nrtsn (ro)\r\nConditions:\r\n  Type                        Status\r\n  PodReadyToStartContainers   False\r\n  Initialized                 True\r\n  Ready                       False\r\n  ContainersReady             False\r\n  PodScheduled                True\r\nVolumes:\r\n  pgdata:\r\n    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\r\n    ClaimName:  cnpg-cluster-1\r\n    ReadOnly:   false\r\n  scratch-data:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:\r\n    SizeLimit:  <unset>\r\n  shm:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:     Memory\r\n    SizeLimit:  <unset>\r\n  superuser-secret:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  cnpg-cluster-superuser\r\n    Optional:    false\r\n  app-secret:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  cnpg-cluster-app\r\n    Optional:    false\r\n  kube-api-access-nrtsn:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  3607\r\n    ConfigMapName:           kube-root-ca.crt\r\n    ConfigMapOptional:       <nil>\r\n    DownwardAPI:             true\r\nQoS Class:                   BestEffort\r\nNode-Selectors:              <none>\r\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\r\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\r\nEvents:\r\n  Type    Reason     Age   From               Message\r\n  ----    ------     ----  ----               -------\r\n  Normal  Scheduled  26m   default-scheduler  Successfully assigned cnpg-database/cnpg-cluster-1-initdb-z9clj to node2\r\n  Normal  Pulled     26m   kubelet            Container image \"ghcr.io/cloudnative-pg/cloudnative-pg:1.23.3\" already present on machine\r\n  Normal  Created    26m   kubelet            Created container bootstrap-controller\r\n  Normal  Started    26m   kubelet            Started container bootstrap-controller\r\n  Normal  Pulling    26m   kubelet            Pulling image \"ghcr.io/cloudnative-pg/postgresql:15.2\"\r\n  Normal  Pulled     25m   kubelet            Successfully pulled image \"ghcr.io/cloudnative-pg/postgresql:15.2\" in 1m19.068s (1m19.068s including waiting)\r\n  Normal  Created    25m   kubelet            Created container initdb\r\n  Normal  Started    25m   kubelet            Started container initdb\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"logger\":\"pg_ctl\",\"msg\":\" done\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-cluster-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"logger\":\"pg_ctl\",\"msg\":\"server started\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-cluster-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"msg\":\"Configuring new PostgreSQL instance\",\"logging_pod\":\"cnpg-cluster-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"msg\":\"Executing post-init SQL instructions\",\"logging_pod\":\"cnpg-cluster-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"msg\":\"Executing post-init template SQL instructions\",\"logging_pod\":\"cnpg-cluster-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"msg\":\"executing Application instructions\",\"logging_pod\":\"cnpg-cluster-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"logger\":\"pg_ctl\",\"msg\":\"pg_ctl: server is running (PID: 33)\\n/usr/lib/postgresql/15/bin/postgres \\\"-D\\\" \\\"/var/lib/postgresql/data/pgdata\\\" \\\"-c\\\" \\\"port=5432\\\" \\\"-c\\\" \\\"unix_socket_directories=/controller/run\\\" \\\"-c\\\" \\\"listen_addresses=127.0.0.1\\\"\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-cluster-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"msg\":\"Shutting down instance\",\"logging_pod\":\"cnpg-cluster-1-initdb\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"mode\":\"fast\",\"timeout\":null}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-cluster-1-initdb\",\"record\":{\"log_time\":\"2024-08-07 00:43:38.598 UTC\",\"process_id\":\"33\",\"session_id\":\"66b2c33a.21\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-08-07 00:43:38 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"received fast shutdown request\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T01:13:40Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-cluster\",\"namespace\":\"cnpg-database\"},\"namespace\":\"cnpg-database\",\"name\":\"cnpg-cluster\",\"reconcileID\":\"746f21f2-6437-4faf-8a0d-9a39a5c242cf\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T01:13:41Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-cluster\",\"namespace\":\"cnpg-database\"},\"namespace\":\"cnpg-database\",\"name\":\"cnpg-cluster\",\"reconcileID\":\"52019bfa-ed3d-4fb8-8c1e-a762bc5708d1\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T01:13:42Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-cluster\",\"namespace\":\"cnpg-database\"},\"namespace\":\"cnpg-database\",\"name\":\"cnpg-cluster\",\"reconcileID\":\"60a1ceb5-794c-4192-9397-1b79319e933b\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T01:13:43Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-cluster\",\"namespace\":\"cnpg-database\"},\"namespace\":\"cnpg-database\",\"name\":\"cnpg-cluster\",\"reconcileID\":\"dd832645-ac6e-4cc3-b9ba-3f3f78fc39d0\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T01:13:44Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-cluster\",\"namespace\":\"cnpg-database\"},\"namespace\":\"cnpg-database\",\"name\":\"cnpg-cluster\",\"reconcileID\":\"61cb1bba-7c05-4ba2-82ba-a62885b66dd8\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T01:13:45Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-cluster\",\"namespace\":\"cnpg-database\"},\"namespace\":\"cnpg-database\",\"name\":\"cnpg-cluster\",\"reconcileID\":\"7309533e-31ec-4235-b1e2-e63ac8c9f5f6\",\"latestGeneratedNode\":1}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nmousetraphun@gmail.com\n### Version\nolder in 1.23.x\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nWhile doing my initial deployment I notice that it always got stuck after completing the \"cnpg-cluster-1-initdb\", checking the logs of it I see that it start but then it got shutdown. From the operator all that I see is the error of the title of this issue.\n### Cluster resource\n```shell\nName:             cnpg-cluster-1-initdb-z9clj\r\nNamespace:        cnpg-database\r\nPriority:         0\r\nService Account:  cnpg-cluster\r\nNode:             node2/192.168.88.202\r\nStart Time:       Wed, 07 Aug 2024 00:42:16 +0000\r\nLabels:           batch.kubernetes.io/controller-uid=afc0e4dd-5485-48d4-a468-c9e5859df8c4\r\n                  batch.kubernetes.io/job-name=cnpg-cluster-1-initdb\r\n                  cnpg.io/cluster=cnpg-cluster\r\n                  cnpg.io/instanceName=cnpg-cluster-1\r\n                  cnpg.io/jobRole=initdb\r\n                  controller-uid=afc0e4dd-5485-48d4-a468-c9e5859df8c4\r\n                  job-name=cnpg-cluster-1-initdb\r\nAnnotations:      cni.projectcalico.org/containerID: ada0bee1e7733875ba10ad29ace85779c3b39b9ce1d4ee19e65a63970d1e9e01\r\n                  cni.projectcalico.org/podIP:\r\n                  cni.projectcalico.org/podIPs:\r\nStatus:           Succeeded\r\nSeccompProfile:   RuntimeDefault\r\nIP:               172.168.104.3\r\nIPs:\r\n  IP:           172.168.104.3\r\nControlled By:  Job/cnpg-cluster-1-initdb\r\nInit Containers:\r\n  bootstrap-controller:\r\n    Container ID:    containerd://6254af8b43c4c395038b9da674b484f79ee8b3667455639b03e633a691f406b8\r\n    Image:           ghcr.io/cloudnative-pg/cloudnative-pg:1.23.3\r\n    Image ID:        ghcr.io/cloudnative-pg/cloudnative-pg@sha256:09e65ad4891b4a45052d1334171d86e9ede1625c4cb04d372e3c8699cb4529ae\r\n    Port:            <none>\r\n    Host Port:       <none>\r\n    SeccompProfile:  RuntimeDefault\r\n    Command:\r\n      /manager\r\n      bootstrap\r\n      /controller/manager\r\n      --log-level=info\r\n    State:          Terminated\r\n      Reason:       Completed\r\n      Exit Code:    0\r\n      Started:      Wed, 07 Aug 2024 00:42:16 +0000\r\n      Finished:     Wed, 07 Aug 2024 00:42:16 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Environment:    <none>\r\n    Mounts:\r\n      /controller from scratch-data (rw)\r\n      /dev/shm from shm (rw)\r\n      /etc/app-secret from app-secret (rw)\r\n      /etc/superuser-secret from superuser-secret (rw)\r\n      /run from scratch-data (rw)\r\n      /var/lib/postgresql/data from pgdata (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nrtsn (ro)\r\nContainers:\r\n  initdb:\r\n    Container ID:    containerd://ceb1c64fbb50448c0c436e09b1a2bbfcdf9e018d8406eafb0ac9264c1067ca2a\r\n    Image:           ghcr.io/cloudnative-pg/postgresql:15.2\r\n    Image ID:        ghcr.io/cloudnative-pg/postgresql@sha256:4c5c7266afcaffb736e6181d9bb71f05c67cba971afd9aab76207e6be312c546\r\n    Port:            <none>\r\n    Host Port:       <none>\r\n    SeccompProfile:  RuntimeDefault\r\n    Command:\r\n      /controller/manager\r\n      instance\r\n      init\r\n      --initdb-flags\r\n      --encoding=UTF8 --lc-collate=C --lc-ctype=C\r\n      --app-db-name\r\n      app\r\n      --app-user\r\n      app\r\n      --log-level=info\r\n    State:          Terminated\r\n      Reason:       Completed\r\n      Exit Code:    0\r\n      Started:      Wed, 07 Aug 2024 00:43:37 +0000\r\n      Finished:     Wed, 07 Aug 2024 00:43:38 +0000\r\n    Ready:          False\r\n    Restart Count:  0\r\n    Environment:\r\n      PGDATA:        /var/lib/postgresql/data/pgdata\r\n      POD_NAME:      cnpg-cluster-1-initdb\r\n      NAMESPACE:     cnpg-database\r\n      CLUSTER_NAME:  cnpg-cluster\r\n      PGPORT:        5432\r\n      PGHOST:        /controller/run\r\n    Mounts:\r\n      /controller from scratch-data (rw)\r\n      /dev/shm from shm (rw)\r\n      /etc/app-secret from app-secret (rw)\r\n      /etc/superuser-secret from superuser-secret (rw)\r\n      /run from scratch-data (rw)\r\n      /var/lib/postgresql/data from pgdata (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nrtsn (ro)\r\nConditions:\r\n  Type                        Status\r\n  PodReadyToStartContainers   False\r\n  Initialized                 True\r\n  Ready                       False\r\n  ContainersReady             False\r\n  PodScheduled                True\r\nVolumes:\r\n  pgdata:\r\n    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\r\n    ClaimName:  cnpg-cluster-1\r\n    ReadOnly:   false\r\n  scratch-data:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:\r\n    SizeLimit:  <unset>\r\n  shm:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:     Memory\r\n    SizeLimit:  <unset>\r\n  superuser-secret:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  cnpg-cluster-superuser\r\n    Optional:    false\r\n  app-secret:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  cnpg-cluster-app\r\n    Optional:    false\r\n  kube-api-access-nrtsn:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  3607\r\n    ConfigMapName:           kube-root-ca.crt\r\n    ConfigMapOptional:       <nil>\r\n    DownwardAPI:             true\r\nQoS Class:                   BestEffort\r\nNode-Selectors:              <none>\r\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\r\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\r\nEvents:\r\n  Type    Reason     Age   From               Message\r\n  ----    ------     ----  ----               -------\r\n  Normal  Scheduled  26m   default-scheduler  Successfully assigned cnpg-database/cnpg-cluster-1-initdb-z9clj to node2\r\n  Normal  Pulled     26m   kubelet            Container image \"ghcr.io/cloudnative-pg/cloudnative-pg:1.23.3\" already present on machine\r\n  Normal  Created    26m   kubelet            Created container bootstrap-controller\r\n  Normal  Started    26m   kubelet            Started container bootstrap-controller\r\n  Normal  Pulling    26m   kubelet            Pulling image \"ghcr.io/cloudnative-pg/postgresql:15.2\"\r\n  Normal  Pulled     25m   kubelet            Successfully pulled image \"ghcr.io/cloudnative-pg/postgresql:15.2\" in 1m19.068s (1m19.068s including waiting)\r\n  Normal  Created    25m   kubelet            Created container initdb\r\n  Normal  Started    25m   kubelet            Started container initdb\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"logger\":\"pg_ctl\",\"msg\":\" done\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-cluster-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"logger\":\"pg_ctl\",\"msg\":\"server started\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-cluster-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"msg\":\"Configuring new PostgreSQL instance\",\"logging_pod\":\"cnpg-cluster-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"msg\":\"Executing post-init SQL instructions\",\"logging_pod\":\"cnpg-cluster-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"msg\":\"Executing post-init template SQL instructions\",\"logging_pod\":\"cnpg-cluster-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"msg\":\"executing Application instructions\",\"logging_pod\":\"cnpg-cluster-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"logger\":\"pg_ctl\",\"msg\":\"pg_ctl: server is running (PID: 33)\\n/usr/lib/postgresql/15/bin/postgres \\\"-D\\\" \\\"/var/lib/postgresql/data/pgdata\\\" \\\"-c\\\" \\\"port=5432\\\" \\\"-c\\\" \\\"unix_socket_directories=/controller/run\\\" \\\"-c\\\" \\\"listen_addresses=127.0.0.1\\\"\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-cluster-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"msg\":\"Shutting down instance\",\"logging_pod\":\"cnpg-cluster-1-initdb\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"mode\":\"fast\",\"timeout\":null}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T00:43:38Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-cluster-1-initdb\",\"record\":{\"log_time\":\"2024-08-07 00:43:38.598 UTC\",\"process_id\":\"33\",\"session_id\":\"66b2c33a.21\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-08-07 00:43:38 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"received fast shutdown request\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T01:13:40Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-cluster\",\"namespace\":\"cnpg-database\"},\"namespace\":\"cnpg-database\",\"name\":\"cnpg-cluster\",\"reconcileID\":\"746f21f2-6437-4faf-8a0d-9a39a5c242cf\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T01:13:41Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-cluster\",\"namespace\":\"cnpg-database\"},\"namespace\":\"cnpg-database\",\"name\":\"cnpg-cluster\",\"reconcileID\":\"52019bfa-ed3d-4fb8-8c1e-a762bc5708d1\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T01:13:42Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-cluster\",\"namespace\":\"cnpg-database\"},\"namespace\":\"cnpg-database\",\"name\":\"cnpg-cluster\",\"reconcileID\":\"60a1ceb5-794c-4192-9397-1b79319e933b\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T01:13:43Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-cluster\",\"namespace\":\"cnpg-database\"},\"namespace\":\"cnpg-database\",\"name\":\"cnpg-cluster\",\"reconcileID\":\"dd832645-ac6e-4cc3-b9ba-3f3f78fc39d0\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T01:13:44Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-cluster\",\"namespace\":\"cnpg-database\"},\"namespace\":\"cnpg-database\",\"name\":\"cnpg-cluster\",\"reconcileID\":\"61cb1bba-7c05-4ba2-82ba-a62885b66dd8\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-08-07T01:13:45Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-cluster\",\"namespace\":\"cnpg-database\"},\"namespace\":\"cnpg-database\",\"name\":\"cnpg-cluster\",\"reconcileID\":\"7309533e-31ec-4235-b1e2-e63ac8c9f5f6\",\"latestGeneratedNode\":1}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI'm having the same issue here \ud83d\ude22\n---\nfacing this too, using following:  \r\n```\r\napiVersion: v1\r\nkind: PersistentVolume\r\nmetadata:\r\n  name: postgres-0\r\nspec:\r\n  capacity:\r\n    storage: 1Gi\r\n  accessModes:\r\n    - ReadWriteOnce\r\n  hostPath:\r\n    path: /mnt/postgres-data-0\r\n---\r\napiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: postgres-0\r\nspec:\r\n  accessModes:\r\n    - ReadWriteOnce\r\n  storageClassName: \"\"\r\n  volumeName: postgres-0\r\n  resources:\r\n    requests:\r\n      storage: 1Gi\r\n---\r\napiVersion: v1\r\nkind: PersistentVolume\r\nmetadata:\r\n  name: postgres-1\r\nspec:\r\n  capacity:\r\n    storage: 1Gi\r\n  accessModes:\r\n    - ReadWriteOnce\r\n  hostPath:\r\n    path: /mnt/postgres-data-1\r\n---\r\napiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: postgres-1\r\nspec:\r\n  accessModes:\r\n    - ReadWriteOnce\r\n  storageClassName: \"\"\r\n  volumeName: postgres-1\r\n  resources:\r\n    requests:\r\n      storage: 1Gi\r\n---\r\napiVersion: v1\r\nkind: PersistentVolume\r\nmetadata:\r\n  name: postgres-2\r\nspec:\r\n  capacity:\r\n    storage: 1Gi\r\n  accessModes:\r\n    - ReadWriteOnce\r\n  hostPath:\r\n    path: /mnt/postgres-data-2\r\n---\r\napiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: postgres-2\r\nspec:\r\n  accessModes:\r\n    - ReadWriteOnce\r\n  storageClassName: \"\"\r\n  volumeName: postgres-2\r\n  resources:\r\n    requests:\r\n      storage: 1Gi\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: postgres\r\nspec:\r\n  affinity:\r\n    enablePodAntiAffinity: true\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:15.2\r\n  instances: 1\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  postgresql:\r\n    parameters:\r\n      huge_pages: \"off\"\r\n  storage:\r\n    resizeInUseVolumes: false\r\n    size: 1Gi\r\n    pvcTemplate:\r\n      dataSource:\r\n        apiGroup: v1\r\n        name: postgres-0\r\n        kind: PersistentVolumeClaim\r\n```\n---\nsame here... after deleting all pods and pvcs of an existing cluster manually the operator doesn't spin up new ones.\r\nk8s 1.29, cnpg operator helm install 0.18.2\n---\nI'm facing the same issue. Anyone here come up with a work around for this?\n---\nFacing the same issue also\n---\nAlso have trhis same issue. InitJob starts but the cluster is not being (re)deployed. It only happens to a cluster that is being reployed with the same resources.\n---\nIn our case deleting the clusters.postgresql.cnpg.io resource and re creating it solved the problem.\r\nBear in mind we used Retain storageclass for the PVs\r\nCNPG: 1.24.0\r\nK8s: v1.28.13(RKE2)\n---\nHi @marthydavid When deleting a postgres cluster with a Retain claim on PVs, does this result data lost when the cluster get recreated?\n---\n@Lolliip0p you are correct if you mark the storageclass retain the PV Claim will not be deleted from the volume so in this case the volume will be reused upon recreation of the clusters.postgresql.cnpg.io resource.\r\nBut you need to use this type of storageclass beforehand.\r\nIn the storageclass spec there's this field:\r\n**reclaimPolicy: Retain**\r\nExample: \r\n```yaml\r\napiVersion: storage.k8s.io/v1\r\nkind: StorageClass\r\nmetadata:\r\n  annotations:\r\n    storageclass.beta.kubernetes.io/is-default-class: \"true\"\r\n    storageclass.kubernetes.io/is-default-class: \"true\"\r\n  name: openebs-zfspv\r\nallowVolumeExpansion: true\r\nparameters:\r\n  fstype: zfs\r\n  poolname: data\r\n  shared: \"no\"\r\nprovisioner: zfs.csi.openebs.io\r\nreclaimPolicy: Retain\r\nvolumeBindingMode: WaitForFirstConsumer\r\n```\n---\nI'm getting this quite often even when I completely remove the PVC, PV, Cluster-object and the complete helm chart of the affected application.\r\nSeems to be quite related to latest releases of CNPG to be frank\n---\nWell I now have to helm charts, that I completely nuke every trace off.\r\nAnd yet every damn reinstall it ends up with this stupid error.\r\nIts so bad and untraceable that I'm even rethinking my choice of going for CNPG to begin with.\r\nI am lucky this wasn't a production cluster. But otherwise I would be royally screwed.\r\n@sxd @mnencia  @gbartolini\r\nCan this PLEASE be triaged. This is for sure not limited to people with remaining PVs or PVCs.\n---\nquick glance showed that cnpg [heavily relies](https://github.com/cloudnative-pg/cloudnative-pg/blob/ebe20200dee0bd8ed357ab459b6e5fdc30d12855/internal/controller/cluster_restore.go#L241) on [annotations](https://cloudnative-pg.io/documentation/1.23/labels_annotations/) in its logic. \r\ni got the same err msg when i precreated pvc and cnpg cannot find appropritate annotation.\r\ni let cnpg create pvc itself and cluster is running now.\n---\n> quick glance showed that cnpg [heavily relies](https://github.com/cloudnative-pg/cloudnative-pg/blob/ebe20200dee0bd8ed357ab459b6e5fdc30d12855/internal/controller/cluster_restore.go#L241) on [annotations](https://cloudnative-pg.io/documentation/1.23/labels_annotations/) in its logic. i got the same err msg when i precreated pvc and cnpg cannot find appropritate annotation. i let cnpg create pvc itself and cluster is running now.\r\nI have no PVC or PV, everything is 100% a clean slate, so it should be creating them.\r\nThis error shouldnt show if there is no PVC or PV afaik.\n---\n>I have no PVC or PV, everything is 100% a clean slate, so it should be creating them.\r\nThis error shouldnt show if there is no PVC or PV afaik.\r\nprobably, idk :) but code [shows](https://github.com/cloudnative-pg/cloudnative-pg/blob/ebe20200dee0bd8ed357ab459b6e5fdc30d12855/internal/controller/cluster_create.go#L1048) why it could happen. \r\nmaybe some orphans objects are still existed. \r\nyou can check it easily with something like this:\r\n```shell\r\nkubectl get pvc -A | grep cnpg.io \r\n```\n---\n> > I have no PVC or PV, everything is 100% a clean slate, so it should be creating them.\r\n> > This error shouldnt show if there is no PVC or PV afaik.\r\n> \r\n> probably, idk :) but code [shows](https://github.com/cloudnative-pg/cloudnative-pg/blob/ebe20200dee0bd8ed357ab459b6e5fdc30d12855/internal/controller/cluster_create.go#L1048) why it could happen.\r\n> \r\n> maybe some orphans objects are still existed. you can check it easily with something like this:\r\n> \r\n> ```shell\r\n> kubectl get pvc -A | grep cnpg.io \r\n> ```\r\nWhen I say no objects exist, I mean it.\r\nNONE.\r\nIncluding their namespace.\n---\nI have run into this issue as well - cant reinstall cnpg cluster. So there is no fix / workaround at the moment ?\n---\nFacing the same issue. I deleted the PVCs and the pods, expecting the pods to be recreated, but I am seeing this log in the operator.\n```\n{\"level\":\"info\",\"ts\":\"2024-10-30T13:49:58.991988455Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"recovered-database-name\",\"namespace\":\"backup3\"},\"namespace\":\"backup3\",\"name\":\"recovered-database-name\",\"reconcileID\":\"0f61ae67-8d0e-44da-8c1e-d1eed82c09a3\",\"latestGeneratedNode\":3}\n```\nCNPG: 1.24.1 by Helm\nK8s: v1.27.16\nPostgres: 16.0\nMy cluster yml\n```yml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  creationTimestamp: '2024-10-28T19:41:35Z'\n  generation: 4\n  labels:\n    objectset.rio.cattle.io/hash: ac277acec1e31bd7f1462b8dd625c9f54dec5170\n  managedFields:\n    - apiVersion: postgresql.cnpg.io/v1\n      fieldsType: FieldsV1\n      fieldsV1:\n        f:status:\n          .: {}\n          f:availableArchitectures: {}\n          f:certificates:\n            .: {}\n            f:clientCASecret: {}\n            f:expirations:\n              .: {}\n              f:database-name-ca: {}\n              f:database-name-replication: {}\n              f:database-name-server: {}\n            f:replicationTLSSecret: {}\n            f:serverAltDNSNames: {}\n            f:serverCASecret: {}\n            f:serverTLSSecret: {}\n          f:cloudNativePGCommitHash: {}\n          f:cloudNativePGOperatorHash: {}\n          f:conditions: {}\n          f:configMapResourceVersion:\n            .: {}\n            f:metrics:\n              .: {}\n              f:cnpg-default-monitoring: {}\n          f:currentPrimary: {}\n          f:currentPrimaryTimestamp: {}\n          f:firstRecoverabilityPoint: {}\n          f:firstRecoverabilityPointByMethod:\n            .: {}\n            f:barmanObjectStore: {}\n          f:image: {}\n          f:lastSuccessfulBackup: {}\n          f:lastSuccessfulBackupByMethod:\n            .: {}\n            f:barmanObjectStore: {}\n          f:latestGeneratedNode: {}\n          f:managedRolesStatus: {}\n          f:phase: {}\n          f:phaseReason: {}\n          f:poolerIntegrations:\n            .: {}\n            f:pgBouncerIntegration: {}\n          f:readService: {}\n          f:secretsResourceVersion:\n            .: {}\n            f:applicationSecretVersion: {}\n            f:clientCaSecretVersion: {}\n            f:replicationSecretVersion: {}\n            f:serverCaSecretVersion: {}\n            f:serverSecretVersion: {}\n            f:superuserSecretVersion: {}\n          f:switchReplicaClusterStatus: {}\n          f:targetPrimary: {}\n          f:targetPrimaryTimestamp: {}\n          f:timelineID: {}\n          f:topology:\n            .: {}\n            f:successfullyExtracted: {}\n          f:writeService: {}\n      manager: manager\n      operation: Update\n      subresource: status\n      time: '2024-10-30T13:11:57Z'\n    - apiVersion: postgresql.cnpg.io/v1\n      fieldsType: FieldsV1\n      fieldsV1:\n        f:metadata:\n          f:annotations:\n            .: {}\n            f:objectset.rio.cattle.io/applied: {}\n            f:objectset.rio.cattle.io/id: {}\n          f:labels:\n            .: {}\n            f:objectset.rio.cattle.io/hash: {}\n        f:spec:\n          .: {}\n          f:backup:\n            .: {}\n            f:barmanObjectStore:\n              .: {}\n              f:data:\n                .: {}\n                f:compression: {}\n              f:destinationPath: {}\n              f:endpointURL: {}\n              f:s3Credentials:\n                .: {}\n                f:accessKeyId:\n                  .: {}\n                  f:key: {}\n                  f:name: {}\n                f:secretAccessKey:\n                  .: {}\n                  f:key: {}\n                  f:name: {}\n              f:serverName: {}\n              f:wal:\n                .: {}\n                f:compression: {}\n            f:retentionPolicy: {}\n            f:target: {}\n          f:bootstrap:\n            .: {}\n            f:initdb:\n              .: {}\n              f:database: {}\n              f:owner: {}\n              f:secret:\n                .: {}\n                f:name: {}\n          f:enablePDB: {}\n          f:enableSuperuserAccess: {}\n          f:failoverDelay: {}\n          f:imageName: {}\n          f:instances: {}\n          f:logLevel: {}\n          f:maxSyncReplicas: {}\n          f:minSyncReplicas: {}\n          f:monitoring:\n            .: {}\n            f:disableDefaultQueries: {}\n            f:enablePodMonitor: {}\n          f:postgresGID: {}\n          f:postgresUID: {}\n          f:primaryUpdateMethod: {}\n          f:primaryUpdateStrategy: {}\n          f:replicationSlots:\n            .: {}\n            f:highAvailability:\n              .: {}\n              f:enabled: {}\n              f:slotPrefix: {}\n            f:updateInterval: {}\n          f:resources:\n            .: {}\n            f:limits:\n              .: {}\n              f:memory: {}\n            f:requests:\n              .: {}\n              f:memory: {}\n          f:smartShutdownTimeout: {}\n          f:startDelay: {}\n          f:stopDelay: {}\n          f:storage:\n            .: {}\n            f:resizeInUseVolumes: {}\n            f:size: {}\n          f:superuserSecret:\n            .: {}\n            f:name: {}\n          f:switchoverDelay: {}\n      manager: agent\n      operation: Update\n      time: '2024-10-30T13:19:03Z'\n  name: database-name\n  namespace: backup3\n  resourceVersion: '203404973'\n  uid: 2f21d5af-43e9-4010-8da7-939c0e7297e7\nspec:\n  affinity:\n    podAntiAffinityType: preferred\n  backup:\n    barmanObjectStore:\n      data:\n        compression: gzip\n      destinationPath: ****\n      endpointURL: ****\n      s3Credentials:\n        accessKeyId:\n          key: MINIO_ACCESS_KEY_ID\n          name: minio-secret\n        secretAccessKey:\n          key: MINIO_SECRET_ACCESS_KEY\n          name: minio-secret\n      serverName: backup-database-name-3\n      wal:\n        compression: gzip\n    retentionPolicy: 1d\n    target: prefer-standby\n  bootstrap:\n    initdb:\n      database: database-name\n      encoding: UTF8\n      localeCType: C\n      localeCollate: C\n      owner: database-name\n      secret:\n        name: database-name-app-user\n  enablePDB: true\n  enableSuperuserAccess: true\n  failoverDelay: 0\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.0\n  instances: 3\n  logLevel: info\n  maxSyncReplicas: 0\n  minSyncReplicas: 0\n  monitoring:\n    customQueriesConfigMap:\n      - key: queries\n        name: cnpg-default-monitoring\n    disableDefaultQueries: false\n    enablePodMonitor: true\n  postgresGID: 26\n  postgresUID: 26\n  postgresql:\n    parameters:\n      archive_mode: 'on'\n      archive_timeout: 5min\n      dynamic_shared_memory_type: posix\n      full_page_writes: 'on'\n      log_destination: csvlog\n      log_directory: /controller/log\n      log_filename: postgres\n      log_rotation_age: '0'\n      log_rotation_size: '0'\n      log_truncate_on_rotation: 'false'\n      logging_collector: 'on'\n      max_parallel_workers: '32'\n      max_replication_slots: '32'\n      max_worker_processes: '32'\n      shared_memory_type: mmap\n      shared_preload_libraries: ''\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.3\n      wal_keep_size: 512MB\n      wal_level: logical\n      wal_log_hints: 'on'\n      wal_receiver_timeout: 5s\n      wal_sender_timeout: 5s\n    syncReplicaElectionConstraint:\n      enabled: false\n  primaryUpdateMethod: restart\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n      slotPrefix: _cnpg_\n    synchronizeReplicas:\n      enabled: true\n    updateInterval: 30\n  resources:\n    limits:\n      memory: 2001Mi\n    requests:\n      memory: 1000Mi\n  smartShutdownTimeout: 180\n  startDelay: 3600\n  stopDelay: 1800\n  storage:\n    resizeInUseVolumes: true\n    size: 20Gi\n  superuserSecret:\n    name: database-name-app-superuser\n  switchoverDelay: 3600\nstatus:\n  availableArchitectures:\n    - goArch: amd64\n      hash: 575b8d5080a718a1b1c8e6febcb6ccfde6cf546aa1a253acd7336226494ba784\n    - goArch: arm64\n      hash: bab50cc05e920db8bd118118323ef8003201dd3ba0642bbdee87cfdde1672e3e\n  certificates:\n    clientCASecret: database-name-ca\n    expirations:\n      database-name-ca: 2025-01-26 19:36:35 +0000 UTC\n      database-name-replication: 2025-01-26 19:36:35 +0000 UTC\n      database-name-server: 2025-01-26 19:36:35 +0000 UTC\n    replicationTLSSecret: database-name-replication\n    serverAltDNSNames:\n      - database-name-rw\n      - database-name-rw.backup3\n      - database-name-rw.backup3.svc\n      - database-name-rw.backup3.svc.cluster.local\n      - database-name-r\n      - database-name-r.backup3\n      - database-name-r.backup3.svc\n      - database-name-r.backup3.svc.cluster.local\n      - database-name-ro\n      - database-name-ro.backup3\n      - database-name-ro.backup3.svc\n      - database-name-ro.backup3.svc.cluster.local\n    serverCASecret: database-name-ca\n    serverTLSSecret: database-name-server\n  cloudNativePGCommitHash: 3f96930d\n  cloudNativePGOperatorHash: 575b8d5080a718a1b1c8e6febcb6ccfde6cf546aa1a253acd7336226494ba784\n  conditions:\n    - lastTransitionTime: '2024-10-30T13:08:23Z'\n      message: Cluster Is Not Ready\n      reason: ClusterIsNotReady\n      status: 'False'\n      type: Ready\n    - lastTransitionTime: '2024-10-28T19:42:51Z'\n      message: Continuous archiving is working\n      reason: ContinuousArchivingSuccess\n      status: 'True'\n      type: ContinuousArchiving\n    - lastTransitionTime: '2024-10-30T00:00:11Z'\n      message: Backup was successful\n      reason: LastBackupSucceeded\n      status: 'True'\n      type: LastBackupSucceeded\n  configMapResourceVersion:\n    metrics:\n      cnpg-default-monitoring: '202252074'\n  currentPrimary: database-name-1\n  currentPrimaryTimestamp: '2024-10-28T19:42:48.786910Z'\n  firstRecoverabilityPoint: '2024-10-29T00:00:08Z'\n  firstRecoverabilityPointByMethod:\n    barmanObjectStore: '2024-10-29T00:00:08Z'\n  image: ghcr.io/cloudnative-pg/postgresql:16.0\n  lastSuccessfulBackup: '2024-10-30T00:00:10Z'\n  lastSuccessfulBackupByMethod:\n    barmanObjectStore: '2024-10-30T00:00:10Z'\n  latestGeneratedNode: 3\n  managedRolesStatus: {}\n  phase: Waiting for the instances to become active\n  phaseReason: Some instances are not yet active. Please wait.\n  poolerIntegrations:\n    pgBouncerIntegration: {}\n  readService: database-name-r\n  secretsResourceVersion:\n    applicationSecretVersion: '202252028'\n    clientCaSecretVersion: '202252034'\n    replicationSecretVersion: '202252041'\n    serverCaSecretVersion: '202252034'\n    serverSecretVersion: '202252037'\n    superuserSecretVersion: '202252029'\n  switchReplicaClusterStatus: {}\n  targetPrimary: database-name-1\n  targetPrimaryTimestamp: '2024-10-28T19:41:35.194133Z'\n  timelineID: 1\n  topology:\n    successfullyExtracted: true\n  writeService: database-name-rw\n```\n---\nI found at least one case where this could be reproduced somewhat:\nCases where somehow the storageClass was invalid and it wasn't creating PVCs might, in some cases, not start as well with this error.\n---\n> I found at least one case where this could be reproduced somewhat: Cases where somehow the storageClass was invalid and it wasn't creating PVCs might, in some cases, not start as well with this error.\n@PrivatePuffin  Did you manage to get it to work? In my case, I am using Longhorn. I see the storage being provisioned on longhorn, then they are detached again after the initdb fails to provision. Worked perfectly before. But I decided to redo the cnpg install - uninstalled everything , including all storage items and namespaces. Ensured there was nothing left and rebooted the cluster. Still cant get it installed again. Always goes into an error loop during the provision of the first node in the cnpg cluster.\n---\n@vacquah Can you give me the name of your storageClass?\n---\n> [@vacquah](https://github.com/vacquah) Can you give me the name of your storageClass?\nlonghorn-postgres-storage\n---\nIs there no solution to this? I still cant install.\n---\nSame here, cordoned the nodes, deleted pods + pvcs, -> `refusing to create the primary instance while the latest generated serial is not zero`\nNuking the cluster as well helped. But the replicas fail to join the first instance coming up because there's too much load on it (too many connections) ... would've expected that there are some conns reserved for admin / cluster purposes.\n---\nIn my case, this happens when I manually delete the pvc/pv.\nSee also: https://github.com/cloudnative-pg/cloudnative-pg/blob/b3b6dbe7899d27dd65b42d4ecd89c9d0c3d780b0/internal/controller/cluster_create.go#L1063\nIf you really know what you are doing, you can override the lastGeneratedNode.\n```sh\nkubectl patch clusters.postgresql.cnpg.io mydb --type=merge --subresource status --patch 'status: {latestGeneratedNode: 0}'\n```\nThan the cluster comes back for me.\n---\n> In my case, this happens when I manually delete the pvc/pv. See also:\n> \n> [cloudnative-pg/internal/controller/cluster_create.go](https://github.com/cloudnative-pg/cloudnative-pg/blob/b3b6dbe7899d27dd65b42d4ecd89c9d0c3d780b0/internal/controller/cluster_create.go#L1063)\n> \n> Line 1063 in [b3b6dbe](/cloudnative-pg/cloudnative-pg/commit/b3b6dbe7899d27dd65b42d4ecd89c9d0c3d780b0)\n> \n>  contextLogger.Info(\"refusing to create the primary instance while the latest generated serial is not zero\", \nI can confirm this, although I'm not sure why deleting and recreating the cluster resource wouldn't solve it.\n> If you really know what you are doing, you can override the lastGeneratedNode.\n> \n> kubectl patch clusters.postgresql.cnpg.io mydb --type=merge --subresource status --patch 'status: {latestGeneratedNode: 0}'\n> Than the cluster comes back for me.\nThis is good shit!\n---\nI had totally missed this issue. We will look into it ASAP. Thanks.\n---\n> facing this too, using following:\n> \n> ```\n> apiVersion: v1\n> kind: PersistentVolume\n> metadata:\n>   name: postgres-0\n> spec:\n>   capacity:\n>     storage: 1Gi\n>   accessModes:\n>     - ReadWriteOnce\n>   hostPath:\n>     path: /mnt/postgres-data-0\n> ---\n> apiVersion: v1\n> kind: PersistentVolumeClaim\n> metadata:\n>   name: postgres-0\n> spec:\n>   accessModes:\n>     - ReadWriteOnce\n>   storageClassName: \"\"\n>   volumeName: postgres-0\n>   resources:\n>     requests:\n>       storage: 1Gi\n> ... <snipped>\n> ```\nWhy do you create the storage before-hand instead of using dynamic provisioning?\n---\n> Why do you create the storage before-hand instead of using dynamic provisioning?\nMight be best to ignore this weird setup when bug tracing this.\nAs most of the other people affected here have the same issue without weird customizations.\nFunnily enough, I thought I had tagged you already multiple times here @gbartolini ...\nBut seems I forgot to actually do it xD\n---\nI can see why creating storage before hand can make sense in some scenarios, Hosting multiple databases on the same cluster, but using different backend storage locations, due to different requirements, i.e. some databases need a specific storage location, due to security constraints, while others can \"share\" the default storage location.\nDynamically provisioning a PV/PVC-setup does impose a naming complexity and making it harder to look for potential issues. <- First world problem, I know, but i like to have full control over all naming, not only due to corporate naming conventions.\n---\n> I can see why creating storage before hand can make sense in some scenarios...\nCan we please just **all** ignore this?\nAs it has absolutely zero to do with the issue.\nIts rather annoying getting notifications for comments like these that have nothing really to do with the issue itself."
    },
    {
        "title": "Webhook only warns about nodeMaintenance window if used in PDB-disabling mode",
        "id": 2451574863,
        "state": "open",
        "first": "nodeMaintenanceWindow is also used with reusePVC: false which does not disable PDBs, leading to an irrelevant warning emitted here.\r\nOriginally Introduced: bf4248a33d3ffcec2a99c9bb5b5e7f94bc80ce91 \r\nRef: https://github.com/cloudnative-pg/cloudnative-pg/issues/2570#issuecomment-1884962321",
        "messages": "nodeMaintenanceWindow is also used with reusePVC: false which does not disable PDBs, leading to an irrelevant warning emitted here.\r\nOriginally Introduced: bf4248a33d3ffcec2a99c9bb5b5e7f94bc80ce91 \r\nRef: https://github.com/cloudnative-pg/cloudnative-pg/issues/2570#issuecomment-1884962321"
    },
    {
        "title": "Append namespace in backup path",
        "id": 2450108808,
        "state": "open",
        "first": "This PR is an attempt to solve issue #5216.\r\nThis adds in an option on the configuration of the backup path to include the namespace by appending it to the end of the path. This makes the path unique to the cluster even if the name is the same as another one in a different namespace.",
        "messages": "This PR is an attempt to solve issue #5216.\r\nThis adds in an option on the configuration of the backup path to include the namespace by appending it to the end of the path. This makes the path unique to the cluster even if the name is the same as another one in a different namespace."
    },
    {
        "title": "[Feature]: Separate enablePDB settings for primary and replicas",
        "id": 2447783905,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nThe enablePDB: setting creates 2 PDB's: one for primary and 1 for replicas.\r\nSince there is only 1 primary running, the default alerting in Kubernetes /Openshift  immediately generates an alert.\r\nA PDB is not needed in my opinion\r\n### Describe the solution you'd like\r\nInstead of 1 enablePDB setting that hits 2 PDB's, add 2 separate configurable flags for the primary and replicas. e.g.:  enablePDB-primary and  enablePDB-replicas\r\nTo handle backwardscompatibility, both the original enablePDB and the new flags could exist next to eachother, having the new flags to override enablePDB\r\ne.g. after determining the value van enablePDB, additionaly check if enablePDB-primary is set to false: do not create the primary pdb.\r\n### Describe alternatives you've considered\r\nsetting enablePDB to false and manually creating a PDB\r\n(we rather see the operator handle this)\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nNo\r\n### Are you willing to actively contribute to this feature?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nThe enablePDB: setting creates 2 PDB's: one for primary and 1 for replicas.\r\nSince there is only 1 primary running, the default alerting in Kubernetes /Openshift  immediately generates an alert.\r\nA PDB is not needed in my opinion\r\n### Describe the solution you'd like\r\nInstead of 1 enablePDB setting that hits 2 PDB's, add 2 separate configurable flags for the primary and replicas. e.g.:  enablePDB-primary and  enablePDB-replicas\r\nTo handle backwardscompatibility, both the original enablePDB and the new flags could exist next to eachother, having the new flags to override enablePDB\r\ne.g. after determining the value van enablePDB, additionaly check if enablePDB-primary is set to false: do not create the primary pdb.\r\n### Describe alternatives you've considered\r\nsetting enablePDB to false and manually creating a PDB\r\n(we rather see the operator handle this)\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nNo\r\n### Are you willing to actively contribute to this feature?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: Backups using Azure Blob Storage don't work on Azure Government Cloud",
        "id": 2444854193,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nHelm\n### What happened?\nI'm trying to set up backups on azure blob storage in an azure government cloud tenant but that fails. I found a relevant issue on the barman repo [here](https://github.com/EnterpriseDB/barman/issues/884). I've tried with and without the `storage-account` secret\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: main-cluster\r\nspec:\r\n  instances: 2\r\n  inheritedMetadata:\r\n    labels:\r\n      azure.workload.identity/use: \"true\"\r\n  serviceAccountTemplate:\r\n    metadata:\r\n      labels:\r\n        azure.workload.identity/use: \"true\"\r\n      annotations:\r\n        azure.workload.identity/client-id: \"<client-id>\"\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: https://<account-name>.blob.core.usgovcloudapi.net/postgres/backup\r\n      azureCredentials:\r\n        inheritFromAzureAD: true\r\n        # storageAccount:\r\n        #   name: storage-account\r\n        #   key: name\r\n      wal:\r\n        compression: gzip\r\n        encryption: AES256\r\n      data:\r\n        compression: gzip\r\n        encryption: AES256\r\n        immediateCheckpoint: false\r\n        jobs: 2\r\n    retentionPolicy: \"30d\"\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-08-02T10:44:08Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"2024-08-02 10:44:08,917 [1321] ERROR: Barman cloud WAL archiver exception: A connection string must be provided when using emulated storage\",\"pipe\":\"stderr\",\"logging_pod\":\"main-cluster-2\"}\r\n{\"level\":\"error\",\"ts\":\"2024-08-02T10:44:08Z\",\"msg\":\"Error invoking barman-cloud-wal-archive\",\"logging_pod\":\"main-cluster-2\",\"walName\":\"pg_wal/00000002.history\",\"currentPrimary\":\"main-cluster-2\",\"targetPrimary\":\"main-cluster-2\",\"options\":[\"--gzip\",\"-e\",\"AES256\",\"--cloud-provider\",\"azure-blob-storage\",\"--credential\",\"managed-identity\",\"https://<account-name>.blob.core.usgovcloudapi.net/postgres/backup\",\"main-cluster\",\"pg_wal/00000002.history\"],\"exitCode\":-1,\"error\":\"exit status 4\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/barman/archiver.(*WALArchiver).Archive\\n\\tpkg/management/barman/archiver/archiver.go:186\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/barman/archiver.(*WALArchiver).ArchiveList.func1\\n\\tpkg/management/barman/archiver/archiver.go:131\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nHelm\n### What happened?\nI'm trying to set up backups on azure blob storage in an azure government cloud tenant but that fails. I found a relevant issue on the barman repo [here](https://github.com/EnterpriseDB/barman/issues/884). I've tried with and without the `storage-account` secret\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: main-cluster\r\nspec:\r\n  instances: 2\r\n  inheritedMetadata:\r\n    labels:\r\n      azure.workload.identity/use: \"true\"\r\n  serviceAccountTemplate:\r\n    metadata:\r\n      labels:\r\n        azure.workload.identity/use: \"true\"\r\n      annotations:\r\n        azure.workload.identity/client-id: \"<client-id>\"\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: https://<account-name>.blob.core.usgovcloudapi.net/postgres/backup\r\n      azureCredentials:\r\n        inheritFromAzureAD: true\r\n        # storageAccount:\r\n        #   name: storage-account\r\n        #   key: name\r\n      wal:\r\n        compression: gzip\r\n        encryption: AES256\r\n      data:\r\n        compression: gzip\r\n        encryption: AES256\r\n        immediateCheckpoint: false\r\n        jobs: 2\r\n    retentionPolicy: \"30d\"\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-08-02T10:44:08Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"2024-08-02 10:44:08,917 [1321] ERROR: Barman cloud WAL archiver exception: A connection string must be provided when using emulated storage\",\"pipe\":\"stderr\",\"logging_pod\":\"main-cluster-2\"}\r\n{\"level\":\"error\",\"ts\":\"2024-08-02T10:44:08Z\",\"msg\":\"Error invoking barman-cloud-wal-archive\",\"logging_pod\":\"main-cluster-2\",\"walName\":\"pg_wal/00000002.history\",\"currentPrimary\":\"main-cluster-2\",\"targetPrimary\":\"main-cluster-2\",\"options\":[\"--gzip\",\"-e\",\"AES256\",\"--cloud-provider\",\"azure-blob-storage\",\"--credential\",\"managed-identity\",\"https://<account-name>.blob.core.usgovcloudapi.net/postgres/backup\",\"main-cluster\",\"pg_wal/00000002.history\"],\"exitCode\":-1,\"error\":\"exit status 4\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/barman/archiver.(*WALArchiver).Archive\\n\\tpkg/management/barman/archiver/archiver.go:186\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/barman/archiver.(*WALArchiver).ArchiveList.func1\\n\\tpkg/management/barman/archiver/archiver.go:131\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Docs]: Replica clusters topic contains broken links to missing \"About PostgreSQL Roles\" section",
        "id": 2442698990,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nPR #5167 added a new section to the Replica clusters topic, [\"About PostgreSQL Roles\"](https://github.com/cloudnative-pg/cloudnative-pg/blob/8a3d693b1c7c4b32877fecc19def75d506848d38/docs/src/replica_cluster.md#about-postgresql-roles). This is then referenced in four new links elsewhere on the revised page.\r\nSOME of the new content was backported to docs for 1.22 and 1.23, including three of those links. However, the section itself was not backported, leaving the links broken.\n### Describe what additions need to be done to the documentation\nThe links should either be removed from 1.22 and 1.23 docs, or the section backported. \n### Describe what pages need to change in the documentation, if any\nhttps://cloudnative-pg.io/documentation/1.23/replica_cluster/\r\nhttps://cloudnative-pg.io/documentation/1.22/replica_cluster/\r\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nPR #5167 added a new section to the Replica clusters topic, [\"About PostgreSQL Roles\"](https://github.com/cloudnative-pg/cloudnative-pg/blob/8a3d693b1c7c4b32877fecc19def75d506848d38/docs/src/replica_cluster.md#about-postgresql-roles). This is then referenced in four new links elsewhere on the revised page.\r\nSOME of the new content was backported to docs for 1.22 and 1.23, including three of those links. However, the section itself was not backported, leaving the links broken.\n### Describe what additions need to be done to the documentation\nThe links should either be removed from 1.22 and 1.23 docs, or the section backported. \n### Describe what pages need to change in the documentation, if any\nhttps://cloudnative-pg.io/documentation/1.23/replica_cluster/\r\nhttps://cloudnative-pg.io/documentation/1.22/replica_cluster/\r\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Add a flag to add the namespace to the backup cluster name",
        "id": 2442325150,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI have multiple clusters that all have the same name but exist within different namespaces. We do this becuase we have multiple development environments for testing. For instance I have dev1, dev2, dev3 as the name space, all of them have a cluster called \"api-data\" within them.\r\nWe also make use of ArgoCD and Kustomize to provision these resources in K8s.\r\nWhen provisioning the backup I currently have to create multiple files within kustomize that define the backup location that basically adds the name space as a unique identifier to the path. This results in a massive number of files that basically all contain the following:\r\n```yaml\r\nspec:\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3://psql-backup-storage/devX\r\n```\r\nI'd like to have a single bucket that contains all the backups. I'd also like to not have to specify a unique path in many files. The uniqueness of the cluster is actually the namespace + the cluster name. Therefore I would like to have a flag that tells the cluster to add the cluster name and namespace to the path when creating the files.\r\nSomething like this:\r\n```yaml\r\nspec:\r\n  backup:\r\n    barmanObjectStore:\r\n      includeNamespace: true\r\n      destinationPath: s3://psql-backup-storage\r\n```\r\nWith the result being `s3://psql-backup-storage/<namespace>/<cluster>`.\n### Describe the solution you'd like\nI'd like a flag in the barman section that allows me to have the namespace be part of the generated path that is used to write files to the object storage.\n### Describe alternatives you've considered\nWrite the base paths manually in kustomize, which generates many many files with very little difference.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI have multiple clusters that all have the same name but exist within different namespaces. We do this becuase we have multiple development environments for testing. For instance I have dev1, dev2, dev3 as the name space, all of them have a cluster called \"api-data\" within them.\r\nWe also make use of ArgoCD and Kustomize to provision these resources in K8s.\r\nWhen provisioning the backup I currently have to create multiple files within kustomize that define the backup location that basically adds the name space as a unique identifier to the path. This results in a massive number of files that basically all contain the following:\r\n```yaml\r\nspec:\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3://psql-backup-storage/devX\r\n```\r\nI'd like to have a single bucket that contains all the backups. I'd also like to not have to specify a unique path in many files. The uniqueness of the cluster is actually the namespace + the cluster name. Therefore I would like to have a flag that tells the cluster to add the cluster name and namespace to the path when creating the files.\r\nSomething like this:\r\n```yaml\r\nspec:\r\n  backup:\r\n    barmanObjectStore:\r\n      includeNamespace: true\r\n      destinationPath: s3://psql-backup-storage\r\n```\r\nWith the result being `s3://psql-backup-storage/<namespace>/<cluster>`.\n### Describe the solution you'd like\nI'd like a flag in the barman section that allows me to have the namespace be part of the generated path that is used to write files to the object storage.\n### Describe alternatives you've considered\nWrite the base paths manually in kustomize, which generates many many files with very little difference.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI have attempted to add this feature in with this PR, as you can see in the timeline: #5228"
    },
    {
        "title": "[Docs]: Missing Advice for Assigning Resource Requests/Limits to Operator Pods",
        "id": 2441236077,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nI can't find any advice in any of the documentation about how to properly size resource requests/limits for operator pods.\r\nI found the [Helm chart which supports the `resources` key](https://github.com/cloudnative-pg/charts/blob/main/charts/cloudnative-pg/README.md) so I know _how_ to set them, but what's missing is any advice about what to actually put there.\r\nThis is somewhat necessary when deploying to a GKE Autopilot cluster, as it [uses pod resource requests to allocate nodes](https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-resource-requests).\r\nIt has some [relatively sane defaults](https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-resource-requests#defaults) (`cpu: 500m`, `memory: 2Gi` for the general compute class), but I am unsure if this is going to be sufficient for the operator pod.\r\nThe other problem is, without specifying `limits`, the operator pod is going to be given the `Burstable` QoS class by default, which means it may get evicted when resources are constrained. That does not seem ideal.\r\nIt seems, at the very least, I need to add explicit limits and requests identical to the defaults to ensure the operator pod(s) are assigned the `Guaranteed` QoS class.\r\nI scanned the [default manifest](https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.23/releases/cnpg-1.23.3.yaml) and found the `cnpg-controller-manager` deployment actually has default resource requests, and they seem very small:\r\n```yaml\r\n        resources:\r\n          limits:\r\n            cpu: 100m\r\n            memory: 200Mi\r\n          requests:\r\n            cpu: 100m\r\n            memory: 100Mi\r\n```\r\nAdditionally, since `requests.memory` is smaller than `limits.memory`, this would seem to give the operator pods the `Burstable` QoS class, which I thought was undesirable. I don't see a `PodDisruptionBudget` in the manifest to control when operator pods are evicted.\r\nIn general, I guess I would like to see more documentation covering how to configure the operator itself for reliability.\n### Describe what additions need to be done to the documentation\nAdd advice about what resource requests/limits to assign to operator pods, or document that there _is_ a default and where to find it (though not necessarily what those values are, if subject to change).\n### Describe what pages need to change in the documentation, if any\nEither:\r\n* Resource Management (https://cloudnative-pg.io/documentation/1.23/resource_management/)\r\n* Or Operator Configuration (https://cloudnative-pg.io/documentation/1.23/operator_conf/)\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nI can't find any advice in any of the documentation about how to properly size resource requests/limits for operator pods.\r\nI found the [Helm chart which supports the `resources` key](https://github.com/cloudnative-pg/charts/blob/main/charts/cloudnative-pg/README.md) so I know _how_ to set them, but what's missing is any advice about what to actually put there.\r\nThis is somewhat necessary when deploying to a GKE Autopilot cluster, as it [uses pod resource requests to allocate nodes](https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-resource-requests).\r\nIt has some [relatively sane defaults](https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-resource-requests#defaults) (`cpu: 500m`, `memory: 2Gi` for the general compute class), but I am unsure if this is going to be sufficient for the operator pod.\r\nThe other problem is, without specifying `limits`, the operator pod is going to be given the `Burstable` QoS class by default, which means it may get evicted when resources are constrained. That does not seem ideal.\r\nIt seems, at the very least, I need to add explicit limits and requests identical to the defaults to ensure the operator pod(s) are assigned the `Guaranteed` QoS class.\r\nI scanned the [default manifest](https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.23/releases/cnpg-1.23.3.yaml) and found the `cnpg-controller-manager` deployment actually has default resource requests, and they seem very small:\r\n```yaml\r\n        resources:\r\n          limits:\r\n            cpu: 100m\r\n            memory: 200Mi\r\n          requests:\r\n            cpu: 100m\r\n            memory: 100Mi\r\n```\r\nAdditionally, since `requests.memory` is smaller than `limits.memory`, this would seem to give the operator pods the `Burstable` QoS class, which I thought was undesirable. I don't see a `PodDisruptionBudget` in the manifest to control when operator pods are evicted.\r\nIn general, I guess I would like to see more documentation covering how to configure the operator itself for reliability.\n### Describe what additions need to be done to the documentation\nAdd advice about what resource requests/limits to assign to operator pods, or document that there _is_ a default and where to find it (though not necessarily what those values are, if subject to change).\n### Describe what pages need to change in the documentation, if any\nEither:\r\n* Resource Management (https://cloudnative-pg.io/documentation/1.23/resource_management/)\r\n* Or Operator Configuration (https://cloudnative-pg.io/documentation/1.23/operator_conf/)\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct+1 on this, I also want to add that the helm chart for the operator has no actual resources requests/limits defined, so installing a release with default values will always work granted there is available memory/cpu to run the operator on the cluster - but the quickstart manifests that @abonander referenced are too restrictive and I had seen the operator go in CrashLoopBackoff due to OOMKilled, both with cloudnative-pg 1.23.3 and 1.24.0-RC1 - just tried that today. \r\nIt looks like the operator takes at least 350/400MB memory, even with no PG cluster defined - so I\u2019d suggest at least to uniform the helm values and the quickstart manifests - with no limits or higher limits (I\u2019d say at least 500MB)"
    },
    {
        "title": "[Feature]: In-place recovery from backup",
        "id": 2438241528,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nCurrently,\r\n> In CloudNativePG, you can't perform recovery in place on an existing cluster. Recovery is instead a way to bootstrap a new Postgres cluster starting from an available physical backup.\r\nWhile it is amazing to have this capability, the ergonomics are not great, especially if needing to recover a database in an emergency scenario.\r\nThe current recovery mechanism has the following friction points:\r\n1. Requires changing the cluster spec (`bootstrap.recovery` + `externalClusters`)\r\n2. Requires creating a new cluster\r\n3. Requires deleting the old cluster\r\n### Describe the solution you'd like\r\nIt seems like a cluster that has the `backup` spec configured has all of the information it needs find available backups and initiate an in-place recovery.\r\nTo make it easier to recover an existing cluster to one of its backups, I'd propose the following:\r\n1. Add a `kubectl-cnpg recover` command to the cnpg plugin that exposes the same fields as `bootstrap.recovery` on the `Cluster` spec.\r\n2. When calling that command:\r\n   1. Internally, reset the cluster by removing the pods and state.\r\n   2. Execute the logic that is currently run when bootstrapping a new cluster with the `bootstrap.recovery` field set using the options supplied to the CLI.\r\nFor the end user, this would make recovery in an emergency situation much faster and less error-prone. In effect, it would require a user to just run a single command such as `kubectl cnpg recover <cluster> --source=myBackup --targetTime=\"2023-08-11 11:14:21.00000+02\"`.\r\n### Describe alternatives you've considered\r\nAlternatively, you could supply a `recovery` field on the Cluster spec outside of the `bootstrap`. When that field changes, an in-place recovery is initiated.\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nNo\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nCurrently,\r\n> In CloudNativePG, you can't perform recovery in place on an existing cluster. Recovery is instead a way to bootstrap a new Postgres cluster starting from an available physical backup.\r\nWhile it is amazing to have this capability, the ergonomics are not great, especially if needing to recover a database in an emergency scenario.\r\nThe current recovery mechanism has the following friction points:\r\n1. Requires changing the cluster spec (`bootstrap.recovery` + `externalClusters`)\r\n2. Requires creating a new cluster\r\n3. Requires deleting the old cluster\r\n### Describe the solution you'd like\r\nIt seems like a cluster that has the `backup` spec configured has all of the information it needs find available backups and initiate an in-place recovery.\r\nTo make it easier to recover an existing cluster to one of its backups, I'd propose the following:\r\n1. Add a `kubectl-cnpg recover` command to the cnpg plugin that exposes the same fields as `bootstrap.recovery` on the `Cluster` spec.\r\n2. When calling that command:\r\n   1. Internally, reset the cluster by removing the pods and state.\r\n   2. Execute the logic that is currently run when bootstrapping a new cluster with the `bootstrap.recovery` field set using the options supplied to the CLI.\r\nFor the end user, this would make recovery in an emergency situation much faster and less error-prone. In effect, it would require a user to just run a single command such as `kubectl cnpg recover <cluster> --source=myBackup --targetTime=\"2023-08-11 11:14:21.00000+02\"`.\r\n### Describe alternatives you've considered\r\nAlternatively, you could supply a `recovery` field on the Cluster spec outside of the `bootstrap`. When that field changes, an in-place recovery is initiated.\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nNo\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductGreat and very useful features. But it is very important to confirm the correct restore to your target time.\n---\nAbsolutely, I would imagine if implemented in the CLI, there would be a confirmation prompt that required re-entering / confirming the information.\r\nI would also propose that the existing backups should _not_ be dropped on the restore.\r\nI believe this is already the default behavior:\r\n> The operator includes a safety check to ensure a cluster doesn't overwrite a storage bucket that contained information. A cluster that would overwrite existing storage remains in the state Setting up primary with pods in an error state. The pod logs show: ERROR: WAL archive check failed for server\r\nrecoveredCluster: Expected empty archive.\r\nThat way if you were to restore to the wrong backup / point-in-time, you could always retry without data loss.\r\nOf course, this would require manual intervention in order to have backups working again after the recovery, but I think this is fine given the primary goal here is simply to minimize the friction of executing the recovery itself. (Note that you can enable automatic overwrites by setting the `cnpg.io/skipEmptyWalArchiveCheck` annotation).\n---\nA way to have cnpg bootstrap a cluster from backups would be amazing. My D/R steps would be greatly reduced because right now I need to edit the YAML for the pg cluster for recovering a pg cluster.\r\nCloudnativePG needs to have the cluster spec updated with the new `.spec.backup.barmanObjectStore.serverName` and `.spec.bootstrap.recovery.source`\r\nMy current `Cluster`\r\n```yaml\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: postgres16\r\nspec:\r\n  instances: 3\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.4-28\r\n  primaryUpdateStrategy: unsupervised\r\n  storage:\r\n    size: 20Gi\r\n    storageClass: openebs-hostpath\r\n  superuserSecret:\r\n    name: cloudnative-pg-secret\r\n  enableSuperuserAccess: true\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  backup:\r\n    retentionPolicy: 30d\r\n    barmanObjectStore: &barmanObjectStore\r\n      data:\r\n        compression: bzip2\r\n      wal:\r\n        compression: bzip2\r\n        maxParallel: 8\r\n      destinationPath: s3://cloudnative-pg/\r\n      endpointURL: http://minio.default.svc.cluster.local:9000\r\n      # Note: serverName version needs to be incremented\r\n      # when recovering from an existing cnpg cluster\r\n      serverName: &currentCluster postgres16-v6\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: cloudnative-pg-secret\r\n          key: aws-access-key-id\r\n        secretAccessKey:\r\n          name: cloudnative-pg-secret\r\n          key: aws-secret-access-key\r\n  # Note: previousCluster needs to be set to the name of the previous\r\n  # cluster when recovering from an existing cnpg cluster\r\n  bootstrap:\r\n    recovery:\r\n      source: &previousCluster postgres16-v5\r\n  # Note: externalClusters is needed when recovering from an existing cnpg cluster\r\n  externalClusters:\r\n    - name: *previousCluster\r\n      barmanObjectStore:\r\n        <<: *barmanObjectStore\r\n        serverName: *previousCluster\r\n```\r\nIf there was an option exposed to force cnpg to recover on a new cluster that would be awesome.  I know Crunchy PGO supports this feature, it would be nice if CNPG followed suit.\n---\nI agree the current design is simply not infrastructure-as-code compliant enough\n---\n@onedr0p Not sure what you mean by this:\r\n> A way to have cnpg bootstrap a cluster from backups would be amazing. \r\nThe ability to bootstrap a cluster from backups already exists. This feature request is a way to initiate a recovery on a cluster that has already been bootstrapped. That is currently not possible using native CNPG functionality.\n---\nMy request looks more like https://github.com/cloudnative-pg/cloudnative-pg/issues/5778 so I'll bring it over there, sorry for the noise!\n---\n@gbartolini Wanted to touch base on this one. This has quickly become one of the most \ud83d\udc4d\ud83c\udffb feature requests for the project, but it is still in the triage phase. I don't feel comfortable proceeding with an implementation until a project maintainer provides some directional guidance on the preferred approach here.\n---\nI like how it is done in [Percona Operator for MySQL](https://docs.percona.com/percona-operator-for-mysql/pxc/backups-restore.html#restore-the-cluster-with-point-in-time-recovery). You just define `Restore` custom resource with all required parameters, perform `kubectl apply` and recovery process is running. No need to even touch `Cluster` CR and IMO this is more declarative than running CLI command. As a bonus we can have history of recovery attempts as Kubernetes objects and in general it can be better integrated with monitoring tools. However, having both methods (CLI and Custom Resource) would be great!\n---\n> I like how it is done in [Percona Operator for MySQL](https://docs.percona.com/percona-operator-for-mysql/pxc/backups-restore.html#restore-the-cluster-with-point-in-time-recovery). You just define `Restore` custom resource with all required parameters, perform `kubectl apply` and recovery process is running. No need to even touch `Cluster` CR and IMO this is more declarative than running CLI command. As a bonus we can have history of recovery attempts as Kubernetes objects and in general it can be better integrated with monitoring tools. However, having both methods (CLI and Custom Resource) would be great!\nI'm not sure that works any better with IaC.\n---\nHaving a Restore CRD wouldn't influence the ability of this to work IaC. In fact, it actually provides more options.\nThe `kubectl-cnpg recover` CLI command would simply create the CR. Additionally, you could now also create a Recovery using your existing favorite IaC tooling (`kubectl`, `helm`, `terraform`, etc.).\nThis also matches that pattern implemented in other popular backup / restore solutions for Kubernetes such as Velero."
    },
    {
        "title": "[Bug]: detection of most advanced replica, and primary, fail for replica clusters",
        "id": 2438079061,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njaime.silvela@mailfene.com\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nThere are several cases in the codebase where we rely on having the slice of instances sorted,\r\nwith the [0] element being the primary, and the [1] element being the most advanced replica.\r\n`pkg/postgres/status.go`\r\n``` go\r\n// PostgresqlStatusList is a list of PostgreSQL status received from the Pods\r\n// that can be sorted considering the replication status\r\ntype PostgresqlStatusList struct {\r\n\tItems []PostgresqlStatus `json:\"items\"`\r\n}\r\n```\r\nWe have already seen bugs with this approach:\r\n- In replica clusters, the designated primary may not be sorted as the first item, since `IsPrimary` is false for all pods in a replica cluster, and if the designated primary is at the same LSN with another pod, alphabetical order will be used\r\n- We have seen a previous issue where a pod that was not streaming would have the \"LSN\" read as zero, and would climb to the top of the list\r\nMaking the choice of primary and most advanced replica by getting a list and sorting it, is also hard to read.\r\nWe should have a more behavioral interface (in the words of Armando)\r\nIn the `status` plugin we show the instances as sorted by the current criterium.\r\nYou can see in this replica cluster, the designated primary is the second item.\r\n``` txt\r\nInstances status\r\nName                                            Database Size  Current LSN  Replication role              Status  QoS         Manager Version  Node\r\n----                                            -------------  -----------  ----------------              ------  ---         ---------------  ----\r\ncluster-example-with-volume-snapshot-replica-1  29 MB          0/9000060    Standby (in Replica Cluster)  OK      BestEffort  1.23.2           pg-operator-e2e-v1-30-2-worker\r\ncluster-example-with-volume-snapshot-replica-2  29 MB          0/9000060    Designated primary            OK      BestEffort  1.23.2           pg-operator-e2e-v1-30-2-worker\r\ncluster-example-with-volume-snapshot-replica-3  29 MB          0/9000060    Standby (in Replica Cluster)  OK      BestEffort  1.23.2           pg-operator-e2e-v1-30-2-worker\r\n```\n### Cluster resource\n```shell\nPlain example of a replica cluster, but any replica cluster will do.\r\n yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-replica-tls\r\nspec:\r\n  instances: 3\r\n  bootstrap:\r\n    pg_basebackup:\r\n      source: cluster-example\r\n  replica:\r\n    primary: cluster-example\r\n    source: cluster-example\r\n  storage:\r\n    size: 1Gi\r\n  externalClusters:\r\n  - name: cluster-example\r\n    connectionParameters:\r\n      host: cluster-example-rw.default.svc\r\n      user: streaming_replica\r\n      sslmode: verify-full\r\n      dbname: postgres\r\n    sslKey:\r\n      name: cluster-example-replication\r\n      key: tls.key\r\n    sslCert:\r\n      name: cluster-example-replication\r\n      key: tls.crt\r\n    sslRootCert:\r\n      name: cluster-example-ca\r\n      key: ca.crt\r\n```\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njaime.silvela@mailfene.com\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nThere are several cases in the codebase where we rely on having the slice of instances sorted,\r\nwith the [0] element being the primary, and the [1] element being the most advanced replica.\r\n`pkg/postgres/status.go`\r\n``` go\r\n// PostgresqlStatusList is a list of PostgreSQL status received from the Pods\r\n// that can be sorted considering the replication status\r\ntype PostgresqlStatusList struct {\r\n\tItems []PostgresqlStatus `json:\"items\"`\r\n}\r\n```\r\nWe have already seen bugs with this approach:\r\n- In replica clusters, the designated primary may not be sorted as the first item, since `IsPrimary` is false for all pods in a replica cluster, and if the designated primary is at the same LSN with another pod, alphabetical order will be used\r\n- We have seen a previous issue where a pod that was not streaming would have the \"LSN\" read as zero, and would climb to the top of the list\r\nMaking the choice of primary and most advanced replica by getting a list and sorting it, is also hard to read.\r\nWe should have a more behavioral interface (in the words of Armando)\r\nIn the `status` plugin we show the instances as sorted by the current criterium.\r\nYou can see in this replica cluster, the designated primary is the second item.\r\n``` txt\r\nInstances status\r\nName                                            Database Size  Current LSN  Replication role              Status  QoS         Manager Version  Node\r\n----                                            -------------  -----------  ----------------              ------  ---         ---------------  ----\r\ncluster-example-with-volume-snapshot-replica-1  29 MB          0/9000060    Standby (in Replica Cluster)  OK      BestEffort  1.23.2           pg-operator-e2e-v1-30-2-worker\r\ncluster-example-with-volume-snapshot-replica-2  29 MB          0/9000060    Designated primary            OK      BestEffort  1.23.2           pg-operator-e2e-v1-30-2-worker\r\ncluster-example-with-volume-snapshot-replica-3  29 MB          0/9000060    Standby (in Replica Cluster)  OK      BestEffort  1.23.2           pg-operator-e2e-v1-30-2-worker\r\n```\n### Cluster resource\n```shell\nPlain example of a replica cluster, but any replica cluster will do.\r\n yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-replica-tls\r\nspec:\r\n  instances: 3\r\n  bootstrap:\r\n    pg_basebackup:\r\n      source: cluster-example\r\n  replica:\r\n    primary: cluster-example\r\n    source: cluster-example\r\n  storage:\r\n    size: 1Gi\r\n  externalClusters:\r\n  - name: cluster-example\r\n    connectionParameters:\r\n      host: cluster-example-rw.default.svc\r\n      user: streaming_replica\r\n      sslmode: verify-full\r\n      dbname: postgres\r\n    sslKey:\r\n      name: cluster-example-replication\r\n      key: tls.key\r\n    sslCert:\r\n      name: cluster-example-replication\r\n      key: tls.crt\r\n    sslRootCert:\r\n      name: cluster-example-ca\r\n      key: ca.crt\r\n```\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Shared backup object storage configuration multiple every clusters",
        "id": 2433980620,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI plan on creating a bunch of small-ish clusters using the operator. I'd like these to be backuped to object storage.\r\nBut, to ease configuration, i'd prefer not having to configure dozens of different object storage credentials/buckets/accounts/policies.\r\n### Describe the solution you'd like\nI'd like to give the operator a set of credentials with the required permissions to create user/policies/buckets on the object storage. \r\nI'd then only set a flag while creating a cluster ala `backup.toSharedLocation: the_location`.\r\n### Describe alternatives you've considered\nConfiguring backups for every cluster manually. \r\nI'd hoped to avoid that.\n### Additional context\nIMHO this would fit very well with the \"autopilot\" level claim of the operator.\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI plan on creating a bunch of small-ish clusters using the operator. I'd like these to be backuped to object storage.\r\nBut, to ease configuration, i'd prefer not having to configure dozens of different object storage credentials/buckets/accounts/policies.\r\n### Describe the solution you'd like\nI'd like to give the operator a set of credentials with the required permissions to create user/policies/buckets on the object storage. \r\nI'd then only set a flag while creating a cluster ala `backup.toSharedLocation: the_location`.\r\n### Describe alternatives you've considered\nConfiguring backups for every cluster manually. \r\nI'd hoped to avoid that.\n### Additional context\nIMHO this would fit very well with the \"autopilot\" level claim of the operator.\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductYou could use a single s3 user and bucket for multiple clusters. In the bucket CNPG will create a folder per cluster anyway.\n---\n> You could use a single s3 user and bucket for multiple clusters. In the bucket CNPG will create a folder per cluster anyway.\r\nWhile that is good to know, that doesnt really solve my problem."
    },
    {
        "title": "[Bug]: point in time recovery fails from on demand backup when using targetTime",
        "id": 2433794744,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nNote new to cloudnative-pg, this happened during testing of cloudnative-pg to see if it something I would use. \r\nI created a database, put some data in it (its not actively used by any application). Added scheduled backups to a minio instance. So far it all works.\r\nWhen i try to recover to a point in time backup from the minio instance using a scheduled backup it works\r\n```yaml\r\n  bootstrap:\r\n    recovery:\r\n      source: cnpg-woop-db\r\n      recoveryTarget:\r\n        targetTime: \"2024-07-28 01:00:00.00000+00\"\r\n(scheduled backup was done at midnight)\r\n```\r\nWhen i create a manual backup:\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Backup\r\nmetadata:\r\n  name: backup-example1\r\n  namespace: data\r\nspec:\r\n  method: barmanObjectStore\r\n  cluster:\r\n    name: cnpg-woop-db\r\n```\r\nor by using the kubectl plugin:\r\n```bash\r\nkubectl cnpg backup cnpg-woop-db -n data\r\n```\r\nThe backup gets created and is visible in minio/ k8s backup list\r\nHowever if i then try to restore to this backup using the `targetTime` like posted above (but then with an updated targettime) it fails to bootstrap the cluster and the started job fails (logs below)\r\nWhen doing the same thing but then using the backup name the restore works fine.\r\n```yaml\r\n      backup:\r\n        name: backup-example1\r\nor\r\n      backup:\r\n        name: cnpg-woop-db-20240728102002\r\n```\r\nBootstrap used for recovery: \r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cnpg-foo-db-frombackup\r\nspec:\r\n  instances: 1\r\n  bootstrap:\r\n    recovery:\r\n      backup:\r\n        name: cnpg-woop-db-20240728102002\r\n      # source: cnpg-woop-db\r\n      # recoveryTarget:\r\n      #   targetTime: \"2024-07-28 11:00:00.00000+00\"\r\n  storage:\r\n    storageClass: longhorn-crypto\r\n    size: 10Gi\r\n  superuserSecret:\r\n    name: cnpg-woop-db-app\r\n  externalClusters:\r\n    - name: cnpg-woop-db\r\n      barmanObjectStore:\r\n        destinationPath: \"s3://postgres/\"\r\n        endpointURL: \"http://minio:9000\"\r\n        s3Credentials:\r\n          accessKeyId:\r\n            name: aws-creds\r\n            key: ACCESS_KEY_ID\r\n          secretAccessKey:\r\n            name: aws-creds\r\n            key: ACCESS_SECRET_KEY\r\n        wal:\r\n          maxParallel: 8\r\n```\n### Cluster resource\n_No response_\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:00Z\",\"msg\":\"Recovering from external cluster\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"sourceName\":\"cnpg-woop-db\"}\r\n2024-07-28T09:56:00.797511212+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:00Z\",\"msg\":\"Target backup found\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"backup\":{\"backup_name\":\"backup-20240728065333\",\"backup_label\":\"'START WAL LOCATION: 0/8B000028 (file 00000001000000000000008B)\\\\nCHECKPOINT LOCATION: 0/8B000060\\\\nBACKUP METHOD: streamed\\\\nBACKUP FROM: primary\\\\nSTART TIME: 2024-07-28 06:53:33 UTC\\\\nLABEL: Barman backup cloud 20240728T065333\\\\nSTART TIMELINE: 1\\\\n'\",\"begin_time\":\"Sun Jul 28 06:53:33 2024\",\"end_time\":\"Sun Jul 28 06:53:35 2024\",\"BeginTime\":\"2024-07-28T06:53:33Z\",\"EndTime\":\"2024-07-28T06:53:35Z\",\"begin_wal\":\"00000001000000000000008B\",\"end_wal\":\"00000001000000000000008B\",\"begin_xlog\":\"0/8B000028\",\"end_xlog\":\"0/8B0019F0\",\"systemid\":\"7396357834682834968\",\"backup_id\":\"20240728T065333\",\"error\":\"\",\"timeline\":1}}\r\n2024-07-28T09:56:01.200815989+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:01Z\",\"msg\":\"Starting barman-cloud-restore\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"options\":[\"--endpoint-url\",\"http://minio:9000\",\"s3://postgres/\",\"cnpg-woop-db\",\"20240728T065333\",\"--cloud-provider\",\"aws-s3\",\"/var/lib/postgresql/data/pgdata\"]}\r\n2024-07-28T09:56:01.952418772+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:01Z\",\"msg\":\"Restore completed\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:01Z\",\"msg\":\"Creating new data directory\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"pgdata\":\"/controller/recovery/datadir_480069336\",\"initDbOptions\":[\"--username\",\"postgres\",\"-D\",\"/controller/recovery/datadir_480069336\",\"--no-sync\"]}\r\n2024-07-28T09:56:02.315721536+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"initdb\",\"msg\":\"The files belonging to this database system will be owned by user \\\"postgres\\\".\\nThis user must also own the server process.\\n\\nThe database cluster will be initialized with locale \\\"en_US.utf8\\\".\\nThe default database encoding has accordingly been set to \\\"UTF8\\\".\\nThe default text search configuration will be set to \\\"english\\\".\\n\\nData page checksums are disabled.\\n\\nfixing permissions on existing directory /controller/recovery/datadir_480069336 ... ok\\ncreating subdirectories ... ok\\nselecting dynamic shared memory implementation ... posix\\nselecting default max_connections ... 100\\nselecting default shared_buffers ... 128MB\\nselecting default time zone ... Etc/UTC\\ncreating configuration files ... ok\\nrunning bootstrap script ... ok\\nperforming post-bootstrap initialization ... ok\\n\\nSync to disk skipped.\\nThe data directory might become corrupt if the operating system crashes.\\n\\n\\nSuccess. You can now start the database server using:\\n\\n    pg_ctl -D /controller/recovery/datadir_480069336 -l logfile start\\n\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n2024-07-28T09:56:02.315736039+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"initdb\",\"msg\":\"initdb: warning: enabling \\\"trust\\\" authentication for local connections\\ninitdb: hint: You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb.\\n\",\"pipe\":\"stderr\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n2024-07-28T09:56:02.321977948+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"msg\":\"Installed configuration file\r\n\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"pgdata\":\"/controller/recovery/datadir_480069336\",\"filename\":\"pg_hba.conf\"}\r\n2024-07-28T09:56:02.324859966+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"msg\":\"Installed configuration file\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"pgdata\":\"/controller/recovery/datadir_480069336\",\"filename\":\"pg_ident.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"msg\":\"Installed configuration file\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"pgdata\":\"/controller/recovery/datadir_480069336\",\"filename\":\"custom.conf\"}\r\n2024-07-28T09:56:02.362881366+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"msg\":\"Generated recovery configuration\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"configuration\":\"recovery_target_action = promote\\nrestore_command = 'barman-cloud-wal-restore --endpoint-url http://minio:9000 s3://postgres/ cnpg-woop-db --cloud-provider aws-s3 %f %p'\\nrecovery_target_time = '2024-07-28 06:55:00.00000+00'\\nrecovery_target_inclusive = true\\n\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"msg\":\"Aligned PostgreSQL configuration to satisfy both pg_controldata and cluster spec\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"enforcedParams\":{\"max_connections\":\"100\",\"max_locks_per_transaction\":\"64\",\"max_prepared_transactions\":\"0\",\"max_wal_senders\":\"10\",\"max_worker_processes\":\"32\"},\"controldataParams\":{\"max_connections\":100,\"max_locks_per_transaction\":64,\"max_prepared_transactions\":0,\"max_wal_senders\":10,\"max_worker_processes\":32},\"clusterParams\":{\"max_worker_processes\":32}}\r\n2024-07-28T09:56:02.372517177+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"msg\":\"Starting up instance\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"options\":[\"start\",\"-w\",\"-D\",\"/var/lib/postgresql/data/pgdata\",\"-o\",\"-c port=5432 -c unix_socket_directories=/controller/run\",\"-t 40000000\",\"-o\",\"-c listen_addresses='127.0.0.1'\"]}\r\n2024-07-28T09:56:02.395539272+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"pg_ctl\",\"msg\":\"waiting for server to start....2024-07-28 06:56:02.395 UTC [36] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"pg_ctl\",\"msg\":\"2024-07-28 06:56:02.395 UTC [36] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:02.395 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:02.395852596+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:02.395 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 16.3 (Debian 16.3-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:02.395862455+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:02.395 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"127.0.0.1\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:02.401721168+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:02.401 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\r\n\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:02.408 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was interrupted; last known up at 2024-07-28 06:53:33 UTC\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:02.408490391+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:02.408 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"creating missing WAL directory \\\"pg_wal/archive_status\\\"\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:03Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:03.060 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting point-in-time recovery to 2024-07-28 06:55:00+00\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:03.060479156+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:03Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:03.060 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting backup recovery with redo LSN 0/8B000028, checkpoint LSN 0/8B000060, on timeline ID 1\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:03.466600434+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:03Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:03.466 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000001000000000000008B\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:03.610868649+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:03Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:03.610 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"redo st\r\narts at 0/8B000028\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.027 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000001000000000000008C\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.442 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"completed backup recovery with redo LSN 0/8B000028 and end LSN 0/8B0019F0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:04.442247017+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.442 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"9\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"consistent recovery state reached at 0/8B0019F0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:04.442248526+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.442 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is ready to accept read-only connections\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:04.480908725+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"pg_ctl\",\"msg\":\".. done\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n2024-07-28T09:56:04.480920659+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"pg_ctl\",\"msg\":\"server started\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n2024-07-28T09:56:04.586217547+03:00 {\"level\r\n\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"msg\":\"Checking if the server is still in recovery\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"recovery\":true}\r\n2024-07-28T09:56:04.689257663+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.689 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"10\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"redo done at 0/8C000410 system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 1.07 s\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.689 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"11\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"last completed transaction was at log time 2024-07-28 06:53:36.485066+00\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:04.689274180+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.689 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"12\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"XX000\",\"message\":\"recovery ended before configured recovery target was reached\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.689 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 40) exited with exit code 1\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:04.690048565+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.689 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"terminating any other active server processes\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:04.690246388+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.690 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"shutting down due to startup process failure\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.691 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"9\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:09.590136827+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:09Z\",\"logger\":\"pg_ctl\",\"msg\":\"pg_ctl: no server running\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n2024-07-28T09:56:09.590153014+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:09Z\",\"msg\":\"Error while deactivating instance\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"err\":\"instance is not running\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:09Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n2024-07-28T09:56:09.590162365+03:00 {\"level\":\"error\",\"ts\":\"2024-07-28T06:56:09Z\",\"msg\":\"Error while restoring a backup\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"error\":\"while waiting for PostgreSQL to stop recovery mode: error while reading results of pg_is_in_recovery: failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): dial error: dial unix /controller/run/.s.PGSQL.5432: connect: no such file or directory\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/cmd.go:92\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.NewCmd.func2\\n\\tinternal/cmd/manager/instance/restore/cmd.go:63\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.22.4/x64/src/runtime/proc.go:271\"}\r\nAll following jobs:\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:59:30Z\",\"msg\":\"PGData already exists, can't overwrite\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nNote new to cloudnative-pg, this happened during testing of cloudnative-pg to see if it something I would use. \r\nI created a database, put some data in it (its not actively used by any application). Added scheduled backups to a minio instance. So far it all works.\r\nWhen i try to recover to a point in time backup from the minio instance using a scheduled backup it works\r\n```yaml\r\n  bootstrap:\r\n    recovery:\r\n      source: cnpg-woop-db\r\n      recoveryTarget:\r\n        targetTime: \"2024-07-28 01:00:00.00000+00\"\r\n(scheduled backup was done at midnight)\r\n```\r\nWhen i create a manual backup:\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Backup\r\nmetadata:\r\n  name: backup-example1\r\n  namespace: data\r\nspec:\r\n  method: barmanObjectStore\r\n  cluster:\r\n    name: cnpg-woop-db\r\n```\r\nor by using the kubectl plugin:\r\n```bash\r\nkubectl cnpg backup cnpg-woop-db -n data\r\n```\r\nThe backup gets created and is visible in minio/ k8s backup list\r\nHowever if i then try to restore to this backup using the `targetTime` like posted above (but then with an updated targettime) it fails to bootstrap the cluster and the started job fails (logs below)\r\nWhen doing the same thing but then using the backup name the restore works fine.\r\n```yaml\r\n      backup:\r\n        name: backup-example1\r\nor\r\n      backup:\r\n        name: cnpg-woop-db-20240728102002\r\n```\r\nBootstrap used for recovery: \r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cnpg-foo-db-frombackup\r\nspec:\r\n  instances: 1\r\n  bootstrap:\r\n    recovery:\r\n      backup:\r\n        name: cnpg-woop-db-20240728102002\r\n      # source: cnpg-woop-db\r\n      # recoveryTarget:\r\n      #   targetTime: \"2024-07-28 11:00:00.00000+00\"\r\n  storage:\r\n    storageClass: longhorn-crypto\r\n    size: 10Gi\r\n  superuserSecret:\r\n    name: cnpg-woop-db-app\r\n  externalClusters:\r\n    - name: cnpg-woop-db\r\n      barmanObjectStore:\r\n        destinationPath: \"s3://postgres/\"\r\n        endpointURL: \"http://minio:9000\"\r\n        s3Credentials:\r\n          accessKeyId:\r\n            name: aws-creds\r\n            key: ACCESS_KEY_ID\r\n          secretAccessKey:\r\n            name: aws-creds\r\n            key: ACCESS_SECRET_KEY\r\n        wal:\r\n          maxParallel: 8\r\n```\n### Cluster resource\n_No response_\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:00Z\",\"msg\":\"Recovering from external cluster\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"sourceName\":\"cnpg-woop-db\"}\r\n2024-07-28T09:56:00.797511212+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:00Z\",\"msg\":\"Target backup found\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"backup\":{\"backup_name\":\"backup-20240728065333\",\"backup_label\":\"'START WAL LOCATION: 0/8B000028 (file 00000001000000000000008B)\\\\nCHECKPOINT LOCATION: 0/8B000060\\\\nBACKUP METHOD: streamed\\\\nBACKUP FROM: primary\\\\nSTART TIME: 2024-07-28 06:53:33 UTC\\\\nLABEL: Barman backup cloud 20240728T065333\\\\nSTART TIMELINE: 1\\\\n'\",\"begin_time\":\"Sun Jul 28 06:53:33 2024\",\"end_time\":\"Sun Jul 28 06:53:35 2024\",\"BeginTime\":\"2024-07-28T06:53:33Z\",\"EndTime\":\"2024-07-28T06:53:35Z\",\"begin_wal\":\"00000001000000000000008B\",\"end_wal\":\"00000001000000000000008B\",\"begin_xlog\":\"0/8B000028\",\"end_xlog\":\"0/8B0019F0\",\"systemid\":\"7396357834682834968\",\"backup_id\":\"20240728T065333\",\"error\":\"\",\"timeline\":1}}\r\n2024-07-28T09:56:01.200815989+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:01Z\",\"msg\":\"Starting barman-cloud-restore\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"options\":[\"--endpoint-url\",\"http://minio:9000\",\"s3://postgres/\",\"cnpg-woop-db\",\"20240728T065333\",\"--cloud-provider\",\"aws-s3\",\"/var/lib/postgresql/data/pgdata\"]}\r\n2024-07-28T09:56:01.952418772+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:01Z\",\"msg\":\"Restore completed\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:01Z\",\"msg\":\"Creating new data directory\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"pgdata\":\"/controller/recovery/datadir_480069336\",\"initDbOptions\":[\"--username\",\"postgres\",\"-D\",\"/controller/recovery/datadir_480069336\",\"--no-sync\"]}\r\n2024-07-28T09:56:02.315721536+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"initdb\",\"msg\":\"The files belonging to this database system will be owned by user \\\"postgres\\\".\\nThis user must also own the server process.\\n\\nThe database cluster will be initialized with locale \\\"en_US.utf8\\\".\\nThe default database encoding has accordingly been set to \\\"UTF8\\\".\\nThe default text search configuration will be set to \\\"english\\\".\\n\\nData page checksums are disabled.\\n\\nfixing permissions on existing directory /controller/recovery/datadir_480069336 ... ok\\ncreating subdirectories ... ok\\nselecting dynamic shared memory implementation ... posix\\nselecting default max_connections ... 100\\nselecting default shared_buffers ... 128MB\\nselecting default time zone ... Etc/UTC\\ncreating configuration files ... ok\\nrunning bootstrap script ... ok\\nperforming post-bootstrap initialization ... ok\\n\\nSync to disk skipped.\\nThe data directory might become corrupt if the operating system crashes.\\n\\n\\nSuccess. You can now start the database server using:\\n\\n    pg_ctl -D /controller/recovery/datadir_480069336 -l logfile start\\n\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n2024-07-28T09:56:02.315736039+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"initdb\",\"msg\":\"initdb: warning: enabling \\\"trust\\\" authentication for local connections\\ninitdb: hint: You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb.\\n\",\"pipe\":\"stderr\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n2024-07-28T09:56:02.321977948+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"msg\":\"Installed configuration file\r\n\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"pgdata\":\"/controller/recovery/datadir_480069336\",\"filename\":\"pg_hba.conf\"}\r\n2024-07-28T09:56:02.324859966+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"msg\":\"Installed configuration file\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"pgdata\":\"/controller/recovery/datadir_480069336\",\"filename\":\"pg_ident.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"msg\":\"Installed configuration file\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"pgdata\":\"/controller/recovery/datadir_480069336\",\"filename\":\"custom.conf\"}\r\n2024-07-28T09:56:02.362881366+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"msg\":\"Generated recovery configuration\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"configuration\":\"recovery_target_action = promote\\nrestore_command = 'barman-cloud-wal-restore --endpoint-url http://minio:9000 s3://postgres/ cnpg-woop-db --cloud-provider aws-s3 %f %p'\\nrecovery_target_time = '2024-07-28 06:55:00.00000+00'\\nrecovery_target_inclusive = true\\n\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"msg\":\"Aligned PostgreSQL configuration to satisfy both pg_controldata and cluster spec\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"enforcedParams\":{\"max_connections\":\"100\",\"max_locks_per_transaction\":\"64\",\"max_prepared_transactions\":\"0\",\"max_wal_senders\":\"10\",\"max_worker_processes\":\"32\"},\"controldataParams\":{\"max_connections\":100,\"max_locks_per_transaction\":64,\"max_prepared_transactions\":0,\"max_wal_senders\":10,\"max_worker_processes\":32},\"clusterParams\":{\"max_worker_processes\":32}}\r\n2024-07-28T09:56:02.372517177+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"msg\":\"Starting up instance\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"options\":[\"start\",\"-w\",\"-D\",\"/var/lib/postgresql/data/pgdata\",\"-o\",\"-c port=5432 -c unix_socket_directories=/controller/run\",\"-t 40000000\",\"-o\",\"-c listen_addresses='127.0.0.1'\"]}\r\n2024-07-28T09:56:02.395539272+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"pg_ctl\",\"msg\":\"waiting for server to start....2024-07-28 06:56:02.395 UTC [36] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"pg_ctl\",\"msg\":\"2024-07-28 06:56:02.395 UTC [36] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:02.395 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:02.395852596+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:02.395 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 16.3 (Debian 16.3-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:02.395862455+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:02.395 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"127.0.0.1\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:02.401721168+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:02.401 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\r\n\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:02.408 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was interrupted; last known up at 2024-07-28 06:53:33 UTC\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:02.408490391+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:02.408 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"creating missing WAL directory \\\"pg_wal/archive_status\\\"\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:03Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:03.060 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting point-in-time recovery to 2024-07-28 06:55:00+00\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:03.060479156+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:03Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:03.060 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting backup recovery with redo LSN 0/8B000028, checkpoint LSN 0/8B000060, on timeline ID 1\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:03.466600434+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:03Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:03.466 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000001000000000000008B\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:03.610868649+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:03Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:03.610 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"redo st\r\narts at 0/8B000028\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.027 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000001000000000000008C\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.442 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"completed backup recovery with redo LSN 0/8B000028 and end LSN 0/8B0019F0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:04.442247017+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.442 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"9\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"consistent recovery state reached at 0/8B0019F0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:04.442248526+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.442 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is ready to accept read-only connections\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:04.480908725+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"pg_ctl\",\"msg\":\".. done\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n2024-07-28T09:56:04.480920659+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"pg_ctl\",\"msg\":\"server started\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n2024-07-28T09:56:04.586217547+03:00 {\"level\r\n\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"msg\":\"Checking if the server is still in recovery\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"recovery\":true}\r\n2024-07-28T09:56:04.689257663+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.689 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"10\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"redo done at 0/8C000410 system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 1.07 s\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.689 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"11\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"last completed transaction was at log time 2024-07-28 06:53:36.485066+00\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:04.689274180+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.689 UTC\",\"process_id\":\"40\",\"session_id\":\"66a5eb82.28\",\"session_line_num\":\"12\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"XX000\",\"message\":\"recovery ended before configured recovery target was reached\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.689 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 40) exited with exit code 1\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:04.690048565+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.689 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"terminating any other active server processes\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:04.690246388+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.690 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"shutting down due to startup process failure\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"record\":{\"log_time\":\"2024-07-28 06:56:04.691 UTC\",\"process_id\":\"36\",\"session_id\":\"66a5eb82.24\",\"session_line_num\":\"9\",\"session_start_time\":\"2024-07-28 06:56:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2024-07-28T09:56:09.590136827+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:09Z\",\"logger\":\"pg_ctl\",\"msg\":\"pg_ctl: no server running\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n2024-07-28T09:56:09.590153014+03:00 {\"level\":\"info\",\"ts\":\"2024-07-28T06:56:09Z\",\"msg\":\"Error while deactivating instance\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"err\":\"instance is not running\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:56:09Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\r\n2024-07-28T09:56:09.590162365+03:00 {\"level\":\"error\",\"ts\":\"2024-07-28T06:56:09Z\",\"msg\":\"Error while restoring a backup\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\",\"error\":\"while waiting for PostgreSQL to stop recovery mode: error while reading results of pg_is_in_recovery: failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): dial error: dial unix /controller/run/.s.PGSQL.5432: connect: no such file or directory\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/cmd.go:92\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.NewCmd.func2\\n\\tinternal/cmd/manager/instance/restore/cmd.go:63\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.22.4/x64/src/runtime/proc.go:271\"}\r\nAll following jobs:\r\n{\"level\":\"info\",\"ts\":\"2024-07-28T06:59:30Z\",\"msg\":\"PGData already exists, can't overwrite\",\"logging_pod\":\"cnpg-foo-db-frombackup8-1-full-recovery\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI don't add anything to the topic except sharing I have the same problem, which might not be isolated.\n---\nSame here.\n---\n\u9047\u5230\u4e86\u4e00\u6837\u7684\u95ee\u9898,\u6574\u4f53\u6062\u590d\u65f6\u62a5\u9519:\nwhile waiting for PostgreSQL to stop recovery mode: error while reading results of pg_is_in_recovery: failed to connect to `user=postgres database=postgres`\n---\nI\u2019m experiencing the same issue as described above.  Has there been any progress or workaround for this? \n**Thanks!**\n---\nSame issue here\n---\n> I\u2019m experiencing the same issue as described above. Has there been any progress or workaround for this?\n> \n> **Thanks!**\nDon't use on demand backup but use a scheduled one as i mentioned in the initial post."
    },
    {
        "title": "[Bug]: Import dbjob is not properly working in a cluster with istio setup",
        "id": 2432236885,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.23.2\r\n### What version of Kubernetes are you using?\r\nv1.24.7 (unsupported)\r\n### What is your Kubernetes environment?\r\nSelf-managed: kind (evaluation)\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\nI am trying to deploy cnpg operator and cluster with helm chart. I am using istio in my cluster and in operator I have added this annotations and its starting as expected. But in db cluster helm chart there is a job initdb which generates a pod. That is not configurable and I can not add this annotations in the job so its failing with below's error. I am using helm chart v0.14.0 for operator and v0.0.9 for cluster.\r\n```\r\n  annotations:\r\n    proxy.istio.io/config: '{ \"holdApplicationUntilProxyStarts\": true }'\r\n    traffic.sidecar.istio.io/excludeInboundPorts: \"9443\"\r\n    traffic.sidecar.istio.io/excludeInboundPorts: \"443\"\r\n    sidecar.istio.io/inject: \"false\"\r\n``` \r\n### Cluster resource\r\n_No response_\r\n### Relevant log output\r\n```shell\r\nkubectl -n cnpg logs cnpg-cluster-1-initdb-vws6m\r\n{\"level\":\"error\",\"ts\":1721997831.4323683,\"msg\":\"Error while bootstrapping data directory\",\"logging_pod\":\"cnpg-cluster-1\",\"error\":\"Get \\\"https://10.56.0.1:443/apis/postgresql.cnpg.io/v1/namespaces/cnpg/clusters/cnpg-cluster\\\": dial tcp 10.56.0.1:443: connect: connection refused\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:165\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.initSubCommand\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:124\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.NewCmd.func1\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:88\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.5.0/command.go:872\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.5.0/command.go:990\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.5.0/command.go:918\\nmain.main\\n\\tcmd/manager/main.go:61\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.17.11/x64/src/runtime/proc.go:255\"}\r\nError: Get \"https://10.56.0.1:443/apis/postgresql.cnpg.io/v1/namespaces/cnpg/clusters/cnpg-cluster\": dial tcp 10.56.0.1:443: connect: connection refused\r\n```\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.23.2\r\n### What version of Kubernetes are you using?\r\nv1.24.7 (unsupported)\r\n### What is your Kubernetes environment?\r\nSelf-managed: kind (evaluation)\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\nI am trying to deploy cnpg operator and cluster with helm chart. I am using istio in my cluster and in operator I have added this annotations and its starting as expected. But in db cluster helm chart there is a job initdb which generates a pod. That is not configurable and I can not add this annotations in the job so its failing with below's error. I am using helm chart v0.14.0 for operator and v0.0.9 for cluster.\r\n```\r\n  annotations:\r\n    proxy.istio.io/config: '{ \"holdApplicationUntilProxyStarts\": true }'\r\n    traffic.sidecar.istio.io/excludeInboundPorts: \"9443\"\r\n    traffic.sidecar.istio.io/excludeInboundPorts: \"443\"\r\n    sidecar.istio.io/inject: \"false\"\r\n``` \r\n### Cluster resource\r\n_No response_\r\n### Relevant log output\r\n```shell\r\nkubectl -n cnpg logs cnpg-cluster-1-initdb-vws6m\r\n{\"level\":\"error\",\"ts\":1721997831.4323683,\"msg\":\"Error while bootstrapping data directory\",\"logging_pod\":\"cnpg-cluster-1\",\"error\":\"Get \\\"https://10.56.0.1:443/apis/postgresql.cnpg.io/v1/namespaces/cnpg/clusters/cnpg-cluster\\\": dial tcp 10.56.0.1:443: connect: connection refused\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:165\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.initSubCommand\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:124\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.NewCmd.func1\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:88\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.5.0/command.go:872\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.5.0/command.go:990\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.5.0/command.go:918\\nmain.main\\n\\tcmd/manager/main.go:61\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.17.11/x64/src/runtime/proc.go:255\"}\r\nError: Get \"https://10.56.0.1:443/apis/postgresql.cnpg.io/v1/namespaces/cnpg/clusters/cnpg-cluster\": dial tcp 10.56.0.1:443: connect: connection refused\r\n```\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductHi @kndoni \r\nThis is how I install cloudnative-pg with Istio.\r\nkubectl create ns cnpg-system\r\nkubectl label namespace cnpg-system istio-injection=enabled\r\nkubectl apply -f ./peer.yaml\r\n```bash\r\n---\r\napiVersion: security.istio.io/v1beta1\r\nkind: PeerAuthentication\r\nmetadata:\r\n  name: operator-tls-simple\r\n  namespace: cnpg-system\r\nspec:\r\n  selector:\r\n    matchLabels:\r\n      app.kubernetes.io/name: cloudnative-pg\r\n  mtls:\r\n    mode: STRICT\r\n  portLevelMtls:\r\n    9443:\r\n      mode: PERMISSIVE\r\n```\r\nkubectl apply --server-side -f https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.23/releases/cnpg-1.23.2.yaml\r\nI created a helm chart to install my Cluster:\r\nkubectl create ns my-pg-cluster\r\nkubectl label namespace my-pg-cluster istio-injection=enabled\r\nhelm upgrade --install pg-cluster -n my-pg-cluster ./pg-cluster/ -f ./pg-cluster/values.yaml\r\n### Make sure to create a headless service\r\n```bash\r\n---\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: {{ include \"postgresql.fullname\" . }}-headless\r\n  namespace: {{ .Release.Namespace }}\r\nspec:\r\n  type: ClusterIP\r\n  ports:\r\n  - port: 5432\r\n    targetPort: 5432\r\n    name: tcp-{{ include \"postgresql.fullname\" . }}\r\n  - port: 8000\r\n    targetPort: 8000\r\n    name: tcp-{{ include \"postgresql.fullname\" . }}-status\r\n  selector:\r\n    cnpg.io/cluster: {{ include \"postgresql.fullname\" . }}\r\n  clusterIP: None\r\n```\r\nIf you need to configure initdb pods etc.\r\n```bash\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: {{ include \"postgresql.fullname\" . }}\r\n  namespace: {{ .Release.Namespace }}\r\n  labels:\r\n    {{- include \"postgresql.labels\" . | nindent 4 }}\r\nspec:\r\n  ......\r\n  inheritedMetadata:       # <-----\r\n    annotations:\r\n      proxy.istio.io/config: '{\"holdApplicationUntilProxyStarts\": true}'\r\n    labels:\r\n      app: {{ include \"postgresql.fullname\" . }}\r\n      version: {{ .Chart.Version }}\r\n```\r\nGood luck\r\n//Victor"
    },
    {
        "title": "[Feature]: Backup Configuration for OSS Bucket in CNPG Database Cluster",
        "id": 2431768362,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI have encountered an issue while setting up a backup for an Aliyun Cloud OSS bucket, which is S3 compatible. Here is the [Link](https://www.alibabacloud.com/help/en/oss/developer-reference/use-amazon-s3-sdks-to-access-oss#section-jmf-a67-hat)\r\nThe default configuration does not seem to be working as expected.\r\n### Describe the solution you'd like\n I was able to get it to work by creating a profile inside the database container and setting specific values in that profile.\r\n Here are the values,\r\n ```\r\ns3 =\r\n  addressing_style = virtual\r\n```\r\n After configured the profile, I ran the `barman-cloud-backup` command with the configured profile as a parameter, and it worked as expected.\n### Describe alternatives you've considered\nHow can we apply this configuration to a CNPG database cluster? Any guidance or suggestions would be greatly appreciated. Thank you.\r\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI have encountered an issue while setting up a backup for an Aliyun Cloud OSS bucket, which is S3 compatible. Here is the [Link](https://www.alibabacloud.com/help/en/oss/developer-reference/use-amazon-s3-sdks-to-access-oss#section-jmf-a67-hat)\r\nThe default configuration does not seem to be working as expected.\r\n### Describe the solution you'd like\n I was able to get it to work by creating a profile inside the database container and setting specific values in that profile.\r\n Here are the values,\r\n ```\r\ns3 =\r\n  addressing_style = virtual\r\n```\r\n After configured the profile, I ran the `barman-cloud-backup` command with the configured profile as a parameter, and it worked as expected.\n### Describe alternatives you've considered\nHow can we apply this configuration to a CNPG database cluster? Any guidance or suggestions would be greatly appreciated. Thank you.\r\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: mTLS Authentication for Pooler",
        "id": 2429677685,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently password-based auth is the only supported way to authenticate through the connection pooler. Client certificates work on the database itself as configured for the cluster, including configurations for external issuers like Cert Manager etc., but it would be nice to do the same through the pooler as well.\n### Describe the solution you'd like\nPgBouncer supports client cert authentication. It would be nice to add this to the pooler CRD so that it can be configured, or directly infer which secret contains the CA for verification from the `Cluster` object.\n### Describe alternatives you've considered\nAlternatives include:\r\n1. Direct connections to the database using short-lived certificates\r\n2. Using short-lived dynamic credentials through something like Hashicorp Vault\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently password-based auth is the only supported way to authenticate through the connection pooler. Client certificates work on the database itself as configured for the cluster, including configurations for external issuers like Cert Manager etc., but it would be nice to do the same through the pooler as well.\n### Describe the solution you'd like\nPgBouncer supports client cert authentication. It would be nice to add this to the pooler CRD so that it can be configured, or directly infer which secret contains the CA for verification from the `Cluster` object.\n### Describe alternatives you've considered\nAlternatives include:\r\n1. Direct connections to the database using short-lived certificates\r\n2. Using short-lived dynamic credentials through something like Hashicorp Vault\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductAnyone interested contributing to this?\n---\nI added a help-wanted tag to this, given that I feel this needs support from external contributors; feel free to change it if you disagree, @gbartolini."
    },
    {
        "title": "[Feature]: Add a prometheus metric that increments whenever the operator performs a failover",
        "id": 2428213206,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nMonitoring failovers and correlating those with application downtime is important for debugging. Having this metric handy will help operators correlate failures better.\n### Describe the solution you'd like\nA custom counter emitted by the operator that counts to the number of failovers for a given cluster. It should have labels for src and dest pod name, and timestamp.\n### Describe alternatives you've considered\nIt might be possible to achieve this externally using existing metrics, but I have not explored that.\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nMonitoring failovers and correlating those with application downtime is important for debugging. Having this metric handy will help operators correlate failures better.\n### Describe the solution you'd like\nA custom counter emitted by the operator that counts to the number of failovers for a given cluster. It should have labels for src and dest pod name, and timestamp.\n### Describe alternatives you've considered\nIt might be possible to achieve this externally using existing metrics, but I have not explored that.\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: \"Cluster is in healthy state\" despite 0 running pods",
        "id": 2427613827,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.27\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nYAML manifest\n### What happened?\nHi! I am trying to check how well operator prepared for detecting different Postgres malfunctions. Recently I've tried to simulate scenario with Postgres failing to start:\r\n```\r\n\u279c  ~ kubectl exec --stdin --tty postgresql-cluster-1 -- /bin/bash\r\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\r\npostgres@postgresql-cluster-1:/$ mv /var/lib/postgresql/data/pgdata/global/pg_control /var/lib/postgresql/data/pgdata/global/pg_control_bck\r\npostgres@postgresql-cluster-1:/$ ps aux | grep postgres\r\npostgres       1  0.1  0.3 1280364 59440 ?       Ssl  12:38   0:04 /controller/manager instance run --log-level=info\r\npostgres      26  0.0  0.2 235368 41892 ?        S    12:38   0:01 postgres -D /var/lib/postgresql/data/pgdata\r\npostgres      27  0.0  0.0  73816  6348 ?        Ss   12:38   0:00 postgres: postgresql-cluster: logger\r\npostgres      28  0.0  0.1 235624 17588 ?        Ss   12:38   0:00 postgres: postgresql-cluster: checkpointer\r\npostgres      29  0.0  0.0 235592  6732 ?        Ss   12:38   0:00 postgres: postgresql-cluster: background writer\r\npostgres      32  0.0  0.0 235368 11480 ?        Ss   12:38   0:00 postgres: postgresql-cluster: walwriter\r\npostgres      33  0.0  0.0 237244  9392 ?        Ss   12:38   0:00 postgres: postgresql-cluster: autovacuum launcher\r\npostgres      34  0.0  0.0 235472  8188 ?        Ss   12:38   0:00 postgres: postgresql-cluster: archiver last was 000000010000000000000005\r\npostgres      35  0.0  0.0 237176  9472 ?        Ss   12:38   0:00 postgres: postgresql-cluster: logical replication launcher\r\npostgres    2942  0.0  0.0   7636  4324 pts/0    Ss   13:27   0:00 /bin/bash\r\npostgres    2980  0.0  0.0  10072  1592 pts/0    R+   13:28   0:00 ps aux\r\npostgres    2981  0.0  0.0   6480  2340 pts/0    S+   13:28   0:00 grep postgres\r\npostgres@postgresql-cluster-1:/$ kill -9 26\r\npostgres@postgresql-cluster-1:/$ command terminated with exit code 137\r\n```\r\nDespite the fact that now I have a fully broken cluster, cluster phase does not reflect that:\r\n```\r\n\u279c  ~ kubectl cnpg  status postgresql-cluster\r\nCluster Summary\r\nName:                postgresql-cluster\r\nNamespace:           ***\r\nPostgreSQL Image:    ***\r\nPrimary instance:    postgresql-cluster-1\r\nPrimary start time:  2024-07-23 15:32:04 +0000 UTC (uptime 21h58m59s)\r\nStatus:              Cluster in healthy state\r\nInstances:           1\r\nReady instances:     0\r\nCertificates Status\r\nCertificate Name                Expiration Date                Days Left Until Expiration\r\n----------------                ---------------                --------------------------\r\ncafile                          2030-12-31 09:37:37 +0000 UTC  2350.84\r\ncert-postgres-secret            2025-07-23 10:00:00 +0000 UTC  363.85\r\npostgresql-cluster-ca           2024-10-21 15:26:18 +0000 UTC  89.08\r\npostgresql-cluster-replication  2024-10-21 15:26:18 +0000 UTC  89.08\r\nContinuous Backup status\r\nFirst Point of Recoverability:  2024-07-23T15:32:24Z\r\nNo Primary instance found\r\nPhysical backups\r\nPrimary instance not found\r\nStreaming Replication status\r\nNot configured\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nManaged roles status\r\nNo roles managed\r\nTablespaces status\r\nNo managed tablespaces\r\nPod Disruption Budgets status\r\nName                        Role     Expected Pods  Current Healthy  Minimum Desired Healthy  Disruptions Allowed\r\n----                        ----     -------------  ---------------  -----------------------  -------------------\r\npostgresql-cluster-primary  primary  1              0                1                        0\r\nInstances status\r\nName                  Database Size  Current LSN  Replication role  Status             QoS         Manager Version  Node\r\n----                  -------------  -----------  ----------------  ------             ---         ---------------  ----\r\npostgresql-cluster-1  -              -            -                 pod not available  Guaranteed  -                xxx\r\n```\r\nIn operator logs there are endless messages regarding pod connection errors:\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-07-24T13:32:51Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgresql-cluster\",\"namespace\":\"***\"},\"namespace\":\"***\",\"name\":\"postgresql-cluster\",\"reconcileID\":\"87e29fe9-4ffe-412d-a96b-b40108c4f1c3\",\"name\":\"postgresql-cluster-1\",\"error\":\"Get \\\"http://10.113.49.217:8000/pg/status\\\": dial tcp 10.113.49.217:8000: connect: connection refused\"}\r\n``` \r\nFrom my point of view, the expected behavior in such a case is to reflect the actual degraded state of the cluster in the cluster phase/conditions.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.27\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nYAML manifest\n### What happened?\nHi! I am trying to check how well operator prepared for detecting different Postgres malfunctions. Recently I've tried to simulate scenario with Postgres failing to start:\r\n```\r\n\u279c  ~ kubectl exec --stdin --tty postgresql-cluster-1 -- /bin/bash\r\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\r\npostgres@postgresql-cluster-1:/$ mv /var/lib/postgresql/data/pgdata/global/pg_control /var/lib/postgresql/data/pgdata/global/pg_control_bck\r\npostgres@postgresql-cluster-1:/$ ps aux | grep postgres\r\npostgres       1  0.1  0.3 1280364 59440 ?       Ssl  12:38   0:04 /controller/manager instance run --log-level=info\r\npostgres      26  0.0  0.2 235368 41892 ?        S    12:38   0:01 postgres -D /var/lib/postgresql/data/pgdata\r\npostgres      27  0.0  0.0  73816  6348 ?        Ss   12:38   0:00 postgres: postgresql-cluster: logger\r\npostgres      28  0.0  0.1 235624 17588 ?        Ss   12:38   0:00 postgres: postgresql-cluster: checkpointer\r\npostgres      29  0.0  0.0 235592  6732 ?        Ss   12:38   0:00 postgres: postgresql-cluster: background writer\r\npostgres      32  0.0  0.0 235368 11480 ?        Ss   12:38   0:00 postgres: postgresql-cluster: walwriter\r\npostgres      33  0.0  0.0 237244  9392 ?        Ss   12:38   0:00 postgres: postgresql-cluster: autovacuum launcher\r\npostgres      34  0.0  0.0 235472  8188 ?        Ss   12:38   0:00 postgres: postgresql-cluster: archiver last was 000000010000000000000005\r\npostgres      35  0.0  0.0 237176  9472 ?        Ss   12:38   0:00 postgres: postgresql-cluster: logical replication launcher\r\npostgres    2942  0.0  0.0   7636  4324 pts/0    Ss   13:27   0:00 /bin/bash\r\npostgres    2980  0.0  0.0  10072  1592 pts/0    R+   13:28   0:00 ps aux\r\npostgres    2981  0.0  0.0   6480  2340 pts/0    S+   13:28   0:00 grep postgres\r\npostgres@postgresql-cluster-1:/$ kill -9 26\r\npostgres@postgresql-cluster-1:/$ command terminated with exit code 137\r\n```\r\nDespite the fact that now I have a fully broken cluster, cluster phase does not reflect that:\r\n```\r\n\u279c  ~ kubectl cnpg  status postgresql-cluster\r\nCluster Summary\r\nName:                postgresql-cluster\r\nNamespace:           ***\r\nPostgreSQL Image:    ***\r\nPrimary instance:    postgresql-cluster-1\r\nPrimary start time:  2024-07-23 15:32:04 +0000 UTC (uptime 21h58m59s)\r\nStatus:              Cluster in healthy state\r\nInstances:           1\r\nReady instances:     0\r\nCertificates Status\r\nCertificate Name                Expiration Date                Days Left Until Expiration\r\n----------------                ---------------                --------------------------\r\ncafile                          2030-12-31 09:37:37 +0000 UTC  2350.84\r\ncert-postgres-secret            2025-07-23 10:00:00 +0000 UTC  363.85\r\npostgresql-cluster-ca           2024-10-21 15:26:18 +0000 UTC  89.08\r\npostgresql-cluster-replication  2024-10-21 15:26:18 +0000 UTC  89.08\r\nContinuous Backup status\r\nFirst Point of Recoverability:  2024-07-23T15:32:24Z\r\nNo Primary instance found\r\nPhysical backups\r\nPrimary instance not found\r\nStreaming Replication status\r\nNot configured\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nManaged roles status\r\nNo roles managed\r\nTablespaces status\r\nNo managed tablespaces\r\nPod Disruption Budgets status\r\nName                        Role     Expected Pods  Current Healthy  Minimum Desired Healthy  Disruptions Allowed\r\n----                        ----     -------------  ---------------  -----------------------  -------------------\r\npostgresql-cluster-primary  primary  1              0                1                        0\r\nInstances status\r\nName                  Database Size  Current LSN  Replication role  Status             QoS         Manager Version  Node\r\n----                  -------------  -----------  ----------------  ------             ---         ---------------  ----\r\npostgresql-cluster-1  -              -            -                 pod not available  Guaranteed  -                xxx\r\n```\r\nIn operator logs there are endless messages regarding pod connection errors:\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-07-24T13:32:51Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgresql-cluster\",\"namespace\":\"***\"},\"namespace\":\"***\",\"name\":\"postgresql-cluster\",\"reconcileID\":\"87e29fe9-4ffe-412d-a96b-b40108c4f1c3\",\"name\":\"postgresql-cluster-1\",\"error\":\"Get \\\"http://10.113.49.217:8000/pg/status\\\": dial tcp 10.113.49.217:8000: connect: connection refused\"}\r\n``` \r\nFrom my point of view, the expected behavior in such a case is to reflect the actual degraded state of the cluster in the cluster phase/conditions.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductCan you please try with the latest snapshot? See https://cloudnative-pg.io/documentation/current/installation_upgrade/#testing-the-latest-development-snapshot"
    },
    {
        "title": "[Bug]: PersistentVolumeClaim is invalid: spec.accessModes: Forbidden: may not use ReadWriteOncePod with other access modes",
        "id": 2426913368,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ncnpg_lau11@a.jlv6.com\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nProvisioner: driver.longhorn.io\r\nThe Operator is unable to create PVC(s) with the requested access mode (ReadWriteOncePod). I understand that the ReadWriteOncePod accss mode can't be mixed with other access modes, but I can't figure out why the Operator is trying to create a PVC resource with anything else than the requested access mode in the Cluster definition.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cloudnative-pg-1\r\n  namespace: postgresql\r\nspec:\r\n  description: \"Postgresql Cluster 1\"\r\n  inheritedMetadata:\r\n    labels:\r\n      k8s-app: \"cnpg\"\r\n      cnpg.io/cluster: \"cloudnative-pg-1\"\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.3\r\n  # imagePullSecret is only required if the images are located in a private registry\r\n  # imagePullSecrets:\r\n  #   - name: private_registry_access\r\n  instances: 3\r\n  postgresUID: 26\r\n  postgresGID: 26\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n  postgresql:\r\n    parameters:\r\n      auto_explain.log_min_duration: '10s'\r\n      log_checkpoints: 'on'\r\n      log_connections: 'on'\r\n      password_encryption: 'scram-sha-256'\r\n      pg_stat_statements.max: '10000'\r\n      pg_stat_statements.track: 'all'\r\n      shared_buffers: '4GB'\r\n      wal_compression: 'on'\r\n    pg_hba:\r\n      - host all all 10.52.0.0/16 scram-sha-256\r\n  bootstrap:\r\n    initdb:\r\n      database: notused\r\n#     owner: app\r\n#     secret:\r\n#       name: cluster-example-app-user\r\n    # Alternative bootstrap method: start from a backup\r\n    #recovery:\r\n    #  backup:\r\n    #    name: backup-example\r\n  enableSuperuserAccess: true\r\n  superuserSecret:\r\n    name: cnpg-1-superuser\r\n  storage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOncePod\r\n      resources:\r\n        requests:\r\n          storage: 300Gi\r\n      storageClassName: stor-1r\r\n      volumeMode: Filesystem\r\n      resizeInUseVolumes: true\r\n  walStorage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOncePod\r\n      resources:\r\n        requests:\r\n          storage: 30Gi\r\n      storageClassName: stor-1r\r\n      volumeMode: Filesystem\r\n      resizeInUseVolumes: true\r\n  backup:\r\n    volumeSnapshot:\r\n      className: stor-vsc\r\n  # barmanObjectStore:\r\n  #   destinationPath: s3://cluster-example-full-backup/\r\n  #   endpointURL: http://custom-endpoint:1234\r\n  #   s3Credentials:\r\n  #     accessKeyId:\r\n  #       name: backup-creds\r\n  #       key: ACCESS_KEY_ID\r\n  #     secretAccessKey:\r\n  #       name: backup-creds\r\n  #       key: ACCESS_SECRET_KEY\r\n  #   wal:\r\n  #     compression: gzip\r\n  #     encryption: AES256\r\n  #   data:\r\n  #     compression: gzip\r\n  #     encryption: AES256\r\n  #     immediateCheckpoint: false\r\n  #     jobs: 2\r\n    retentionPolicy: \"7d\"\r\n  #  resources:\r\n  #    requests:\r\n  #      memory: \"512Mi\"\r\n  #      cpu: \"1\"\r\n  #    limits:\r\n  #      memory: \"1Gi\"\r\n  #      cpu: \"2\"\r\n  affinity:\r\n    enablePodAntiAffinity: false\r\n    additionalPodAntiAffinity:\r\n      requiredDuringSchedulingIgnoredDuringExecution:\r\n      - topologyKey: kubernetes.io/hostname\r\n        labelSelector:\r\n          matchLabels:\r\n            cnpg.io/cluster: \"cloudnative-pg-1\"\r\n    nodeSelector:\r\n      82219: \"cloudnative-pg-cluster-1\"\r\n  enablePDB: true\n```\n### Relevant log output\n```shell\n{\"level\":\"error\",\"ts\":\"2024-07-24T08:16:43\",\"msg\":\"Reconciler error\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cloudnative-pg-1\",\"namespace\":\"postgresql\"},\"namespace\":\"postgresql\",\"name\":\"cloudnative-pg-1\",\"reconcileID\":\"4e7e90a1-a904-4fce-930b-a3ac255639fb\",\"error\":\"unable to create a PVC: cloudnative-pg-1-1 for this node (nodeSerial: 1): PersistentVolumeClaim \\\"cloudnative-pg-1-1\\\" is invalid: spec.accessModes: Forbidden: may not use ReadWriteOncePod with other access modes\"\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ncnpg_lau11@a.jlv6.com\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nProvisioner: driver.longhorn.io\r\nThe Operator is unable to create PVC(s) with the requested access mode (ReadWriteOncePod). I understand that the ReadWriteOncePod accss mode can't be mixed with other access modes, but I can't figure out why the Operator is trying to create a PVC resource with anything else than the requested access mode in the Cluster definition.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cloudnative-pg-1\r\n  namespace: postgresql\r\nspec:\r\n  description: \"Postgresql Cluster 1\"\r\n  inheritedMetadata:\r\n    labels:\r\n      k8s-app: \"cnpg\"\r\n      cnpg.io/cluster: \"cloudnative-pg-1\"\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.3\r\n  # imagePullSecret is only required if the images are located in a private registry\r\n  # imagePullSecrets:\r\n  #   - name: private_registry_access\r\n  instances: 3\r\n  postgresUID: 26\r\n  postgresGID: 26\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n  postgresql:\r\n    parameters:\r\n      auto_explain.log_min_duration: '10s'\r\n      log_checkpoints: 'on'\r\n      log_connections: 'on'\r\n      password_encryption: 'scram-sha-256'\r\n      pg_stat_statements.max: '10000'\r\n      pg_stat_statements.track: 'all'\r\n      shared_buffers: '4GB'\r\n      wal_compression: 'on'\r\n    pg_hba:\r\n      - host all all 10.52.0.0/16 scram-sha-256\r\n  bootstrap:\r\n    initdb:\r\n      database: notused\r\n#     owner: app\r\n#     secret:\r\n#       name: cluster-example-app-user\r\n    # Alternative bootstrap method: start from a backup\r\n    #recovery:\r\n    #  backup:\r\n    #    name: backup-example\r\n  enableSuperuserAccess: true\r\n  superuserSecret:\r\n    name: cnpg-1-superuser\r\n  storage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOncePod\r\n      resources:\r\n        requests:\r\n          storage: 300Gi\r\n      storageClassName: stor-1r\r\n      volumeMode: Filesystem\r\n      resizeInUseVolumes: true\r\n  walStorage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOncePod\r\n      resources:\r\n        requests:\r\n          storage: 30Gi\r\n      storageClassName: stor-1r\r\n      volumeMode: Filesystem\r\n      resizeInUseVolumes: true\r\n  backup:\r\n    volumeSnapshot:\r\n      className: stor-vsc\r\n  # barmanObjectStore:\r\n  #   destinationPath: s3://cluster-example-full-backup/\r\n  #   endpointURL: http://custom-endpoint:1234\r\n  #   s3Credentials:\r\n  #     accessKeyId:\r\n  #       name: backup-creds\r\n  #       key: ACCESS_KEY_ID\r\n  #     secretAccessKey:\r\n  #       name: backup-creds\r\n  #       key: ACCESS_SECRET_KEY\r\n  #   wal:\r\n  #     compression: gzip\r\n  #     encryption: AES256\r\n  #   data:\r\n  #     compression: gzip\r\n  #     encryption: AES256\r\n  #     immediateCheckpoint: false\r\n  #     jobs: 2\r\n    retentionPolicy: \"7d\"\r\n  #  resources:\r\n  #    requests:\r\n  #      memory: \"512Mi\"\r\n  #      cpu: \"1\"\r\n  #    limits:\r\n  #      memory: \"1Gi\"\r\n  #      cpu: \"2\"\r\n  affinity:\r\n    enablePodAntiAffinity: false\r\n    additionalPodAntiAffinity:\r\n      requiredDuringSchedulingIgnoredDuringExecution:\r\n      - topologyKey: kubernetes.io/hostname\r\n        labelSelector:\r\n          matchLabels:\r\n            cnpg.io/cluster: \"cloudnative-pg-1\"\r\n    nodeSelector:\r\n      82219: \"cloudnative-pg-cluster-1\"\r\n  enablePDB: true\n```\n### Relevant log output\n```shell\n{\"level\":\"error\",\"ts\":\"2024-07-24T08:16:43\",\"msg\":\"Reconciler error\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cloudnative-pg-1\",\"namespace\":\"postgresql\"},\"namespace\":\"postgresql\",\"name\":\"cloudnative-pg-1\",\"reconcileID\":\"4e7e90a1-a904-4fce-930b-a3ac255639fb\",\"error\":\"unable to create a PVC: cloudnative-pg-1-1 for this node (nodeSerial: 1): PersistentVolumeClaim \\\"cloudnative-pg-1-1\\\" is invalid: spec.accessModes: Forbidden: may not use ReadWriteOncePod with other access modes\"\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI believe we should go further than this and actually make `ReadWritePodOnce` the default access mode for any PVC. Thoughts?\n---\nHi @gbartolini - thank you for the amazing work you've done/are doing on the operator.\nI can't think of a single reason for not wanting the psql cluster node to have exclusive access to its storage. Any other process requiring access to it should just run in a sidecar container. So yes, I'd make it the default access mode.\n---\nBTW, I tried with this example:\n```yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: angus\nspec:\n  instances: 3\n  storage:\n    pvcTemplate:\n      accessModes:\n        - ReadWriteOncePod\n      resources:\n        requests:\n          storage: 1Gi\n      storageClassName: standard\n      volumeMode: Filesystem\n```\nThe template functionality works. As you can see it tries to create a PVC with access mode RWOP, but fails as the default local provision on Kind doesn't support that access mode:\n```\n$kubectl events\nLAST SEEN              TYPE      REASON                        OBJECT                              MESSAGE\n12m                    Normal    CreatingServiceAccount        Cluster/angus                       Creating ServiceAccount\n12m (x2 over 12m)      Normal    NoPods                        PodDisruptionBudget/angus-primary   No matching pods found\n12m                    Normal    CreatingPodDisruptionBudget   Cluster/angus                       Creating PodDisruptionBudget angus-primary\n12m                    Normal    CreatingPodDisruptionBudget   Cluster/angus                       Creating PodDisruptionBudget angus\n12m (x2 over 12m)      Normal    NoPods                        PodDisruptionBudget/angus           No matching pods found\n12m                    Normal    SuccessfulCreate              Job/angus-1-initdb                  Created pod: angus-1-initdb-vwmj7\n12m                    Normal    WaitForFirstConsumer          PersistentVolumeClaim/angus-1       waiting for first consumer to be created before binding\n12m                    Normal    CreatingRole                  Cluster/angus                       Creating Cluster Role\n12m                    Normal    CreatingInstance              Cluster/angus                       Primary instance (initdb)\n4m35s (x6 over 12m)    Normal    Provisioning                  PersistentVolumeClaim/angus-1       External provisioner is provisioning volume for claim \"default/angus-1\"\n4m35s (x6 over 12m)    Warning   ProvisioningFailed            PersistentVolumeClaim/angus-1       failed to provision volume with StorageClass \"standard\": Only support ReadWriteOnce access mode\n2m20s                  Warning   FailedScheduling              Pod/angus-1-initdb-vwmj7            running PreBind plugin \"VolumeBinding\": binding volumes: context deadline exceeded\n2m14s (x43 over 12m)   Normal    ExternalProvisioning          PersistentVolumeClaim/angus-1       Waiting for a volume to be created either by the external provisioner 'rancher.io/local-path' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.\n```"
    },
    {
        "title": "[Feature]: add replication users outside cluster in kubectl plugin",
        "id": 2413174916,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nThe `kubectl status` plugin has a section for the replication status where we list the standby instances from the cluster.\r\nWe may have replica clusters that are also streaming from the primary, but we would not know.\r\nHowever, the `pg_stat_replication` view would easily offer this info.\r\n``` txt\r\nselect application_name, usename FROM pg_catalog.pg_stat_replication;\r\n application_name |      usename      \r\n------------------+-------------------\r\n cluster-sample-3 | streaming_replica\r\n cluster-sample-1 | streaming_replica\r\n cluster-sample3  | streaming_replica\r\n```\r\nwhere for this same configuration, the status plugin shows\r\n``` txt\r\nStreaming Replication status\r\nReplication Slots Enabled\r\nName              Sent LSN    Write LSN   Flush LSN   Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority  Replication Slot\r\n----              --------    ---------   ---------   ----------  ---------  ---------  ----------  -----      ----------  -------------  ----------------\r\ncluster-sample-1  0/10000060  0/10000060  0/10000060  0/10000060  00:00:00   00:00:00   00:00:00    streaming  async       0              active\r\ncluster-sample-3  0/10000060  0/10000060  0/10000060  0/10000060  00:00:00   00:00:00   00:00:00    streaming  async       0              active\r\n```\r\nIn addition, when calling `kubectl status` on a replica cluster, we don't display the name of the source cluster.\r\n``` txt\r\nReplica Cluster Summary\r\nName:                cluster-sample3\r\nNamespace:           edb3\r\nSystem ID:           7392244288816541717\r\nPostgreSQL Image:    ghcr.io/cloudnative-pg/postgresql:15.7\r\nDesignated primary:  cluster-sample3-1\r\nSource cluster:      active\r\n``` \r\n### Describe the solution you'd like\r\n1. add a section for replica users that are outside the cluster. Say *External Replication Status*\r\n``` txt\r\nStreaming Replication status\r\nReplication Slots Enabled\r\nName              Sent LSN    Write LSN   Flush LSN   Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority  Replication Slot\r\n----              --------    ---------   ---------   ----------  ---------  ---------  ----------  -----      ----------  -------------  ----------------\r\ncluster-sample-1  0/10000060  0/10000060  0/10000060  0/10000060  00:00:00   00:00:00   00:00:00    streaming  async       0              active\r\ncluster-sample-3  0/10000060  0/10000060  0/10000060  0/10000060  00:00:00   00:00:00   00:00:00    streaming  async       0              active\r\n---\r\nExternal Replication Status\r\nName               Sent LSN    Write LSN   Flush LSN   Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority  Replication Slot\r\n----               --------    ---------   ---------   ----------  ---------  ---------  ----------  -----      ----------  -------------  ----------------\r\ncluster-sample3-1  0/10000060  0/10000060  0/10000060  0/10000060  00:00:00   00:00:00   00:00:00    streaming  async       0              active\r\n```\r\n2. add the name/namespace of the source cluster for a replica cluster. Something like\r\n```\r\nReplica Cluster Summary\r\nName:                cluster-sample3\r\nNamespace:           edb3\r\n< .. snipped .. >\r\nDesignated primary:  cluster-sample3-1\r\nSource cluster:      edb/cluster-sample\r\nSource cluster:      active\r\nPrimary start time:  2024-07-17 09:05:49 +0000 UTC (uptime 24m50s)\r\n```\r\nThe lines `Source cluster: active` and `Primary start time:  XYZ` exist already.\r\nImplementation-wise, it's in `pkg/management/postgres/probes.go` that we filter out the replication stats that don't belong to the cluster.\r\n``` go\r\n\trows, err := superUserDB.Query(\r\n\t\t`SELECT\r\n\t\t\tapplication_name,\r\n\t\t\t<- snipped ->\r\n\t\tFROM pg_catalog.pg_stat_replication\r\n\t\tWHERE application_name ~ $1 AND usename = $2`,\r\n\t\tfmt.Sprintf(\"%s-[0-9]+$\", instance.ClusterName),\r\n```\r\nAnd only the replication stats from the same cluster are added here:\r\n``` go\r\n// PgStatReplicationList is a list of PgStatReplication reported by the primary instance\r\ntype PgStatReplicationList []PgStatReplication\r\n```\r\nWe might add a new section like so:\r\n``` go\r\n// PgStatReplicationListExternal is a list of PgStatReplication reported by the primary instance\r\n// for replication clients that are outside the cluster i.e. replica clusters\r\ntype PgStatReplicationListExternal []PgStatReplication\r\n```\r\n### Describe alternatives you've considered\r\nThe proposed implementation as above is easy and we can decide to do it that way, or add a new field in `PgStatReplication` to track whether a replication user is internal or external.\r\n**Concerns**\r\nThe `pg_stat_replication` view does not know about Kubernetes namespaces.\r\nWe could have cases where the names collide.\r\nI've done a test with a replica cluster having the same name as the source cluster.\r\nIn the view, the application name shows a new entry called `cluster-sample` which I guess means that in the replica cluster, we set the `application_name` to be the cluster name, without the numeric suffix.\r\n``` txt\r\nselect application_name, usename FROM pg_catalog.pg_stat_replication;\r\n application_name |      usename      \r\n------------------+-------------------\r\n cluster-sample-3 | streaming_replica\r\n cluster-sample-1 | streaming_replica\r\n cluster-sample3  | streaming_replica\r\n cluster-sample   | streaming_replica\r\n```\r\nThis raises two concerns:\r\n- if we had two replica clusters in different namespaces but the same cluster name, the entries in `pg_stat_replication` would collide\r\n- if we created a replica cluster named as one of the source's replica instances (e.g. `cluster-sample-1` in the example above), the entries would collide too\r\n**Extra idea**\r\nThe results of `pg_stat_replication`, and of the `kubectl status` would be a lot easier to understand at a glance if we added the Kubernetes namespace to the `application_name`. \r\n``` txt\r\nselect application_name, usename FROM pg_catalog.pg_stat_replication;\r\n application_name      |      usename      \r\n-----------------------+-------------------\r\n edb_cluster-sample-3  | streaming_replica\r\n edb_cluster-sample-1  | streaming_replica\r\n edb3_cluster-sample3  | streaming_replica\r\n```\r\n``` txt\r\nStreaming Replication status\r\n<- snipped ->\r\nExternal Replication Status\r\nName                    Sent LSN    Write LSN   Flush LSN   Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority  Replication Slot\r\n----                    --------    ---------   ---------   ----------  ---------  ---------  ----------  -----      ----------  -------------  ----------------\r\nedb3_cluster-sample3-1  0/10000060  0/10000060  0/10000060  0/10000060  00:00:00   00:00:00   00:00:00    streaming  async       0              active\r\n```\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nThe `kubectl status` plugin has a section for the replication status where we list the standby instances from the cluster.\r\nWe may have replica clusters that are also streaming from the primary, but we would not know.\r\nHowever, the `pg_stat_replication` view would easily offer this info.\r\n``` txt\r\nselect application_name, usename FROM pg_catalog.pg_stat_replication;\r\n application_name |      usename      \r\n------------------+-------------------\r\n cluster-sample-3 | streaming_replica\r\n cluster-sample-1 | streaming_replica\r\n cluster-sample3  | streaming_replica\r\n```\r\nwhere for this same configuration, the status plugin shows\r\n``` txt\r\nStreaming Replication status\r\nReplication Slots Enabled\r\nName              Sent LSN    Write LSN   Flush LSN   Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority  Replication Slot\r\n----              --------    ---------   ---------   ----------  ---------  ---------  ----------  -----      ----------  -------------  ----------------\r\ncluster-sample-1  0/10000060  0/10000060  0/10000060  0/10000060  00:00:00   00:00:00   00:00:00    streaming  async       0              active\r\ncluster-sample-3  0/10000060  0/10000060  0/10000060  0/10000060  00:00:00   00:00:00   00:00:00    streaming  async       0              active\r\n```\r\nIn addition, when calling `kubectl status` on a replica cluster, we don't display the name of the source cluster.\r\n``` txt\r\nReplica Cluster Summary\r\nName:                cluster-sample3\r\nNamespace:           edb3\r\nSystem ID:           7392244288816541717\r\nPostgreSQL Image:    ghcr.io/cloudnative-pg/postgresql:15.7\r\nDesignated primary:  cluster-sample3-1\r\nSource cluster:      active\r\n``` \r\n### Describe the solution you'd like\r\n1. add a section for replica users that are outside the cluster. Say *External Replication Status*\r\n``` txt\r\nStreaming Replication status\r\nReplication Slots Enabled\r\nName              Sent LSN    Write LSN   Flush LSN   Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority  Replication Slot\r\n----              --------    ---------   ---------   ----------  ---------  ---------  ----------  -----      ----------  -------------  ----------------\r\ncluster-sample-1  0/10000060  0/10000060  0/10000060  0/10000060  00:00:00   00:00:00   00:00:00    streaming  async       0              active\r\ncluster-sample-3  0/10000060  0/10000060  0/10000060  0/10000060  00:00:00   00:00:00   00:00:00    streaming  async       0              active\r\n---\r\nExternal Replication Status\r\nName               Sent LSN    Write LSN   Flush LSN   Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority  Replication Slot\r\n----               --------    ---------   ---------   ----------  ---------  ---------  ----------  -----      ----------  -------------  ----------------\r\ncluster-sample3-1  0/10000060  0/10000060  0/10000060  0/10000060  00:00:00   00:00:00   00:00:00    streaming  async       0              active\r\n```\r\n2. add the name/namespace of the source cluster for a replica cluster. Something like\r\n```\r\nReplica Cluster Summary\r\nName:                cluster-sample3\r\nNamespace:           edb3\r\n< .. snipped .. >\r\nDesignated primary:  cluster-sample3-1\r\nSource cluster:      edb/cluster-sample\r\nSource cluster:      active\r\nPrimary start time:  2024-07-17 09:05:49 +0000 UTC (uptime 24m50s)\r\n```\r\nThe lines `Source cluster: active` and `Primary start time:  XYZ` exist already.\r\nImplementation-wise, it's in `pkg/management/postgres/probes.go` that we filter out the replication stats that don't belong to the cluster.\r\n``` go\r\n\trows, err := superUserDB.Query(\r\n\t\t`SELECT\r\n\t\t\tapplication_name,\r\n\t\t\t<- snipped ->\r\n\t\tFROM pg_catalog.pg_stat_replication\r\n\t\tWHERE application_name ~ $1 AND usename = $2`,\r\n\t\tfmt.Sprintf(\"%s-[0-9]+$\", instance.ClusterName),\r\n```\r\nAnd only the replication stats from the same cluster are added here:\r\n``` go\r\n// PgStatReplicationList is a list of PgStatReplication reported by the primary instance\r\ntype PgStatReplicationList []PgStatReplication\r\n```\r\nWe might add a new section like so:\r\n``` go\r\n// PgStatReplicationListExternal is a list of PgStatReplication reported by the primary instance\r\n// for replication clients that are outside the cluster i.e. replica clusters\r\ntype PgStatReplicationListExternal []PgStatReplication\r\n```\r\n### Describe alternatives you've considered\r\nThe proposed implementation as above is easy and we can decide to do it that way, or add a new field in `PgStatReplication` to track whether a replication user is internal or external.\r\n**Concerns**\r\nThe `pg_stat_replication` view does not know about Kubernetes namespaces.\r\nWe could have cases where the names collide.\r\nI've done a test with a replica cluster having the same name as the source cluster.\r\nIn the view, the application name shows a new entry called `cluster-sample` which I guess means that in the replica cluster, we set the `application_name` to be the cluster name, without the numeric suffix.\r\n``` txt\r\nselect application_name, usename FROM pg_catalog.pg_stat_replication;\r\n application_name |      usename      \r\n------------------+-------------------\r\n cluster-sample-3 | streaming_replica\r\n cluster-sample-1 | streaming_replica\r\n cluster-sample3  | streaming_replica\r\n cluster-sample   | streaming_replica\r\n```\r\nThis raises two concerns:\r\n- if we had two replica clusters in different namespaces but the same cluster name, the entries in `pg_stat_replication` would collide\r\n- if we created a replica cluster named as one of the source's replica instances (e.g. `cluster-sample-1` in the example above), the entries would collide too\r\n**Extra idea**\r\nThe results of `pg_stat_replication`, and of the `kubectl status` would be a lot easier to understand at a glance if we added the Kubernetes namespace to the `application_name`. \r\n``` txt\r\nselect application_name, usename FROM pg_catalog.pg_stat_replication;\r\n application_name      |      usename      \r\n-----------------------+-------------------\r\n edb_cluster-sample-3  | streaming_replica\r\n edb_cluster-sample-1  | streaming_replica\r\n edb3_cluster-sample3  | streaming_replica\r\n```\r\n``` txt\r\nStreaming Replication status\r\n<- snipped ->\r\nExternal Replication Status\r\nName                    Sent LSN    Write LSN   Flush LSN   Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority  Replication Slot\r\n----                    --------    ---------   ---------   ----------  ---------  ---------  ----------  -----      ----------  -------------  ----------------\r\nedb3_cluster-sample3-1  0/10000060  0/10000060  0/10000060  0/10000060  00:00:00   00:00:00   00:00:00    streaming  async       0              active\r\n```\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: Certificated issued from cert-manager is not being mounted correctly",
        "id": 2410605650,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nHelm\n### What happened?\nI have issued an certificate from cert-manager following the documentation steps described [here](https://cloudnative-pg.io/documentation/1.16/certificates/)\r\n```\r\napiVersion: cert-manager.io/v1\r\nkind: Certificate\r\nmetadata:\r\n  name: my-postgres-server-cert\r\n  namespace: postgresql-operator-test\r\nspec:\r\n  secretName: my-postgres-server-cert\r\n  usages:\r\n    - server auth\r\n  commonName: cnpg-webhook-service.postgresql-operator-test.svc\r\n  dnsNames:\r\n    - cnpg-webhook-service\r\n    - cnpg-webhook-service.postgresql-operator-test\r\n    - cnpg-webhook-service.postgresql-operator-test.svc\r\n    - cnpg-webhook-service.postgresql-operator-test.svc.cluster.local\r\n  issuerRef:\r\n    name: issuer\r\n    kind: ClusterIssuer\r\n    group: cert-manager.io\r\n``` \r\nAnd then I have mounted it to the certificates: block section under clusters helm chart \r\n```\r\ncluster:\r\n  certificates:\r\n     serverTLSSecret: my-postgres-server-cert\r\n     serverCASecret: my-postgres-server-cert\r\n``` \r\nThe certificate is successfully issued because I can see it in the cluster events but again when I try to deploy the helm chart I notice that the cnpg-ca-secret and cnpg-webhook-cert objects are still automatically created by the operator and in the mutatingwebhookconfiguration in the caBundle I can see that still see that Issuer: OU = postgresql-operator-test, CN = cnpg-ca-secret certificate is being used.\r\nAny ideas what might be wrong? \r\nI should also mention that istio mtls is being used in the cluster.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nHelm\n### What happened?\nI have issued an certificate from cert-manager following the documentation steps described [here](https://cloudnative-pg.io/documentation/1.16/certificates/)\r\n```\r\napiVersion: cert-manager.io/v1\r\nkind: Certificate\r\nmetadata:\r\n  name: my-postgres-server-cert\r\n  namespace: postgresql-operator-test\r\nspec:\r\n  secretName: my-postgres-server-cert\r\n  usages:\r\n    - server auth\r\n  commonName: cnpg-webhook-service.postgresql-operator-test.svc\r\n  dnsNames:\r\n    - cnpg-webhook-service\r\n    - cnpg-webhook-service.postgresql-operator-test\r\n    - cnpg-webhook-service.postgresql-operator-test.svc\r\n    - cnpg-webhook-service.postgresql-operator-test.svc.cluster.local\r\n  issuerRef:\r\n    name: issuer\r\n    kind: ClusterIssuer\r\n    group: cert-manager.io\r\n``` \r\nAnd then I have mounted it to the certificates: block section under clusters helm chart \r\n```\r\ncluster:\r\n  certificates:\r\n     serverTLSSecret: my-postgres-server-cert\r\n     serverCASecret: my-postgres-server-cert\r\n``` \r\nThe certificate is successfully issued because I can see it in the cluster events but again when I try to deploy the helm chart I notice that the cnpg-ca-secret and cnpg-webhook-cert objects are still automatically created by the operator and in the mutatingwebhookconfiguration in the caBundle I can see that still see that Issuer: OU = postgresql-operator-test, CN = cnpg-ca-secret certificate is being used.\r\nAny ideas what might be wrong? \r\nI should also mention that istio mtls is being used in the cluster.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductEven with operator generated certificates its not working I receive this error when I try to deploy the cluster\r\n```\r\nInternal error occurred: failed calling webhook \\\"mcluster.cnpg.io\\\": failed to call webhook: Post \\\"https://cnpg-webhook-service.postgresql-operator-test.svc:443/mutate-postgresql-cnpg-io-v1-cluster?timeout=10s\\\": x509: certificate is not valid for any names, but wanted to match cnpg-webhook-service.postgresql-operator-test.svc\\n\\n\\n\"\r\n```"
    },
    {
        "title": "[Bug]: Backup faling to provision PVC due to wrong size",
        "id": 2407531770,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.23.2\r\n### What version of Kubernetes are you using?\r\n1.30 (unsupported)\r\n### What is your Kubernetes environment?\r\nOther\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nCNPG is failing to create the backup pod because it's requesting an invalid size for the PVC.\r\n### Cluster resource\r\n```yaml\r\napiVersion: v1\r\nitems:\r\n- apiVersion: postgresql.cnpg.io/v1\r\n  kind: Cluster\r\n  metadata:\r\n    creationTimestamp: \"2023-09-06T03:19:03Z\"\r\n    generation: 8\r\n    labels:\r\n      kustomize.toolkit.fluxcd.io/name: apps\r\n      kustomize.toolkit.fluxcd.io/namespace: flux-system\r\n    name: postgres\r\n    namespace: gotosocial\r\n    resourceVersion: \"303176500\"\r\n    uid: 221ee73b-c322-442f-a76e-3fdecc5f8665\r\n  spec:\r\n    affinity:\r\n      podAntiAffinityType: required\r\n    backup:\r\n      barmanObjectStore:\r\n        destinationPath: s3://readygo-backups/barman\r\n        endpointURL: https://s3.us-west-004.backblazeb2.com\r\n        s3Credentials:\r\n          accessKeyId:\r\n            key: aws_access_key_id\r\n            name: backblaze-auth\r\n          secretAccessKey:\r\n            key: aws_secret_access_key\r\n            name: backblaze-auth\r\n        serverName: gotosocial-2\r\n        wal:\r\n          compression: snappy\r\n          encryption: AES256\r\n          maxParallel: 4\r\n      retentionPolicy: 90d\r\n      target: prefer-standby\r\n    bootstrap:\r\n      initdb:\r\n        database: app\r\n        encoding: UTF8\r\n        localeCType: C\r\n        localeCollate: C\r\n        owner: app\r\n    enablePDB: true\r\n    enableSuperuserAccess: true\r\n    failoverDelay: 0\r\n    imageName: ghcr.io/cloudnative-pg/postgresql:15.3\r\n    instances: 2\r\n    logLevel: info\r\n    maxSyncReplicas: 0\r\n    minSyncReplicas: 0\r\n    monitoring:\r\n      customQueriesConfigMap:\r\n      - key: queries\r\n        name: cnpg-default-monitoring\r\n      disableDefaultQueries: false\r\n      enablePodMonitor: false\r\n    nodeMaintenanceWindow:\r\n      inProgress: true\r\n      reusePVC: true\r\n    postgresGID: 26\r\n    postgresUID: 26\r\n    postgresql:\r\n      parameters:\r\n        archive_mode: \"on\"\r\n        archive_timeout: 5min\r\n        dynamic_shared_memory_type: posix\r\n        log_destination: csvlog\r\n        log_directory: /controller/log\r\n        log_filename: postgres\r\n        log_rotation_age: \"0\"\r\n        log_rotation_size: \"0\"\r\n        log_truncate_on_rotation: \"false\"\r\n        logging_collector: \"on\"\r\n        max_parallel_workers: \"32\"\r\n        max_replication_slots: \"32\"\r\n        max_worker_processes: \"32\"\r\n        shared_memory_type: mmap\r\n        shared_preload_libraries: \"\"\r\n        ssl_max_protocol_version: TLSv1.3\r\n        ssl_min_protocol_version: TLSv1.3\r\n        wal_keep_size: 512MB\r\n        wal_level: logical\r\n        wal_log_hints: \"on\"\r\n        wal_receiver_timeout: 5s\r\n        wal_sender_timeout: 5s\r\n      syncReplicaElectionConstraint:\r\n        enabled: false\r\n    primaryUpdateMethod: restart\r\n    primaryUpdateStrategy: unsupervised\r\n    replicationSlots:\r\n      highAvailability:\r\n        enabled: true\r\n        slotPrefix: _cnpg_\r\n      synchronizeReplicas:\r\n        enabled: true\r\n      updateInterval: 30\r\n    resources: {}\r\n    smartShutdownTimeout: 180\r\n    startDelay: 30\r\n    stopDelay: 30\r\n    storage:\r\n      resizeInUseVolumes: true\r\n      size: 275Gi\r\n    switchoverDelay: 40000000\r\n  status:\r\n    availableArchitectures:\r\n    - goArch: amd64\r\n      hash: e43340cd2ccfa2a8120cf5de6035fe4b18d799bb2feabf99a91434dd9ba92e4c\r\n    - goArch: arm64\r\n      hash: f453a8cb50a418ff9cac24c818b9e155b80487b4152444e48d187161ecdfc0eb\r\n    certificates:\r\n      clientCASecret: postgres-ca\r\n      expirations:\r\n        postgres-ca: 2024-08-10 08:36:41 +0000 UTC\r\n        postgres-replication: 2024-08-10 08:36:41 +0000 UTC\r\n        postgres-server: 2024-08-10 08:36:41 +0000 UTC\r\n      replicationTLSSecret: postgres-replication\r\n      serverAltDNSNames:\r\n      - postgres-rw\r\n      - postgres-rw.gotosocial\r\n      - postgres-rw.gotosocial.svc\r\n      - postgres-r\r\n      - postgres-r.gotosocial\r\n      - postgres-r.gotosocial.svc\r\n      - postgres-ro\r\n      - postgres-ro.gotosocial\r\n      - postgres-ro.gotosocial.svc\r\n      serverCASecret: postgres-ca\r\n      serverTLSSecret: postgres-server\r\n    cloudNativePGCommitHash: 4bef8412\r\n    cloudNativePGOperatorHash: f453a8cb50a418ff9cac24c818b9e155b80487b4152444e48d187161ecdfc0eb\r\n    conditions:\r\n    - lastTransitionTime: \"2024-07-14T17:31:51Z\"\r\n      message: Cluster is Ready\r\n      reason: ClusterIsReady\r\n      status: \"True\"\r\n      type: Ready\r\n    - lastTransitionTime: \"2024-07-14T15:33:41Z\"\r\n      message: Continuous archiving is working\r\n      reason: ContinuousArchivingSuccess\r\n      status: \"True\"\r\n      type: ContinuousArchiving\r\n    - lastTransitionTime: \"2024-07-14T15:39:18Z\"\r\n      message: Backup was successful\r\n      reason: LastBackupSucceeded\r\n      status: \"True\"\r\n      type: LastBackupSucceeded\r\n    configMapResourceVersion:\r\n      metrics:\r\n        cnpg-default-monitoring: \"268745742\"\r\n    currentPrimary: postgres-2\r\n    currentPrimaryTimestamp: \"2023-09-28T02:33:55.730850Z\"\r\n    firstRecoverabilityPoint: \"2024-04-14T00:02:14Z\"\r\n    firstRecoverabilityPointByMethod:\r\n      barmanObjectStore: \"2024-04-14T00:02:14Z\"\r\n    healthyPVC:\r\n    - postgres-1\r\n    - postgres-2\r\n    image: ghcr.io/cloudnative-pg/postgresql:15.3\r\n    instanceNames:\r\n    - postgres-1\r\n    - postgres-2\r\n    instances: 2\r\n    instancesReportedState:\r\n      postgres-1:\r\n        isPrimary: false\r\n        timeLineID: 2\r\n      postgres-2:\r\n        isPrimary: true\r\n        timeLineID: 2\r\n    instancesStatus:\r\n      healthy:\r\n      - postgres-1\r\n      - postgres-2\r\n    lastSuccessfulBackup: \"2024-07-14T15:39:04Z\"\r\n    lastSuccessfulBackupByMethod:\r\n      barmanObjectStore: \"2024-07-14T15:39:04Z\"\r\n    latestGeneratedNode: 2\r\n    managedRolesStatus: {}\r\n    phase: Cluster in healthy state\r\n    poolerIntegrations:\r\n      pgBouncerIntegration: {}\r\n    pvcCount: 2\r\n    readService: postgres-r\r\n    readyInstances: 2\r\n    secretsResourceVersion:\r\n      applicationSecretVersion: \"297491747\"\r\n      clientCaSecretVersion: \"278564069\"\r\n      replicationSecretVersion: \"278564073\"\r\n      serverCaSecretVersion: \"278564069\"\r\n      serverSecretVersion: \"278564071\"\r\n      superuserSecretVersion: \"297491746\"\r\n    switchReplicaClusterStatus: {}\r\n    targetPrimary: postgres-2\r\n    targetPrimaryTimestamp: \"2023-10-21T16:17:15.482218Z\"\r\n    timelineID: 2\r\n    topology:\r\n      instances:\r\n        postgres-1: {}\r\n        postgres-2: {}\r\n      nodesUsed: 2\r\n      successfullyExtracted: true\r\n    writeService: postgres-rw\r\nkind: List\r\nmetadata:\r\n  resourceVersion: \"\"\r\n```\r\n### Relevant log output\r\n```shell\r\nWarning   ProvisioningFailed     persistentvolumeclaim/postgres-backup   failed to provision volume with StorageClass \"topolvm-provisioner\": error getting handle for DataSource Type PersistentVolumeClaim by Name postgres-2: error, new PVC request must be greater than or equal in size to the specified PVC data source, requested 268435456000 but source is 295279001600\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.23.2\r\n### What version of Kubernetes are you using?\r\n1.30 (unsupported)\r\n### What is your Kubernetes environment?\r\nOther\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nCNPG is failing to create the backup pod because it's requesting an invalid size for the PVC.\r\n### Cluster resource\r\n```yaml\r\napiVersion: v1\r\nitems:\r\n- apiVersion: postgresql.cnpg.io/v1\r\n  kind: Cluster\r\n  metadata:\r\n    creationTimestamp: \"2023-09-06T03:19:03Z\"\r\n    generation: 8\r\n    labels:\r\n      kustomize.toolkit.fluxcd.io/name: apps\r\n      kustomize.toolkit.fluxcd.io/namespace: flux-system\r\n    name: postgres\r\n    namespace: gotosocial\r\n    resourceVersion: \"303176500\"\r\n    uid: 221ee73b-c322-442f-a76e-3fdecc5f8665\r\n  spec:\r\n    affinity:\r\n      podAntiAffinityType: required\r\n    backup:\r\n      barmanObjectStore:\r\n        destinationPath: s3://readygo-backups/barman\r\n        endpointURL: https://s3.us-west-004.backblazeb2.com\r\n        s3Credentials:\r\n          accessKeyId:\r\n            key: aws_access_key_id\r\n            name: backblaze-auth\r\n          secretAccessKey:\r\n            key: aws_secret_access_key\r\n            name: backblaze-auth\r\n        serverName: gotosocial-2\r\n        wal:\r\n          compression: snappy\r\n          encryption: AES256\r\n          maxParallel: 4\r\n      retentionPolicy: 90d\r\n      target: prefer-standby\r\n    bootstrap:\r\n      initdb:\r\n        database: app\r\n        encoding: UTF8\r\n        localeCType: C\r\n        localeCollate: C\r\n        owner: app\r\n    enablePDB: true\r\n    enableSuperuserAccess: true\r\n    failoverDelay: 0\r\n    imageName: ghcr.io/cloudnative-pg/postgresql:15.3\r\n    instances: 2\r\n    logLevel: info\r\n    maxSyncReplicas: 0\r\n    minSyncReplicas: 0\r\n    monitoring:\r\n      customQueriesConfigMap:\r\n      - key: queries\r\n        name: cnpg-default-monitoring\r\n      disableDefaultQueries: false\r\n      enablePodMonitor: false\r\n    nodeMaintenanceWindow:\r\n      inProgress: true\r\n      reusePVC: true\r\n    postgresGID: 26\r\n    postgresUID: 26\r\n    postgresql:\r\n      parameters:\r\n        archive_mode: \"on\"\r\n        archive_timeout: 5min\r\n        dynamic_shared_memory_type: posix\r\n        log_destination: csvlog\r\n        log_directory: /controller/log\r\n        log_filename: postgres\r\n        log_rotation_age: \"0\"\r\n        log_rotation_size: \"0\"\r\n        log_truncate_on_rotation: \"false\"\r\n        logging_collector: \"on\"\r\n        max_parallel_workers: \"32\"\r\n        max_replication_slots: \"32\"\r\n        max_worker_processes: \"32\"\r\n        shared_memory_type: mmap\r\n        shared_preload_libraries: \"\"\r\n        ssl_max_protocol_version: TLSv1.3\r\n        ssl_min_protocol_version: TLSv1.3\r\n        wal_keep_size: 512MB\r\n        wal_level: logical\r\n        wal_log_hints: \"on\"\r\n        wal_receiver_timeout: 5s\r\n        wal_sender_timeout: 5s\r\n      syncReplicaElectionConstraint:\r\n        enabled: false\r\n    primaryUpdateMethod: restart\r\n    primaryUpdateStrategy: unsupervised\r\n    replicationSlots:\r\n      highAvailability:\r\n        enabled: true\r\n        slotPrefix: _cnpg_\r\n      synchronizeReplicas:\r\n        enabled: true\r\n      updateInterval: 30\r\n    resources: {}\r\n    smartShutdownTimeout: 180\r\n    startDelay: 30\r\n    stopDelay: 30\r\n    storage:\r\n      resizeInUseVolumes: true\r\n      size: 275Gi\r\n    switchoverDelay: 40000000\r\n  status:\r\n    availableArchitectures:\r\n    - goArch: amd64\r\n      hash: e43340cd2ccfa2a8120cf5de6035fe4b18d799bb2feabf99a91434dd9ba92e4c\r\n    - goArch: arm64\r\n      hash: f453a8cb50a418ff9cac24c818b9e155b80487b4152444e48d187161ecdfc0eb\r\n    certificates:\r\n      clientCASecret: postgres-ca\r\n      expirations:\r\n        postgres-ca: 2024-08-10 08:36:41 +0000 UTC\r\n        postgres-replication: 2024-08-10 08:36:41 +0000 UTC\r\n        postgres-server: 2024-08-10 08:36:41 +0000 UTC\r\n      replicationTLSSecret: postgres-replication\r\n      serverAltDNSNames:\r\n      - postgres-rw\r\n      - postgres-rw.gotosocial\r\n      - postgres-rw.gotosocial.svc\r\n      - postgres-r\r\n      - postgres-r.gotosocial\r\n      - postgres-r.gotosocial.svc\r\n      - postgres-ro\r\n      - postgres-ro.gotosocial\r\n      - postgres-ro.gotosocial.svc\r\n      serverCASecret: postgres-ca\r\n      serverTLSSecret: postgres-server\r\n    cloudNativePGCommitHash: 4bef8412\r\n    cloudNativePGOperatorHash: f453a8cb50a418ff9cac24c818b9e155b80487b4152444e48d187161ecdfc0eb\r\n    conditions:\r\n    - lastTransitionTime: \"2024-07-14T17:31:51Z\"\r\n      message: Cluster is Ready\r\n      reason: ClusterIsReady\r\n      status: \"True\"\r\n      type: Ready\r\n    - lastTransitionTime: \"2024-07-14T15:33:41Z\"\r\n      message: Continuous archiving is working\r\n      reason: ContinuousArchivingSuccess\r\n      status: \"True\"\r\n      type: ContinuousArchiving\r\n    - lastTransitionTime: \"2024-07-14T15:39:18Z\"\r\n      message: Backup was successful\r\n      reason: LastBackupSucceeded\r\n      status: \"True\"\r\n      type: LastBackupSucceeded\r\n    configMapResourceVersion:\r\n      metrics:\r\n        cnpg-default-monitoring: \"268745742\"\r\n    currentPrimary: postgres-2\r\n    currentPrimaryTimestamp: \"2023-09-28T02:33:55.730850Z\"\r\n    firstRecoverabilityPoint: \"2024-04-14T00:02:14Z\"\r\n    firstRecoverabilityPointByMethod:\r\n      barmanObjectStore: \"2024-04-14T00:02:14Z\"\r\n    healthyPVC:\r\n    - postgres-1\r\n    - postgres-2\r\n    image: ghcr.io/cloudnative-pg/postgresql:15.3\r\n    instanceNames:\r\n    - postgres-1\r\n    - postgres-2\r\n    instances: 2\r\n    instancesReportedState:\r\n      postgres-1:\r\n        isPrimary: false\r\n        timeLineID: 2\r\n      postgres-2:\r\n        isPrimary: true\r\n        timeLineID: 2\r\n    instancesStatus:\r\n      healthy:\r\n      - postgres-1\r\n      - postgres-2\r\n    lastSuccessfulBackup: \"2024-07-14T15:39:04Z\"\r\n    lastSuccessfulBackupByMethod:\r\n      barmanObjectStore: \"2024-07-14T15:39:04Z\"\r\n    latestGeneratedNode: 2\r\n    managedRolesStatus: {}\r\n    phase: Cluster in healthy state\r\n    poolerIntegrations:\r\n      pgBouncerIntegration: {}\r\n    pvcCount: 2\r\n    readService: postgres-r\r\n    readyInstances: 2\r\n    secretsResourceVersion:\r\n      applicationSecretVersion: \"297491747\"\r\n      clientCaSecretVersion: \"278564069\"\r\n      replicationSecretVersion: \"278564073\"\r\n      serverCaSecretVersion: \"278564069\"\r\n      serverSecretVersion: \"278564071\"\r\n      superuserSecretVersion: \"297491746\"\r\n    switchReplicaClusterStatus: {}\r\n    targetPrimary: postgres-2\r\n    targetPrimaryTimestamp: \"2023-10-21T16:17:15.482218Z\"\r\n    timelineID: 2\r\n    topology:\r\n      instances:\r\n        postgres-1: {}\r\n        postgres-2: {}\r\n      nodesUsed: 2\r\n      successfullyExtracted: true\r\n    writeService: postgres-rw\r\nkind: List\r\nmetadata:\r\n  resourceVersion: \"\"\r\n```\r\n### Relevant log output\r\n```shell\r\nWarning   ProvisioningFailed     persistentvolumeclaim/postgres-backup   failed to provision volume with StorageClass \"topolvm-provisioner\": error getting handle for DataSource Type PersistentVolumeClaim by Name postgres-2: error, new PVC request must be greater than or equal in size to the specified PVC data source, requested 268435456000 but source is 295279001600\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductThe ticket doesn't contain much information. Can you please elaborate on both the issue and what the expected behaviour is? Thanks."
    },
    {
        "title": "[Bug]: Unable to create cluster, stuck in endless loop with error :refusing to create the primary instance while the latest generated serial is not zero",
        "id": 2406754892,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nallardkrings@gmail.com\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nI am trying to create a cluster but it gets stuck in a loop. I have seen that a similar issue was reported in previous versions of CNPG reporting the same behaviour restoring from a backup, but that issue has been resolved.\n### Cluster resource\n```shell\nName:         postgres13\r\nNamespace:    postgres\r\nLabels:       <none>\r\nAnnotations:  <none>\r\nAPI Version:  postgresql.cnpg.io/v1\r\nKind:         Cluster\r\nMetadata:\r\n  Creation Timestamp:  2024-07-13T07:09:06Z\r\n  Generation:          1\r\n  Resource Version:    4117499\r\n  UID:                 632073c5-355d-4ea8-b773-7ff6e4b66b56\r\nSpec:\r\n  Affinity:\r\n    Pod Anti Affinity Type:  preferred\r\n  Bootstrap:\r\n    Initdb:\r\n      Database:        app\r\n      Encoding:        UTF8\r\n      Locale C Type:   C\r\n      Locale Collate:  C\r\n      Owner:           app\r\n      Post Init SQL:\r\n        CREATE USER admin WITH PASSWORD 'xxxxxxxx'\r\n        ALTER USER admin WITH SUPERUSER\r\n        CREATE USER harbor WITH PASSWORD 'harbor'\r\n        CREATE DATABASE harbor OWNER harbor\r\n  Enable PDB:               true\r\n  Enable Superuser Access:  false\r\n  Failover Delay:           0\r\n  Image Name:               ghcr.io/cloudnative-pg/postgresql:13.14-3\r\n  Instances:                3\r\n  Log Level:                info\r\n  Max Sync Replicas:        0\r\n  Min Sync Replicas:        0\r\n  Monitoring:\r\n    Custom Queries Config Map:\r\n      Key:                    queries\r\n      Name:                   cnpg-default-monitoring\r\n    Disable Default Queries:  false\r\n    Enable Pod Monitor:       false\r\n  Postgres GID:               26\r\n  Postgres UID:               26\r\n  Postgresql:\r\n    Parameters:\r\n      archive_mode:                on\r\n      archive_timeout:             5min\r\n      dynamic_shared_memory_type:  posix\r\n      log_destination:             csvlog\r\n      log_directory:               /controller/log\r\n      log_filename:                postgres\r\n      log_rotation_age:            0\r\n      log_rotation_size:           0\r\n      log_truncate_on_rotation:    false\r\n      logging_collector:           on\r\n      max_parallel_workers:        32\r\n      max_replication_slots:       32\r\n      max_worker_processes:        32\r\n      shared_memory_type:          mmap\r\n      shared_preload_libraries:    \r\n      ssl_max_protocol_version:    TLSv1.3\r\n      ssl_min_protocol_version:    TLSv1.3\r\n      wal_keep_size:               512MB\r\n      wal_level:                   logical\r\n      wal_log_hints:               on\r\n      wal_receiver_timeout:        5s\r\n      wal_sender_timeout:          5s\r\n    Sync Replica Election Constraint:\r\n      Enabled:              false\r\n  Primary Update Method:    restart\r\n  Primary Update Strategy:  unsupervised\r\n  Replication Slots:\r\n    High Availability:\r\n      Enabled:      true\r\n      Slot Prefix:  _cnpg_\r\n    Synchronize Replicas:\r\n      Enabled:        true\r\n    Update Interval:  30\r\n  Resources:\r\n  Smart Shutdown Timeout:  180\r\n  Start Delay:             3600\r\n  Stop Delay:              1800\r\n  Storage:\r\n    Resize In Use Volumes:  true\r\n    Size:                   5Gi\r\n  Superuser Secret:\r\n    Name:            superuser-secret\r\n  Switchover Delay:  3600\r\nStatus:\r\n  Available Architectures:\r\n    Go Arch:  amd64\r\n    Hash:     e43340cd2ccfa2a8120cf5de6035fe4b18d799bb2feabf99a91434dd9ba92e4c\r\n    Go Arch:  arm64\r\n    Hash:     f453a8cb50a418ff9cac24c818b9e155b80487b4152444e48d187161ecdfc0eb\r\n  Certificates:\r\n    Client CA Secret:  postgres13-ca\r\n    Expirations:\r\n      postgres13-ca:           2024-10-11 07:04:07 +0000 UTC\r\n      postgres13-replication:  2024-10-11 07:04:07 +0000 UTC\r\n      postgres13-server:       2024-10-11 07:04:07 +0000 UTC\r\n    Replication TLS Secret:    postgres13-replication\r\n    Server Alt DNS Names:\r\n      postgres13-rw\r\n      postgres13-rw.postgres\r\n      postgres13-rw.postgres.svc\r\n      postgres13-r\r\n      postgres13-r.postgres\r\n      postgres13-r.postgres.svc\r\n      postgres13-ro\r\n      postgres13-ro.postgres\r\n      postgres13-ro.postgres.svc\r\n    Server CA Secret:             postgres13-ca\r\n    Server TLS Secret:            postgres13-server\r\n  Cloud Native PG Commit Hash:    4bef8412\r\n  Cloud Native PG Operator Hash:  f453a8cb50a418ff9cac24c818b9e155b80487b4152444e48d187161ecdfc0eb\r\n  Conditions:\r\n    Last Transition Time:  2024-07-13T07:09:07Z\r\n    Message:               Cluster Is Not Ready\r\n    Reason:                ClusterIsNotReady\r\n    Status:                False\r\n    Type:                  Ready\r\n  Config Map Resource Version:\r\n    Metrics:\r\n      Cnpg - Default - Monitoring:  3940226\r\n  Image:                            ghcr.io/cloudnative-pg/postgresql:13.14-3\r\n  Job Count:                        1\r\n  Latest Generated Node:            1\r\n  Managed Roles Status:\r\n  Phase:         Setting up primary\r\n  Phase Reason:  Creating primary instance postgres13-1\r\n  Pooler Integrations:\r\n    Pg Bouncer Integration:\r\n  Read Service:  postgres13-r\r\n  Secrets Resource Version:\r\n    Application Secret Version:  4117456\r\n    Client Ca Secret Version:    4117453\r\n    Replication Secret Version:  4117455\r\n    Server Ca Secret Version:    4117453\r\n    Server Secret Version:       4117454\r\n  Switch Replica Cluster Status:\r\n  Target Primary:            postgres13-1\r\n  Target Primary Timestamp:  2024-07-13T07:09:07.726213Z\r\n  Topology:\r\n    Successfully Extracted:  true\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-07-13T07:33:44Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres13\",\"namespace\":\"postgres\"},\"namespace\":\"postgres\",\"name\":\"postgres13\",\"reconcileID\":\"7152ce93-1371-466e-8d3e-9207fc7a8494\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-07-13T07:33:46Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres13\",\"namespace\":\"postgres\"},\"namespace\":\"postgres\",\"name\":\"postgres13\",\"reconcileID\":\"32204707-06b9-4379-af9a-d78cda8d3ee8\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-07-13T07:33:47Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres13\",\"namespace\":\"postgres\"},\"namespace\":\"postgres\",\"name\":\"postgres13\",\"reconcileID\":\"6b18e01f-16cc-4525-84c0-330af4fd5aa2\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-07-13T07:33:48Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres13\",\"namespace\":\"postgres\"},\"namespace\":\"postgres\",\"name\":\"postgres13\",\"reconcileID\":\"8a57dc93-95fa-4390-a806-f85f107f70a0\",\"latestGeneratedNode\":1}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nallardkrings@gmail.com\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nI am trying to create a cluster but it gets stuck in a loop. I have seen that a similar issue was reported in previous versions of CNPG reporting the same behaviour restoring from a backup, but that issue has been resolved.\n### Cluster resource\n```shell\nName:         postgres13\r\nNamespace:    postgres\r\nLabels:       <none>\r\nAnnotations:  <none>\r\nAPI Version:  postgresql.cnpg.io/v1\r\nKind:         Cluster\r\nMetadata:\r\n  Creation Timestamp:  2024-07-13T07:09:06Z\r\n  Generation:          1\r\n  Resource Version:    4117499\r\n  UID:                 632073c5-355d-4ea8-b773-7ff6e4b66b56\r\nSpec:\r\n  Affinity:\r\n    Pod Anti Affinity Type:  preferred\r\n  Bootstrap:\r\n    Initdb:\r\n      Database:        app\r\n      Encoding:        UTF8\r\n      Locale C Type:   C\r\n      Locale Collate:  C\r\n      Owner:           app\r\n      Post Init SQL:\r\n        CREATE USER admin WITH PASSWORD 'xxxxxxxx'\r\n        ALTER USER admin WITH SUPERUSER\r\n        CREATE USER harbor WITH PASSWORD 'harbor'\r\n        CREATE DATABASE harbor OWNER harbor\r\n  Enable PDB:               true\r\n  Enable Superuser Access:  false\r\n  Failover Delay:           0\r\n  Image Name:               ghcr.io/cloudnative-pg/postgresql:13.14-3\r\n  Instances:                3\r\n  Log Level:                info\r\n  Max Sync Replicas:        0\r\n  Min Sync Replicas:        0\r\n  Monitoring:\r\n    Custom Queries Config Map:\r\n      Key:                    queries\r\n      Name:                   cnpg-default-monitoring\r\n    Disable Default Queries:  false\r\n    Enable Pod Monitor:       false\r\n  Postgres GID:               26\r\n  Postgres UID:               26\r\n  Postgresql:\r\n    Parameters:\r\n      archive_mode:                on\r\n      archive_timeout:             5min\r\n      dynamic_shared_memory_type:  posix\r\n      log_destination:             csvlog\r\n      log_directory:               /controller/log\r\n      log_filename:                postgres\r\n      log_rotation_age:            0\r\n      log_rotation_size:           0\r\n      log_truncate_on_rotation:    false\r\n      logging_collector:           on\r\n      max_parallel_workers:        32\r\n      max_replication_slots:       32\r\n      max_worker_processes:        32\r\n      shared_memory_type:          mmap\r\n      shared_preload_libraries:    \r\n      ssl_max_protocol_version:    TLSv1.3\r\n      ssl_min_protocol_version:    TLSv1.3\r\n      wal_keep_size:               512MB\r\n      wal_level:                   logical\r\n      wal_log_hints:               on\r\n      wal_receiver_timeout:        5s\r\n      wal_sender_timeout:          5s\r\n    Sync Replica Election Constraint:\r\n      Enabled:              false\r\n  Primary Update Method:    restart\r\n  Primary Update Strategy:  unsupervised\r\n  Replication Slots:\r\n    High Availability:\r\n      Enabled:      true\r\n      Slot Prefix:  _cnpg_\r\n    Synchronize Replicas:\r\n      Enabled:        true\r\n    Update Interval:  30\r\n  Resources:\r\n  Smart Shutdown Timeout:  180\r\n  Start Delay:             3600\r\n  Stop Delay:              1800\r\n  Storage:\r\n    Resize In Use Volumes:  true\r\n    Size:                   5Gi\r\n  Superuser Secret:\r\n    Name:            superuser-secret\r\n  Switchover Delay:  3600\r\nStatus:\r\n  Available Architectures:\r\n    Go Arch:  amd64\r\n    Hash:     e43340cd2ccfa2a8120cf5de6035fe4b18d799bb2feabf99a91434dd9ba92e4c\r\n    Go Arch:  arm64\r\n    Hash:     f453a8cb50a418ff9cac24c818b9e155b80487b4152444e48d187161ecdfc0eb\r\n  Certificates:\r\n    Client CA Secret:  postgres13-ca\r\n    Expirations:\r\n      postgres13-ca:           2024-10-11 07:04:07 +0000 UTC\r\n      postgres13-replication:  2024-10-11 07:04:07 +0000 UTC\r\n      postgres13-server:       2024-10-11 07:04:07 +0000 UTC\r\n    Replication TLS Secret:    postgres13-replication\r\n    Server Alt DNS Names:\r\n      postgres13-rw\r\n      postgres13-rw.postgres\r\n      postgres13-rw.postgres.svc\r\n      postgres13-r\r\n      postgres13-r.postgres\r\n      postgres13-r.postgres.svc\r\n      postgres13-ro\r\n      postgres13-ro.postgres\r\n      postgres13-ro.postgres.svc\r\n    Server CA Secret:             postgres13-ca\r\n    Server TLS Secret:            postgres13-server\r\n  Cloud Native PG Commit Hash:    4bef8412\r\n  Cloud Native PG Operator Hash:  f453a8cb50a418ff9cac24c818b9e155b80487b4152444e48d187161ecdfc0eb\r\n  Conditions:\r\n    Last Transition Time:  2024-07-13T07:09:07Z\r\n    Message:               Cluster Is Not Ready\r\n    Reason:                ClusterIsNotReady\r\n    Status:                False\r\n    Type:                  Ready\r\n  Config Map Resource Version:\r\n    Metrics:\r\n      Cnpg - Default - Monitoring:  3940226\r\n  Image:                            ghcr.io/cloudnative-pg/postgresql:13.14-3\r\n  Job Count:                        1\r\n  Latest Generated Node:            1\r\n  Managed Roles Status:\r\n  Phase:         Setting up primary\r\n  Phase Reason:  Creating primary instance postgres13-1\r\n  Pooler Integrations:\r\n    Pg Bouncer Integration:\r\n  Read Service:  postgres13-r\r\n  Secrets Resource Version:\r\n    Application Secret Version:  4117456\r\n    Client Ca Secret Version:    4117453\r\n    Replication Secret Version:  4117455\r\n    Server Ca Secret Version:    4117453\r\n    Server Secret Version:       4117454\r\n  Switch Replica Cluster Status:\r\n  Target Primary:            postgres13-1\r\n  Target Primary Timestamp:  2024-07-13T07:09:07.726213Z\r\n  Topology:\r\n    Successfully Extracted:  true\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-07-13T07:33:44Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres13\",\"namespace\":\"postgres\"},\"namespace\":\"postgres\",\"name\":\"postgres13\",\"reconcileID\":\"7152ce93-1371-466e-8d3e-9207fc7a8494\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-07-13T07:33:46Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres13\",\"namespace\":\"postgres\"},\"namespace\":\"postgres\",\"name\":\"postgres13\",\"reconcileID\":\"32204707-06b9-4379-af9a-d78cda8d3ee8\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-07-13T07:33:47Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres13\",\"namespace\":\"postgres\"},\"namespace\":\"postgres\",\"name\":\"postgres13\",\"reconcileID\":\"6b18e01f-16cc-4525-84c0-330af4fd5aa2\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-07-13T07:33:48Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres13\",\"namespace\":\"postgres\"},\"namespace\":\"postgres\",\"name\":\"postgres13\",\"reconcileID\":\"8a57dc93-95fa-4390-a806-f85f107f70a0\",\"latestGeneratedNode\":1}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conductmy fault!\r\nI dit not delete the PVC\u2019s from a previous installation\r\nIssue ,may be closed!\n---\nFor me the issue persists even after deleting all PVCs, PVs, CRDs and namespace of a previous installation."
    },
    {
        "title": "[Feature]: Handling pvc volume shrink by recreating instances",
        "id": 2405838907,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nSome times we want to give volumes high amount of size when we want to have lots of changes (for Vacuums or High number of transactions [that causes high disk usage due to WAL]\n### Describe the solution you'd like\nI think by creating instances one by one (just like the restart command)\r\nwe will be able to do this\n### Describe alternatives you've considered\nSetting quota on storage directly (impossible some times)\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nSome times we want to give volumes high amount of size when we want to have lots of changes (for Vacuums or High number of transactions [that causes high disk usage due to WAL]\n### Describe the solution you'd like\nI think by creating instances one by one (just like the restart command)\r\nwe will be able to do this\n### Describe alternatives you've considered\nSetting quota on storage directly (impossible some times)\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI would be interested in that feature as well. We enabled compression on our Timescale tables which freed around 90% of diskspace. It would be great to shrink the disk now as well."
    },
    {
        "title": "[Bug]: Setting custom UID/GID causes initdb to fail",
        "id": 2403671331,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.27\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nHelm\n### What happened?\nWhen creating a `Cluster` with `postgresUID` and `postgresGID` set to values that do not exist in the base image (in `/etc/passwd` and `/etc/group` typically), initdb will fail to initialize a new data directory.\r\nThis is a well-documented issue that is fixed in the entrypoint scripts within the base `postgres` upstream images:\r\n- https://github.com/docker-library/postgres/pull/448\r\n- https://hub.docker.com/_/postgres ('Arbitrary --user Notes' section)\r\n- https://github.com/docker-library/postgres/blob/master/15/bookworm/docker-entrypoint.sh#L74\r\nIdeally, this would be handled via [nss_wrapper](https://cwrap.org/nss_wrapper.html) here as well.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-example\r\nspec:\r\n  instances: 3\r\n  storage:\r\n    size: 1Gi\r\n    storageClass: managed\r\n  resources:\r\n    limits:\r\n      cpu: \"4\"\r\n      memory: 4Gi\r\n    requests:\r\n      cpu: 100m\r\n      memory: 100Mi\r\n  postgresUID: 1001\r\n  postgresGID: 1001\n```\n### Relevant log output\n```shell\nFor example, I was trying to create a new `Cluster` with the effective UID/GID of 1001/1001:\r\n{\"level\":\"info\",\"ts\":\"2024-07-11T16:42:40Z\",\"msg\":\"Creating new data directory\",\"logging_pod\":\"cluster-example-1-initdb\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"initDbOptions\":[\"--username\",\"postgres\",\"-D\",\"/var/lib/postgresql/data/pgdata\",\"--encoding=UTF8\",\"--lc-collate=C\",\"--lc-ctype=C\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-07-11T16:42:40Z\",\"logger\":\"initdb\",\"msg\":\"initdb: could not look up effective user ID 1001: user does not exist\\n\",\"pipe\":\"stderr\",\"logging_pod\":\"cluster-example-1-initdb\"}\r\n{\"level\":\"error\",\"ts\":\"2024-07-11T16:42:40Z\",\"msg\":\"Error while bootstrapping data directory\",\"logging_pod\":\"cluster-example-1-initdb\",\"error\":\"error while creating the PostgreSQL instance: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.initSubCommand\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:151\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.NewCmd.func2\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:104\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.22.4/x64/src/runtime/proc.go:271\"}\r\nError: error while creating the PostgreSQL instance: exit status 1\r\n```\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.27\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nHelm\n### What happened?\nWhen creating a `Cluster` with `postgresUID` and `postgresGID` set to values that do not exist in the base image (in `/etc/passwd` and `/etc/group` typically), initdb will fail to initialize a new data directory.\r\nThis is a well-documented issue that is fixed in the entrypoint scripts within the base `postgres` upstream images:\r\n- https://github.com/docker-library/postgres/pull/448\r\n- https://hub.docker.com/_/postgres ('Arbitrary --user Notes' section)\r\n- https://github.com/docker-library/postgres/blob/master/15/bookworm/docker-entrypoint.sh#L74\r\nIdeally, this would be handled via [nss_wrapper](https://cwrap.org/nss_wrapper.html) here as well.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-example\r\nspec:\r\n  instances: 3\r\n  storage:\r\n    size: 1Gi\r\n    storageClass: managed\r\n  resources:\r\n    limits:\r\n      cpu: \"4\"\r\n      memory: 4Gi\r\n    requests:\r\n      cpu: 100m\r\n      memory: 100Mi\r\n  postgresUID: 1001\r\n  postgresGID: 1001\n```\n### Relevant log output\n```shell\nFor example, I was trying to create a new `Cluster` with the effective UID/GID of 1001/1001:\r\n{\"level\":\"info\",\"ts\":\"2024-07-11T16:42:40Z\",\"msg\":\"Creating new data directory\",\"logging_pod\":\"cluster-example-1-initdb\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"initDbOptions\":[\"--username\",\"postgres\",\"-D\",\"/var/lib/postgresql/data/pgdata\",\"--encoding=UTF8\",\"--lc-collate=C\",\"--lc-ctype=C\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-07-11T16:42:40Z\",\"logger\":\"initdb\",\"msg\":\"initdb: could not look up effective user ID 1001: user does not exist\\n\",\"pipe\":\"stderr\",\"logging_pod\":\"cluster-example-1-initdb\"}\r\n{\"level\":\"error\",\"ts\":\"2024-07-11T16:42:40Z\",\"msg\":\"Error while bootstrapping data directory\",\"logging_pod\":\"cluster-example-1-initdb\",\"error\":\"error while creating the PostgreSQL instance: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.initSubCommand\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:151\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.NewCmd.func2\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:104\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.22.4/x64/src/runtime/proc.go:271\"}\r\nError: error while creating the PostgreSQL instance: exit status 1\r\n```\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI've been working on a PR for this once the issue is past triage.\n---\nHi @yaraskm,\r\nI am also experiencing a similar issue, but in my case, it's specifically related to using the EFS storage class in EKS. I'm not sure if the custom UID is causing the issue.\r\n### Environment\r\n- **Kubernetes Version:** 1.30\r\n- **CloudNativePG Version:** 1.23.3\r\n- **EFS Driver:** v2.0.5-eksbuild.1 installed as EKS addon\r\n### Issue\r\nWhen creating a PG cluster with EFS as the storage class, the `initdb` process fails due to incorrect ownership of the data directory.\r\n### Logs\r\nHere are the relevant log entries:\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-08-02T11:17:01Z\",\"msg\":\"Creating new data directory\",\"logging_pod\":\"pg-cluster-1-initdb\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"initDbOptions\":[\"--username\",\"postgres\",\"-D\",\"/var/lib/postgresql/data/pgdata\",\"--encoding=UTF8\",\"--lc-collate=C\",\"--lc-ctype=C\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-08-02T11:17:02Z\",\"logger\":\"initdb\",\"msg\":\"The files belonging to this database system will be owned by user \\\"postgres\\\".\\nThis user must also own the server process.\\n\\nThe database cluster will be initialized with this locale configuration:\\n  locale provider:   libc\\n  LC_COLLATE:  C\\n  LC_CTYPE:    C\\n  LC_MESSAGES: en_US.utf8\\n  LC_MONETARY: en_US.utf8\\n  LC_NUMERIC:  en_US.utf8\\n  LC_TIME:     en_US.utf8\\nThe default text search configuration will be set to \\\"english\\\".\\n\\nData page checksums are disabled.\\n\\ncreating directory /var/lib/postgresql/data/pgdata ... ok\\ncreating subdirectories ... ok\\nselecting dynamic shared memory implementation ... posix\\nselecting default \\\"max_connections\\\" ... 20\\nselecting default \\\"shared_buffers\\\" ... 400kB\\nselecting default time zone ... Etc/UTC\\ncreating configuration files ... ok\\nrunning bootstrap script ... \",\"pipe\":\"stdout\",\"logging_pod\":\"pg-cluster-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2024-08-02T11:17:02Z\",\"logger\":\"initdb\",\"msg\":\"2024-08-02 11:17:02.575 UTC [63] FATAL:  data directory \\\"/var/lib/postgresql/data/pgdata\\\" has wrong ownership\\n2024-08-02 11:17:02.575 UTC [63] HINT:  The server must be started by the user that owns the data directory.\\nchild process exited with exit code 1\\ninitdb: removing data directory \\\"/var/lib/postgresql/data/pgdata\\\"\\n\",\"pipe\":\"stderr\",\"logging_pod\":\"pg-cluster-1-initdb\"}\r\n{\"level\":\"error\",\"ts\":\"2024-08-02T11:17:02Z\",\"msg\":\"Error while bootstrapping data directory\",\"logging_pod\":\"pg-cluster-1-initdb\",\"error\":\"error while creating the PostgreSQL instance: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.initSubCommand\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:151\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.NewCmd.func2\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:104\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.22.5/x64/src/runtime/proc.go:271\"}\r\nError: error while creating the PostgreSQL instance: exit status 1\r\n```\r\n### Additional Context\r\n- I have ensured all components are updated to their latest versions, including the EFS driver installed via the EKS addon.\r\n- The custom UID is set to 0, but it is unclear if this is the root cause of the issue.\r\nIt seems the ownership of the data directory is not set correctly for the `postgres` user when using EFS. Any guidance or solutions to address this would be greatly appreciated.\n---\nHi @dkargatzis ,\r\nAre you setting a custom group as well? The permissions issue should be covered by the `fsGroup` parameter being added to the securityContext for the Pod generated by the `Cluster` object. I'm just not sure if the group needs to be set as well for the fsGroup parameter to be added. You could add your `Cluster` object and maybe the security context as well for one of your Cluster pods?\r\nI'm still trying to solve some issues in the PR I was working on, but I think I'm close. Once I get it working for UID >= 1000, I'll try setting it to 0 as well.\n---\n@yaraskm, thanks for your prompt response. I've just attempted the following steps but still encountered the same issue:\r\n1. **Updated Helm Chart Configuration:**\r\nModified the `values.yaml` file to set the `fsGroup` parameter and adjusted `runAsNonRoot`. Tried each change individually and both together.\r\n2. **Reinstalled the Helm Chart:**\r\n   Used the updated `values.yaml` file to reinstall the Helm chart.\r\n3. **Reapplied the PostgreSQL Cluster Configuration:**\r\n   Inspected the logs of the pg-cluster-initdb pods, which still show the ownership issue.\n---\nFYI, I've also created another EFS storage class without custom UID and the ownership issue still persists."
    },
    {
        "title": "[Bug]: the database system is starting up",
        "id": 2402873524,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ndroslean@gmail.com\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nYAML manifest\n### What happened?\nCreated the cluster. The first instance is OK. The 2nd instance is failing with\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-07-11T10:48:18Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"opihr-postgres-2\",\"record\":{\"log_time\":\"2024-07-11 10:48:18.928 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"44183\",\"connection_from\":\"[local]\",\"session_id\":\"668fb872.ac97\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-11 10:48:18 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n```\n### Cluster resource\n```shell\n- apiVersion: postgresql.cnpg.io/v1\r\n  kind: Cluster\r\n  metadata:\r\n    name: opihr-postgres\r\n    namespace: opihr\r\n  spec:\r\n    postgresql:\r\n      parameters:\r\n        max_connections: \"500\"\r\n    instances: 2\r\n    bootstrap:\r\n      initdb:\r\n        database: opihr\r\n        owner: opihr\r\n    storage:\r\n      size: 20Gi\r\n    backup:\r\n      volumeSnapshot:\r\n        className: do-block-storage-snapshot-postgres-class\r\n        labels:\r\n          app: opihr\r\n        online: true\r\n      target: primary\r\n      retentionPolicy: 10d\r\n    monitoring:\r\n      enablePodMonitor: true\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ndroslean@gmail.com\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nYAML manifest\n### What happened?\nCreated the cluster. The first instance is OK. The 2nd instance is failing with\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-07-11T10:48:18Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"opihr-postgres-2\",\"record\":{\"log_time\":\"2024-07-11 10:48:18.928 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"44183\",\"connection_from\":\"[local]\",\"session_id\":\"668fb872.ac97\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-11 10:48:18 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n```\n### Cluster resource\n```shell\n- apiVersion: postgresql.cnpg.io/v1\r\n  kind: Cluster\r\n  metadata:\r\n    name: opihr-postgres\r\n    namespace: opihr\r\n  spec:\r\n    postgresql:\r\n      parameters:\r\n        max_connections: \"500\"\r\n    instances: 2\r\n    bootstrap:\r\n      initdb:\r\n        database: opihr\r\n        owner: opihr\r\n    storage:\r\n      size: 20Gi\r\n    backup:\r\n      volumeSnapshot:\r\n        className: do-block-storage-snapshot-postgres-class\r\n        labels:\r\n          app: opihr\r\n        online: true\r\n      target: primary\r\n      retentionPolicy: 10d\r\n    monitoring:\r\n      enablePodMonitor: true\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct@droslean did you find a solution for this? previously working cluster ran into the same issue, 2 instances working, 3rd never comes up with the same message. r+w operations work, wal works, backup works\n---\nI had a extensions which failed to load, have you added any?\n---\nMine turned out to be an underlying issue with the pvc / PV. Deleted both, identified an issue with rook-ceph that prevented them being recreated and the issue was resolved."
    },
    {
        "title": "[Bug]: password authentication failed for user",
        "id": 2402869933,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ndroslean@gmail.com\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nYAML manifest\n### What happened?\nCreated the cluster. The instances are generating error:\r\n```console\r\n{\"level\":\"info\",\"ts\":\"2024-07-11T10:44:25Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"nordes-postgres-1\",\"record\":{\"log_time\":\"2024-07-11 10:44:25.267 UTC\",\"user_name\":\"nordes\",\"database_name\":\"nordes\",\"process_id\":\"117234\",\"connection_from\":\"10.244.1.79:55590\",\"session_id\":\"668fb789.1c9f2\",\"session_line_num\":\"1\",\"command_tag\":\"authentication\",\"session_start_time\":\"2024-07-11 10:44:25 UTC\",\"virtual_transaction_id\":\"5/6556\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"28P01\",\"message\":\"password authentication failed for user \\\"nordes\\\"\",\"detail\":\"Connection matched file \\\"/var/lib/postgresql/data/pgdata/pg_hba.conf\\\" line 25: \\\"host all all all scram-sha-256\\\"\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n```\n### Cluster resource\n```shell\n- apiVersion: postgresql.cnpg.io/v1\r\n  kind: Cluster\r\n  metadata:\r\n    name: nordes-postgres\r\n    namespace: nordes\r\n  spec:\r\n    postgresql:\r\n      parameters:\r\n        max_connections: \"500\"\r\n    instances: 2\r\n    bootstrap:\r\n      initdb:\r\n        database: nordes\r\n        owner: nordes\r\n    storage:\r\n      size: 20Gi\r\n    backup:\r\n      volumeSnapshot:\r\n        className: do-block-storage-snapshot-postgres-class\r\n        labels:\r\n          app: nordes\r\n        online: true\r\n      target: primary\r\n      retentionPolicy: 10d\r\n    monitoring:\r\n      enablePodMonitor: true\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ndroslean@gmail.com\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nYAML manifest\n### What happened?\nCreated the cluster. The instances are generating error:\r\n```console\r\n{\"level\":\"info\",\"ts\":\"2024-07-11T10:44:25Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"nordes-postgres-1\",\"record\":{\"log_time\":\"2024-07-11 10:44:25.267 UTC\",\"user_name\":\"nordes\",\"database_name\":\"nordes\",\"process_id\":\"117234\",\"connection_from\":\"10.244.1.79:55590\",\"session_id\":\"668fb789.1c9f2\",\"session_line_num\":\"1\",\"command_tag\":\"authentication\",\"session_start_time\":\"2024-07-11 10:44:25 UTC\",\"virtual_transaction_id\":\"5/6556\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"28P01\",\"message\":\"password authentication failed for user \\\"nordes\\\"\",\"detail\":\"Connection matched file \\\"/var/lib/postgresql/data/pgdata/pg_hba.conf\\\" line 25: \\\"host all all all scram-sha-256\\\"\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n```\n### Cluster resource\n```shell\n- apiVersion: postgresql.cnpg.io/v1\r\n  kind: Cluster\r\n  metadata:\r\n    name: nordes-postgres\r\n    namespace: nordes\r\n  spec:\r\n    postgresql:\r\n      parameters:\r\n        max_connections: \"500\"\r\n    instances: 2\r\n    bootstrap:\r\n      initdb:\r\n        database: nordes\r\n        owner: nordes\r\n    storage:\r\n      size: 20Gi\r\n    backup:\r\n      volumeSnapshot:\r\n        className: do-block-storage-snapshot-postgres-class\r\n        labels:\r\n          app: nordes\r\n        online: true\r\n      target: primary\r\n      retentionPolicy: 10d\r\n    monitoring:\r\n      enablePodMonitor: true\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct@droslean Do you still have this issue. I can not reproduce it, the log you paste means the authentication is failed on that line, which probably is because user/password is not match.\n---\nSimilar problem here.\r\nOld DB deployed with [bitnamicharts/postgresql](https://github.com/bitnami/charts/tree/main/bitnami/postgresql).\r\nTesting with CNPG:\r\n```\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-microservice\r\n  namespace: netbox\r\nspec:\r\n  instances: 2\r\n  bootstrap:\r\n    initdb:\r\n      import:\r\n        type: microservice\r\n        databases:\r\n          - netbox\r\n        source:\r\n          externalCluster: postgresql\r\n  storage:\r\n    size: 1Gi\r\n  externalClusters:\r\n    - name: postgresql\r\n      connectionParameters:\r\n        # Use the correct IP or host name for the source database\r\n        host: postgresql-hl.netbox\r\n        user: netbox\r\n        dbname: netbox\r\n      password:\r\n        name: pgpassword-secret\r\n        key: PGPASSWORD\r\n```\r\nProvided the very same password as the OLD one but I am getting:\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-10-11T13:45:51Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cluster-microservice-1\",\"record\":{\"log_time\":\"2024-10-11 13:45:51.443 UTC\",\"user_name\":\"netbox\",\"database_name\":\"netbox\",\"process_id\":\"5039\",\"connection_from\":\"172.25.39.79:52514\",\"session_id\":\"67092c0f.13af\",\"session_line_num\":\"1\",\"command_tag\":\"authentication\",\"session_start_time\":\"2024-10-11 13:45:51 UTC\",\"virtual_transaction_id\":\"4/5979\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"28P01\",\"message\":\"password authentication failed for user \\\"netbox\\\"\",\"detail\":\"Role \\\"netbox\\\" does not exist.\\nConnection matched file \\\"/var/lib/postgresql/data/pgdata/pg_hba.conf\\\" line 25: \\\"host all all all scram-sha-256\\\"\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n```\r\nTurning the application back to the OLD Postgres DB works.\r\nMaybe I'm just missing something?"
    },
    {
        "title": "[Feature]: add TLS support in the manager metrics port",
        "id": 2402639474,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nAs part of increased security for CloudNativePG, the prometheus metrics port\r\nserved in port 9187 should support TLS\n### Describe the solution you'd like\nWe should enable a new CLI option for the manager to enable TLS. Defaulting to false.\r\nPossibly add a new mount for the certificates, similar to what we have for the webhook certificates\r\nI.e. in `/config/manager/manager.yaml`\r\n```\r\n- mountPath: /run/secrets/cnpg.io/metrics\r\n  name: metrics-certificates\r\n```\r\n### Describe alternatives you've considered\nWe could use an environment variable for the option.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nAs part of increased security for CloudNativePG, the prometheus metrics port\r\nserved in port 9187 should support TLS\n### Describe the solution you'd like\nWe should enable a new CLI option for the manager to enable TLS. Defaulting to false.\r\nPossibly add a new mount for the certificates, similar to what we have for the webhook certificates\r\nI.e. in `/config/manager/manager.yaml`\r\n```\r\n- mountPath: /run/secrets/cnpg.io/metrics\r\n  name: metrics-certificates\r\n```\r\n### Describe alternatives you've considered\nWe could use an environment variable for the option.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]:  Data Loss During Postgres Cluster Restore with cnpg Operator",
        "id": 2400563290,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nneeraj.s@paynearby.in\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Amazon EKS\n### How did you install the operator?\nHelm\n### What happened?\nData Loss During Postgres Cluster Restore with cnpg Operator\r\nScenario:\r\n1.I set up a production-ready Postgres cluster using the cnpg operator.\r\n2. I implemented a backup strategy with:\r\n3. Volume snapshots: Hourly snapshots for physical backups.\r\n4.AWS S3 with Barman: Continuous archiving of WAL files to S3 (default 5-minute interval).\r\n5.The last successful backup occurred at 12:30 PM.\r\n6.Transactions occurred between 12:30 PM and 1:00 PM, recorded in WAL files and potentially archived in S3.\r\n7.The cluster unexpectedly crashed.\r\nRecovery Attempt:\r\n1.I attempted to restore the cluster using the latest volume snapshot (12:30 PM) and the WAL files stored in S3.\r\n2.While the volume snapshot successfully restored data up to 12:30 PM, the data from the subsequent transactions (12:30 PM to\r\n1:00 PM) seems to be missing.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: postgres-replica\r\n  namespace: postgres\r\nspec:\r\n  description: \"Cluster Demo for DoEKS\"\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:15.6\r\n  instances: 1\r\n#  envFrom:\r\n#    - configMapRef:\r\n#        name: postgres-replica-confiMap\r\n  startDelay: 300\r\n  stopDelay: 300\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n    updateInterval: 300\r\n  # unsupervised will provide rolling update in which first all replicas will be updated and then primary updated and switchover takes automatically\r\n  primaryUpdateStrategy: unsupervised\r\n  postgresql:\r\n    parameters:\r\n      # shared_buffers represent the memory available for caching. Recommended is 25% of memory allocated for pod\r\n      shared_buffers: 128MB\r\n      pg_stat_statements.max: '10000'\r\n      pg_stat_statements.track: all\r\n      auto_explain.log_min_duration: '10s'\r\n    pg_hba:\r\n      # - hostssl app all all cert\r\n#      - host app app all password\r\n      - host jenkins all all password\r\n  logLevel: warning\r\n  storage:\r\n    storageClass: ebs-sc\r\n    size: 20Gi\r\n  walStorage:\r\n    storageClass: ebs-sc\r\n    size: 10Gi\r\n#  monitoring:\r\n#    enablePodMonitor: true\r\n  bootstrap:\r\n    recovery:\r\n      source: postgres\r\n      volumeSnapshots:\r\n        storage:\r\n          name: snapshot-cluster-cold-backup-20240710105431\r\n          kind: VolumeSnapshot\r\n          apiGroup: snapshot.storage.k8s.io\r\n        walStorage:\r\n          name: snapshot-cluster-cold-backup-20240710105431-wal\r\n          kind: VolumeSnapshot\r\n          apiGroup: snapshot.storage.k8s.io\r\n  externalClusters:\r\n    - name: postgres\r\n#      connectionParameters:\r\n#        host: postgres-rw.postgres.svc\r\n#        user: jenkins\r\n#        dbname: build_matrix\r\n#      password:\r\n#        name: cluster-jenkins-user\r\n#        key: password\r\n      barmanObjectStore:\r\n        destinationPath: s3://nbt-postgres-backup\r\n        s3Credentials:\r\n          inheritFromIAMRole: true\r\n        wal:\r\n          maxParallel: 8\r\n  enableSuperuserAccess: true\r\n  superuserSecret:\r\n    name: cluster-superuser\r\n# Recommended is to set the request and limit same which refers to Guaranteed Quality of service\r\n  resources:\r\n    requests:\r\n      memory: \"512Mi\"\r\n      cpu: \"0.5\"\r\n    limits:\r\n      memory: \"5124Gi\"\r\n      cpu: \"0.5\"\r\n  affinity:\r\n    enablePodAntiAffinity: false\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: false\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nneeraj.s@paynearby.in\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Amazon EKS\n### How did you install the operator?\nHelm\n### What happened?\nData Loss During Postgres Cluster Restore with cnpg Operator\r\nScenario:\r\n1.I set up a production-ready Postgres cluster using the cnpg operator.\r\n2. I implemented a backup strategy with:\r\n3. Volume snapshots: Hourly snapshots for physical backups.\r\n4.AWS S3 with Barman: Continuous archiving of WAL files to S3 (default 5-minute interval).\r\n5.The last successful backup occurred at 12:30 PM.\r\n6.Transactions occurred between 12:30 PM and 1:00 PM, recorded in WAL files and potentially archived in S3.\r\n7.The cluster unexpectedly crashed.\r\nRecovery Attempt:\r\n1.I attempted to restore the cluster using the latest volume snapshot (12:30 PM) and the WAL files stored in S3.\r\n2.While the volume snapshot successfully restored data up to 12:30 PM, the data from the subsequent transactions (12:30 PM to\r\n1:00 PM) seems to be missing.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: postgres-replica\r\n  namespace: postgres\r\nspec:\r\n  description: \"Cluster Demo for DoEKS\"\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:15.6\r\n  instances: 1\r\n#  envFrom:\r\n#    - configMapRef:\r\n#        name: postgres-replica-confiMap\r\n  startDelay: 300\r\n  stopDelay: 300\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n    updateInterval: 300\r\n  # unsupervised will provide rolling update in which first all replicas will be updated and then primary updated and switchover takes automatically\r\n  primaryUpdateStrategy: unsupervised\r\n  postgresql:\r\n    parameters:\r\n      # shared_buffers represent the memory available for caching. Recommended is 25% of memory allocated for pod\r\n      shared_buffers: 128MB\r\n      pg_stat_statements.max: '10000'\r\n      pg_stat_statements.track: all\r\n      auto_explain.log_min_duration: '10s'\r\n    pg_hba:\r\n      # - hostssl app all all cert\r\n#      - host app app all password\r\n      - host jenkins all all password\r\n  logLevel: warning\r\n  storage:\r\n    storageClass: ebs-sc\r\n    size: 20Gi\r\n  walStorage:\r\n    storageClass: ebs-sc\r\n    size: 10Gi\r\n#  monitoring:\r\n#    enablePodMonitor: true\r\n  bootstrap:\r\n    recovery:\r\n      source: postgres\r\n      volumeSnapshots:\r\n        storage:\r\n          name: snapshot-cluster-cold-backup-20240710105431\r\n          kind: VolumeSnapshot\r\n          apiGroup: snapshot.storage.k8s.io\r\n        walStorage:\r\n          name: snapshot-cluster-cold-backup-20240710105431-wal\r\n          kind: VolumeSnapshot\r\n          apiGroup: snapshot.storage.k8s.io\r\n  externalClusters:\r\n    - name: postgres\r\n#      connectionParameters:\r\n#        host: postgres-rw.postgres.svc\r\n#        user: jenkins\r\n#        dbname: build_matrix\r\n#      password:\r\n#        name: cluster-jenkins-user\r\n#        key: password\r\n      barmanObjectStore:\r\n        destinationPath: s3://nbt-postgres-backup\r\n        s3Credentials:\r\n          inheritFromIAMRole: true\r\n        wal:\r\n          maxParallel: 8\r\n  enableSuperuserAccess: true\r\n  superuserSecret:\r\n    name: cluster-superuser\r\n# Recommended is to set the request and limit same which refers to Guaranteed Quality of service\r\n  resources:\r\n    requests:\r\n      memory: \"512Mi\"\r\n      cpu: \"0.5\"\r\n    limits:\r\n      memory: \"5124Gi\"\r\n      cpu: \"0.5\"\r\n  affinity:\r\n    enablePodAntiAffinity: false\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: false\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductCan you please explain how you took the backup (is it a cold backup?)? I just wanted to let you know that you only provide recovery information. Also, are you sure you are generating committed transactions from when you took the backup?\r\nP.S.: Verify your limits, as you are asking for 5TB of RAM\n---\nThanks for replying gbartolini\r\nAttaching main cluster manifest in which I am taking backup\r\n[postrgres-cluster.txt](https://github.com/user-attachments/files/16164847/postrgres-cluster.txt)\r\nAlso, are you sure you are generating committed transactions from when you took the backup?\r\nYes, I had verified that transactions were committed"
    },
    {
        "title": "[Bug]: SystemAccount dont have permissions to get secret with minio credentials for backup",
        "id": 2391373702,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nHi\r\nCreated a cluster without backup section. Later I've added the backup section and applied it\r\nOn applying with the backup changes, I've received a log message saying lack of permissions.\r\nThe pods are running with the correct service account\r\nAnd:\r\n```\r\nkubectl auth can-i get secrets/minio-secret --as=system:serviceaccount:auto-sapo-sandbox-pgsql:auto-stats-sandbox                           \r\nyes\r\n```\r\nAny ideas?\r\nThanks in advance\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: auto-stats-sandbox\r\nspec:\r\n  instances: 3\r\n  affinity:\r\n    enablePodAntiAffinity: true #default value\r\n    topologyKey: kubernetes.io/hostname #defaul value\r\n    podAntiAffinityType: preferred #default value\r\n    tolerations:\r\n      - key: \"postgres\"\r\n        operator: \"Equal\"\r\n        value: \"true\"\r\n        effect: \"NoSchedule\"\r\n    nodeSelector:\r\n      postgres: \"true\"\r\n  superuserSecret:\r\n    name: superuser-secret\r\n  bootstrap:\r\n    initdb:\r\n      database: stats\r\n      owner: auto\r\n      secret:\r\n        name: auto-secret\r\n  storage:\r\n    storageClass: longhorn-encrypted\r\n    size: 1Gi\r\n  walStorage:\r\n    storageClass: longhorn-encrypted\r\n    size: 1Gi\r\n  backup:\r\n    barmanObjectStore:\r\n      endpointURL: \"https://minio.example.pt\"\r\n      destinationPath: s3://backups/example\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: minio-secret\r\n          key: accesskey\r\n        secretAccessKey:\r\n          name: minio-secret\r\n          key: secretkey\r\n      wal:\r\n        compression: gzip\r\n        encryption: AES256\r\n      data:\r\n        compression: gzip\r\n        encryption: AES256\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-07-04T19:48:53Z\",\"msg\":\"backup credentials don't yet have access permissions. Will retry reconciliation loop\",\"logging_pod\":\"auto-stats-sandbox-3\"}\r\n{\"level\":\"error\",\"ts\":\"2024-07-04T19:48:53Z\",\"msg\":\"while getting recover credentials\",\"logging_pod\":\"auto-stats-sandbox-3\",\"error\":\"while getting secret minio-secret: secrets \\\"minio-secret\\\" is forbidden: User \\\"system:serviceaccount:auto-sapo-sandbox-pgsql:auto-stats-sandbox\\\" cannot get resource \\\"secrets\\\" in API group \\\"\\\" in the namespace \\\"auto-sapo-sandbox-pgsql\\\"\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).updateWALRestoreSettingsCache\\n\\tinternal/management/controller/cache.go:71\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).updateCacheFromCluster\\n\\tinternal/management/controller/cache.go:47\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).Reconcile\\n\\tinternal/management/controller/instance_controller.go:122\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/internal/controller/controller.go:114\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/internal/controller/controller.go:311\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/internal/controller/controller.go:261\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/internal/controller/controller.go:222\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nHi\r\nCreated a cluster without backup section. Later I've added the backup section and applied it\r\nOn applying with the backup changes, I've received a log message saying lack of permissions.\r\nThe pods are running with the correct service account\r\nAnd:\r\n```\r\nkubectl auth can-i get secrets/minio-secret --as=system:serviceaccount:auto-sapo-sandbox-pgsql:auto-stats-sandbox                           \r\nyes\r\n```\r\nAny ideas?\r\nThanks in advance\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: auto-stats-sandbox\r\nspec:\r\n  instances: 3\r\n  affinity:\r\n    enablePodAntiAffinity: true #default value\r\n    topologyKey: kubernetes.io/hostname #defaul value\r\n    podAntiAffinityType: preferred #default value\r\n    tolerations:\r\n      - key: \"postgres\"\r\n        operator: \"Equal\"\r\n        value: \"true\"\r\n        effect: \"NoSchedule\"\r\n    nodeSelector:\r\n      postgres: \"true\"\r\n  superuserSecret:\r\n    name: superuser-secret\r\n  bootstrap:\r\n    initdb:\r\n      database: stats\r\n      owner: auto\r\n      secret:\r\n        name: auto-secret\r\n  storage:\r\n    storageClass: longhorn-encrypted\r\n    size: 1Gi\r\n  walStorage:\r\n    storageClass: longhorn-encrypted\r\n    size: 1Gi\r\n  backup:\r\n    barmanObjectStore:\r\n      endpointURL: \"https://minio.example.pt\"\r\n      destinationPath: s3://backups/example\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: minio-secret\r\n          key: accesskey\r\n        secretAccessKey:\r\n          name: minio-secret\r\n          key: secretkey\r\n      wal:\r\n        compression: gzip\r\n        encryption: AES256\r\n      data:\r\n        compression: gzip\r\n        encryption: AES256\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-07-04T19:48:53Z\",\"msg\":\"backup credentials don't yet have access permissions. Will retry reconciliation loop\",\"logging_pod\":\"auto-stats-sandbox-3\"}\r\n{\"level\":\"error\",\"ts\":\"2024-07-04T19:48:53Z\",\"msg\":\"while getting recover credentials\",\"logging_pod\":\"auto-stats-sandbox-3\",\"error\":\"while getting secret minio-secret: secrets \\\"minio-secret\\\" is forbidden: User \\\"system:serviceaccount:auto-sapo-sandbox-pgsql:auto-stats-sandbox\\\" cannot get resource \\\"secrets\\\" in API group \\\"\\\" in the namespace \\\"auto-sapo-sandbox-pgsql\\\"\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).updateWALRestoreSettingsCache\\n\\tinternal/management/controller/cache.go:71\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).updateCacheFromCluster\\n\\tinternal/management/controller/cache.go:47\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).Reconcile\\n\\tinternal/management/controller/instance_controller.go:122\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/internal/controller/controller.go:114\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/internal/controller/controller.go:311\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/internal/controller/controller.go:261\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/internal/controller/controller.go:222\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductMy bad\r\nI've installed the operator with helm chart. I had the same behaviour without any log messages. I've upgraded the operator and then the messages showed. Before upgrade I've deleted the cluster.\r\nUsing RKE2 1.24\n---\nHello @zeitler, if I understand correctly we can close the issue, if needed feel free to reopen it :)\n---\nHi\r\nMy bad * 2 :)\r\nMy bad... I forgot to say how the operator installation was and with kubernetes version I was.\r\nThe issue still exists\n---\nI have the same issue. If I create a new cluster with the backup section it works fine. But if I add it to an existing cluster then there is a permission error (like described above).\r\nThe permissions for the service account seems to be correct.\n---\nThe same error occurs,\r\nwhen updating the Cluster via apply via new ObjectStorage while leaving the endpoint the same.\r\nHowever, the k8s Role receives the update correctly.\n---\n@armru, same problem on my side after updating backup section and backup credentials\n---\nHello, I've reopened the issue, I will try to take a look as soon as I have some free cycles\n---\nHello, I did manage to reproduce the following issue:\r\n```\r\n{\"level\":\"error\",\"ts\":\"2024-08-13T10:43:12Z\",\"msg\":\"while getting recover credentials\",\"logging_pod\":\"pg-backup-minio-1\",\"error\":\"while getting secret backup-storage-creds: secrets \\\"backup-storage-creds\\\" is forbidden: User \\\"system:serviceaccount:cluster-backup-minio-2747:pg-backup-minio\\\" cannot get resource \\\"secrets\\\" in API group \\\"\\\" in the namespace \\\"cluster-backup-minio-2747\\\"\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).updateWALRestoreSettingsCache\\n\\tinternal/management/controller/cache.go:71\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).updateCacheFromCluster\\n\\tinternal/management/controller/cache.go:47\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).Reconcile\\n\\tinternal/management/controller/instance_controller.go:126\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\tvendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:114\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tvendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:311\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tvendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:261\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tvendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:222\"\r\n```\r\nIt went away after a couple of reconciliation loops (as expected). The instance manager started correctly uploading files to the bucket endpoint.\r\nIf your issue persists, I think to proceed, I need a full step-by-step guide on how to reproduce the issue in a way that persists over time\n---\nhi \r\njust did another try. \r\nJust using \r\n```\r\n...\r\n    wal:\r\n        compression: gzip\r\n```\r\nNo encryption and without data backup. Just because I wanted that way and not looking for a solution.\r\nAfter applying the manifest, I did \r\n`kubectl cnpg reload`\r\n`kubectl cnpg restart`\r\nAnd wal backups show up in minio.\r\nTomorrow I'll try to test it normally, without the encryption because my minio instance don't allow it yet\n---\nHi, does the problem persist even after waiting for several minutes? I tried several times to reproduce it without any success. The error appears, but it goes away after a couple of reconciliation loops.\r\nIt would help if you would provide me with a reproducible scenario with step by step guide\n---\nSame error as @costigator and @FischerLGLN.\nI got the permission error when updating a cluster with the backup backup spec.\nDelete it and recreate it with the backup spec existing, it works fine.\n---\nWe also got the same issue.\n```\n{\"level\":\"error\",\"ts\":\"2024-11-21T12:34:16.457952685Z\",\"msg\":\"while getting recover credentials\",\"logger\":\"instance-manager\",\"logging_pod\":\"mnd-reach-staging-17-1\",\"controller\":\"instance-cluster\",\"controllerGroup\":\"[postgresql.cnpg.io](http://postgresql.cnpg.io/)\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"mnd-reach-staging-17\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"mnd-reach-staging-17\",\"reconcileID\":\"79daec0a-5238-4c9b-8b75-1e2beba4b95b\",\"logging_pod\":\"mnd-reach-staging-17-1\",\"error\":\"while getting secret cloudnativepg-minio-credentials: secrets \\\"cloudnativepg-minio-credentials\\\" is forbidden: User \\\"system:serviceaccount:default:mnd-reach-staging-17\\\" cannot get resource \\\"secrets\\\" in API group \\\"\\\" in the namespace \\\"default\\\"\",\"stacktrace\":\"[github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error](http://github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error)\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\[ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).updateWALRestoreSettingsCache](http://ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).updateWALRestoreSettingsCache)\\n\\tinternal/management/controller/cache.go:71\\[ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).updateCacheFromCluster](http://ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).updateCacheFromCluster)\\n\\tinternal/management/controller/cache.go:45\\[ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).Reconcile](http://ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).Reconcile)\\n\\tinternal/management/controller/instance_controller.go:126\\[nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Reconcile](http://nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Reconcile)\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:116\\[nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).reconcileHandler](http://nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).reconcileHandler)\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:303\\[nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).processNextWorkItem](http://nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).processNextWorkItem)\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:263\\[nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Start.func2.2](http://nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Start.func2.2)\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.19.0/pkg/internal/controller/controller.go:224\"}\n```\nWe tried removing backup section and adding it back. It didn't fix our problem. Once we changed\n`enableSuperuserAccess` to `false` (from `true`) backups started working. I found this thread after our \"fix\" so now Im thinking that the `enableSuperuserAccess` change was just a coincidence from backup starting working. Maybe it was just triggering a reconciliation loop to happened. \ud83e\udd37"
    },
    {
        "title": "[Bug]: new user don't has access to imported database",
        "id": 2386461580,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.23.2\r\n### What version of Kubernetes are you using?\r\n1.29\r\n### What is your Kubernetes environment?\r\nSelf-managed: k3s\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\n1. I recovered an existing cluster \"old-cluster\" with database \"old-cluster\" into a new cluster \"new-cluster\r\n2. when trying to connect from hasura with the credentials from secret \"new-cluster-app\" leads to error:\r\n`permission denied for database old-cluster`\r\nWhen I instead connect via e.g. dbweaver, It seems that the connection was successful.\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: new-cluster\r\nspec:\r\n  bootstrap:\r\n    recovery:\r\n      database: old-cluster\r\n      owner: new-cluster\r\n      source: old-cluster\r\n  externalClusters:\r\n  - barmanObjectStore:\r\n      azureCredentials:\r\n        ...\r\n    name: old-cluster\r\n```\r\n### Relevant log output\r\n```\r\n-07-02T14:41:52.286+0000\",\"type\":\"startup\"}\r\n{\"detail\":{\"info\":{\"database_url\":\"postgres://new-cluster:...@new-cluster-rw.vision-ai-hub.svc.cluster.local:5432/old-cluster?sslmode=require\",\"retries\":1},\"kind\":\"postgres_connection\"},\"level\":\"info\",\"timestamp\":\"2024-07-02T14:41:52.286+0000\",\"type\":\"startup\"}\r\n{\"detail\":{\"info\":{\"code\":\"unexpected\",\"error\":\"database query error\",\"internal\":{\"arguments\":[],\"error\":{\"description\":null,\"exec_status\":\"FatalError\",\"hint\":null,\"message\":\"permission denied for database old-cluster\",\"status_code\":\"42501\"},\"prepared\":false,\"statement\":\"CREATE SCHEMA hdb_catalog\"},\"path\":\"$\"},\"kind\":\"catalog_migrate\"},\"level\":\"error\",\"timestamp\":\"2024-07-02T14:41:52.286+0000\",\"type\":\"startup\"}\r\n{\"error\":\"database query error\",\"path\":\"$\",\"code\":\"unexpected\",\"internal\":{\"arguments\":[],\"error\":{\"description\":null,\"exec_status\":\"FatalError\",\"hint\":null,\"message\":\"permission denied for database old-cluster\",\"status_code\":\"42501\"},\"prepared\":false,\"statement\":\"CREATE SCHEMA hdb_catalog\"}}\r\n```\r\n### Additional Debugging\r\nI checked some access levels with psql and turns out, that the new user \"new-cluster\" is definitely not \"owner\" of recovered database \"old-cluster\". But this should be the case, according to docs. What when wrong here?\r\n```shell\r\nkubectl cnpg psql new-cluster\r\npostgres=> \\c old-cluster\r\nYou are now connected to database \"old-cluster\" as user \"postgres\".\r\nold-cluster=# \\du\r\n                                       List of roles\r\n     Role name     |                         Attributes                         | Member of \r\n-------------------+------------------------------------------------------------+-----------\r\n old-cluster       |                                                            | {}\r\n new-cluster       |                                                            | {}\r\n postgres          | Superuser, Create role, Create DB, Replication, Bypass RLS | {}\r\n streaming_replica | Replication                                                | {}\r\nold-cluster=# SELECT current_user;\r\n current_user \r\n--------------\r\n postgres\r\n(1 row)\r\nold-cluster=# set role \"new-cluster\"\r\nold-cluster-# ;\r\nSET\r\nold-cluster=> SELECT datname, \r\n       pg_catalog.pg_get_userbyid(datdba) as owner, \r\n       has_database_privilege(datname, 'CONNECT') as can_connect,\r\n       has_database_privilege(datname, 'CREATE') as can_create,\r\n       has_database_privilege(datname, 'TEMP') as can_temp\r\nFROM pg_database\r\nWHERE datname = 'old-cluster';\r\n datname | owner  | can_connect | can_create | can_temp \r\n---------+--------+-------------+------------+----------\r\n old-cluster  | old-cluster | t           | f          | t\r\n(1 row)\r\nold-cluster=> SELECT nspname AS schema,\r\n       pg_catalog.pg_get_userbyid(nspowner) AS owner,\r\n       has_schema_privilege('new-cluster', nspname, 'CREATE') AS can_create,\r\n       has_schema_privilege('new-cluster', nspname, 'USAGE') AS can_usage\r\nFROM pg_catalog.pg_namespace\r\nWHERE nspname NOT LIKE 'pg_%' AND nspname != 'information_schema';\r\n   schema    |       owner       | can_create | can_usage \r\n-------------+-------------------+------------+-----------\r\n public      | pg_database_owner | f          | t\r\n hdb_catalog | old-cluster       | f          | f\r\n(2 rows)\r\nold-cluster=> -- Check table privileges\r\nSELECT table_catalog, \r\n       table_schema, \r\n       table_name, \r\n       privilege_type\r\nFROM information_schema.role_table_grants\r\nWHERE grantee = 'new-cluster' AND table_catalog = 'old-cluster';\r\n table_catalog | table_schema | table_name | privilege_type \r\n---------------+--------------+------------+----------------\r\n(0 rows)\r\nold-cluster=> \r\n``` \r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.23.2\r\n### What version of Kubernetes are you using?\r\n1.29\r\n### What is your Kubernetes environment?\r\nSelf-managed: k3s\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\n1. I recovered an existing cluster \"old-cluster\" with database \"old-cluster\" into a new cluster \"new-cluster\r\n2. when trying to connect from hasura with the credentials from secret \"new-cluster-app\" leads to error:\r\n`permission denied for database old-cluster`\r\nWhen I instead connect via e.g. dbweaver, It seems that the connection was successful.\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: new-cluster\r\nspec:\r\n  bootstrap:\r\n    recovery:\r\n      database: old-cluster\r\n      owner: new-cluster\r\n      source: old-cluster\r\n  externalClusters:\r\n  - barmanObjectStore:\r\n      azureCredentials:\r\n        ...\r\n    name: old-cluster\r\n```\r\n### Relevant log output\r\n```\r\n-07-02T14:41:52.286+0000\",\"type\":\"startup\"}\r\n{\"detail\":{\"info\":{\"database_url\":\"postgres://new-cluster:...@new-cluster-rw.vision-ai-hub.svc.cluster.local:5432/old-cluster?sslmode=require\",\"retries\":1},\"kind\":\"postgres_connection\"},\"level\":\"info\",\"timestamp\":\"2024-07-02T14:41:52.286+0000\",\"type\":\"startup\"}\r\n{\"detail\":{\"info\":{\"code\":\"unexpected\",\"error\":\"database query error\",\"internal\":{\"arguments\":[],\"error\":{\"description\":null,\"exec_status\":\"FatalError\",\"hint\":null,\"message\":\"permission denied for database old-cluster\",\"status_code\":\"42501\"},\"prepared\":false,\"statement\":\"CREATE SCHEMA hdb_catalog\"},\"path\":\"$\"},\"kind\":\"catalog_migrate\"},\"level\":\"error\",\"timestamp\":\"2024-07-02T14:41:52.286+0000\",\"type\":\"startup\"}\r\n{\"error\":\"database query error\",\"path\":\"$\",\"code\":\"unexpected\",\"internal\":{\"arguments\":[],\"error\":{\"description\":null,\"exec_status\":\"FatalError\",\"hint\":null,\"message\":\"permission denied for database old-cluster\",\"status_code\":\"42501\"},\"prepared\":false,\"statement\":\"CREATE SCHEMA hdb_catalog\"}}\r\n```\r\n### Additional Debugging\r\nI checked some access levels with psql and turns out, that the new user \"new-cluster\" is definitely not \"owner\" of recovered database \"old-cluster\". But this should be the case, according to docs. What when wrong here?\r\n```shell\r\nkubectl cnpg psql new-cluster\r\npostgres=> \\c old-cluster\r\nYou are now connected to database \"old-cluster\" as user \"postgres\".\r\nold-cluster=# \\du\r\n                                       List of roles\r\n     Role name     |                         Attributes                         | Member of \r\n-------------------+------------------------------------------------------------+-----------\r\n old-cluster       |                                                            | {}\r\n new-cluster       |                                                            | {}\r\n postgres          | Superuser, Create role, Create DB, Replication, Bypass RLS | {}\r\n streaming_replica | Replication                                                | {}\r\nold-cluster=# SELECT current_user;\r\n current_user \r\n--------------\r\n postgres\r\n(1 row)\r\nold-cluster=# set role \"new-cluster\"\r\nold-cluster-# ;\r\nSET\r\nold-cluster=> SELECT datname, \r\n       pg_catalog.pg_get_userbyid(datdba) as owner, \r\n       has_database_privilege(datname, 'CONNECT') as can_connect,\r\n       has_database_privilege(datname, 'CREATE') as can_create,\r\n       has_database_privilege(datname, 'TEMP') as can_temp\r\nFROM pg_database\r\nWHERE datname = 'old-cluster';\r\n datname | owner  | can_connect | can_create | can_temp \r\n---------+--------+-------------+------------+----------\r\n old-cluster  | old-cluster | t           | f          | t\r\n(1 row)\r\nold-cluster=> SELECT nspname AS schema,\r\n       pg_catalog.pg_get_userbyid(nspowner) AS owner,\r\n       has_schema_privilege('new-cluster', nspname, 'CREATE') AS can_create,\r\n       has_schema_privilege('new-cluster', nspname, 'USAGE') AS can_usage\r\nFROM pg_catalog.pg_namespace\r\nWHERE nspname NOT LIKE 'pg_%' AND nspname != 'information_schema';\r\n   schema    |       owner       | can_create | can_usage \r\n-------------+-------------------+------------+-----------\r\n public      | pg_database_owner | f          | t\r\n hdb_catalog | old-cluster       | f          | f\r\n(2 rows)\r\nold-cluster=> -- Check table privileges\r\nSELECT table_catalog, \r\n       table_schema, \r\n       table_name, \r\n       privilege_type\r\nFROM information_schema.role_table_grants\r\nWHERE grantee = 'new-cluster' AND table_catalog = 'old-cluster';\r\n table_catalog | table_schema | table_name | privilege_type \r\n---------------+--------------+------------+----------------\r\n(0 rows)\r\nold-cluster=> \r\n``` \r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductBTW ... this is not an imported database. This is a physical copy (recovery) of the original cluster.\n---\nokay, but with `owner: new-cluster` I told cnpg to create an new user `new-cluster` with ownership of `old-cluster` db, no?"
    },
    {
        "title": "[Bug]: Replica does not come up: checkpoint not found in timeline",
        "id": 2384221299,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nOLM\n### What happened?\nWe ran into a problem with WAL not being able to be shipped quick enough and it wanted to failover.  This did not work (another now fixed bug report afair).\r\nThe replica failed and we destroyed the old replica and tried creating a new one.  Unfortunately it always fails after the `-init` container finishes (successfully) when starting the replica (`core-royalties-pg16-9`).\r\nThe relevant log line might be\r\n```\r\nLatest checkpoint is at 19AA/14000060 on timeline 20, but in the history of the requested timeline, the server forked off from that timeline at 18FC/2E000110.\r\n```\r\nThe primary server is pretty busy writing WAL, so it might just take too long.  I don't know what to do though.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    cnpg.io/reloadedAt: \"2024-03-13T16:47:47.298812+01:00\"\r\n    kubectl.kubernetes.io/restartedAt: \"2024-03-13T17:03:56+01:00\"\r\n  creationTimestamp: \"2024-03-07T10:01:10Z\"\r\n  generation: 14\r\n  name: core-royalties-pg16\r\n  namespace: db\r\n  resourceVersion: \"562754887\"\r\n  uid: b59ffc31-d8d9-4156-b18b-4c59d9d148ae\r\nspec:\r\n  affinity:\r\n    nodeAffinity:\r\n      requiredDuringSchedulingIgnoredDuringExecution:\r\n        nodeSelectorTerms:\r\n        - matchExpressions:\r\n          - key: node-role.kubernetes.io/db\r\n            operator: Exists\r\n    podAntiAffinityType: required\r\n    tolerations:\r\n    - effect: NoSchedule\r\n      key: dedicated\r\n      operator: Equal\r\n      value: db\r\n  backup:\r\n    barmanObjectStore:\r\n      data:\r\n        compression: snappy\r\n      destinationPath: s3://db-core-royalties-wal/cnpg/core-royalties-pg16/\r\n      endpointURL: <redacted>\r\n      s3Credentials:\r\n        accessKeyId:\r\n          key: AWS_ACCESS_KEY_ID\r\n          name: core-royalties-wal\r\n        secretAccessKey:\r\n          key: AWS_SECRET_ACCESS_KEY\r\n          name: core-royalties-wal\r\n      wal:\r\n        compression: snappy\r\n        maxParallel: 4\r\n    retentionPolicy: 10d\r\n    target: prefer-standby\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enablePDB: true\r\n  enableSuperuserAccess: false\r\n  failoverDelay: 0\r\n  imageName: image-registry.openshift-image-registry.svc:5000/db/spilo-citus:16-20240604\r\n  instances: 2\r\n  logLevel: info\r\n  managed:\r\n    roles: [<redacted>]\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_connections: \"1000\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_wal_size: \"8192\"\r\n      max_worker_processes: \"32\"\r\n      pg_stat_statements.track: all\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_level: logical\r\n      wal_log_hints: \"on\"\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    pg_hba:\r\n    - hostssl all all all scram-sha-256\r\n    shared_preload_libraries:\r\n    - citus\r\n    - pg_uuidv7\r\n    - pg_squeeze\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: supervised\r\n  projectedVolumeTemplate:\r\n    sources:\r\n    - configMap:\r\n        items:\r\n        - key: archive_gzip.sh\r\n          mode: 365\r\n          path: scripts/archive_gzip.sh\r\n        - key: archive_zstd.sh\r\n          mode: 365\r\n          path: scripts/archive_zstd.sh\r\n        - key: head_gzip.sh\r\n          mode: 365\r\n          path: scripts/head_gzip.sh\r\n        - key: head_zstd.sh\r\n          mode: 365\r\n          path: scripts/head_zstd.sh\r\n        - key: unarchive_gzip.sh\r\n          mode: 365\r\n          path: scripts/unarchive_gzip.sh\r\n        - key: unarchive_zstd.sh\r\n          mode: 365\r\n          path: scripts/unarchive_zstd.sh\r\n        name: core-royalties-scripts\r\n    - configMap:\r\n        items:\r\n        - key: BUCKET_HOST\r\n          path: db-archives/BUCKET_HOST\r\n        - key: BUCKET_NAME\r\n          path: db-archives/BUCKET_NAME\r\n        - key: BUCKET_PORT\r\n          path: db-archives/BUCKET_PORT\r\n        - key: BUCKET_REGION\r\n          path: db-archives/BUCKET_REGION\r\n        - key: BUCKET_SUBREGION\r\n          path: db-archives/BUCKET_SUBREGION\r\n        name: core-royalties-db-archives\r\n    - secret:\r\n        items:\r\n        - key: AWS_ACCESS_KEY_ID\r\n          path: db-archives/AWS_ACCESS_KEY_ID\r\n        - key: AWS_SECRET_ACCESS_KEY\r\n          path: db-archives/AWS_SECRET_ACCESS_KEY\r\n        name: core-royalties-db-archives\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    synchronizeReplicas:\r\n      enabled: true\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      cpu: \"18\"\r\n      memory: 64Gi\r\n    requests:\r\n      cpu: \"6\"\r\n      memory: 32Gi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 1500Gi\r\n    storageClass: local-nvme\r\n  switchoverDelay: 3600\r\n  tablespaces:\r\n  - name: extended_storage\r\n    owner:\r\n      name: postgres\r\n    storage:\r\n      resizeInUseVolumes: true\r\n      size: 6Ti\r\n      storageClass: local-nvme\r\n    temporary: false\r\nstatus:\r\n  availableArchitectures:\r\n  - goArch: amd64\r\n    hash: e43340cd2ccfa2a8120cf5de6035fe4b18d799bb2feabf99a91434dd9ba92e4c\r\n  - goArch: arm64\r\n    hash: f453a8cb50a418ff9cac24c818b9e155b80487b4152444e48d187161ecdfc0eb\r\n  certificates:\r\n    clientCASecret: core-royalties-pg16-ca\r\n    expirations:\r\n      core-royalties-pg16-ca: 2024-08-27 09:51:11 +0000 UTC\r\n      core-royalties-pg16-replication: 2024-08-27 09:51:11 +0000 UTC\r\n      core-royalties-pg16-server: 2024-08-27 09:51:11 +0000 UTC\r\n    replicationTLSSecret: core-royalties-pg16-replication\r\n    serverAltDNSNames:\r\n    - core-royalties-pg16-rw\r\n    - core-royalties-pg16-rw.db\r\n    - core-royalties-pg16-rw.db.svc\r\n    - core-royalties-pg16-r\r\n    - core-royalties-pg16-r.db\r\n    - core-royalties-pg16-r.db.svc\r\n    - core-royalties-pg16-ro\r\n    - core-royalties-pg16-ro.db\r\n    - core-royalties-pg16-ro.db.svc\r\n    serverCASecret: core-royalties-pg16-ca\r\n    serverTLSSecret: core-royalties-pg16-server\r\n  cloudNativePGCommitHash: 4bef8412\r\n  cloudNativePGOperatorHash: e43340cd2ccfa2a8120cf5de6035fe4b18d799bb2feabf99a91434dd9ba92e4c\r\n  conditions:\r\n  - lastTransitionTime: \"2024-06-18T04:48:42Z\"\r\n    message: Cluster Is Not Ready\r\n    reason: ClusterIsNotReady\r\n    status: \"False\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-06-18T04:48:45Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  - lastTransitionTime: \"2024-07-01T05:26:05Z\"\r\n    message: Backup was successful\r\n    reason: LastBackupSucceeded\r\n    status: \"True\"\r\n    type: LastBackupSucceeded\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"525368161\"\r\n  currentPrimary: core-royalties-pg16-4\r\n  currentPrimaryTimestamp: \"2024-06-18T04:48:55.355761Z\"\r\n  firstRecoverabilityPoint: \"2024-06-21T04:24:49Z\"\r\n  firstRecoverabilityPointByMethod:\r\n    barmanObjectStore: \"2024-06-21T04:24:49Z\"\r\n  healthyPVC:\r\n  - core-royalties-pg16-4\r\n  - core-royalties-pg16-4-tbs-extended-storage\r\n  - core-royalties-pg16-9\r\n  - core-royalties-pg16-9-tbs-extended-storage\r\n  image: image-registry.openshift-image-registry.svc:5000/db/spilo-citus:16-20240604\r\n  instanceNames:\r\n  - core-royalties-pg16-4\r\n  - core-royalties-pg16-9\r\n  instances: 2\r\n  instancesReportedState:\r\n    core-royalties-pg16-4:\r\n      isPrimary: true\r\n      timeLineID: 20\r\n    core-royalties-pg16-9:\r\n      isPrimary: false\r\n  instancesStatus:\r\n    healthy:\r\n    - core-royalties-pg16-4\r\n    replicating:\r\n    - core-royalties-pg16-9\r\n  jobCount: 4\r\n  lastFailedBackup: \"2024-05-24T04:59:54Z\"\r\n  lastSuccessfulBackup: \"2024-07-01T05:25:59Z\"\r\n  lastSuccessfulBackupByMethod:\r\n    barmanObjectStore: \"2024-07-01T05:25:59Z\"\r\n  latestGeneratedNode: 9\r\n  managedRolesStatus:\r\n    byStatus:\r\n      not-managed:\r\n      - bot-mmp-syncer-core-royalties\r\n      - bot-download-proxy\r\n      - bot-discogs-syncer\r\n      - api-core-royalties\r\n      - bot-winline-syncer\r\n      - bot-royalty-distribution-calculator\r\n      - robot_zmon\r\n      - standby\r\n      - bot-statement-downloader\r\n      - bot-statement-exporter\r\n      - bot-statement-ingestor\r\n      - bot-statement-sampler\r\n      - bot-external-ems-trend-data-brigde\r\n      - bot-browser-automation\r\n      - bot-sms-gateway\r\n      - bot-smtp-gateway\r\n      - bot-analytics-db-syncer\r\n      - usergroup-read-nearly-all\r\n      - bot-royalty-distribution-report-generator\r\n      - bot-db-cron-core-royalties\r\n      - mara.hartung\r\n      - bot-royalty-distribution-report-email-sender\r\n      - bot-analytics-data-exporter\r\n      - api-dec\r\n      - app\r\n      reconciled:\r\n      - alexander.kops\r\n      - ante.von.postel\r\n      - bot-fxrates-scraper\r\n      - max\r\n      - michi\r\n      - jens\r\n      - tob\r\n      - nicolai.hallberg\r\n      - wolfgang.stoelzle\r\n      - lucie.abendroth\r\n      - jan.kuechler\r\n      - mika\r\n      - tobias\r\n      - frauke.schmidt\r\n      - ilbey.bulut\r\n      - dan\r\n      - juergen.soeder\r\n      reserved:\r\n      - postgres\r\n      - streaming_replica\r\n    passwordStatus:\r\n      alexander.kops:\r\n        transactionID: 315504\r\n      ante.von.postel:\r\n        transactionID: 315505\r\n      bot-fxrates-scraper:\r\n        resourceVersion: \"453797026\"\r\n        transactionID: 384346\r\n      dan:\r\n        transactionID: 315519\r\n      frauke.schmidt:\r\n        transactionID: 315517\r\n      ilbey.bulut:\r\n        transactionID: 315518\r\n      jan.kuechler:\r\n        transactionID: 1749386\r\n      jens:\r\n        transactionID: 315509\r\n      juergen.soeder:\r\n        transactionID: 315506\r\n      lucie.abendroth:\r\n        transactionID: 315513\r\n      max:\r\n        transactionID: 315507\r\n      michi:\r\n        transactionID: 315508\r\n      mika:\r\n        transactionID: 315515\r\n      nicolai.hallberg:\r\n        transactionID: 315511\r\n      tob:\r\n        transactionID: 315510\r\n      tobias:\r\n        transactionID: 315516\r\n      wolfgang.stoelzle:\r\n        transactionID: 315512\r\n  onlineUpdateEnabled: true\r\n  phase: Waiting for the instances to become active\r\n  phaseReason: Some instances are not yet active. Please wait.\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 4\r\n  readService: core-royalties-pg16-r\r\n  readyInstances: 1\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"547666968\"\r\n    clientCaSecretVersion: \"535433707\"\r\n    managedRoleSecretVersion:\r\n      bot-fxrates-scraper-core-royalties-pgcreds: \"453797026\"\r\n    replicationSecretVersion: \"535433709\"\r\n    serverCaSecretVersion: \"535433707\"\r\n    serverSecretVersion: \"535433708\"\r\n  switchReplicaClusterStatus: {}\r\n  tablespacesStatus:\r\n  - name: extended_storage\r\n    owner: postgres\r\n    state: reconciled\r\n  targetPrimary: core-royalties-pg16-4\r\n  targetPrimaryTimestamp: \"2024-06-18T04:48:49.629562Z\"\r\n  timelineID: 20\r\n  topology:\r\n    instances:\r\n      core-royalties-pg16-4: {}\r\n      core-royalties-pg16-9: {}\r\n    nodesUsed: 2\r\n    successfullyExtracted: true\r\n  writeService: core-royalties-pg16-rw\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"core-royalties-pg16-9\",\"version\":\"1.23.2\",\"build\":{\"Version\":\"1.23.2\",\"Commit\":\"4bef8412\",\"Date\":\"2024-06-12\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"setup\",\"msg\":\"starting tablespace manager\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"setup\",\"msg\":\"starting external server manager\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"core-royalties-pg16-9\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"roles_reconciler\",\"msg\":\"skipping the RoleSynchronizer in replicas\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"core-royalties-pg16-9\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"core-royalties-pg16-9\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Found previous run flag\",\"logging_pod\":\"core-royalties-pg16-9\",\"filename\":\"/var/lib/postgresql/data/pgdata/cnpg_initialized-core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"core-royalties-pg16-9\",\"reason\":\"postmaster start up\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7342071844302868515\\nDatabase cluster state:               in production\\npg_control last modified:             Mon 01 Jul 2024 01:39:49 PM UTC\\nLatest checkpoint location:           19AA/14000060\\nLatest checkpoint's REDO location:    19AA/14000028\\nLatest checkpoint's REDO WAL file:    00000014000019AA00000014\\nLatest checkpoint's TimeLineID:       20\\nLatest checkpoint's PrevTimeLineID:   20\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:3739416\\nLatest checkpoint's NextOID:          185538614\\nLatest checkpoint's NextMultiXactId:  75910\\nLatest checkpoint's NextMultiOffset:  234941\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   5\\nLatest checkpoint's oldestActiveXID:  3739416\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 16489\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Mon 01 Jul 2024 01:39:48 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              1000\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           2000\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            cb3f9f6c33cfd47dd626988e8ae23202776f4f80fae330eef968882da2143f71\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"2024-07-01 16:03:22.269 UTC [35] LOG:  number of prepared transactions has not been configured, overriding\",\"pipe\":\"stderr\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"2024-07-01 16:03:22.269 UTC [35] DETAIL:  max_prepared_transactions is now set to 2000\",\"pipe\":\"stderr\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"core-royalties-pg16\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"core-royalties-pg16\",\"reconcileID\":\"a284d17d-18a2-4c82-b595-323209904e09\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"2024-07-01 16:03:22.296 UTC [35] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"2024-07-01 16:03:22.296 UTC [35] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"2024-07-01 16:03:22.296 UTC [35] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"2024-07-01 16:03:22.296 UTC [35] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:22.296 UTC\",\"process_id\":\"35\",\"session_id\":\"6682d34a.23\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:22.296 UTC\",\"process_id\":\"35\",\"session_id\":\"6682d34a.23\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 16.3 (Debian 16.3-1.pgdg120+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:22.296 UTC\",\"process_id\":\"35\",\"session_id\":\"6682d34a.23\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:22.296 UTC\",\"process_id\":\"35\",\"session_id\":\"6682d34a.23\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:22.297 UTC\",\"process_id\":\"35\",\"session_id\":\"6682d34a.23\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:22.299 UTC\",\"process_id\":\"39\",\"session_id\":\"6682d34a.27\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was interrupted; last known up at 2024-07-01 13:39:49 UTC\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000015.history\",\"startTime\":\"2024-07-01T16:03:22Z\",\"endTime\":\"2024-07-01T16:03:22Z\",\"elapsedWalTime\":0.31691575}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000015.history\",\"maxParallel\":4,\"successfulWalRestore\":1,\"failedWalRestore\":3,\"endOfWALStream\":false,\"startTime\":\"2024-07-01T16:03:22Z\",\"downloadStartTime\":\"2024-07-01T16:03:22Z\",\"downloadTotalTime\":0.317032751,\"totalTime\":0.365500254}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:22.695 UTC\",\"process_id\":\"39\",\"session_id\":\"6682d34a.27\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000015.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000016.history\",\"options\":[\"--endpoint-url\",\"<redacted>\",\"--cloud-provider\",\"aws-s3\",\"s3://db-core-royalties-wal/cnpg/core-royalties-pg16/\",\"core-royalties-pg16\"],\"startTime\":\"2024-07-01T16:03:22Z\",\"endTime\":\"2024-07-01T16:03:22Z\",\"elapsedWalTime\":0.228545702}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:23.011 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"84\",\"connection_from\":\"[local]\",\"session_id\":\"6682d34b.54\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-01 16:03:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:23.081 UTC\",\"process_id\":\"39\",\"session_id\":\"6682d34a.27\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"entering standby mode\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:23.081 UTC\",\"process_id\":\"39\",\"session_id\":\"6682d34a.27\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting backup recovery with redo LSN 19AA/12000028, checkpoint LSN 19AA/12000060, on timeline ID 20\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:23.303 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"108\",\"connection_from\":\"[local]\",\"session_id\":\"6682d34b.6c\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-01 16:03:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"core-royalties-pg16\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"core-royalties-pg16\",\"reconcileID\":\"395b52c2-e5b3-4193-8d79-b1bc912a22e1\",\"logging_pod\":\"core-royalties-pg16-9\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:23.304 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"109\",\"connection_from\":\"[local]\",\"session_id\":\"6682d34b.6d\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-01 16:03:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000015.history\",\"startTime\":\"2024-07-01T16:03:23Z\",\"endTime\":\"2024-07-01T16:03:23Z\",\"elapsedWalTime\":0.221317728}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000015.history\",\"maxParallel\":4,\"successfulWalRestore\":1,\"failedWalRestore\":3,\"endOfWALStream\":false,\"startTime\":\"2024-07-01T16:03:23Z\",\"downloadStartTime\":\"2024-07-01T16:03:23Z\",\"downloadTotalTime\":0.22145703,\"totalTime\":0.268732854}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:23.362 UTC\",\"process_id\":\"39\",\"session_id\":\"6682d34a.27\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000015.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:23.414 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"131\",\"connection_from\":\"[local]\",\"session_id\":\"6682d34b.83\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-01 16:03:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:23.648 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"136\",\"connection_from\":\"[local]\",\"session_id\":\"6682d34b.88\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-01 16:03:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000015000019AA00000012\",\"options\":[\"--endpoint-url\",\"<redacted>\",\"--cloud-provider\",\"aws-s3\",\"s3://db-core-royalties-wal/cnpg/core-royalties-pg16/\",\"core-royalties-pg16\"],\"startTime\":\"2024-07-01T16:03:23Z\",\"endTime\":\"2024-07-01T16:03:23Z\",\"elapsedWalTime\":0.277760957}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000014000019AA00000015\",\"startTime\":\"2024-07-01T16:03:23Z\",\"endTime\":\"2024-07-01T16:03:24Z\",\"elapsedWalTime\":0.27732838}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000014000019AA00000014\",\"startTime\":\"2024-07-01T16:03:23Z\",\"endTime\":\"2024-07-01T16:03:24Z\",\"elapsedWalTime\":0.299597147}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000014000019AA00000013\",\"startTime\":\"2024-07-01T16:03:23Z\",\"endTime\":\"2024-07-01T16:03:24Z\",\"elapsedWalTime\":0.300367211}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000014000019AA00000012\",\"startTime\":\"2024-07-01T16:03:23Z\",\"endTime\":\"2024-07-01T16:03:24Z\",\"elapsedWalTime\":0.313373012}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000014000019AA00000012\",\"maxParallel\":4,\"successfulWalRestore\":4,\"failedWalRestore\":0,\"endOfWALStream\":false,\"startTime\":\"2024-07-01T16:03:23Z\",\"downloadStartTime\":\"2024-07-01T16:03:23Z\",\"downloadTotalTime\":0.313483181,\"totalTime\":0.360317842}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:24.169 UTC\",\"process_id\":\"39\",\"session_id\":\"6682d34a.27\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000014000019AA00000012\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:24.176 UTC\",\"process_id\":\"39\",\"session_id\":\"6682d34a.27\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"XX000\",\"message\":\"requested timeline 21 is not a child of this server's history\",\"detail\":\"Latest checkpoint is at 19AA/14000060 on timeline 20, but in the history of the requested timeline, the server forked off from that timeline at 18FC/2E000110.\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:24.177 UTC\",\"process_id\":\"35\",\"session_id\":\"6682d34a.23\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 39) exited with exit code 1\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:24.177 UTC\",\"process_id\":\"35\",\"session_id\":\"6682d34a.23\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"aborting startup due to startup process failure\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:24.180 UTC\",\"process_id\":\"35\",\"session_id\":\"6682d34a.23\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"core-royalties-pg16-9\",\"reason\":\"postmaster has exited\"}\r\n{\"level\":\"error\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"PostgreSQL process exited with errors\",\"logging_pod\":\"core-royalties-pg16-9\",\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:106\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/manager/runnable_group.go:226\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Stopping and waiting for leader election runnables\"}\r\n{\"level\":\"error\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"error received after stop sequence was engaged\",\"error\":\"exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).engageStopProcedure.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/manager/internal.go:499\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"core-royalties-pg16-9\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"core-royalties-pg16-9\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"core-royalties-pg16-9\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Stopping and waiting for caches\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Stopping and waiting for webhooks\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Stopping and waiting for HTTP servers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7342071844302868515\\nDatabase cluster state:               in production\\npg_control last modified:             Mon 01 Jul 2024 01:39:49 PM UTC\\nLatest checkpoint location:           19AA/14000060\\nLatest checkpoint's REDO location:    19AA/14000028\\nLatest checkpoint's REDO WAL file:    00000014000019AA00000014\\nLatest checkpoint's TimeLineID:       20\\nLatest checkpoint's PrevTimeLineID:   20\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:3739416\\nLatest checkpoint's NextOID:          185538614\\nLatest checkpoint's NextMultiXactId:  75910\\nLatest checkpoint's NextMultiOffset:  234941\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   5\\nLatest checkpoint's oldestActiveXID:  3739416\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 16489\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Mon 01 Jul 2024 01:39:48 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              1000\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           2000\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            cb3f9f6c33cfd47dd626988e8ae23202776f4f80fae330eef968882da2143f71\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"core-royalties-pg16-9\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nOLM\n### What happened?\nWe ran into a problem with WAL not being able to be shipped quick enough and it wanted to failover.  This did not work (another now fixed bug report afair).\r\nThe replica failed and we destroyed the old replica and tried creating a new one.  Unfortunately it always fails after the `-init` container finishes (successfully) when starting the replica (`core-royalties-pg16-9`).\r\nThe relevant log line might be\r\n```\r\nLatest checkpoint is at 19AA/14000060 on timeline 20, but in the history of the requested timeline, the server forked off from that timeline at 18FC/2E000110.\r\n```\r\nThe primary server is pretty busy writing WAL, so it might just take too long.  I don't know what to do though.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    cnpg.io/reloadedAt: \"2024-03-13T16:47:47.298812+01:00\"\r\n    kubectl.kubernetes.io/restartedAt: \"2024-03-13T17:03:56+01:00\"\r\n  creationTimestamp: \"2024-03-07T10:01:10Z\"\r\n  generation: 14\r\n  name: core-royalties-pg16\r\n  namespace: db\r\n  resourceVersion: \"562754887\"\r\n  uid: b59ffc31-d8d9-4156-b18b-4c59d9d148ae\r\nspec:\r\n  affinity:\r\n    nodeAffinity:\r\n      requiredDuringSchedulingIgnoredDuringExecution:\r\n        nodeSelectorTerms:\r\n        - matchExpressions:\r\n          - key: node-role.kubernetes.io/db\r\n            operator: Exists\r\n    podAntiAffinityType: required\r\n    tolerations:\r\n    - effect: NoSchedule\r\n      key: dedicated\r\n      operator: Equal\r\n      value: db\r\n  backup:\r\n    barmanObjectStore:\r\n      data:\r\n        compression: snappy\r\n      destinationPath: s3://db-core-royalties-wal/cnpg/core-royalties-pg16/\r\n      endpointURL: <redacted>\r\n      s3Credentials:\r\n        accessKeyId:\r\n          key: AWS_ACCESS_KEY_ID\r\n          name: core-royalties-wal\r\n        secretAccessKey:\r\n          key: AWS_SECRET_ACCESS_KEY\r\n          name: core-royalties-wal\r\n      wal:\r\n        compression: snappy\r\n        maxParallel: 4\r\n    retentionPolicy: 10d\r\n    target: prefer-standby\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enablePDB: true\r\n  enableSuperuserAccess: false\r\n  failoverDelay: 0\r\n  imageName: image-registry.openshift-image-registry.svc:5000/db/spilo-citus:16-20240604\r\n  instances: 2\r\n  logLevel: info\r\n  managed:\r\n    roles: [<redacted>]\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_connections: \"1000\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_wal_size: \"8192\"\r\n      max_worker_processes: \"32\"\r\n      pg_stat_statements.track: all\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_level: logical\r\n      wal_log_hints: \"on\"\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    pg_hba:\r\n    - hostssl all all all scram-sha-256\r\n    shared_preload_libraries:\r\n    - citus\r\n    - pg_uuidv7\r\n    - pg_squeeze\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: supervised\r\n  projectedVolumeTemplate:\r\n    sources:\r\n    - configMap:\r\n        items:\r\n        - key: archive_gzip.sh\r\n          mode: 365\r\n          path: scripts/archive_gzip.sh\r\n        - key: archive_zstd.sh\r\n          mode: 365\r\n          path: scripts/archive_zstd.sh\r\n        - key: head_gzip.sh\r\n          mode: 365\r\n          path: scripts/head_gzip.sh\r\n        - key: head_zstd.sh\r\n          mode: 365\r\n          path: scripts/head_zstd.sh\r\n        - key: unarchive_gzip.sh\r\n          mode: 365\r\n          path: scripts/unarchive_gzip.sh\r\n        - key: unarchive_zstd.sh\r\n          mode: 365\r\n          path: scripts/unarchive_zstd.sh\r\n        name: core-royalties-scripts\r\n    - configMap:\r\n        items:\r\n        - key: BUCKET_HOST\r\n          path: db-archives/BUCKET_HOST\r\n        - key: BUCKET_NAME\r\n          path: db-archives/BUCKET_NAME\r\n        - key: BUCKET_PORT\r\n          path: db-archives/BUCKET_PORT\r\n        - key: BUCKET_REGION\r\n          path: db-archives/BUCKET_REGION\r\n        - key: BUCKET_SUBREGION\r\n          path: db-archives/BUCKET_SUBREGION\r\n        name: core-royalties-db-archives\r\n    - secret:\r\n        items:\r\n        - key: AWS_ACCESS_KEY_ID\r\n          path: db-archives/AWS_ACCESS_KEY_ID\r\n        - key: AWS_SECRET_ACCESS_KEY\r\n          path: db-archives/AWS_SECRET_ACCESS_KEY\r\n        name: core-royalties-db-archives\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    synchronizeReplicas:\r\n      enabled: true\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      cpu: \"18\"\r\n      memory: 64Gi\r\n    requests:\r\n      cpu: \"6\"\r\n      memory: 32Gi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 1500Gi\r\n    storageClass: local-nvme\r\n  switchoverDelay: 3600\r\n  tablespaces:\r\n  - name: extended_storage\r\n    owner:\r\n      name: postgres\r\n    storage:\r\n      resizeInUseVolumes: true\r\n      size: 6Ti\r\n      storageClass: local-nvme\r\n    temporary: false\r\nstatus:\r\n  availableArchitectures:\r\n  - goArch: amd64\r\n    hash: e43340cd2ccfa2a8120cf5de6035fe4b18d799bb2feabf99a91434dd9ba92e4c\r\n  - goArch: arm64\r\n    hash: f453a8cb50a418ff9cac24c818b9e155b80487b4152444e48d187161ecdfc0eb\r\n  certificates:\r\n    clientCASecret: core-royalties-pg16-ca\r\n    expirations:\r\n      core-royalties-pg16-ca: 2024-08-27 09:51:11 +0000 UTC\r\n      core-royalties-pg16-replication: 2024-08-27 09:51:11 +0000 UTC\r\n      core-royalties-pg16-server: 2024-08-27 09:51:11 +0000 UTC\r\n    replicationTLSSecret: core-royalties-pg16-replication\r\n    serverAltDNSNames:\r\n    - core-royalties-pg16-rw\r\n    - core-royalties-pg16-rw.db\r\n    - core-royalties-pg16-rw.db.svc\r\n    - core-royalties-pg16-r\r\n    - core-royalties-pg16-r.db\r\n    - core-royalties-pg16-r.db.svc\r\n    - core-royalties-pg16-ro\r\n    - core-royalties-pg16-ro.db\r\n    - core-royalties-pg16-ro.db.svc\r\n    serverCASecret: core-royalties-pg16-ca\r\n    serverTLSSecret: core-royalties-pg16-server\r\n  cloudNativePGCommitHash: 4bef8412\r\n  cloudNativePGOperatorHash: e43340cd2ccfa2a8120cf5de6035fe4b18d799bb2feabf99a91434dd9ba92e4c\r\n  conditions:\r\n  - lastTransitionTime: \"2024-06-18T04:48:42Z\"\r\n    message: Cluster Is Not Ready\r\n    reason: ClusterIsNotReady\r\n    status: \"False\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-06-18T04:48:45Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  - lastTransitionTime: \"2024-07-01T05:26:05Z\"\r\n    message: Backup was successful\r\n    reason: LastBackupSucceeded\r\n    status: \"True\"\r\n    type: LastBackupSucceeded\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"525368161\"\r\n  currentPrimary: core-royalties-pg16-4\r\n  currentPrimaryTimestamp: \"2024-06-18T04:48:55.355761Z\"\r\n  firstRecoverabilityPoint: \"2024-06-21T04:24:49Z\"\r\n  firstRecoverabilityPointByMethod:\r\n    barmanObjectStore: \"2024-06-21T04:24:49Z\"\r\n  healthyPVC:\r\n  - core-royalties-pg16-4\r\n  - core-royalties-pg16-4-tbs-extended-storage\r\n  - core-royalties-pg16-9\r\n  - core-royalties-pg16-9-tbs-extended-storage\r\n  image: image-registry.openshift-image-registry.svc:5000/db/spilo-citus:16-20240604\r\n  instanceNames:\r\n  - core-royalties-pg16-4\r\n  - core-royalties-pg16-9\r\n  instances: 2\r\n  instancesReportedState:\r\n    core-royalties-pg16-4:\r\n      isPrimary: true\r\n      timeLineID: 20\r\n    core-royalties-pg16-9:\r\n      isPrimary: false\r\n  instancesStatus:\r\n    healthy:\r\n    - core-royalties-pg16-4\r\n    replicating:\r\n    - core-royalties-pg16-9\r\n  jobCount: 4\r\n  lastFailedBackup: \"2024-05-24T04:59:54Z\"\r\n  lastSuccessfulBackup: \"2024-07-01T05:25:59Z\"\r\n  lastSuccessfulBackupByMethod:\r\n    barmanObjectStore: \"2024-07-01T05:25:59Z\"\r\n  latestGeneratedNode: 9\r\n  managedRolesStatus:\r\n    byStatus:\r\n      not-managed:\r\n      - bot-mmp-syncer-core-royalties\r\n      - bot-download-proxy\r\n      - bot-discogs-syncer\r\n      - api-core-royalties\r\n      - bot-winline-syncer\r\n      - bot-royalty-distribution-calculator\r\n      - robot_zmon\r\n      - standby\r\n      - bot-statement-downloader\r\n      - bot-statement-exporter\r\n      - bot-statement-ingestor\r\n      - bot-statement-sampler\r\n      - bot-external-ems-trend-data-brigde\r\n      - bot-browser-automation\r\n      - bot-sms-gateway\r\n      - bot-smtp-gateway\r\n      - bot-analytics-db-syncer\r\n      - usergroup-read-nearly-all\r\n      - bot-royalty-distribution-report-generator\r\n      - bot-db-cron-core-royalties\r\n      - mara.hartung\r\n      - bot-royalty-distribution-report-email-sender\r\n      - bot-analytics-data-exporter\r\n      - api-dec\r\n      - app\r\n      reconciled:\r\n      - alexander.kops\r\n      - ante.von.postel\r\n      - bot-fxrates-scraper\r\n      - max\r\n      - michi\r\n      - jens\r\n      - tob\r\n      - nicolai.hallberg\r\n      - wolfgang.stoelzle\r\n      - lucie.abendroth\r\n      - jan.kuechler\r\n      - mika\r\n      - tobias\r\n      - frauke.schmidt\r\n      - ilbey.bulut\r\n      - dan\r\n      - juergen.soeder\r\n      reserved:\r\n      - postgres\r\n      - streaming_replica\r\n    passwordStatus:\r\n      alexander.kops:\r\n        transactionID: 315504\r\n      ante.von.postel:\r\n        transactionID: 315505\r\n      bot-fxrates-scraper:\r\n        resourceVersion: \"453797026\"\r\n        transactionID: 384346\r\n      dan:\r\n        transactionID: 315519\r\n      frauke.schmidt:\r\n        transactionID: 315517\r\n      ilbey.bulut:\r\n        transactionID: 315518\r\n      jan.kuechler:\r\n        transactionID: 1749386\r\n      jens:\r\n        transactionID: 315509\r\n      juergen.soeder:\r\n        transactionID: 315506\r\n      lucie.abendroth:\r\n        transactionID: 315513\r\n      max:\r\n        transactionID: 315507\r\n      michi:\r\n        transactionID: 315508\r\n      mika:\r\n        transactionID: 315515\r\n      nicolai.hallberg:\r\n        transactionID: 315511\r\n      tob:\r\n        transactionID: 315510\r\n      tobias:\r\n        transactionID: 315516\r\n      wolfgang.stoelzle:\r\n        transactionID: 315512\r\n  onlineUpdateEnabled: true\r\n  phase: Waiting for the instances to become active\r\n  phaseReason: Some instances are not yet active. Please wait.\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 4\r\n  readService: core-royalties-pg16-r\r\n  readyInstances: 1\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"547666968\"\r\n    clientCaSecretVersion: \"535433707\"\r\n    managedRoleSecretVersion:\r\n      bot-fxrates-scraper-core-royalties-pgcreds: \"453797026\"\r\n    replicationSecretVersion: \"535433709\"\r\n    serverCaSecretVersion: \"535433707\"\r\n    serverSecretVersion: \"535433708\"\r\n  switchReplicaClusterStatus: {}\r\n  tablespacesStatus:\r\n  - name: extended_storage\r\n    owner: postgres\r\n    state: reconciled\r\n  targetPrimary: core-royalties-pg16-4\r\n  targetPrimaryTimestamp: \"2024-06-18T04:48:49.629562Z\"\r\n  timelineID: 20\r\n  topology:\r\n    instances:\r\n      core-royalties-pg16-4: {}\r\n      core-royalties-pg16-9: {}\r\n    nodesUsed: 2\r\n    successfullyExtracted: true\r\n  writeService: core-royalties-pg16-rw\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"core-royalties-pg16-9\",\"version\":\"1.23.2\",\"build\":{\"Version\":\"1.23.2\",\"Commit\":\"4bef8412\",\"Date\":\"2024-06-12\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"setup\",\"msg\":\"starting tablespace manager\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"setup\",\"msg\":\"starting external server manager\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"core-royalties-pg16-9\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"roles_reconciler\",\"msg\":\"skipping the RoleSynchronizer in replicas\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"core-royalties-pg16-9\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"core-royalties-pg16-9\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Found previous run flag\",\"logging_pod\":\"core-royalties-pg16-9\",\"filename\":\"/var/lib/postgresql/data/pgdata/cnpg_initialized-core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"core-royalties-pg16-9\",\"reason\":\"postmaster start up\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7342071844302868515\\nDatabase cluster state:               in production\\npg_control last modified:             Mon 01 Jul 2024 01:39:49 PM UTC\\nLatest checkpoint location:           19AA/14000060\\nLatest checkpoint's REDO location:    19AA/14000028\\nLatest checkpoint's REDO WAL file:    00000014000019AA00000014\\nLatest checkpoint's TimeLineID:       20\\nLatest checkpoint's PrevTimeLineID:   20\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:3739416\\nLatest checkpoint's NextOID:          185538614\\nLatest checkpoint's NextMultiXactId:  75910\\nLatest checkpoint's NextMultiOffset:  234941\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   5\\nLatest checkpoint's oldestActiveXID:  3739416\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 16489\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Mon 01 Jul 2024 01:39:48 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              1000\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           2000\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            cb3f9f6c33cfd47dd626988e8ae23202776f4f80fae330eef968882da2143f71\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"2024-07-01 16:03:22.269 UTC [35] LOG:  number of prepared transactions has not been configured, overriding\",\"pipe\":\"stderr\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"2024-07-01 16:03:22.269 UTC [35] DETAIL:  max_prepared_transactions is now set to 2000\",\"pipe\":\"stderr\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"core-royalties-pg16\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"core-royalties-pg16\",\"reconcileID\":\"a284d17d-18a2-4c82-b595-323209904e09\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"2024-07-01 16:03:22.296 UTC [35] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"2024-07-01 16:03:22.296 UTC [35] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"2024-07-01 16:03:22.296 UTC [35] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"2024-07-01 16:03:22.296 UTC [35] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:22.296 UTC\",\"process_id\":\"35\",\"session_id\":\"6682d34a.23\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:22.296 UTC\",\"process_id\":\"35\",\"session_id\":\"6682d34a.23\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 16.3 (Debian 16.3-1.pgdg120+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:22.296 UTC\",\"process_id\":\"35\",\"session_id\":\"6682d34a.23\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:22.296 UTC\",\"process_id\":\"35\",\"session_id\":\"6682d34a.23\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:22.297 UTC\",\"process_id\":\"35\",\"session_id\":\"6682d34a.23\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:22.299 UTC\",\"process_id\":\"39\",\"session_id\":\"6682d34a.27\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was interrupted; last known up at 2024-07-01 13:39:49 UTC\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000015.history\",\"startTime\":\"2024-07-01T16:03:22Z\",\"endTime\":\"2024-07-01T16:03:22Z\",\"elapsedWalTime\":0.31691575}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000015.history\",\"maxParallel\":4,\"successfulWalRestore\":1,\"failedWalRestore\":3,\"endOfWALStream\":false,\"startTime\":\"2024-07-01T16:03:22Z\",\"downloadStartTime\":\"2024-07-01T16:03:22Z\",\"downloadTotalTime\":0.317032751,\"totalTime\":0.365500254}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:22.695 UTC\",\"process_id\":\"39\",\"session_id\":\"6682d34a.27\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000015.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:22Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000016.history\",\"options\":[\"--endpoint-url\",\"<redacted>\",\"--cloud-provider\",\"aws-s3\",\"s3://db-core-royalties-wal/cnpg/core-royalties-pg16/\",\"core-royalties-pg16\"],\"startTime\":\"2024-07-01T16:03:22Z\",\"endTime\":\"2024-07-01T16:03:22Z\",\"elapsedWalTime\":0.228545702}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:23.011 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"84\",\"connection_from\":\"[local]\",\"session_id\":\"6682d34b.54\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-01 16:03:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:23.081 UTC\",\"process_id\":\"39\",\"session_id\":\"6682d34a.27\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"entering standby mode\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:23.081 UTC\",\"process_id\":\"39\",\"session_id\":\"6682d34a.27\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting backup recovery with redo LSN 19AA/12000028, checkpoint LSN 19AA/12000060, on timeline ID 20\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:23.303 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"108\",\"connection_from\":\"[local]\",\"session_id\":\"6682d34b.6c\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-01 16:03:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"core-royalties-pg16\",\"namespace\":\"db\"},\"namespace\":\"db\",\"name\":\"core-royalties-pg16\",\"reconcileID\":\"395b52c2-e5b3-4193-8d79-b1bc912a22e1\",\"logging_pod\":\"core-royalties-pg16-9\",\"err\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is starting up (SQLSTATE 57P03)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:23.304 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"109\",\"connection_from\":\"[local]\",\"session_id\":\"6682d34b.6d\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-01 16:03:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000015.history\",\"startTime\":\"2024-07-01T16:03:23Z\",\"endTime\":\"2024-07-01T16:03:23Z\",\"elapsedWalTime\":0.221317728}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000015.history\",\"maxParallel\":4,\"successfulWalRestore\":1,\"failedWalRestore\":3,\"endOfWALStream\":false,\"startTime\":\"2024-07-01T16:03:23Z\",\"downloadStartTime\":\"2024-07-01T16:03:23Z\",\"downloadTotalTime\":0.22145703,\"totalTime\":0.268732854}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:23.362 UTC\",\"process_id\":\"39\",\"session_id\":\"6682d34a.27\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000015.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:23.414 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"131\",\"connection_from\":\"[local]\",\"session_id\":\"6682d34b.83\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-01 16:03:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:23.648 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"136\",\"connection_from\":\"[local]\",\"session_id\":\"6682d34b.88\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-07-01 16:03:23 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:23Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000015000019AA00000012\",\"options\":[\"--endpoint-url\",\"<redacted>\",\"--cloud-provider\",\"aws-s3\",\"s3://db-core-royalties-wal/cnpg/core-royalties-pg16/\",\"core-royalties-pg16\"],\"startTime\":\"2024-07-01T16:03:23Z\",\"endTime\":\"2024-07-01T16:03:23Z\",\"elapsedWalTime\":0.277760957}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000014000019AA00000015\",\"startTime\":\"2024-07-01T16:03:23Z\",\"endTime\":\"2024-07-01T16:03:24Z\",\"elapsedWalTime\":0.27732838}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000014000019AA00000014\",\"startTime\":\"2024-07-01T16:03:23Z\",\"endTime\":\"2024-07-01T16:03:24Z\",\"elapsedWalTime\":0.299597147}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000014000019AA00000013\",\"startTime\":\"2024-07-01T16:03:23Z\",\"endTime\":\"2024-07-01T16:03:24Z\",\"elapsedWalTime\":0.300367211}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000014000019AA00000012\",\"startTime\":\"2024-07-01T16:03:23Z\",\"endTime\":\"2024-07-01T16:03:24Z\",\"elapsedWalTime\":0.313373012}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"core-royalties-pg16-9\",\"walName\":\"00000014000019AA00000012\",\"maxParallel\":4,\"successfulWalRestore\":4,\"failedWalRestore\":0,\"endOfWALStream\":false,\"startTime\":\"2024-07-01T16:03:23Z\",\"downloadStartTime\":\"2024-07-01T16:03:23Z\",\"downloadTotalTime\":0.313483181,\"totalTime\":0.360317842}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:24.169 UTC\",\"process_id\":\"39\",\"session_id\":\"6682d34a.27\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000014000019AA00000012\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:24.176 UTC\",\"process_id\":\"39\",\"session_id\":\"6682d34a.27\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"XX000\",\"message\":\"requested timeline 21 is not a child of this server's history\",\"detail\":\"Latest checkpoint is at 19AA/14000060 on timeline 20, but in the history of the requested timeline, the server forked off from that timeline at 18FC/2E000110.\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:24.177 UTC\",\"process_id\":\"35\",\"session_id\":\"6682d34a.23\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 39) exited with exit code 1\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:24.177 UTC\",\"process_id\":\"35\",\"session_id\":\"6682d34a.23\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"aborting startup due to startup process failure\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"core-royalties-pg16-9\",\"record\":{\"log_time\":\"2024-07-01 16:03:24.180 UTC\",\"process_id\":\"35\",\"session_id\":\"6682d34a.23\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-07-01 16:03:22 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"core-royalties-pg16-9\",\"reason\":\"postmaster has exited\"}\r\n{\"level\":\"error\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"PostgreSQL process exited with errors\",\"logging_pod\":\"core-royalties-pg16-9\",\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:106\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/manager/runnable_group.go:226\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Stopping and waiting for leader election runnables\"}\r\n{\"level\":\"error\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"error received after stop sequence was engaged\",\"error\":\"exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).engageStopProcedure.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/manager/internal.go:499\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"core-royalties-pg16-9\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"core-royalties-pg16-9\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"core-royalties-pg16-9\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"core-royalties-pg16-9\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Stopping and waiting for caches\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Stopping and waiting for webhooks\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Stopping and waiting for HTTP servers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-01T16:03:24Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7342071844302868515\\nDatabase cluster state:               in production\\npg_control last modified:             Mon 01 Jul 2024 01:39:49 PM UTC\\nLatest checkpoint location:           19AA/14000060\\nLatest checkpoint's REDO location:    19AA/14000028\\nLatest checkpoint's REDO WAL file:    00000014000019AA00000014\\nLatest checkpoint's TimeLineID:       20\\nLatest checkpoint's PrevTimeLineID:   20\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:3739416\\nLatest checkpoint's NextOID:          185538614\\nLatest checkpoint's NextMultiXactId:  75910\\nLatest checkpoint's NextMultiOffset:  234941\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   5\\nLatest checkpoint's oldestActiveXID:  3739416\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 16489\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Mon 01 Jul 2024 01:39:48 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              1000\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           2000\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            cb3f9f6c33cfd47dd626988e8ae23202776f4f80fae330eef968882da2143f71\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"core-royalties-pg16-9\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct@gbartolini \r\nHaving the same problem... master is working correctly but I can't bring up new replica\n---\nThis has been plaguing me for several months as well.\r\nWhat makes this really frustrating for me is that CNPG is quite twitch-y in how it treats replicas, so I've found that even a few seconds of downtime (say, unplugging the ethernet cable on a node with a CNPG-managed Postgres replica) will trigger this issue, and it won't recover without manual intervention.\r\nThe manual intervention that I found to work is to delete the pod + PVC. Except this morning this issue has triggered again, and now that trick isn't working - the replicas are reliably coming up in this crashloop state.\n---\nAlso, I think this issue might be a dupe of #4188 ?\n---\n+1 \nran into the same issue with a replica"
    },
    {
        "title": "[Feature]: Add suffix to default service account name",
        "id": 2381003984,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nOn cluster deployment the operator creates a number of resources, all of common resource types have suffixes added to the name (e.g. svc cluster-rw, cluster-r, numbers at the ends of podnames, etc...), _except_ for the service account, which just uses the cluster name.  This opens up a huge possibility for unexpected conflicts and overwriting of existing service accounts without any notice that it's happened, and can result in all kinds of unexpected problems.\r\n### Describe the solution you'd like\r\nI'd like to have the service account that gets created at cluster deployment to have some kind of suffix appended to the end of the name so that it reduces the chances of inadvertently overwriting any explicitly created service account related to other parts of the same project.\r\n### Describe alternatives you've considered\r\nSome sort of deployment alert that can be thrown by the operator that the service account already exists and/or a refusal to override the Service Account without explicit override permission in the cluster manifest or something.\r\nAlternatively, the ability to specify the service account name on the cluster manifest could also work, but ultimately, my preference would be that any common k8s resources created by the operator would have some id, suffix, or other designation added to distinguish them as pieces created on the backend by the operator.\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nNo\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nOn cluster deployment the operator creates a number of resources, all of common resource types have suffixes added to the name (e.g. svc cluster-rw, cluster-r, numbers at the ends of podnames, etc...), _except_ for the service account, which just uses the cluster name.  This opens up a huge possibility for unexpected conflicts and overwriting of existing service accounts without any notice that it's happened, and can result in all kinds of unexpected problems.\r\n### Describe the solution you'd like\r\nI'd like to have the service account that gets created at cluster deployment to have some kind of suffix appended to the end of the name so that it reduces the chances of inadvertently overwriting any explicitly created service account related to other parts of the same project.\r\n### Describe alternatives you've considered\r\nSome sort of deployment alert that can be thrown by the operator that the service account already exists and/or a refusal to override the Service Account without explicit override permission in the cluster manifest or something.\r\nAlternatively, the ability to specify the service account name on the cluster manifest could also work, but ultimately, my preference would be that any common k8s resources created by the operator would have some id, suffix, or other designation added to distinguish them as pieces created on the backend by the operator.\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nNo\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: postgres lifecycle manager may ignore a failure to apply configuration",
        "id": 2377568782,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njaime.silvela@enterprisedb.com\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nAfter recent refactorings of the instance manager lifecycle manager, the \"runAndWait\" function has a goroutine to apply permissions, and a failure in it will have no effect\r\n``` go\r\nfunc (i *PostgresLifecycle) runPostgresAndWait(ctx context.Context) <-chan error {\r\n\t... snipped ...\r\n\trunPostmasterSession := func() error {\r\n\t\tgo func() {\r\n\t\t\tdefer wg.Done()\r\n\t\t\tif err := configureInstancePermissions(postgresContext, i.instance); err != nil {\r\n\t\t\t\tcontextLogger.Error(err, \"Unable to update PostgreSQL roles and permissions\")\r\n\t\t\t}\r\n\t\t}()\r\n```\r\nThere are several operations that may fail inside that `configureInstancePermission` function.\r\nSome of them may be temporary in nature, like availability of a superuser DB connection.\r\nAnd more seriously, either of these could also fail.\r\n``` go\r\n\thasSuperuser, err := configureStreamingReplicaUser(tx)\r\n\t... snipped ...\r\n\terr = configurePgRewindPrivileges(majorVersion, hasSuperuser, tx)\r\n```\r\nWe should not have a whole section of the lifecycle that could fail silently.\r\nThe refactoring of the lifecycle manager has been done in response to finding some cases where two postmaster processes could coexist, one of them shutting down, the other coming up.\r\nThe could should be redesigned to ensure only one goroutine running `postmasterExitStatus := streamingCmd.Wait()`\r\nmay exist at any given time.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njaime.silvela@enterprisedb.com\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nAfter recent refactorings of the instance manager lifecycle manager, the \"runAndWait\" function has a goroutine to apply permissions, and a failure in it will have no effect\r\n``` go\r\nfunc (i *PostgresLifecycle) runPostgresAndWait(ctx context.Context) <-chan error {\r\n\t... snipped ...\r\n\trunPostmasterSession := func() error {\r\n\t\tgo func() {\r\n\t\t\tdefer wg.Done()\r\n\t\t\tif err := configureInstancePermissions(postgresContext, i.instance); err != nil {\r\n\t\t\t\tcontextLogger.Error(err, \"Unable to update PostgreSQL roles and permissions\")\r\n\t\t\t}\r\n\t\t}()\r\n```\r\nThere are several operations that may fail inside that `configureInstancePermission` function.\r\nSome of them may be temporary in nature, like availability of a superuser DB connection.\r\nAnd more seriously, either of these could also fail.\r\n``` go\r\n\thasSuperuser, err := configureStreamingReplicaUser(tx)\r\n\t... snipped ...\r\n\terr = configurePgRewindPrivileges(majorVersion, hasSuperuser, tx)\r\n```\r\nWe should not have a whole section of the lifecycle that could fail silently.\r\nThe refactoring of the lifecycle manager has been done in response to finding some cases where two postmaster processes could coexist, one of them shutting down, the other coming up.\r\nThe could should be redesigned to ensure only one goroutine running `postmasterExitStatus := streamingCmd.Wait()`\r\nmay exist at any given time.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: Relica get's primary before being fully replicated",
        "id": 2377395319,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ngithub@jan-jansen.net\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\n1. Setup Cluster(2 node) without import section and this writes into a bucket\r\n2. Realize import was missing and now bucket already includes\r\n3. Create New Cluster(2 node) with import section and using the same bucket\r\n4. Realize cluster doesn't writes into non empty\r\n5. Update to max_connection and change the bucket name to an empty path\r\n6. Replica get's killed and Primary starts writing into bucket\r\n7. Replica is ready and Primary get's killed which takes some time\r\n8. During this time data is upload from primary to the bucket and replica did start replicate from bucket\r\n9. Primary is killed and Replica get's primary\r\n10. Now i saw replica doesn't include all data\r\n11. I killed the new Cluster started again.\n### Cluster resource\n_No response_\n### Relevant log output\n```shell\nlogs can be provided via email.\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ngithub@jan-jansen.net\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\n1. Setup Cluster(2 node) without import section and this writes into a bucket\r\n2. Realize import was missing and now bucket already includes\r\n3. Create New Cluster(2 node) with import section and using the same bucket\r\n4. Realize cluster doesn't writes into non empty\r\n5. Update to max_connection and change the bucket name to an empty path\r\n6. Replica get's killed and Primary starts writing into bucket\r\n7. Replica is ready and Primary get's killed which takes some time\r\n8. During this time data is upload from primary to the bucket and replica did start replicate from bucket\r\n9. Primary is killed and Replica get's primary\r\n10. Now i saw replica doesn't include all data\r\n11. I killed the new Cluster started again.\n### Cluster resource\n_No response_\n### Relevant log output\n```shell\nlogs can be provided via email.\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductRe: item 1 - import is optional, the cluster is started and the archive is written. So up to item 4 it is a user problem, which is documented. You should simply erase the content in the bucket and restart the operation.\r\nIn my opinion we should not complicate the behaviour of the operator to improve this - which IMHO it is an initialization error which makes the cluster invalid and requires full rebuild.\r\nI think that once we have CNPG-I with some WAL archiving examples, this might be slightly improved, but I would avoid improving this in the current codebase. What do other maintainers think?\n---\nI saw the `cnpg.io/skipEmptyWalArchiveCheck` annotation and wasn't sure if it would clear the wal archive. It might help to describe that you can clear wal if you see this problem.\r\n see two solutions, don't mark the cluster ready for production workload until initialize is done with the managed services. Or add a line to the production guide what to check before letting production workload on the database."
    },
    {
        "title": "[Feature]: decouple metrics scraping from metrics recomputation",
        "id": 2375903767,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nSeveral metrics, including user defined metrics, are run each time the prometheus metrics endpoint 9187, is scraped. Depending on the configuration of Prometheus, and even perhaps just having some other process doing a GET on :9187/metrics, those SQL queries might run often.\r\nIf a user included some long running queries for monitoring, this could be a CPU hog and even pose a threat to the normal functioning of their cluster.\r\n### Describe the solution you'd like\r\nGenerally, to respond to a GET on the metrics port, we should simply return the current values of the various prometheus counters, histograms etc.\r\nA few select metrics might warrant a refresh on each scrape.\r\nThe recomputation of metrics should happen independently of scrapes.\r\nThe refresh interval should be configurable, and if not set default to something reasonable, e.g. 10 seconds.\r\n``` go\r\ntype MonitoringConfiguration struct {\r\n\t// Default interval for metrics to be recomputed\r\n\tDefaultRefreshInterval time.Duration `json:\"defaultRefreshInterval\"`\r\n```\r\nCreate one or more goroutines where the various counters, gauges etc. get recomputed.\r\nIn addition, enable configuration of refresh rate per metric.\r\nAdd a new field to the UserQuery type (in `pkg/management/postgres/metrics/parser.go`)\r\nCalled RefreshInterval.\r\n``` go\r\ntype UserQuery struct {\r\n\tQuery           string        `yaml:\"query\"`\r\n\tPredicateQuery  string        `yaml:\"predicate_query\"`\r\n\tRefreshInterval time.Duration `yaml:\"refreshInterval\"`\r\n\tMetrics         []Mapping     `yaml:\"metrics\"`\r\n\t...    \r\n}\r\n```\r\nWhich, in the YAML spec, could look like\r\n``` yaml\r\n    pg_database:\r\n      query: |\r\n        SELECT datname\r\n          , pg_catalog.pg_database_size(datname) AS size_bytes\r\n          , pg_catalog.age(datfrozenxid) AS xid_age\r\n          , pg_catalog.mxid_age(datminmxid) AS mxid_age\r\n        FROM pg_catalog.pg_database\r\n      refreshInteval: 15m\r\n      metrics:\r\n        - datname:\r\n            usage: \"LABEL\"\r\n            description: \"Name of the database\"\r\n```\r\nWhen doing the metrics collection in response to an HTTP metrics scrape, the existing values for the Counter/Gauge/etc should be returned immediately.\r\nA goroutine would take care of refreshing the values by running the specified queries at the expected intervals.\r\n### Describe alternatives you've considered\r\nThe prometheus HTTP library has a parameter `MaxRequestsInFlight` which, if set will respond with an HTTP 503 if exceeded.\r\nThat could be a very low effort low resolution safety mechanism.\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nSeveral metrics, including user defined metrics, are run each time the prometheus metrics endpoint 9187, is scraped. Depending on the configuration of Prometheus, and even perhaps just having some other process doing a GET on :9187/metrics, those SQL queries might run often.\r\nIf a user included some long running queries for monitoring, this could be a CPU hog and even pose a threat to the normal functioning of their cluster.\r\n### Describe the solution you'd like\r\nGenerally, to respond to a GET on the metrics port, we should simply return the current values of the various prometheus counters, histograms etc.\r\nA few select metrics might warrant a refresh on each scrape.\r\nThe recomputation of metrics should happen independently of scrapes.\r\nThe refresh interval should be configurable, and if not set default to something reasonable, e.g. 10 seconds.\r\n``` go\r\ntype MonitoringConfiguration struct {\r\n\t// Default interval for metrics to be recomputed\r\n\tDefaultRefreshInterval time.Duration `json:\"defaultRefreshInterval\"`\r\n```\r\nCreate one or more goroutines where the various counters, gauges etc. get recomputed.\r\nIn addition, enable configuration of refresh rate per metric.\r\nAdd a new field to the UserQuery type (in `pkg/management/postgres/metrics/parser.go`)\r\nCalled RefreshInterval.\r\n``` go\r\ntype UserQuery struct {\r\n\tQuery           string        `yaml:\"query\"`\r\n\tPredicateQuery  string        `yaml:\"predicate_query\"`\r\n\tRefreshInterval time.Duration `yaml:\"refreshInterval\"`\r\n\tMetrics         []Mapping     `yaml:\"metrics\"`\r\n\t...    \r\n}\r\n```\r\nWhich, in the YAML spec, could look like\r\n``` yaml\r\n    pg_database:\r\n      query: |\r\n        SELECT datname\r\n          , pg_catalog.pg_database_size(datname) AS size_bytes\r\n          , pg_catalog.age(datfrozenxid) AS xid_age\r\n          , pg_catalog.mxid_age(datminmxid) AS mxid_age\r\n        FROM pg_catalog.pg_database\r\n      refreshInteval: 15m\r\n      metrics:\r\n        - datname:\r\n            usage: \"LABEL\"\r\n            description: \"Name of the database\"\r\n```\r\nWhen doing the metrics collection in response to an HTTP metrics scrape, the existing values for the Counter/Gauge/etc should be returned immediately.\r\nA goroutine would take care of refreshing the values by running the specified queries at the expected intervals.\r\n### Describe alternatives you've considered\r\nThe prometheus HTTP library has a parameter `MaxRequestsInFlight` which, if set will respond with an HTTP 503 if exceeded.\r\nThat could be a very low effort low resolution safety mechanism.\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: restore selected databases",
        "id": 2373866433,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\ncurrently its not possbile to just select a list of dbs to restored from and let the not selected dbs untouched. \r\n### Describe the solution you'd like\r\na list of dbs that should be restored\r\n### Describe alternatives you've considered\r\nmanually using a dump after the restore with the current data, not a good solution\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nN/A\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\ncurrently its not possbile to just select a list of dbs to restored from and let the not selected dbs untouched. \r\n### Describe the solution you'd like\r\na list of dbs that should be restored\r\n### Describe alternatives you've considered\r\nmanually using a dump after the restore with the current data, not a good solution\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nN/A\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "feat: leverage pgbouncer's auth_dbname",
        "id": 2370031901,
        "state": "open",
        "first": "Fixes #1383.\r\nAn initial draft implementation to discuss",
        "messages": "Fixes #1383.\r\nAn initial draft implementation to discuss/test limit=local"
    },
    {
        "title": "[Feature]: Allow users to control TLS verification mode in inter-pod communication",
        "id": 2369607727,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.23.2\r\n### What version of Kubernetes are you using?\r\n1.30 (unsupported)\r\n### What is your Kubernetes environment?\r\nSelf-managed: kind (evaluation)\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\nCurrently, internal communication between pods is verified using CA only because it is not guaranteed that the server certificate contains the name of the cluster-rw service.\r\nThis should be under the user's control, allowing full verification if the server certificate supports it. \r\n### Cluster resource\r\n_No response_\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.23.2\r\n### What version of Kubernetes are you using?\r\n1.30 (unsupported)\r\n### What is your Kubernetes environment?\r\nSelf-managed: kind (evaluation)\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\nCurrently, internal communication between pods is verified using CA only because it is not guaranteed that the server certificate contains the name of the cluster-rw service.\r\nThis should be under the user's control, allowing full verification if the server certificate supports it. \r\n### Cluster resource\r\n_No response_\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductThis is not a bug. It's a feature and should not be backported."
    },
    {
        "title": "[Bug]: Operator deploy in multiple namespace",
        "id": 2365816582,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.30 (unsupported)\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhen I try to deploy operator in multiple namespace it was causing issue. When I deploy the second operator the first operator started crashing \r\ncnpg-controller-manager-58c75cd9fd-msm4k   0/1     CrashLoopBackOff   21 (4s ago)   17d\r\n### Cluster resource\n```shell\nTry to deploy the operator in 2 different namespace of same k8s cluster\n```\n### Relevant log output\n```shell\ncnpg-controller-manager-58c75cd9fd-msm4k   0/1     CrashLoopBackOff   21 (4s ago)   17d\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.30 (unsupported)\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhen I try to deploy operator in multiple namespace it was causing issue. When I deploy the second operator the first operator started crashing \r\ncnpg-controller-manager-58c75cd9fd-msm4k   0/1     CrashLoopBackOff   21 (4s ago)   17d\r\n### Cluster resource\n```shell\nTry to deploy the operator in 2 different namespace of same k8s cluster\n```\n### Relevant log output\n```shell\ncnpg-controller-manager-58c75cd9fd-msm4k   0/1     CrashLoopBackOff   21 (4s ago)   17d\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductHello @sujitkagarwal \r\nAre you using OLM to deploy in different namespaces? using the manifest to deploy in more than one namespace is not supported\r\nCheers\n---\nThanks for the update."
    },
    {
        "title": "[Bug]: changes in backup related secrets are not reloaded automatically",
        "id": 2362372801,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.30 (unsupported)\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nIf the s3 credentials secret is not available at cluster startup, you get errors in the primary instance log.\r\n```\r\n{\"level\":\"error\",\"ts\":\"2024-06-19T12:08:28Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"cluster-example-1\",\"error\":\"failed to get envs: cache miss\",\"stacktrace\":\"...\"}\r\n```\r\nWhen you create the secret, the instance manager doesn't load it until you force a reconciliation manually, regardless of whether you add the \"cnpg.io/reload\" label.\r\nThe same issue happens if you update the password in the secret, for example, to rotate AWS credentials.\n### Cluster resource\n```shell\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-example\r\nspec:\r\n  instances: 1\r\n  storage:\r\n    size: 1Gi\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3://backups/\r\n      endpointURL: http://minio:9000\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: minio\r\n          key: ACCESS_KEY_ID\r\n        secretAccessKey:\r\n          name: minio\r\n          key: ACCESS_SECRET_KEY\r\n      data:\r\n        immediateCheckpoint: true\r\n---\r\napiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: minio\r\nspec:\r\n  accessModes:\r\n    - ReadWriteOnce\r\n  volumeMode: Filesystem\r\n  resources:\r\n    requests:\r\n      storage: 1Gi\r\n---\r\napiVersion: v1\r\ndata:\r\n  ACCESS_KEY_ID: Y2hvb0plaXJvcm9vMm5vcXVvbWVpMnV1Y2Vpc2hldGg=\r\n  ACCESS_SECRET_KEY: b25nZWlxdWVpdG9oTDBxdWVlTG9oa2l1cjJxdWFpbmc=\r\nkind: Secret\r\nmetadata:\r\n  labels:\r\n    cnpg.io/reload: \"\"\r\n  name: minio\r\n---\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: minio\r\nspec:\r\n  selector:\r\n    app: minio\r\n  ports:\r\n    - protocol: TCP\r\n      port: 9000\r\n      targetPort: 9000\r\n---\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: minio\r\n  labels:\r\n    app: minio\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: minio\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: minio\r\n    spec:\r\n      containers:\r\n      - name: minio\r\n        image: minio/minio\r\n        ports:\r\n        - containerPort: 9000\r\n        volumeMounts:\r\n        - name: data\r\n          mountPath: /data\r\n        args:\r\n        - server\r\n        - /data\r\n        env:\r\n        - name: MINIO_ACCESS_KEY\r\n          valueFrom:\r\n            secretKeyRef: \r\n              name: minio\r\n              key: ACCESS_KEY_ID\r\n        - name: MINIO_SECRET_KEY\r\n          valueFrom:\r\n            secretKeyRef: \r\n              name: minio\r\n              key: ACCESS_SECRET_KEY\r\n      volumes:\r\n      - name: data\r\n        persistentVolumeClaim:\r\n          claimName: minio\n```\n### Relevant log output\n```shell\n{\"level\":\"error\",\"ts\":\"2024-06-19T12:08:28Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"cluster-example-1\",\"error\":\"failed to get envs: cache miss\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:95\\ngithub.com/spf13/cobra.(*Command).execute\\n\\t/Users/mnencia/go/pkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\t/Users/mnencia/go/pkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\t/Users/mnencia/go/pkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/Users/mnencia/.asdf/installs/golang/1.22.4/go/src/runtime/proc.go:271\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-19T12:08:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cluster-example-1\",\"record\":{\"log_time\":\"2024-06-19 12:08:28.551 UTC\",\"process_id\":\"39\",\"session_id\":\"6672c904.27\",\"session_line_num\":\"38\",\"session_start_time\":\"2024-06-19 12:03:16 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.2\n### What version of Kubernetes are you using?\n1.30 (unsupported)\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nIf the s3 credentials secret is not available at cluster startup, you get errors in the primary instance log.\r\n```\r\n{\"level\":\"error\",\"ts\":\"2024-06-19T12:08:28Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"cluster-example-1\",\"error\":\"failed to get envs: cache miss\",\"stacktrace\":\"...\"}\r\n```\r\nWhen you create the secret, the instance manager doesn't load it until you force a reconciliation manually, regardless of whether you add the \"cnpg.io/reload\" label.\r\nThe same issue happens if you update the password in the secret, for example, to rotate AWS credentials.\n### Cluster resource\n```shell\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-example\r\nspec:\r\n  instances: 1\r\n  storage:\r\n    size: 1Gi\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3://backups/\r\n      endpointURL: http://minio:9000\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: minio\r\n          key: ACCESS_KEY_ID\r\n        secretAccessKey:\r\n          name: minio\r\n          key: ACCESS_SECRET_KEY\r\n      data:\r\n        immediateCheckpoint: true\r\n---\r\napiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: minio\r\nspec:\r\n  accessModes:\r\n    - ReadWriteOnce\r\n  volumeMode: Filesystem\r\n  resources:\r\n    requests:\r\n      storage: 1Gi\r\n---\r\napiVersion: v1\r\ndata:\r\n  ACCESS_KEY_ID: Y2hvb0plaXJvcm9vMm5vcXVvbWVpMnV1Y2Vpc2hldGg=\r\n  ACCESS_SECRET_KEY: b25nZWlxdWVpdG9oTDBxdWVlTG9oa2l1cjJxdWFpbmc=\r\nkind: Secret\r\nmetadata:\r\n  labels:\r\n    cnpg.io/reload: \"\"\r\n  name: minio\r\n---\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: minio\r\nspec:\r\n  selector:\r\n    app: minio\r\n  ports:\r\n    - protocol: TCP\r\n      port: 9000\r\n      targetPort: 9000\r\n---\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: minio\r\n  labels:\r\n    app: minio\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: minio\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: minio\r\n    spec:\r\n      containers:\r\n      - name: minio\r\n        image: minio/minio\r\n        ports:\r\n        - containerPort: 9000\r\n        volumeMounts:\r\n        - name: data\r\n          mountPath: /data\r\n        args:\r\n        - server\r\n        - /data\r\n        env:\r\n        - name: MINIO_ACCESS_KEY\r\n          valueFrom:\r\n            secretKeyRef: \r\n              name: minio\r\n              key: ACCESS_KEY_ID\r\n        - name: MINIO_SECRET_KEY\r\n          valueFrom:\r\n            secretKeyRef: \r\n              name: minio\r\n              key: ACCESS_SECRET_KEY\r\n      volumes:\r\n      - name: data\r\n        persistentVolumeClaim:\r\n          claimName: minio\n```\n### Relevant log output\n```shell\n{\"level\":\"error\",\"ts\":\"2024-06-19T12:08:28Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"cluster-example-1\",\"error\":\"failed to get envs: cache miss\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:95\\ngithub.com/spf13/cobra.(*Command).execute\\n\\t/Users/mnencia/go/pkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\t/Users/mnencia/go/pkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\t/Users/mnencia/go/pkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/Users/mnencia/.asdf/installs/golang/1.22.4/go/src/runtime/proc.go:271\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-19T12:08:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"cluster-example-1\",\"record\":{\"log_time\":\"2024-06-19 12:08:28.551 UTC\",\"process_id\":\"39\",\"session_id\":\"6672c904.27\",\"session_line_num\":\"38\",\"session_start_time\":\"2024-06-19 12:03:16 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductAs a workaround for people who run into this:\r\n**issue:** After rotating AWS key, the old key is still being used\r\n**workaround:** Do a rolling restart of your cnpg database cluster by using `cnpg promote <clustername> <db>` once for every node in your db cluster. Don't forget to end back at the first one.\r\nTwo node cluster assuming db-1 is current primary:\r\n```\r\ncnpg promote db db-2\r\ncnpg promote db db-1\r\n```\r\nThree node cluster assuming db-1 is current primary:\r\n```\r\ncnpg promote db db-2\r\ncnpg promote db db-3\r\ncnpg promote db db-1\r\n``` \r\nMaybe a cnpg dev can chime in if there is a simpler way.\n---\nWe are also running into this error in cloudnative-pg:1.24.1\r\n```\r\n{\"level\":\"error\",\"ts\":\"2024-10-23T11:19:40.639952284Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"mastodon-postgres-1\",\"error\":\"failed to get envs: cache miss\",\"stacktrace\":\"github.com/cloudnative-pg/machinery/pkg/log.(*logger).Error\\n\\tpkg/mod/github.com/cloudnative-pg/machinery@v0.0.0-20241014090714-c27747f9974b/pkg/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:90\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:68\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/proc.go:272\"}\r\n```\r\nroot cause seems to be mismatched secret names:\r\n- cluster wants: `postgres-s3-credentials`\r\n- real name: `s3-postgres-credentials`\r\nAfter correcting this and performing the steps listed by @mzhaase the cluster is working as expected:\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-10-23T11:44:08.588556106Z\",\"logger\":\"wal-archive\",\"msg\":\"Archived WAL file\",\"logging_pod\":\"mastodon-postgres-1\",\"walName\":\"pg_wal/0000000600000000000000AA.partial\",\"startTime\":\"2024-10-23T11:44:08.234541238Z\",\"endTime\":\"2024-10-23T11:44:08.588531683Z\",\"elapsedWalTime\":0.35399045}\r\n```\n---\n@mnencia I think this was already fixed, can you confirm?"
    },
    {
        "title": "[Bug]: NodeSelector don't work in Pooler affinity spec ",
        "id": 2361582681,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.23.2\r\n### What version of Kubernetes are you using?\r\n1.27\r\n### What is your Kubernetes environment?\r\nCloud: Other\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\nHi, we noticed a bug that if you specify a nodeSelector in the affinity section for a DB instance, it works, but for a pooler instance, a nodeSelector is not added to the deployment\r\nWe can use NodeAffinity, but the question is why the db instance uses the node selector (in the affinity section), but the pooler does not\r\n### Cluster resource\r\n```shell\r\n#Affinity Spec Cluster\r\ncluster:\r\n  affinity:                                                                                                                                                                                                                                                                                  \r\n    nodeSelector:                                                                                                                                                                                                                                                                             \r\n      app: allow                                                                                                                                                                                                                                                                            \r\n    podAntiAffinityType: preferred                                                                                                                                                                                                                                                          \r\n    topologyKey: kubernetes.io/hostname\r\n#Affinity Spec Pooler\r\npooler:\r\n  affinity:                                                                                                                                                                                                                                                                                  \r\n    nodeSelector:                                                                                                                                                                                                                                                                             \r\n      app: allow                                                                                                                                                                                                                                                                            \r\n    podAntiAffinityType: preferred                                                                                                                                                                                                                                                          \r\n    topologyKey: kubernetes.io/hostname\r\n```\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.23.2\r\n### What version of Kubernetes are you using?\r\n1.27\r\n### What is your Kubernetes environment?\r\nCloud: Other\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\nHi, we noticed a bug that if you specify a nodeSelector in the affinity section for a DB instance, it works, but for a pooler instance, a nodeSelector is not added to the deployment\r\nWe can use NodeAffinity, but the question is why the db instance uses the node selector (in the affinity section), but the pooler does not\r\n### Cluster resource\r\n```shell\r\n#Affinity Spec Cluster\r\ncluster:\r\n  affinity:                                                                                                                                                                                                                                                                                  \r\n    nodeSelector:                                                                                                                                                                                                                                                                             \r\n      app: allow                                                                                                                                                                                                                                                                            \r\n    podAntiAffinityType: preferred                                                                                                                                                                                                                                                          \r\n    topologyKey: kubernetes.io/hostname\r\n#Affinity Spec Pooler\r\npooler:\r\n  affinity:                                                                                                                                                                                                                                                                                  \r\n    nodeSelector:                                                                                                                                                                                                                                                                             \r\n      app: allow                                                                                                                                                                                                                                                                            \r\n    podAntiAffinityType: preferred                                                                                                                                                                                                                                                          \r\n    topologyKey: kubernetes.io/hostname\r\n```\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductHello @justlucknb, could you post the full yaml of the applied pooler resource?"
    },
    {
        "title": "[Bug]: Replica cluster monitoring fails if source cluster database is non-default and replica clusters bootstrap stanza does not specify custom database name",
        "id": 2349201544,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nTristan.Otterpohl@Outlook.com\r\n### Version\r\nolder in 1.23.x\r\n### What version of Kubernetes are you using?\r\n1.28\r\n### What is your Kubernetes environment?\r\nOther\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\n## Replicate\r\nSource cluster with non-default `database.initdb` in bootstrap stanza:\r\n```\r\n...\r\nbootstrap:\r\n    initdb:\r\n      database: mydb\r\n      owner: myuser\r\n...\r\n```\r\nand replica cluster with defaulted `database.initdb` config:\r\n```\r\n...\r\nbootstrap:\r\n  recovery:\r\n    source: cluster-source\r\nreplica:\r\n  enabled: true\r\n  source: cluster-source\r\nexternalClusters:\r\n- name: cluster-source\r\n  connectionParameters:\r\n    host: cluster-source-rw.default.svc\r\n    user: myuser\r\n    dbname: mydb\r\n  password:\r\n    name: cluster-source-app\r\n    key: password\r\n...\r\n```\r\nI expect that monitoring would work for the replica cluster\r\n## Error\r\n> Error collecting user query\r\nfailed to connect to `host=/controller/run user=postgres database=app`: server error (FATAL: database \\\u201capp\\\u201d does not exist (SQLSTATE 3D000))\r\n## Cause\r\nThe user query collector tries to connect to the cluster using the database specified in `database.initdb.database`. \r\nThe source cluster has this specified as `mydb` but the replica cluster does not have the `database.initdb` stanza specified since it will be a replica of the source cluster.\r\nSomewhat related to #4640\r\n## Solution\r\nOther than forcing the user to specify the database name in both clusters, which is error prone and contrived, we could have: \r\nThe user query collector connect using a built in database, potentially `postgres`.\r\nThe user query colector connect using the database specified in the source cluster.\r\n### Cluster resource\r\n_No response_\r\n### Relevant log output\r\n```shell\r\nfailed to connect to `host=/controller/run user=postgres database=app`: server error (FATAL: database \\\u201capp\\\u201d does not exist (SQLSTATE 3D000))\r\nstacktrace\u201d:\u201c[github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error](http://github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error)\r\n  pkg/management/log/log.go:125\r\n[github.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/metrics.(*QueriesCollector).collectUserQueries](http://github.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/metrics.(*QueriesCollector).collectUserQueries)\r\n  pkg/management/postgres/metrics/collector.go:131\r\n[github.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/metrics.QueriesCollector.Collect](http://github.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/metrics.QueriesCollector.Collect)\r\n  pkg/management/postgres/metrics/collector.go:64\r\n[github.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics](http://github.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics)\r\n  pkg/management/postgres/webserver/metricserver/pg_collector.go:369\r\n[github.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect](http://github.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect)\r\n  pkg/management/postgres/webserver/metricserver/pg_collector.go:298\r\n[github.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1](http://github.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1)\r\n  pkg/mod/github.com/prometheus/client_golang@v1.19.0/prometheus/registry.go:455\u201d}\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nTristan.Otterpohl@Outlook.com\r\n### Version\r\nolder in 1.23.x\r\n### What version of Kubernetes are you using?\r\n1.28\r\n### What is your Kubernetes environment?\r\nOther\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\n## Replicate\r\nSource cluster with non-default `database.initdb` in bootstrap stanza:\r\n```\r\n...\r\nbootstrap:\r\n    initdb:\r\n      database: mydb\r\n      owner: myuser\r\n...\r\n```\r\nand replica cluster with defaulted `database.initdb` config:\r\n```\r\n...\r\nbootstrap:\r\n  recovery:\r\n    source: cluster-source\r\nreplica:\r\n  enabled: true\r\n  source: cluster-source\r\nexternalClusters:\r\n- name: cluster-source\r\n  connectionParameters:\r\n    host: cluster-source-rw.default.svc\r\n    user: myuser\r\n    dbname: mydb\r\n  password:\r\n    name: cluster-source-app\r\n    key: password\r\n...\r\n```\r\nI expect that monitoring would work for the replica cluster\r\n## Error\r\n> Error collecting user query\r\nfailed to connect to `host=/controller/run user=postgres database=app`: server error (FATAL: database \\\u201capp\\\u201d does not exist (SQLSTATE 3D000))\r\n## Cause\r\nThe user query collector tries to connect to the cluster using the database specified in `database.initdb.database`. \r\nThe source cluster has this specified as `mydb` but the replica cluster does not have the `database.initdb` stanza specified since it will be a replica of the source cluster.\r\nSomewhat related to #4640\r\n## Solution\r\nOther than forcing the user to specify the database name in both clusters, which is error prone and contrived, we could have: \r\nThe user query collector connect using a built in database, potentially `postgres`.\r\nThe user query colector connect using the database specified in the source cluster.\r\n### Cluster resource\r\n_No response_\r\n### Relevant log output\r\n```shell\r\nfailed to connect to `host=/controller/run user=postgres database=app`: server error (FATAL: database \\\u201capp\\\u201d does not exist (SQLSTATE 3D000))\r\nstacktrace\u201d:\u201c[github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error](http://github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error)\r\n  pkg/management/log/log.go:125\r\n[github.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/metrics.(*QueriesCollector).collectUserQueries](http://github.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/metrics.(*QueriesCollector).collectUserQueries)\r\n  pkg/management/postgres/metrics/collector.go:131\r\n[github.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/metrics.QueriesCollector.Collect](http://github.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/metrics.QueriesCollector.Collect)\r\n  pkg/management/postgres/metrics/collector.go:64\r\n[github.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics](http://github.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).collectPgMetrics)\r\n  pkg/management/postgres/webserver/metricserver/pg_collector.go:369\r\n[github.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect](http://github.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres/webserver/metricserver.(*Exporter).Collect)\r\n  pkg/management/postgres/webserver/metricserver/pg_collector.go:298\r\n[github.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1](http://github.com/prometheus/client_golang/prometheus.(*Registry).Gather.func1)\r\n  pkg/mod/github.com/prometheus/client_golang@v1.19.0/prometheus/registry.go:455\u201d}\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: pgbouncer missing metrics error with podMonitor enabled",
        "id": 2344139790,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njack@panfactum.com\n### Version\n1.23.1\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Amazon EKS\n### How did you install the operator?\nHelm\n### What happened?\n1. Set `monitoring.enablePodMonitor` to `true` on the `Pooler` resource.\r\n2. Notice the logs linked below.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Pooler\r\nmetadata:\r\n  creationTimestamp: \"2024-06-08T19:08:27Z\"\r\n  generation: 3\r\n  labels:\r\n    id: pg-rooler-rw-pg-2c0d-7f0bb4544f43b0d8\r\n    panfactum.com/environment: production\r\n    panfactum.com/local: \"false\"\r\n    panfactum.com/module: kube_pg_cluster\r\n    panfactum.com/prevent-lifetime-eviction: \"true\"\r\n    panfactum.com/region: us-east-2\r\n    panfactum.com/root-module: kube_authentik\r\n    panfactum.com/stack-commit: local\r\n    panfactum.com/stack-version: local\r\n    panfactum.com/workload: pg-rooler-rw-pg-2c0d\r\n    test.1/2.3.4.5: test.1.2.3.4.5\r\n    test1: foo\r\n    test2: bar\r\n    test3: baz\r\n    test4: \"42\"\r\n  name: pg-2c0d-pooler-rw\r\n  namespace: authentik\r\n  resourceVersion: \"67328507\"\r\n  uid: dc49acff-f9e6-4fb1-afaf-c7134df4d910\r\nspec:\r\n  cluster:\r\n    name: pg-2c0d\r\n  instances: 2\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  pgbouncer:\r\n    authQuery: SELECT usename, passwd FROM user_search($1)\r\n    authQuerySecret:\r\n      name: pg-pooler-certs-8cbb\r\n    parameters:\r\n      application_name_add_host: \"0\"\r\n      autodb_idle_timeout: \"3600\"\r\n      client_idle_timeout: \"0\"\r\n      client_login_timeout: \"60\"\r\n      default_pool_size: \"20\"\r\n      disable_pqexec: \"0\"\r\n      log_connections: \"0\"\r\n      log_disconnections: \"0\"\r\n      log_pooler_errors: \"1\"\r\n      max_client_conn: \"10000\"\r\n      max_db_connections: \"0\"\r\n      max_prepared_statements: \"0\"\r\n      max_user_connections: \"0\"\r\n      min_pool_size: \"0\"\r\n      query_timeout: \"0\"\r\n      query_wait_timeout: \"120\"\r\n      reserve_pool_size: \"0\"\r\n      reserve_pool_timeout: \"5\"\r\n      server_connect_timeout: \"15\"\r\n      server_fast_close: \"0\"\r\n      server_idle_timeout: \"600\"\r\n      server_lifetime: \"3600\"\r\n      server_login_retry: \"15\"\r\n      stats_period: \"60\"\r\n      tcp_keepalive: \"1\"\r\n      tcp_user_timeout: \"0\"\r\n      verbose: \"0\"\r\n    paused: false\r\n    poolMode: transaction\r\n  template:\r\n    metadata:\r\n      annotations:\r\n        linkerd.io/skip-inbound-ports: \"5432\"\r\n      labels:\r\n        id: pg-rooler-rw-pg-2c0d-7f0bb4544f43b0d8\r\n        panfactum.com/environment: production\r\n        panfactum.com/local: \"false\"\r\n        panfactum.com/module: kube_pg_cluster\r\n        panfactum.com/prevent-lifetime-eviction: \"true\"\r\n        panfactum.com/region: us-east-2\r\n        panfactum.com/root-module: kube_authentik\r\n        panfactum.com/stack-commit: local\r\n        panfactum.com/stack-version: local\r\n        panfactum.com/workload: pg-rooler-rw-pg-2c0d\r\n        test.1/2.3.4.5: test.1.2.3.4.5\r\n        test1: foo\r\n        test2: bar\r\n        test3: baz\r\n        test4: \"42\"\r\n    spec:\r\n      affinity:\r\n        nodeAffinity:\r\n          preferredDuringSchedulingIgnoredDuringExecution:\r\n          - preference:\r\n              matchExpressions:\r\n              - key: panfactum.com/class\r\n                operator: In\r\n                values:\r\n                - burstable\r\n            weight: 50\r\n        podAffinity:\r\n          preferredDuringSchedulingIgnoredDuringExecution:\r\n          - podAffinityTerm:\r\n              labelSelector:\r\n                matchLabels:\r\n                  id: pg-pg-2c0d-e46431d0383c5674\r\n              topologyKey: kubernetes.io/hostname\r\n            weight: 100\r\n          requiredDuringSchedulingIgnoredDuringExecution: ]\r\n        podAntiAffinity:\r\n          requiredDuringSchedulingIgnoredDuringExecution:\r\n          - labelSelector:\r\n              matchLabels:\r\n                id: pg-rooler-rw-pg-2c0d-7f0bb4544f43b0d8\r\n            topologyKey: kubernetes.io/hostname\r\n          - labelSelector:\r\n              matchLabels:\r\n                id: pg-rooler-rw-pg-2c0d-7f0bb4544f43b0d8\r\n            topologyKey: node.kubernetes.io/instance-type\r\n      containers:\r\n      - image: 891377197483.dkr.ecr.us-east-2.amazonaws.com/github/cloudnative-pg/pgbouncer:1.22.1\r\n        lifecycle:\r\n          preStop:\r\n            exec:\r\n              command:\r\n              - /bin/sh\r\n              - -c\r\n              - killall -INT pgbouncer && sleep 120\r\n        name: pgbouncer\r\n        resources:\r\n          limits:\r\n            memory: 80Mi\r\n          requests:\r\n            memory: 50Mi\r\n      priorityClassName: database\r\n      tolerations:\r\n      - effect: NoSchedule\r\n        key: spot\r\n        operator: Equal\r\n        value: \"true\"\r\n      - effect: NoSchedule\r\n        key: burstable\r\n        operator: Equal\r\n        value: \"true\"\r\n      - effect: NoSchedule\r\n        key: arm64\r\n        operator: Equal\r\n        value: \"true\"\r\n      topologySpreadConstraints:\r\n      - labelSelector:\r\n          matchLabels:\r\n            id: pg-rooler-rw-pg-2c0d-7f0bb4544f43b0d8\r\n        maxSkew: 1\r\n        topologyKey: topology.kubernetes.io/zone\r\n        whenUnsatisfiable: DoNotSchedule\r\n  type: rw\r\nstatus:\r\n  instances: 2\r\n  secrets:\r\n    clientCA:\r\n      name: pg-client-certs-2003\r\n      version: \"65063758\"\r\n    pgBouncerSecrets:\r\n      authQuery:\r\n        name: pg-pooler-certs-8cbb\r\n        version: \"65063760\"\r\n    serverCA:\r\n      name: pg-server-certs-a710\r\n      version: \"65063762\"\r\n    serverTLS:\r\n      name: pg-server-certs-a710\r\n      version: \"65063762\"\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:31:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:31:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:32:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:32:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:32:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:32:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:33:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:33:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:33:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:33:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:34:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:34:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:34:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:34:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:35:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:35:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:35:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:35:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:36:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:36:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:36:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:36:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:37:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:37:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:37:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:37:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:38:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:38:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:38:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:38:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:39:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:39:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:39:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:39:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:40:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:40:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:40:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:40:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:41:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:41:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:41:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:41:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:42:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:42:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:42:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:42:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:43:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:43:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:43:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:43:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:44:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:44:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:44:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:44:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:45:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:45:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:45:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:45:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:46:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:46:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:46:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:46:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:47:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:47:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:47:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:47:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:48:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:48:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:48:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:48:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:49:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:49:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:49:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:49:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:50:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:50:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:50:30Z\",\"msg\":\"updated configuration file\",\"name\":\"/controller/configs/pgbouncer.ini\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:50:30Z\",\"msg\":\"record\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-06-10 14:50:30.921 UTC\",\"pid\":\"15\",\"level\":\"LOG\",\"msg\":\"RELOAD command issued\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:50:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:50:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:51:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:51:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:51:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:51:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:52:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:52:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:52:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:52:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:53:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:53:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:53:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:53:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:54:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:54:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:54:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:54:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:55:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:55:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:55:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:55:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njack@panfactum.com\n### Version\n1.23.1\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Amazon EKS\n### How did you install the operator?\nHelm\n### What happened?\n1. Set `monitoring.enablePodMonitor` to `true` on the `Pooler` resource.\r\n2. Notice the logs linked below.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Pooler\r\nmetadata:\r\n  creationTimestamp: \"2024-06-08T19:08:27Z\"\r\n  generation: 3\r\n  labels:\r\n    id: pg-rooler-rw-pg-2c0d-7f0bb4544f43b0d8\r\n    panfactum.com/environment: production\r\n    panfactum.com/local: \"false\"\r\n    panfactum.com/module: kube_pg_cluster\r\n    panfactum.com/prevent-lifetime-eviction: \"true\"\r\n    panfactum.com/region: us-east-2\r\n    panfactum.com/root-module: kube_authentik\r\n    panfactum.com/stack-commit: local\r\n    panfactum.com/stack-version: local\r\n    panfactum.com/workload: pg-rooler-rw-pg-2c0d\r\n    test.1/2.3.4.5: test.1.2.3.4.5\r\n    test1: foo\r\n    test2: bar\r\n    test3: baz\r\n    test4: \"42\"\r\n  name: pg-2c0d-pooler-rw\r\n  namespace: authentik\r\n  resourceVersion: \"67328507\"\r\n  uid: dc49acff-f9e6-4fb1-afaf-c7134df4d910\r\nspec:\r\n  cluster:\r\n    name: pg-2c0d\r\n  instances: 2\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  pgbouncer:\r\n    authQuery: SELECT usename, passwd FROM user_search($1)\r\n    authQuerySecret:\r\n      name: pg-pooler-certs-8cbb\r\n    parameters:\r\n      application_name_add_host: \"0\"\r\n      autodb_idle_timeout: \"3600\"\r\n      client_idle_timeout: \"0\"\r\n      client_login_timeout: \"60\"\r\n      default_pool_size: \"20\"\r\n      disable_pqexec: \"0\"\r\n      log_connections: \"0\"\r\n      log_disconnections: \"0\"\r\n      log_pooler_errors: \"1\"\r\n      max_client_conn: \"10000\"\r\n      max_db_connections: \"0\"\r\n      max_prepared_statements: \"0\"\r\n      max_user_connections: \"0\"\r\n      min_pool_size: \"0\"\r\n      query_timeout: \"0\"\r\n      query_wait_timeout: \"120\"\r\n      reserve_pool_size: \"0\"\r\n      reserve_pool_timeout: \"5\"\r\n      server_connect_timeout: \"15\"\r\n      server_fast_close: \"0\"\r\n      server_idle_timeout: \"600\"\r\n      server_lifetime: \"3600\"\r\n      server_login_retry: \"15\"\r\n      stats_period: \"60\"\r\n      tcp_keepalive: \"1\"\r\n      tcp_user_timeout: \"0\"\r\n      verbose: \"0\"\r\n    paused: false\r\n    poolMode: transaction\r\n  template:\r\n    metadata:\r\n      annotations:\r\n        linkerd.io/skip-inbound-ports: \"5432\"\r\n      labels:\r\n        id: pg-rooler-rw-pg-2c0d-7f0bb4544f43b0d8\r\n        panfactum.com/environment: production\r\n        panfactum.com/local: \"false\"\r\n        panfactum.com/module: kube_pg_cluster\r\n        panfactum.com/prevent-lifetime-eviction: \"true\"\r\n        panfactum.com/region: us-east-2\r\n        panfactum.com/root-module: kube_authentik\r\n        panfactum.com/stack-commit: local\r\n        panfactum.com/stack-version: local\r\n        panfactum.com/workload: pg-rooler-rw-pg-2c0d\r\n        test.1/2.3.4.5: test.1.2.3.4.5\r\n        test1: foo\r\n        test2: bar\r\n        test3: baz\r\n        test4: \"42\"\r\n    spec:\r\n      affinity:\r\n        nodeAffinity:\r\n          preferredDuringSchedulingIgnoredDuringExecution:\r\n          - preference:\r\n              matchExpressions:\r\n              - key: panfactum.com/class\r\n                operator: In\r\n                values:\r\n                - burstable\r\n            weight: 50\r\n        podAffinity:\r\n          preferredDuringSchedulingIgnoredDuringExecution:\r\n          - podAffinityTerm:\r\n              labelSelector:\r\n                matchLabels:\r\n                  id: pg-pg-2c0d-e46431d0383c5674\r\n              topologyKey: kubernetes.io/hostname\r\n            weight: 100\r\n          requiredDuringSchedulingIgnoredDuringExecution: ]\r\n        podAntiAffinity:\r\n          requiredDuringSchedulingIgnoredDuringExecution:\r\n          - labelSelector:\r\n              matchLabels:\r\n                id: pg-rooler-rw-pg-2c0d-7f0bb4544f43b0d8\r\n            topologyKey: kubernetes.io/hostname\r\n          - labelSelector:\r\n              matchLabels:\r\n                id: pg-rooler-rw-pg-2c0d-7f0bb4544f43b0d8\r\n            topologyKey: node.kubernetes.io/instance-type\r\n      containers:\r\n      - image: 891377197483.dkr.ecr.us-east-2.amazonaws.com/github/cloudnative-pg/pgbouncer:1.22.1\r\n        lifecycle:\r\n          preStop:\r\n            exec:\r\n              command:\r\n              - /bin/sh\r\n              - -c\r\n              - killall -INT pgbouncer && sleep 120\r\n        name: pgbouncer\r\n        resources:\r\n          limits:\r\n            memory: 80Mi\r\n          requests:\r\n            memory: 50Mi\r\n      priorityClassName: database\r\n      tolerations:\r\n      - effect: NoSchedule\r\n        key: spot\r\n        operator: Equal\r\n        value: \"true\"\r\n      - effect: NoSchedule\r\n        key: burstable\r\n        operator: Equal\r\n        value: \"true\"\r\n      - effect: NoSchedule\r\n        key: arm64\r\n        operator: Equal\r\n        value: \"true\"\r\n      topologySpreadConstraints:\r\n      - labelSelector:\r\n          matchLabels:\r\n            id: pg-rooler-rw-pg-2c0d-7f0bb4544f43b0d8\r\n        maxSkew: 1\r\n        topologyKey: topology.kubernetes.io/zone\r\n        whenUnsatisfiable: DoNotSchedule\r\n  type: rw\r\nstatus:\r\n  instances: 2\r\n  secrets:\r\n    clientCA:\r\n      name: pg-client-certs-2003\r\n      version: \"65063758\"\r\n    pgBouncerSecrets:\r\n      authQuery:\r\n        name: pg-pooler-certs-8cbb\r\n        version: \"65063760\"\r\n    serverCA:\r\n      name: pg-server-certs-a710\r\n      version: \"65063762\"\r\n    serverTLS:\r\n      name: pg-server-certs-a710\r\n      version: \"65063762\"\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:31:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:31:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:32:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:32:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:32:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:32:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:33:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:33:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:33:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:33:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:34:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:34:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:34:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:34:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:35:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:35:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:35:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:35:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:36:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:36:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:36:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:36:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:37:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:37:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:37:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:37:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:38:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:38:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:38:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:38:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:39:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:39:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:39:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:39:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:40:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:40:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:40:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:40:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:41:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:41:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:41:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:41:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:42:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:42:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:42:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:42:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:43:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:43:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:43:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:43:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:44:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:44:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:44:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:44:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:45:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:45:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:45:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:45:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:46:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:46:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:46:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:46:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:47:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:47:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:47:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:47:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:48:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:48:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:48:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:48:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:49:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:49:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:49:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:49:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:50:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:50:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:50:30Z\",\"msg\":\"updated configuration file\",\"name\":\"/controller/configs/pgbouncer.ini\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:50:30Z\",\"msg\":\"record\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-06-10 14:50:30.921 UTC\",\"pid\":\"15\",\"level\":\"LOG\",\"msg\":\"RELOAD command issued\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:50:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:50:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:51:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:51:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:51:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:51:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:52:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:52:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:52:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:52:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:53:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:53:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:53:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:53:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:54:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:54:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:54:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:54:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:55:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:55:24Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:55:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-10T14:55:47Z\",\"msg\":\"Missing metric\",\"query\":\"SHOW LISTS\",\"metric\":\"peer_pools\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI'm also facing the same issue and using the same version 1.23.1\n---\nSame with 1.24.0\n---\nSame here, v1.24.0.\n---\nHow should we configure our clusters to get pgbouncer metrics? \ud83e\udd14\n---\nThe reason is simply that the cloudnative operator does not know about these two metrics. It looks them up in https://github.com/cloudnative-pg/cloudnative-pg/blob/af56bb29ef230a59bd3ef90d8be34e5af161a466/pkg/management/pgbouncer/metricsserver/lists.go#L46\nSo would guess it is not problem - just the that the operator is not collecting these two metrics. But I don't know why."
    },
    {
        "title": "[Bug]: PostgreSQL switch over endless loop with WAL errors like servers diverged at WAL location",
        "id": 2341829908,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ns@thiswatch.com\n### Version\n1.23.1\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nWe have a test database and did a node restart. After the node restart, the primary wasn't switched over and the 'old' primary on the restarted node started to wait for the 'switch over in progress'\r\nThe same issue happend with our production db when we upgraded to k8s 1.30 (the bug happened at 1.29 for test already, i don't think its a k8s version issue).\r\nWe basically have now 3 instances:\r\n1. old primary, waiting for switch over\r\n2. old secondary, can't connect to the primary and crashes\r\n3. old secondary, just runs. Is still secondary, doesn't seem to do anything in particular\r\nIn parallel the folder for the wal logs getting full. On test it consumed now on one instance over 6 gig at a db which is normally a few hundred megabytes big. on Production for one instance, we are at 40gb from a 5gb db. \r\n### Cluster resource\n```shell\nImportant: At this point, the test cluster is recovered ($collegue did this without asking), the broken instance was number 5.\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/last-applied-configuration: |\r\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"metadata\":{\"annotations\":{},\"labels\":{\"argocd.argoproj.io/instance\":\"test-cnpg-db\"},\"name\":\"thiswatch-test-db-cluster\",\"namespace\":\"test\"},\"spec\":{\"affinity\":{\"enablePodAntiAffinity\":true,\"podAntiAffinityType\":\"required\",\"topologyKey\":\"kubernetes.io/hostname\"},\"backup\":{\"barmanObjectStore\":{\"destinationPath\":\"s3://backups/db/thiswatch/wal/\",\"endpointURL\":\"http://minio.test.svc.cluster.local\",\"s3Credentials\":{\"accessKeyId\":{\"key\":\"CONSOLE_ACCESS_KEY\",\"name\":\"minio-user-backup-thiswatch\"},\"secretAccessKey\":{\"key\":\"CONSOLE_SECRET_KEY\",\"name\":\"minio-user-backup-thiswatch\"}},\"wal\":{\"compression\":\"gzip\",\"maxParallel\":2}},\"volumeSnapshot\":{\"className\":\"zfs-snapshot\"}},\"imageName\":\"ghcr.io/cloudnative-pg/postgresql:16.3\",\"instances\":3,\"monitoring\":{\"enablePodMonitor\":true},\"postgresql\":{\"parameters\":{\"pg_stat_statements.max\":\"10000\",\"pg_stat_statements.track\":\"all\"}},\"primaryUpdateStrategy\":\"unsupervised\",\"storage\":{\"size\":\"10Gi\",\"storageClass\":\"tristram\"}}}\r\n  creationTimestamp: \"2024-04-30T13:30:27Z\"\r\n  generation: 9\r\n  labels:\r\n    argocd.argoproj.io/instance: test-cnpg-db\r\n  name: thiswatch-test-db-cluster\r\n  namespace: test\r\n  resourceVersion: \"131447029\"\r\n  uid: d5699655-2b70-4938-83c8-633970c2ccc1\r\nspec:\r\n  affinity:\r\n    enablePodAntiAffinity: true\r\n    podAntiAffinityType: required\r\n    topologyKey: kubernetes.io/hostname\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3://backups/db/thiswatch/wal/\r\n      endpointURL: http://minio.test.svc.cluster.local\r\n      s3Credentials:\r\n        accessKeyId:\r\n          key: CONSOLE_ACCESS_KEY\r\n          name: minio-user-backup-thiswatch\r\n        secretAccessKey:\r\n          key: CONSOLE_SECRET_KEY\r\n          name: minio-user-backup-thiswatch\r\n      wal:\r\n        compression: gzip\r\n        maxParallel: 2\r\n    target: prefer-standby\r\n    volumeSnapshot:\r\n      className: zfs-snapshot\r\n      online: true\r\n      onlineConfiguration:\r\n        immediateCheckpoint: false\r\n        waitForArchive: true\r\n      snapshotOwnerReference: none\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enablePDB: true\r\n  enableSuperuserAccess: false\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.3\r\n  instances: 3\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      pg_stat_statements.max: \"10000\"\r\n      pg_stat_statements.track: all\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_level: logical\r\n      wal_log_hints: \"on\"\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    synchronizeReplicas:\r\n      enabled: true\r\n    updateInterval: 30\r\n  resources: {}\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 10Gi\r\n    storageClass: tristram\r\n  switchoverDelay: 3600\r\nstatus:\r\n  availableArchitectures:\r\n  - goArch: amd64\r\n    hash: 94527128605ac5100415106fe26c480531d094b3f36626e562a8135f342b89e4\r\n  - goArch: arm64\r\n    hash: 9b7b08592e917ed3b20bb3ae404ea4c0c958bdee73e5411c452d6c464d77f0b4\r\n  certificates:\r\n    clientCASecret: thiswatch-test-db-cluster-ca\r\n    expirations:\r\n      thiswatch-test-db-cluster-ca: 2024-07-29 13:25:27 +0000 UTC\r\n      thiswatch-test-db-cluster-replication: 2024-07-29 13:25:27 +0000 UTC\r\n      thiswatch-test-db-cluster-server: 2024-07-29 13:25:27 +0000 UTC\r\n    replicationTLSSecret: thiswatch-test-db-cluster-replication\r\n    serverAltDNSNames:\r\n    - thiswatch-test-db-cluster-rw\r\n    - thiswatch-test-db-cluster-rw.test\r\n    - thiswatch-test-db-cluster-rw.test.svc\r\n    - thiswatch-test-db-cluster-r\r\n    - thiswatch-test-db-cluster-r.test\r\n    - thiswatch-test-db-cluster-r.test.svc\r\n    - thiswatch-test-db-cluster-ro\r\n    - thiswatch-test-db-cluster-ro.test\r\n    - thiswatch-test-db-cluster-ro.test.svc\r\n    serverCASecret: thiswatch-test-db-cluster-ca\r\n    serverTLSSecret: thiswatch-test-db-cluster-server\r\n  cloudNativePGCommitHash: 336ddf53\r\n  cloudNativePGOperatorHash: 94527128605ac5100415106fe26c480531d094b3f36626e562a8135f342b89e4\r\n  conditions:\r\n  - lastTransitionTime: \"2024-06-08T20:25:13Z\"\r\n    message: Cluster is Ready\r\n    reason: ClusterIsReady\r\n    status: \"True\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-06-08T20:24:58Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  - lastTransitionTime: \"2024-06-08T00:00:35Z\"\r\n    message: Backup was successful\r\n    reason: LastBackupSucceeded\r\n    status: \"True\"\r\n    type: LastBackupSucceeded\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"95383440\"\r\n  currentPrimary: thiswatch-test-db-cluster-1\r\n  currentPrimaryTimestamp: \"2024-06-08T20:07:31.385929Z\"\r\n  firstRecoverabilityPoint: \"2024-01-15T00:00:36Z\"\r\n  firstRecoverabilityPointByMethod:\r\n    volumeSnapshot: \"2024-01-15T00:00:36Z\"\r\n  healthyPVC:\r\n  - thiswatch-test-db-cluster-1\r\n  - thiswatch-test-db-cluster-6\r\n  - thiswatch-test-db-cluster-7\r\n  image: ghcr.io/cloudnative-pg/postgresql:16.3\r\n  instanceNames:\r\n  - thiswatch-test-db-cluster-1\r\n  - thiswatch-test-db-cluster-6\r\n  - thiswatch-test-db-cluster-7\r\n  instances: 3\r\n  instancesReportedState:\r\n    thiswatch-test-db-cluster-1:\r\n      isPrimary: true\r\n      timeLineID: 4\r\n    thiswatch-test-db-cluster-6:\r\n      isPrimary: false\r\n      timeLineID: 4\r\n    thiswatch-test-db-cluster-7:\r\n      isPrimary: false\r\n      timeLineID: 4\r\n  instancesStatus:\r\n    healthy:\r\n    - thiswatch-test-db-cluster-1\r\n    - thiswatch-test-db-cluster-6\r\n    - thiswatch-test-db-cluster-7\r\n  lastSuccessfulBackup: \"2024-06-08T00:00:35Z\"\r\n  lastSuccessfulBackupByMethod:\r\n    volumeSnapshot: \"2024-06-08T00:00:35Z\"\r\n  latestGeneratedNode: 7\r\n  managedRolesStatus: {}\r\n  phase: Cluster in healthy state\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 3\r\n  readService: thiswatch-test-db-cluster-r\r\n  readyInstances: 3\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"95383410\"\r\n    clientCaSecretVersion: \"95383404\"\r\n    replicationSecretVersion: \"95383408\"\r\n    serverCaSecretVersion: \"95383404\"\r\n    serverSecretVersion: \"95383407\"\r\n  switchReplicaClusterStatus: {}\r\n  targetPrimary: thiswatch-test-db-cluster-1\r\n  targetPrimaryTimestamp: \"2024-06-08T20:07:28.326475Z\"\r\n  timelineID: 4\r\n  topology:\r\n    instances:\r\n      thiswatch-test-db-cluster-1: {}\r\n      thiswatch-test-db-cluster-6: {}\r\n      thiswatch-test-db-cluster-7: {}\r\n    nodesUsed: 3\r\n    successfullyExtracted: true\r\n  writeService: thiswatch-test-db-cluster-rw\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: servers diverged at WAL location 0/D1001E48 on timeline 1\",\"pipe\":\"stderr\",\"logging_pod\":\"thiswatch-test-db-cluster-5\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: error: could not find previous WAL record at 0/D1001E48: invalid magic number D110 in WAL segment 0000000100000000000000D1, LSN 0/D1000000, offset 0\",\"pipe\":\"stderr\",\"logging_pod\":\"thiswatch-test-db-cluster-5\"}{\"level\":\"error\",\"ts\":\"2024-06-08T20:20:39Z\",\"msg\":\"Failed to execute pg_rewind\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"thiswatch-test-db-cluster\",\"namespace\":\"test\"},\"namespace\":\"test\",\"name\":\"thiswatch-test-db-cluster\",\"reconcileID\":\"7e793430-8f62-4eb1-aa69-42701692b2a3\",\"logging_pod\":\"thiswatch-test-db-cluster-5\",\"options\":[\"-P\",\"--source-server\",\"host=thiswatch-test-db-cluster-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=thiswatch-test-db-cluster-5 sslmode=verify-ca dbname=postgres\",\"--target-pgdata\",\"/var/lib/postgresql/data/pgdata\",\"--restore-target-wal\"],\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*Instance).Rewind\\n\\tpkg/management/postgres/instance.go:986\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).verifyPgDataCoherenceForPrimary\\n\\tinternal/management/controller/instance_startup.go:255\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).initialize\\n\\tinternal/management/controller/instance_controller.go:352\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).Reconcile\\n\\tinternal/management/controller/instance_controller.go:140\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.17.3/pkg/internal/controller/controller.go:119\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.17.3/pkg/internal/controller/controller.go:316\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.17.3/pkg/internal/controller/controller.go:266\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.17.3/pkg/internal/controller/controller.go:227\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"msg\":\"pg_rewind failed, starting the server to complete the crash recovery\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"thiswatch-test-db-cluster\",\"namespace\":\"test\"},\"namespace\":\"test\",\"name\":\"thiswatch-test-db-cluster\",\"reconcileID\":\"7e793430-8f62-4eb1-aa69-42701692b2a3\",\"logging_pod\":\"thiswatch-test-db-cluster-5\",\"err\":\"error executing pg_rewind: exit status 1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"msg\":\"Waiting for server to complete crash recovery\",\"logging_pod\":\"thiswatch-test-db-cluster-5\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"msg\":\"Starting up instance\",\"logging_pod\":\"thiswatch-test-db-cluster-5\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"options\":[\"start\",\"-w\",\"-D\",\"/var/lib/postgresql/data/pgdata\",\"-o\",\"-c port=5432 -c unix_socket_directories=/controller/run\",\"-t 40000000\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"logger\":\"pg_ctl\",\"msg\":\"waiting for server to start....2024-06-08 20:20:39.790 UTC [1916] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stdout\",\"logging_pod\":\"thiswatch-test-db-cluster-5\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"logger\":\"pg_ctl\",\"msg\":\"2024-06-08 20:20:39.790 UTC [1916] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stdout\",\"logging_pod\":\"thiswatch-test-db-cluster-5\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"thiswatch-test-db-cluster-5\",\"record\":{\"log_time\":\"2024-06-08 20:20:39.790 UTC\",\"process_id\":\"1916\",\"session_id\":\"6664bd17.77c\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-06-08 20:20:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"thiswatch-test-db-cluster-5\",\"record\":{\"log_time\":\"2024-06-08 20:20:39.790 UTC\",\"process_id\":\"1916\",\"session_id\":\"6664bd17.77c\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-06-08 20:20:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 16.3 (Debian 16.3-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"logger\":\"postgres\",\"msg\":\"2024-06-08 20:20:39.790 UTC [1916] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"thiswatch-test-db-cluster-5\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ns@thiswatch.com\n### Version\n1.23.1\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nWe have a test database and did a node restart. After the node restart, the primary wasn't switched over and the 'old' primary on the restarted node started to wait for the 'switch over in progress'\r\nThe same issue happend with our production db when we upgraded to k8s 1.30 (the bug happened at 1.29 for test already, i don't think its a k8s version issue).\r\nWe basically have now 3 instances:\r\n1. old primary, waiting for switch over\r\n2. old secondary, can't connect to the primary and crashes\r\n3. old secondary, just runs. Is still secondary, doesn't seem to do anything in particular\r\nIn parallel the folder for the wal logs getting full. On test it consumed now on one instance over 6 gig at a db which is normally a few hundred megabytes big. on Production for one instance, we are at 40gb from a 5gb db. \r\n### Cluster resource\n```shell\nImportant: At this point, the test cluster is recovered ($collegue did this without asking), the broken instance was number 5.\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/last-applied-configuration: |\r\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"metadata\":{\"annotations\":{},\"labels\":{\"argocd.argoproj.io/instance\":\"test-cnpg-db\"},\"name\":\"thiswatch-test-db-cluster\",\"namespace\":\"test\"},\"spec\":{\"affinity\":{\"enablePodAntiAffinity\":true,\"podAntiAffinityType\":\"required\",\"topologyKey\":\"kubernetes.io/hostname\"},\"backup\":{\"barmanObjectStore\":{\"destinationPath\":\"s3://backups/db/thiswatch/wal/\",\"endpointURL\":\"http://minio.test.svc.cluster.local\",\"s3Credentials\":{\"accessKeyId\":{\"key\":\"CONSOLE_ACCESS_KEY\",\"name\":\"minio-user-backup-thiswatch\"},\"secretAccessKey\":{\"key\":\"CONSOLE_SECRET_KEY\",\"name\":\"minio-user-backup-thiswatch\"}},\"wal\":{\"compression\":\"gzip\",\"maxParallel\":2}},\"volumeSnapshot\":{\"className\":\"zfs-snapshot\"}},\"imageName\":\"ghcr.io/cloudnative-pg/postgresql:16.3\",\"instances\":3,\"monitoring\":{\"enablePodMonitor\":true},\"postgresql\":{\"parameters\":{\"pg_stat_statements.max\":\"10000\",\"pg_stat_statements.track\":\"all\"}},\"primaryUpdateStrategy\":\"unsupervised\",\"storage\":{\"size\":\"10Gi\",\"storageClass\":\"tristram\"}}}\r\n  creationTimestamp: \"2024-04-30T13:30:27Z\"\r\n  generation: 9\r\n  labels:\r\n    argocd.argoproj.io/instance: test-cnpg-db\r\n  name: thiswatch-test-db-cluster\r\n  namespace: test\r\n  resourceVersion: \"131447029\"\r\n  uid: d5699655-2b70-4938-83c8-633970c2ccc1\r\nspec:\r\n  affinity:\r\n    enablePodAntiAffinity: true\r\n    podAntiAffinityType: required\r\n    topologyKey: kubernetes.io/hostname\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3://backups/db/thiswatch/wal/\r\n      endpointURL: http://minio.test.svc.cluster.local\r\n      s3Credentials:\r\n        accessKeyId:\r\n          key: CONSOLE_ACCESS_KEY\r\n          name: minio-user-backup-thiswatch\r\n        secretAccessKey:\r\n          key: CONSOLE_SECRET_KEY\r\n          name: minio-user-backup-thiswatch\r\n      wal:\r\n        compression: gzip\r\n        maxParallel: 2\r\n    target: prefer-standby\r\n    volumeSnapshot:\r\n      className: zfs-snapshot\r\n      online: true\r\n      onlineConfiguration:\r\n        immediateCheckpoint: false\r\n        waitForArchive: true\r\n      snapshotOwnerReference: none\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enablePDB: true\r\n  enableSuperuserAccess: false\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.3\r\n  instances: 3\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      pg_stat_statements.max: \"10000\"\r\n      pg_stat_statements.track: all\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_level: logical\r\n      wal_log_hints: \"on\"\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    synchronizeReplicas:\r\n      enabled: true\r\n    updateInterval: 30\r\n  resources: {}\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 10Gi\r\n    storageClass: tristram\r\n  switchoverDelay: 3600\r\nstatus:\r\n  availableArchitectures:\r\n  - goArch: amd64\r\n    hash: 94527128605ac5100415106fe26c480531d094b3f36626e562a8135f342b89e4\r\n  - goArch: arm64\r\n    hash: 9b7b08592e917ed3b20bb3ae404ea4c0c958bdee73e5411c452d6c464d77f0b4\r\n  certificates:\r\n    clientCASecret: thiswatch-test-db-cluster-ca\r\n    expirations:\r\n      thiswatch-test-db-cluster-ca: 2024-07-29 13:25:27 +0000 UTC\r\n      thiswatch-test-db-cluster-replication: 2024-07-29 13:25:27 +0000 UTC\r\n      thiswatch-test-db-cluster-server: 2024-07-29 13:25:27 +0000 UTC\r\n    replicationTLSSecret: thiswatch-test-db-cluster-replication\r\n    serverAltDNSNames:\r\n    - thiswatch-test-db-cluster-rw\r\n    - thiswatch-test-db-cluster-rw.test\r\n    - thiswatch-test-db-cluster-rw.test.svc\r\n    - thiswatch-test-db-cluster-r\r\n    - thiswatch-test-db-cluster-r.test\r\n    - thiswatch-test-db-cluster-r.test.svc\r\n    - thiswatch-test-db-cluster-ro\r\n    - thiswatch-test-db-cluster-ro.test\r\n    - thiswatch-test-db-cluster-ro.test.svc\r\n    serverCASecret: thiswatch-test-db-cluster-ca\r\n    serverTLSSecret: thiswatch-test-db-cluster-server\r\n  cloudNativePGCommitHash: 336ddf53\r\n  cloudNativePGOperatorHash: 94527128605ac5100415106fe26c480531d094b3f36626e562a8135f342b89e4\r\n  conditions:\r\n  - lastTransitionTime: \"2024-06-08T20:25:13Z\"\r\n    message: Cluster is Ready\r\n    reason: ClusterIsReady\r\n    status: \"True\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-06-08T20:24:58Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  - lastTransitionTime: \"2024-06-08T00:00:35Z\"\r\n    message: Backup was successful\r\n    reason: LastBackupSucceeded\r\n    status: \"True\"\r\n    type: LastBackupSucceeded\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"95383440\"\r\n  currentPrimary: thiswatch-test-db-cluster-1\r\n  currentPrimaryTimestamp: \"2024-06-08T20:07:31.385929Z\"\r\n  firstRecoverabilityPoint: \"2024-01-15T00:00:36Z\"\r\n  firstRecoverabilityPointByMethod:\r\n    volumeSnapshot: \"2024-01-15T00:00:36Z\"\r\n  healthyPVC:\r\n  - thiswatch-test-db-cluster-1\r\n  - thiswatch-test-db-cluster-6\r\n  - thiswatch-test-db-cluster-7\r\n  image: ghcr.io/cloudnative-pg/postgresql:16.3\r\n  instanceNames:\r\n  - thiswatch-test-db-cluster-1\r\n  - thiswatch-test-db-cluster-6\r\n  - thiswatch-test-db-cluster-7\r\n  instances: 3\r\n  instancesReportedState:\r\n    thiswatch-test-db-cluster-1:\r\n      isPrimary: true\r\n      timeLineID: 4\r\n    thiswatch-test-db-cluster-6:\r\n      isPrimary: false\r\n      timeLineID: 4\r\n    thiswatch-test-db-cluster-7:\r\n      isPrimary: false\r\n      timeLineID: 4\r\n  instancesStatus:\r\n    healthy:\r\n    - thiswatch-test-db-cluster-1\r\n    - thiswatch-test-db-cluster-6\r\n    - thiswatch-test-db-cluster-7\r\n  lastSuccessfulBackup: \"2024-06-08T00:00:35Z\"\r\n  lastSuccessfulBackupByMethod:\r\n    volumeSnapshot: \"2024-06-08T00:00:35Z\"\r\n  latestGeneratedNode: 7\r\n  managedRolesStatus: {}\r\n  phase: Cluster in healthy state\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 3\r\n  readService: thiswatch-test-db-cluster-r\r\n  readyInstances: 3\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"95383410\"\r\n    clientCaSecretVersion: \"95383404\"\r\n    replicationSecretVersion: \"95383408\"\r\n    serverCaSecretVersion: \"95383404\"\r\n    serverSecretVersion: \"95383407\"\r\n  switchReplicaClusterStatus: {}\r\n  targetPrimary: thiswatch-test-db-cluster-1\r\n  targetPrimaryTimestamp: \"2024-06-08T20:07:28.326475Z\"\r\n  timelineID: 4\r\n  topology:\r\n    instances:\r\n      thiswatch-test-db-cluster-1: {}\r\n      thiswatch-test-db-cluster-6: {}\r\n      thiswatch-test-db-cluster-7: {}\r\n    nodesUsed: 3\r\n    successfullyExtracted: true\r\n  writeService: thiswatch-test-db-cluster-rw\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: servers diverged at WAL location 0/D1001E48 on timeline 1\",\"pipe\":\"stderr\",\"logging_pod\":\"thiswatch-test-db-cluster-5\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: error: could not find previous WAL record at 0/D1001E48: invalid magic number D110 in WAL segment 0000000100000000000000D1, LSN 0/D1000000, offset 0\",\"pipe\":\"stderr\",\"logging_pod\":\"thiswatch-test-db-cluster-5\"}{\"level\":\"error\",\"ts\":\"2024-06-08T20:20:39Z\",\"msg\":\"Failed to execute pg_rewind\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"thiswatch-test-db-cluster\",\"namespace\":\"test\"},\"namespace\":\"test\",\"name\":\"thiswatch-test-db-cluster\",\"reconcileID\":\"7e793430-8f62-4eb1-aa69-42701692b2a3\",\"logging_pod\":\"thiswatch-test-db-cluster-5\",\"options\":[\"-P\",\"--source-server\",\"host=thiswatch-test-db-cluster-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=thiswatch-test-db-cluster-5 sslmode=verify-ca dbname=postgres\",\"--target-pgdata\",\"/var/lib/postgresql/data/pgdata\",\"--restore-target-wal\"],\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*Instance).Rewind\\n\\tpkg/management/postgres/instance.go:986\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).verifyPgDataCoherenceForPrimary\\n\\tinternal/management/controller/instance_startup.go:255\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).initialize\\n\\tinternal/management/controller/instance_controller.go:352\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).Reconcile\\n\\tinternal/management/controller/instance_controller.go:140\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.17.3/pkg/internal/controller/controller.go:119\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.17.3/pkg/internal/controller/controller.go:316\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.17.3/pkg/internal/controller/controller.go:266\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.17.3/pkg/internal/controller/controller.go:227\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"msg\":\"pg_rewind failed, starting the server to complete the crash recovery\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"thiswatch-test-db-cluster\",\"namespace\":\"test\"},\"namespace\":\"test\",\"name\":\"thiswatch-test-db-cluster\",\"reconcileID\":\"7e793430-8f62-4eb1-aa69-42701692b2a3\",\"logging_pod\":\"thiswatch-test-db-cluster-5\",\"err\":\"error executing pg_rewind: exit status 1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"msg\":\"Waiting for server to complete crash recovery\",\"logging_pod\":\"thiswatch-test-db-cluster-5\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"msg\":\"Starting up instance\",\"logging_pod\":\"thiswatch-test-db-cluster-5\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"options\":[\"start\",\"-w\",\"-D\",\"/var/lib/postgresql/data/pgdata\",\"-o\",\"-c port=5432 -c unix_socket_directories=/controller/run\",\"-t 40000000\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"logger\":\"pg_ctl\",\"msg\":\"waiting for server to start....2024-06-08 20:20:39.790 UTC [1916] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stdout\",\"logging_pod\":\"thiswatch-test-db-cluster-5\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"logger\":\"pg_ctl\",\"msg\":\"2024-06-08 20:20:39.790 UTC [1916] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stdout\",\"logging_pod\":\"thiswatch-test-db-cluster-5\"}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"thiswatch-test-db-cluster-5\",\"record\":{\"log_time\":\"2024-06-08 20:20:39.790 UTC\",\"process_id\":\"1916\",\"session_id\":\"6664bd17.77c\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-06-08 20:20:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"thiswatch-test-db-cluster-5\",\"record\":{\"log_time\":\"2024-06-08 20:20:39.790 UTC\",\"process_id\":\"1916\",\"session_id\":\"6664bd17.77c\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-06-08 20:20:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 16.3 (Debian 16.3-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-06-08T20:20:39Z\",\"logger\":\"postgres\",\"msg\":\"2024-06-08 20:20:39.790 UTC [1916] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"thiswatch-test-db-cluster-5\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductWe run into a similar issue.\r\nDB Switchover during node-maintenance (cordon worker nodes). \r\nAfter node-maintenace (uncordon) \r\nStandby came back online but constantly crashlooping with\r\n```\r\n...\r\n{\"level\":\"info\",\"ts\":\"2024-07-26T09:04:17Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: connected to server\",\"pipe\":\"stderr\",\"logging_pod\":\"database-prod-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-26T09:04:17Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: servers diverged at WAL location 0/60000A0 on timeline 1\",\"pipe\":\"stderr\",\"logging_pod\":\"database-prod-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-26T09:04:17Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: error: could not find previous WAL record at 0/60000A0: record with incorrect prev-link 79/6C6B6565 at 0/60000A0\",\"pipe\":\"stderr\",\"logging_pod\":\"database-prod-1\"}\r\n{\"level\":\"error\",\"ts\":\"2024-07-26T09:04:17Z\",\"msg\":\"Failed to execute pg_rewind\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"database-prod\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"database-prod\",\"reconcileID\":\"b73ae35b-3a57-4b36-87f6-bf8f551f9c3a\",\"logging_pod\":\"database-prod-1\",\"options\":[\"-P\",\"--source-server\",\"host=database-prod-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=database-prod-1 sslmode=verify-ca dbname=postgres\",\"--target-pgdata\",\"/var/lib/postgresql/data/pgdata\",\"--restore-target-wal\"],\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*Instance).Rewind\\n\\tpkg/management/postgres/instance.go:993\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).verifyPgDataCoherenceForPrimary\\n\\tinternal/management/controller/instance_startup.go:255\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).initialize\\n\\tinternal/management/controller/instance_controller.go:357\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).Reconcile\\n\\tinternal/management/controller/instance_controller.go:140\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/internal/controller/controller.go:114\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/internal/controller/controller.go:311\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/internal/controller/controller.go:261\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/internal/controller/controller.go:222\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-26T09:04:17Z\",\"msg\":\"pg_rewind failed, starting the server to complete the crash recovery\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"database-prod\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"database-prod\",\"reconcileID\":\"b73ae35b-3a57-4b36-87f6-bf8f551f9c3a\",\"logging_pod\":\"database-prod-1\",\"err\":\"error executing pg_rewind: exit status 1\"}\r\n....\r\n```\r\nuntil all pvc space are occupied\r\n```\r\n...\r\n{\"level\":\"info\",\"ts\":\"2024-07-26T09:33:35Z\",\"logger\":\"wal-archive\",\"msg\":\"Refusing to archive WAL when there is a switchover in progress\",\"logging_pod\":\"database-prod-1\",\"currentPrimary\":\"database-prod-2\",\"targetPrimary\":\"database-prod-2\",\"podName\":\"database-prod-1\"}\r\n...\r\n{\"level\":\"info\",\"ts\":\"2024-07-26T09:42:09Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-07-26 09:42:09.087 UTC [28] FATAL:  could not write to file \\\"pg_wal/xlogtemp.28\\\": No space left on device\",\"pipe\":\"stderr\",\"logging_pod\":\"database-prod-1\"}\r\n...\r\n```\r\nWe end up deleting the database-prod-1 pod and pvc to create a new standby\r\ncnpg: 1.23.2\r\npostgresql:16.3-4\n---\nWe see that the pg_wal in the primary container starts to explode in size. We have given our prod db generous space, but eventually it will run out as all the others. Why does pg_wal get so big? We have set up WAL backup to S3 and snapshot backups. \r\n@HaveFun83 do you see the same for your cluster?\r\n```shell\r\ndu -h /var/lib/postgresql/data/pgdata\r\n8.3M\t/var/lib/postgresql/data/pgdata/base/5\r\n512\t/var/lib/postgresql/data/pgdata/base/pgsql_tmp\r\n8.3M\t/var/lib/postgresql/data/pgdata/base/1\r\n5.7G\t/var/lib/postgresql/data/pgdata/base/16385\r\n7.8M\t/var/lib/postgresql/data/pgdata/base/4\r\n5.7G\t/var/lib/postgresql/data/pgdata/base\r\n265K\t/var/lib/postgresql/data/pgdata/pg_subtrans\r\n512\t/var/lib/postgresql/data/pgdata/pg_stat\r\n393K\t/var/lib/postgresql/data/pgdata/pg_stat_tmp\r\n512\t/var/lib/postgresql/data/pgdata/pg_logical/snapshots\r\n512\t/var/lib/postgresql/data/pgdata/pg_logical/mappings\r\n14K\t/var/lib/postgresql/data/pgdata/pg_logical\r\n512\t/var/lib/postgresql/data/pgdata/pg_commit_ts\r\n512\t/var/lib/postgresql/data/pgdata/pg_notify\r\n512\t/var/lib/postgresql/data/pgdata/pg_tblspc\r\n113K\t/var/lib/postgresql/data/pgdata/pg_xact\r\n512\t/var/lib/postgresql/data/pgdata/pg_serial\r\n9.0K\t/var/lib/postgresql/data/pgdata/pg_multixact/members\r\n9.0K\t/var/lib/postgresql/data/pgdata/pg_multixact/offsets\r\n19K\t/var/lib/postgresql/data/pgdata/pg_multixact\r\n1.9M\t/var/lib/postgresql/data/pgdata/pg_wal/archive_status\r\n44G\t/var/lib/postgresql/data/pgdata/pg_wal\r\n512\t/var/lib/postgresql/data/pgdata/pg_twophase\r\n512\t/var/lib/postgresql/data/pgdata/pg_snapshots\r\n512\t/var/lib/postgresql/data/pgdata/pg_dynshmem\r\n669K\t/var/lib/postgresql/data/pgdata/global\r\n5.0K\t/var/lib/postgresql/data/pgdata/pg_replslot/_cnpg_thiswatch_prod_db_cluster_3\r\n5.0K\t/var/lib/postgresql/data/pgdata/pg_replslot/_cnpg_thiswatch_prod_db_cluster_2\r\n11K\t/var/lib/postgresql/data/pgdata/pg_replslot\r\n49G\t/var/lib/postgresql/data/pgdata\r\n```\n---\n@b-thiswatch \r\nThe filesystem was already full but if I remember correctly the space was occupied by `pg_wal` directory.\r\nWe end up deleting the database-prod-1 pod and pvc to create a new standby\n---\n@HaveFun83 \r\nThanks for the update. Seems like we either do something wrong or this is a problem. The database itself as you can see is just ~6gb. I was under the impression that saving the WAL on S3 serves the purpose of keeping the WAL while also keeping the volume \"small\", but I might be mistaken.\n---\nAre there any remediation steps that can be suggested for this? As I see it the issue is the backup is not being done / done on time and so the wal archive keeps filling up. By changing archive_mode to off the wal should be able to be deleted but instead it just keeps filling up. I tried setting archive mode to off via:\r\n```yaml\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: 'off'\r\n```\r\nBut that hasn't been implemented on the DB - `SHOW archive_mode;` -> on. \r\nSo is there any way to deal with this as it seems like there is no way to flush the wal and thus you are stuck in this loop while it tries to reconcile. In the operator for instance I am seeing \r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-09-18T11:31:33Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pg-main-v1\",\"namespace\":\"pg-main-v1\"},\"namespace\":\"pg-main-v1\",\"name\":\"pg-main-v1\",\"reconcileID\":\"\r\ncabf0566-a01d-4eb1-be8b-7f80ab9408b6\",\"name\":\"pg-main-v1-1\",\"error\":\"error status code: 500, body: failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is shutting down (SQLSTATE 57P03)\\n\"}\r\n```\r\nSo I think the reconciliation is in a loop while it waits for stats on the primary which had its wal fill up. \r\n```\r\npostgres {\"level\":\"error\",\"ts\":\"2024-09-18T11:22:47Z\",\"msg\":\"Reconciler error\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pg-main-v1\",\"namespace\":\"pg-main-v1\"},\"namespace\":\"pg-main-v1\",\"name\":\"pg-main-v1\",\"reconcileID\":\r\n\"3043ef01-381a-4fdf-8a89-56b39c9c0523\",\"error\":\"error executing pg_rewind: exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/internal/controller/controller.go:324\\ns\r\nigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/internal/controller/controller.go:261\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.\r\nio/controller-runtime@v0.18.4/pkg/internal/controller/controller.go:222\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2024-09-18T11:22:56Z\",\"msg\":\"Unable to collect metrics\",\"logging_pod\":\"pg-main-v1-1\",\"error\":\"failed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): dial error: dial unix /controller/run/.s.PGSQL.5432: c\r\nonnect: no such file or directory\"}\r\n```\n---\nUsing k8s operator chart version 0.22.0 I have a very similar issue: the first time the WAL backup is done and as well the snapshot but all the following barman backups failing with following error:\r\n`{\"level\":\"error\",\"ts\":\"2024-09-19T08:41:39Z\",\"logger\":\"wal-archive\",\"msg\":\"Error invoking barman-cloud-check-wal-archive\",\"logging_pod\":\"mydb-postgres-2\",\"currentPrimary\":\"desp-postgres-2\",\"targetPrimary\":\"mydb-postgres-2\",\"options\":[\"--endpoint-url\",\"https://minio.mydomain.com\",\"--cloud-provider\",\"aws-s3\",\"s3://postgres-backup-dev/\",\"desp-postgres\"],\"exitCode\":-1,\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/barman/archiver.(*WALArchiver).CheckWalArchiveDestination\\n\\tpkg/management/barman/archiver/archiver.go:257\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.checkWalArchive\\n\\tinternal/cmd/manager/walarchive/cmd.go:417\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.run\\n\\tinternal/cmd/manager/walarchive/cmd.go:214\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:89\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.22.5/x64/src/runtime/proc.go:271\"}\r\n`\n---\n> We run into a similar issue. DB Switchover during node-maintenance (cordon worker nodes). After node-maintenace (uncordon) Standby came back online but constantly crashlooping with\r\n> \r\n> ```\r\n> ...\r\n> {\"level\":\"info\",\"ts\":\"2024-07-26T09:04:17Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: connected to server\",\"pipe\":\"stderr\",\"logging_pod\":\"database-prod-1\"}\r\n> {\"level\":\"info\",\"ts\":\"2024-07-26T09:04:17Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: servers diverged at WAL location 0/60000A0 on timeline 1\",\"pipe\":\"stderr\",\"logging_pod\":\"database-prod-1\"}\r\n> {\"level\":\"info\",\"ts\":\"2024-07-26T09:04:17Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: error: could not find previous WAL record at 0/60000A0: record with incorrect prev-link 79/6C6B6565 at 0/60000A0\",\"pipe\":\"stderr\",\"logging_pod\":\"database-prod-1\"}\r\n> {\"level\":\"error\",\"ts\":\"2024-07-26T09:04:17Z\",\"msg\":\"Failed to execute pg_rewind\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"database-prod\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"database-prod\",\"reconcileID\":\"b73ae35b-3a57-4b36-87f6-bf8f551f9c3a\",\"logging_pod\":\"database-prod-1\",\"options\":[\"-P\",\"--source-server\",\"host=database-prod-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=database-prod-1 sslmode=verify-ca dbname=postgres\",\"--target-pgdata\",\"/var/lib/postgresql/data/pgdata\",\"--restore-target-wal\"],\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*Instance).Rewind\\n\\tpkg/management/postgres/instance.go:993\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).verifyPgDataCoherenceForPrimary\\n\\tinternal/management/controller/instance_startup.go:255\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).initialize\\n\\tinternal/management/controller/instance_controller.go:357\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).Reconcile\\n\\tinternal/management/controller/instance_controller.go:140\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/internal/controller/controller.go:114\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/internal/controller/controller.go:311\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/internal/controller/controller.go:261\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.3/pkg/internal/controller/controller.go:222\"}\r\n> {\"level\":\"info\",\"ts\":\"2024-07-26T09:04:17Z\",\"msg\":\"pg_rewind failed, starting the server to complete the crash recovery\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"database-prod\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"database-prod\",\"reconcileID\":\"b73ae35b-3a57-4b36-87f6-bf8f551f9c3a\",\"logging_pod\":\"database-prod-1\",\"err\":\"error executing pg_rewind: exit status 1\"}\r\n> ....\r\n> ```\r\n> \r\n> until all pvc space are occupied\r\n> \r\n> ```\r\n> ...\r\n> {\"level\":\"info\",\"ts\":\"2024-07-26T09:33:35Z\",\"logger\":\"wal-archive\",\"msg\":\"Refusing to archive WAL when there is a switchover in progress\",\"logging_pod\":\"database-prod-1\",\"currentPrimary\":\"database-prod-2\",\"targetPrimary\":\"database-prod-2\",\"podName\":\"database-prod-1\"}\r\n> ...\r\n> {\"level\":\"info\",\"ts\":\"2024-07-26T09:42:09Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-07-26 09:42:09.087 UTC [28] FATAL:  could not write to file \\\"pg_wal/xlogtemp.28\\\": No space left on device\",\"pipe\":\"stderr\",\"logging_pod\":\"database-prod-1\"}\r\n> ...\r\n> ```\r\n> \r\n> We end up deleting the database-prod-1 pod and pvc to create a new standby\r\n> \r\n> cnpg: 1.23.2 postgresql:16.3-4\r\nThis is a different issue. You ran out of space. This has been fixed in 1.24.0 with WAL disk exhaustion safeguard.\n---\n> We see that the pg_wal in the primary container starts to explode in size. We have given our prod db generous space, but eventually it will run out as all the others. Why does pg_wal get so big? We have set up WAL backup to S3 and snapshot backups.\r\npg_wal can get big for 3 main reasons:\r\n1. actual spike in WAL production\r\n2. issues with the WAL archiver\r\n3. issues with replicas (you have replication slots - see below and don't be fooled by the size of these directories)\r\n> 5.0K\t/var/lib/postgresql/data/pgdata/pg_replslot/_cnpg_thiswatch_prod_db_cluster_3\r\n> 5.0K\t/var/lib/postgresql/data/pgdata/pg_replslot/_cnpg_thiswatch_prod_db_cluster_2\r\nVersion 1.24 fixes the avalanche effect.\n---\n> @b-thiswatch The filesystem was already full but if I remember correctly the space was occupied by `pg_wal` directory. We end up deleting the database-prod-1 pod and pvc to create a new standby\r\nThis has been fixed in 1.24\n---\nCan you please make sure you can reproduce this issue in 1.24 or the current trunk? THanks.\n---\nThis is still an issue for me as it seems like I need to get into a healthy state to have my wal flushed and because of the same issues from #5767 this has been getting jammed up and I end up having to allocate huge amounts of disk to get over this loop. My DB is 400GB right now but after hitting this issue many times where the only way to get to healthy state I have grown the volume to 1.5TB otherwise cluster is stuck.\n---\nIt is still not clear how to reproduce this. Can you please provide details?\n---\n> It is still not clear how to reproduce this. Can you please provide details?\nI think we just experienced this problem, it looks very similar. \nHaven't attempted to reproduce yet, but can describe what we were doing here as it might work as a usable starting point to find a reproduction case.\n1. Spin up a cluster with 2 instances, 1000Gb disk; request and limit 1 CPU and 4Gb memory\n2. Launch an \"application\" that has pgbench in the same namespace with request for 7 CPU and 512Mb memory\n3. Exec into the pgbench pod and run `pgbench -i -IdtGvp -s 3000 && pgbench -c 50 -j 10 -P 60 -r -T 3600`\n4. After around 40-45 minutes (about 5 minutes after init is completed and benchmark has started), Something (unclear what) caused a termination signal to be sent to the primary instance.\n5. During shutdown we see lots of logs, but from what I can gather, the gist of it is:\n\t- Primary pod is shutting down\n\t- Request a final checkpoint before demotion\n\t- Unable to do checkpoint because postgres is already shutting down and refuses connection\n\t- Shut down pod anyway\n6. Standby is promoted, old primary attempts to start and is unable to run pg_rewind because:\n\t- Servers diverged\n\t- Could not restore file from archive\n\t- Could not find previous WAL record\n7. Attempt to start server to complete crash recovery, but fail because wal-archiver refuses to archive WAL while switchover in progress\n8. Goto 6\nSee the attached logs for details.\nWill possibly attempt to reproduce later today, once we have completed the benchmarks we wanted to do.\n[logs-2024-11-12.json](https://github.com/user-attachments/files/17717476/logs-2024-11-12.json)\n[cluster.yaml.txt](https://github.com/user-attachments/files/17717485/cluster.yaml.txt)\n[deployment.yaml.txt](https://github.com/user-attachments/files/17717484/deployment.yaml.txt)\n---\nI have managed to create a script that seems to consistently reproduce this problem in our environment.\nRan it 5 times, and the issue happens every time.\nProblem is triggered within 5 minutes after running the script.\nYou need to adapt `bug-repro.yaml.txt` to your environment.\nThe script assumes API-server access from pgbench-cnpg-bug-repro pod.\n[bug-repro.sh.txt](https://github.com/user-attachments/files/17805833/bug-repro.sh.txt)\n[bug-repro.yaml.txt](https://github.com/user-attachments/files/17805834/bug-repro.yaml.txt)\n---\nMight be related to #5969\n---\nLogs:\n<details>\n```txt\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"db-cnpg-cluster-1\",\"version\":\"1.24.0\",\"build\":{\"Version\":\"1.24.0\",\"Commit\":\"5fe5bb6b\",\"Date\":\"2024-08-22\"}}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"logger\":\"setup\",\"msg\":\"Checking for free disk space for WALs before starting PostgreSQL\",\"logging_pod\":\"db-cnpg-cluster-1\"}\n2025-01-02 11:46:09 {\"level\":\"error\",\"ts\":\"2025-01-02T10:46:09Z\",\"logger\":\"setup\",\"msg\":\"Error while checking if there is enough disk space for WALs, skipping\",\"logging_pod\":\"db-cnpg-cluster-1\",\"error\":\"while running pg_controldata to detect WAL segment size: while executing pg_controldata: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.runSubCommand\\n\\tinternal/cmd/manager/instance/run/cmd.go:144\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.NewCmd.func2.1\\n\\tinternal/cmd/manager/instance/run/cmd.go:101\\nk8s.io/client-go/util/retry.OnError.func1\\n\\tpkg/mod/k8s.io/client-go@v0.30.3/util/retry/util.go:51\\nk8s.io/apimachinery/pkg/util/wait.runConditionWithCrashProtection\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/wait.go:145\\nk8s.io/apimachinery/pkg/util/wait.ExponentialBackoff\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/backoff.go:461\\nk8s.io/client-go/util/retry.OnError\\n\\tpkg/mod/k8s.io/client-go@v0.30.3/util/retry/util.go:50\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.NewCmd.func2\\n\\tinternal/cmd/manager/instance/run/cmd.go:100\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.22.6/x64/src/runtime/proc.go:271\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"logger\":\"setup\",\"msg\":\"starting tablespace manager\",\"logging_pod\":\"db-cnpg-cluster-1\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"logger\":\"setup\",\"msg\":\"starting external server manager\",\"logging_pod\":\"db-cnpg-cluster-1\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"db-cnpg-cluster-1\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"db-cnpg-cluster-1\",\"address\":\":8000\",\"hasTLS\":true}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"db-cnpg-cluster-1\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"db-cnpg-cluster-1\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"db-cnpg-cluster-1\",\"address\":\":9187\",\"hasTLS\":false}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"db-cnpg-cluster-1\",\"address\":\"localhost:8010\",\"hasTLS\":false}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n2025-01-02 11:46:09 {\"level\":\"error\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Error waiting on the PostgreSQL process\",\"logging_pod\":\"db-cnpg-cluster-1\",\"error\":\"stat /var/lib/postgresql/data/pgdata: no such file or directory\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start.func1\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:102\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:112\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/manager/runnable_group.go:226\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Stopping and waiting for leader election runnables\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"db-cnpg-cluster-1\"}\n2025-01-02 11:46:09 {\"level\":\"error\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"error received after stop sequence was engaged\",\"error\":\"stat /var/lib/postgresql/data/pgdata: no such file or directory\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).engageStopProcedure.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/manager/internal.go:499\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"db-cnpg-cluster-1\"}\n2025-01-02 11:46:09 {\"level\":\"error\",\"ts\":\"2025-01-02T10:46:09Z\",\"logger\":\"controller-runtime.source.EventHandler\",\"msg\":\"failed to get informer from cache\",\"error\":\"Timeout: failed waiting for *v1.Cluster Informer to sync\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/internal/source.(*Kind[...]).Start.func1.1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/internal/source/kind.go:76\\nk8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/loop.go:53\\nk8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/loop.go:54\\nk8s.io/apimachinery/pkg/util/wait.PollUntilContextCancel\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/poll.go:33\\nsigs.k8s.io/controller-runtime/pkg/internal/source.(*Kind[...]).Start.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/internal/source/kind.go:64\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n2025-01-02 11:46:09 {\"level\":\"error\",\"ts\":\"2025-01-02T10:46:09Z\",\"logger\":\"controller-runtime.source.EventHandler\",\"msg\":\"failed to get informer from cache\",\"error\":\"Timeout: failed waiting for *v1.Cluster Informer to sync\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/internal/source.(*Kind[...]).Start.func1.1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/internal/source/kind.go:76\\nk8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/loop.go:53\\nk8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/loop.go:54\\nk8s.io/apimachinery/pkg/util/wait.PollUntilContextCancel\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/poll.go:33\\nsigs.k8s.io/controller-runtime/pkg/internal/source.(*Kind[...]).Start.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/internal/source/kind.go:64\"}\n2025-01-02 11:46:09 {\"level\":\"error\",\"ts\":\"2025-01-02T10:46:09Z\",\"logger\":\"controller-runtime.source.EventHandler\",\"msg\":\"failed to get informer from cache\",\"error\":\"Timeout: failed waiting for *v1.Cluster Informer to sync\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/internal/source.(*Kind[...]).Start.func1.1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/internal/source/kind.go:76\\nk8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/loop.go:53\\nk8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/loop.go:54\\nk8s.io/apimachinery/pkg/util/wait.PollUntilContextCancel\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/poll.go:33\\nsigs.k8s.io/controller-runtime/pkg/internal/source.(*Kind[...]).Start.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/internal/source/kind.go:64\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"db-cnpg-cluster-1\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"db-cnpg-cluster-1\",\"address\":\":9187\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"db-cnpg-cluster-1\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"db-cnpg-cluster-1\",\"address\":\":8000\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"db-cnpg-cluster-1\",\"address\":\"localhost:8010\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Stopping and waiting for caches\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Stopping and waiting for webhooks\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Stopping and waiting for HTTP servers\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\n2025-01-02 11:46:09 {\"level\":\"info\",\"ts\":\"2025-01-02T10:46:09Z\",\"logger\":\"setup\",\"msg\":\"Checking for free disk space for WALs after PostgreSQL finished\",\"logging_pod\":\"db-cnpg-cluster-1\"}\n2025-01-02 11:46:09 {\"level\":\"error\",\"ts\":\"2025-01-02T10:46:09Z\",\"logger\":\"setup\",\"msg\":\"Error while checking if there is enough disk space for WALs, skipping\",\"logging_pod\":\"db-cnpg-cluster-1\",\"error\":\"while running pg_controldata to detect WAL segment size: while executing pg_controldata: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.runSubCommand\\n\\tinternal/cmd/manager/instance/run/cmd.go:305\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.NewCmd.func2.1\\n\\tinternal/cmd/manager/instance/run/cmd.go:101\\nk8s.io/client-go/util/retry.OnError.func1\\n\\tpkg/mod/k8s.io/client-go@v0.30.3/util/retry/util.go:51\\nk8s.io/apimachinery/pkg/util/wait.runConditionWithCrashProtection\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/wait.go:145\\nk8s.io/apimachinery/pkg/util/wait.ExponentialBackoff\\n\\tpkg/mod/k8s.io/apimachinery@v0.30.3/pkg/util/wait/backoff.go:461\\nk8s.io/client-go/util/retry.OnError\\n\\tpkg/mod/k8s.io/client-go@v0.30.3/util/retry/util.go:50\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.NewCmd.func2\\n\\tinternal/cmd/manager/instance/run/cmd.go:100\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.22.6/x64/src/runtime/proc.go:271\"}\n```\n</details>\nYesterday everything worked. After computer reboot database cant start. \n* show \"no free space\" error (Its false i have over 300GB free space)\n* cant find `pgdata`\nMy setup: windows 10 with WSL2 and Docker Desktop"
    },
    {
        "title": "[Bug]: Pooler login failing after migrating to Postgres 16",
        "id": 2341214484,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nnick@e3b0c442.dev\n### Version\n1.23.1\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nAfter upgrading a cluster from Postgres 15 to 16, the pooler refuses connections with `FATAL: bouncer config error`. Looking at the pooler logs, there are attempts to log into a database on the system that does not exist. However, if I update the app configuration to access the database's RW endpoint directly. login succeeds and the app operates as expected.\r\nThe upgrade migration was done using the pubsub live migration method. The database that is being referenced did exist and hold the application's data at one point in time, however in a backup/restore operation after moving onto CNPG I restored into the default `app` database with the default `app` user. That restore was done well before this upgrade attempt.\r\nIn order to attempt to isolate the issue and make sure some other cruft didn't come in the migration, I dumped just the `app` database manually and restored it in similar fashion to a clean `Cluster`. The pooler fails in a similar fashion.\n### Cluster resource\n```shell\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: pg16\r\n  namespace: paperless\r\nspec:\r\n  instances: 2\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.3\r\n  affinity:\r\n    topologyKey: topology.kubernetes.io/zone \r\n    podAntiAffinityType: preferred\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3://<REDACTED>/\r\n      endpointURL: https://<REDACTED>\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: <REDACTED>-pg-backup\r\n          key: access-key-id\r\n        secretAccessKey:\r\n          name: <REDACTED>-pg-backup\r\n          key: secret-access-key\r\n        region:\r\n          name: <REDACTED>-pg-backup\r\n          key: region\r\n      data:\r\n        compression: bzip2\r\n      wal:\r\n        compression: bzip2\r\n    retentionPolicy: \"7d\"\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  storage:\r\n    size: 4Gi\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-06-07T21:33:26Z\",\"msg\":\"record\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-06-07 21:33:26.923 UTC\",\"pid\":\"12\",\"level\":\"LOG\",\"msg\":\"C-0x5604b56b75f0: paperless/(nouser)@10.42.10.158:54952 closing because: server login has been failing, try again later (server_login_retry) (age=0s)\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-06-07T21:33:26Z\",\"msg\":\"record\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-06-07 21:33:26.923 UTC\",\"pid\":\"12\",\"level\":\"WARNING\",\"msg\":\"C-0x5604b56b75f0: paperless/(nouser)@10.42.10.158:54952 pooler error: server login has been failing, try again later (server_login_retry)\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-06-07T21:33:26Z\",\"msg\":\"record\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-06-07 21:33:26.949 UTC\",\"pid\":\"12\",\"level\":\"LOG\",\"msg\":\"S-0x5604b56d1560: paperless/cnpg_pooler_pgbouncer@10.43.230.19:5432 new connection to server (from 10.42.6.33:36834)\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-06-07T21:33:26Z\",\"msg\":\"record\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-06-07 21:33:26.961 UTC\",\"pid\":\"12\",\"level\":\"LOG\",\"msg\":\"S-0x5604b56d1560: paperless/cnpg_pooler_pgbouncer@10.43.230.19:5432 SSL established: TLSv1.3/TLS_AES_256_GCM_SHA384/ECDH=prime256v1\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-06-07T21:33:26Z\",\"msg\":\"record\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-06-07 21:33:26.963 UTC\",\"pid\":\"12\",\"level\":\"WARNING\",\"msg\":\"server login failed: FATAL database \\\"paperless\\\" does not exist\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-06-07T21:33:26Z\",\"msg\":\"record\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-06-07 21:33:26.963 UTC\",\"pid\":\"12\",\"level\":\"LOG\",\"msg\":\"S-0x5604b56d1560: paperless/cnpg_pooler_pgbouncer@10.43.230.19:5432 closing because: login failed (age=0s)\"}}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nnick@e3b0c442.dev\n### Version\n1.23.1\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nAfter upgrading a cluster from Postgres 15 to 16, the pooler refuses connections with `FATAL: bouncer config error`. Looking at the pooler logs, there are attempts to log into a database on the system that does not exist. However, if I update the app configuration to access the database's RW endpoint directly. login succeeds and the app operates as expected.\r\nThe upgrade migration was done using the pubsub live migration method. The database that is being referenced did exist and hold the application's data at one point in time, however in a backup/restore operation after moving onto CNPG I restored into the default `app` database with the default `app` user. That restore was done well before this upgrade attempt.\r\nIn order to attempt to isolate the issue and make sure some other cruft didn't come in the migration, I dumped just the `app` database manually and restored it in similar fashion to a clean `Cluster`. The pooler fails in a similar fashion.\n### Cluster resource\n```shell\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: pg16\r\n  namespace: paperless\r\nspec:\r\n  instances: 2\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.3\r\n  affinity:\r\n    topologyKey: topology.kubernetes.io/zone \r\n    podAntiAffinityType: preferred\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3://<REDACTED>/\r\n      endpointURL: https://<REDACTED>\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: <REDACTED>-pg-backup\r\n          key: access-key-id\r\n        secretAccessKey:\r\n          name: <REDACTED>-pg-backup\r\n          key: secret-access-key\r\n        region:\r\n          name: <REDACTED>-pg-backup\r\n          key: region\r\n      data:\r\n        compression: bzip2\r\n      wal:\r\n        compression: bzip2\r\n    retentionPolicy: \"7d\"\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  storage:\r\n    size: 4Gi\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-06-07T21:33:26Z\",\"msg\":\"record\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-06-07 21:33:26.923 UTC\",\"pid\":\"12\",\"level\":\"LOG\",\"msg\":\"C-0x5604b56b75f0: paperless/(nouser)@10.42.10.158:54952 closing because: server login has been failing, try again later (server_login_retry) (age=0s)\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-06-07T21:33:26Z\",\"msg\":\"record\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-06-07 21:33:26.923 UTC\",\"pid\":\"12\",\"level\":\"WARNING\",\"msg\":\"C-0x5604b56b75f0: paperless/(nouser)@10.42.10.158:54952 pooler error: server login has been failing, try again later (server_login_retry)\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-06-07T21:33:26Z\",\"msg\":\"record\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-06-07 21:33:26.949 UTC\",\"pid\":\"12\",\"level\":\"LOG\",\"msg\":\"S-0x5604b56d1560: paperless/cnpg_pooler_pgbouncer@10.43.230.19:5432 new connection to server (from 10.42.6.33:36834)\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-06-07T21:33:26Z\",\"msg\":\"record\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-06-07 21:33:26.961 UTC\",\"pid\":\"12\",\"level\":\"LOG\",\"msg\":\"S-0x5604b56d1560: paperless/cnpg_pooler_pgbouncer@10.43.230.19:5432 SSL established: TLSv1.3/TLS_AES_256_GCM_SHA384/ECDH=prime256v1\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-06-07T21:33:26Z\",\"msg\":\"record\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-06-07 21:33:26.963 UTC\",\"pid\":\"12\",\"level\":\"WARNING\",\"msg\":\"server login failed: FATAL database \\\"paperless\\\" does not exist\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-06-07T21:33:26Z\",\"msg\":\"record\",\"pipe\":\"stderr\",\"record\":{\"timestamp\":\"2024-06-07 21:33:26.963 UTC\",\"pid\":\"12\",\"level\":\"LOG\",\"msg\":\"S-0x5604b56d1560: paperless/cnpg_pooler_pgbouncer@10.43.230.19:5432 closing because: login failed (age=0s)\"}}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI also tried a microservice import of just the app database into a new pg16 cluster from the old pg15 cluster with the same result.\n---\nI've tried migrating a second database from Postgres 15 to 16 with a similar result. Going to dig a bit further into this and see if I can figure anything else out.\n---\nPool works as expected on a completely fresh cluster, so there's nothing inherently at issue there.\n---\nInterestingly enough, doing a pg_restore of the dump I took yesterday into this new clean database also doesn't cause the pool to stop working. Diving into this. (The same case failed yesterday except that I hadn't checked the pool on the virgin cluster first)\n---\nOK, I was able to find a sequence that works.\r\n1. Manually dump the old database (`pg_dump -Fc app > app.dump`)\r\n2. Start a clean cluster\r\n3. Start a new pooler for that cluster\r\n4. Connect to the new cluster via the pooler\r\n5. Manually restore the dump (`pg_restore --no-acl --no-owner --role app -d app app.dump`)\r\nFollowing this sequence, the pooler works as expected and the app is able to connect to the pooler.\r\nMy suspicion, having no knowledge about the internals of the pooler, is that when it connects for the first time, it does some setup in the target database. However, if that data is already there, the setup bails/fails. I suspect there is some small difference between what is in the database and what the pooler is trying to set up, enough to cause the failure.\r\nWhen the cluster is set up to restore or bootstrap, the restore is done before the pooler connects, which leads to the failure.\r\nDuring the restore, there was one warning:\r\n```\r\npostgres@pg16-1:~/data$ pg_restore --no-acl --no-owner --role app -d app app.dump \r\npg_restore: error: could not execute query: ERROR:  function \"user_search\" already exists with same argument types\r\nCommand was: CREATE FUNCTION public.user_search(uname text) RETURNS TABLE(usename name, passwd text)\r\n    LANGUAGE sql SECURITY DEFINER\r\n    AS $_$SELECT usename, passwd FROM pg_catalog.pg_shadow WHERE usename=$1;$_$;\r\npg_restore: warning: errors ignored on restore: 1\r\n```\r\nI checked this definition in both the source and destination databases and it appeared identical, so I don't believe this specific thing was it.\r\nI will note that these databases I'm trying to migrate were originally on different platforms and have been migrated to CloudNativePG over time. They started on Stackgres, were moved to Percona Postgres Operator, and then most recently to CloudNativePG. There very well could be some cruft left in from the prior platforms' implementations.\r\nI suspect that, now that things have been \"corrected,\" future migrations of these databases will not have this issue."
    },
    {
        "title": "[E2E]: Add pgaudit tests",
        "id": 2335762688,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCNPG assumes pgaudit is installed in the operand images, but we don't have an E2E test that verifies that the extension is present and compatible.\n### Describe the solution you'd like\nWe should add an E2E test that enables the extension and verify the generated logs are correctly parsed.\n### Describe alternatives you've considered\nCurrently, the pgaudit code is well covered by unit tests, which are great for verifying that we don't break the feature. However, an E2E test is needed to confirm that the extension is present and check that the latest changes in pgaudit don't break compatibility.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCNPG assumes pgaudit is installed in the operand images, but we don't have an E2E test that verifies that the extension is present and compatible.\n### Describe the solution you'd like\nWe should add an E2E test that enables the extension and verify the generated logs are correctly parsed.\n### Describe alternatives you've considered\nCurrently, the pgaudit code is well covered by unit tests, which are great for verifying that we don't break the feature. However, an E2E test is needed to confirm that the extension is present and check that the latest changes in pgaudit don't break compatibility.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conductdon't we only assume it's installed if the user tries to configure it? so i'd say the issue is not whether the extension is present or compatible in our operand images, but rather that with the default operand image, it works and we parse logs as intended. no?"
    },
    {
        "title": "fix: make restore job failed if wal replay is paused ",
        "id": 2324946422,
        "state": "open",
        "first": "closes: #4721 \r\nPartially closes https://github.com/cloudnative-pg/cloudnative-pg/issues/2478 https://github.com/cloudnative-pg/cloudnative-pg/issues/2337",
        "messages": "closes: #4721 \r\nPartially closes https://github.com/cloudnative-pg/cloudnative-pg/issues/2478 https://github.com/cloudnative-pg/cloudnative-pg/issues/2337"
    },
    {
        "title": "[Bug]: Avoid restore job hang if max_connection is increased after last basebackup",
        "id": 2324894638,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.1\n### What version of Kubernetes are you using?\n1.30 (unsupported)\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nThis is the followup of  https://github.com/cloudnative-pg/cloudnative-pg/issues/2478 https://github.com/cloudnative-pg/cloudnative-pg/issues/2337\r\nDuring restore job, if the enforced params like max_connection is increased between base backup and restore points, we can use a higher value defined in the restore yaml file to make the restore complete. but if we did not define a higher value in restore yaml file, the restore job will hang forever\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.1\n### What version of Kubernetes are you using?\n1.30 (unsupported)\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nThis is the followup of  https://github.com/cloudnative-pg/cloudnative-pg/issues/2478 https://github.com/cloudnative-pg/cloudnative-pg/issues/2337\r\nDuring restore job, if the enforced params like max_connection is increased between base backup and restore points, we can use a higher value defined in the restore yaml file to make the restore complete. but if we did not define a higher value in restore yaml file, the restore job will hang forever\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: Scaleway S3 Http Errors",
        "id": 2322642348,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nirsanvanwel@gmail.com\n### Version\n1.23.1\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nBarman Cloud backups via S3 works with Amazon S3, Wasabi, Tebi.io etc but with Scaleway Object Storage (https://www.scaleway.com/en/object-storage/) we are getting errors. It does manage to submit some WAL files but is throwing Http Header related errors.\r\nI think if more configuration options we available this might be fixable. \r\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nirsanvanwel@gmail.com\n### Version\n1.23.1\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nBarman Cloud backups via S3 works with Amazon S3, Wasabi, Tebi.io etc but with Scaleway Object Storage (https://www.scaleway.com/en/object-storage/) we are getting errors. It does manage to submit some WAL files but is throwing Http Header related errors.\r\nI think if more configuration options we available this might be fixable. \r\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductHave you got some examples of errors ?\n---\nWe have exactly the same issues - please check the following error messages @pchovelon :\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-07-16T09:27:32Z\",\"logger\":\"wal-archive\",\"msg\":\"Failed archiving WAL: PostgreSQL will retry\",\"logging_pod\":\"airflow-pg-1\",\"walName\":\"pg_wal/00000007.history\",\"startTime\":\"2024-07-16T09:27:32Z\",\"endTime\":\"2024-07-16T09:27:32Z\",\"elapsedWalTime\":0.32404131,\"error\":\"unexpected failure invoking barman-cloud-wal-archive: exit status 4\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-16T09:27:33Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"2024-07-16 09:27:33,525 [83859] WARNING: Failed to parse headers (url=https://xxxxxxx.s3.fr-par.scw.cloud:443/scw-cnpg-dev-stg-testing/airflow-pg/wals/0000000300000000/000000030000000000000010.partial.gz): [MissingHeaderBodySeparatorDefect()], unparsed data: 'HTTP/1.1 400 Bad request\\\\r\\\\nContent-length: 90\\\\r\\\\nCache-Control: no-cache\\\\r\\\\nConnection: close\\\\r\\\\nContent-Type: text/html\\\\r\\\\n\\\\r\\\\n'\",\"pipe\":\"stderr\",\"logging_pod\":\"airflow-pg-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-16T09:27:33Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"Traceback (most recent call last):\",\"pipe\":\"stderr\",\"logging_pod\":\"airflow-pg-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-16T09:27:33Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"  File \\\"/usr/local/lib/python3.9/dist-packages/urllib3/connectionpool.py\\\", line 487, in _make_request\",\"pipe\":\"stderr\",\"logging_pod\":\"airflow-pg-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-16T09:27:33Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"    assert_header_parsing(httplib_response.msg)\",\"pipe\":\"stderr\",\"logging_pod\":\"airflow-pg-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-16T09:27:33Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"  File \\\"/usr/local/lib/python3.9/dist-packages/urllib3/util/response.py\\\", line 91, in assert_header_parsing\",\"pipe\":\"stderr\",\"logging_pod\":\"airflow-pg-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-16T09:27:33Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"    raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)\",\"pipe\":\"stderr\",\"logging_pod\":\"airflow-pg-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-16T09:27:33Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"urllib3.exceptions.HeaderParsingError: [MissingHeaderBodySeparatorDefect()], unparsed data: 'HTTP/1.1 400 Bad request\\\\r\\\\nContent-length: 90\\\\r\\\\nCache-Control: no-cache\\\\r\\\\nConnection: close\\\\r\\\\nContent-Type: text/html\\\\r\\\\n\\\\r\\\\n'\",\"pipe\":\"stderr\",\"logging_pod\":\"airflow-pg-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-07-16T09:27:33Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"2024-07-16 09:27:33,528 [83859] ERROR: Barman cloud WAL archiver exception: An error occurred () when calling the PutObject operation: \",\"pipe\":\"stderr\",\"logging_pod\":\"airflow-pg-1\"}\r\n```\r\nWe opened also an issue at Barman here:\r\nhttps://github.com/EnterpriseDB/barman/issues/962\n---\nI am encountering a similar issue with S3 on OVH. After deep debug, it seems that the S3 provider need the 'region' parameter to be define. \r\nSo I add region in `barmanObjectStore/s3Credentials`.\r\nHere's the patch I used with Kustomize\r\n```yaml\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster\r\nspec:\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: \"s3://my-bucket/\"\r\n      endpointURL: \"https://s3.rbx.io.cloud.ovh.net/\"\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: cluster-pg-s3\r\n          key: ACCESS_KEY_ID\r\n        secretAccessKey:\r\n          name: cluster-pg-s3\r\n          key: ACCESS_SECRET_KEY\r\n        region:\r\n          name: cluster-pg-s3\r\n          key: S3_REGION\r\n```\r\nwith `cluster-pg-s3` as :\r\n```\r\nACCESS_KEY_ID=XXX\r\nACCESS_SECRET_KEY=XXX\r\nS3_REGION=rbx\r\n```\n---\n@ivanwel an example will be added in the documentation : \nhttps://github.com/cloudnative-pg/cloudnative-pg/pull/6143\n---\nMy pull request was merged. I think this could be closed."
    },
    {
        "title": "[Bug]: Operator secrets were deleted during K8s upgrade process",
        "id": 2322160707,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\nolder in 1.22.x\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nHelm\n### What happened?\nDuring RKE2 Kubernetes upgrade from 1.27.10 to 1.28.8 after deleting the secondary POD instance on a node it did not come on up the other worker node. On investigation it was observed that the app-secret and superuser-secrets were deleted so they could not be mounted. Although the secrets were re-created from the Primary instance again., this was not observed on any other cluster running CNPG Postgres. Has anyone else faced such or similar issue during upgrades?\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\nolder in 1.22.x\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nHelm\n### What happened?\nDuring RKE2 Kubernetes upgrade from 1.27.10 to 1.28.8 after deleting the secondary POD instance on a node it did not come on up the other worker node. On investigation it was observed that the app-secret and superuser-secrets were deleted so they could not be mounted. Although the secrets were re-created from the Primary instance again., this was not observed on any other cluster running CNPG Postgres. Has anyone else faced such or similar issue during upgrades?\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductHello @abhishekrajshah \r\nThis looks more like an issue on the Kubernetes side, since the operator doesn't delete a secret unless you destroy the cluster, was the upgrade process affected something else besides CNPG secrets?\r\nCheers!"
    },
    {
        "title": "[Feature]: make instance manager more unit-testable with Postgres interface",
        "id": 2319135939,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nIn the instance manager, the Postgres binary is often called in the middle\r\nof functions. For example, in `func (info InitInfo) CreateDataDirectory()`\r\nThere is\r\n``` go\r\n\tinitdbCmd := exec.Command(constants.InitdbName, options...) // #nosec\r\n\terr := execlog.RunBuffering(initdbCmd, constants.InitdbName)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"error while creating the PostgreSQL instance: %w\", err)\r\n\t}\r\n```\r\nHaving this dependency makes it impossible to write a unit test for `CreateDataDirectory`.\r\nThe solution we end up taking is to write small functions that skirt around the dependencies,\r\nand then unit test those.\r\nBut those tests are generally not very significant, while the significant calls remain untested.\r\n### Describe the solution you'd like\r\nWe should have all calls to the `postgres` binaries behind an interface.\r\nThat way, we could have meaningful unit tests for many operations that\r\nare now impossible to test properly.\r\nSomething along the lines of\r\n``` go\r\ntype PgExecutor interface {\r\n\tInitdb(...string) error\r\n\tPgCtl(subcommand string, options ...string) error\r\n\t// ...\r\n}\r\n```\r\n### Describe alternatives you've considered\r\nThe current approach is to have small functions that skirt around the bits that call PG binaries.\r\nWe end up with more unit tests that are each not very significant.\r\nAnd they also suffer from testing \"implementation, not interface\".\r\n### Additional context\r\nRelated to https://github.com/cloudnative-pg/cloudnative-pg/issues/2222\r\nbut narrower.\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nIn the instance manager, the Postgres binary is often called in the middle\r\nof functions. For example, in `func (info InitInfo) CreateDataDirectory()`\r\nThere is\r\n``` go\r\n\tinitdbCmd := exec.Command(constants.InitdbName, options...) // #nosec\r\n\terr := execlog.RunBuffering(initdbCmd, constants.InitdbName)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"error while creating the PostgreSQL instance: %w\", err)\r\n\t}\r\n```\r\nHaving this dependency makes it impossible to write a unit test for `CreateDataDirectory`.\r\nThe solution we end up taking is to write small functions that skirt around the dependencies,\r\nand then unit test those.\r\nBut those tests are generally not very significant, while the significant calls remain untested.\r\n### Describe the solution you'd like\r\nWe should have all calls to the `postgres` binaries behind an interface.\r\nThat way, we could have meaningful unit tests for many operations that\r\nare now impossible to test properly.\r\nSomething along the lines of\r\n``` go\r\ntype PgExecutor interface {\r\n\tInitdb(...string) error\r\n\tPgCtl(subcommand string, options ...string) error\r\n\t// ...\r\n}\r\n```\r\n### Describe alternatives you've considered\r\nThe current approach is to have small functions that skirt around the bits that call PG binaries.\r\nWe end up with more unit tests that are each not very significant.\r\nAnd they also suffer from testing \"implementation, not interface\".\r\n### Additional context\r\nRelated to https://github.com/cloudnative-pg/cloudnative-pg/issues/2222\r\nbut narrower.\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Docs]:  ImageCatalog installation example not working with a git softlink",
        "id": 2316955186,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\r\n### What problem in the existing documentation this issue aims to solve?\r\nThe documentation for image catalogs says:\r\n> You can install the [latest version of the cluster catalog for the PostgreSQL Container Images](https://raw.githubusercontent.com/cloudnative-pg/postgres-containers/main/Debian/ClusterImageCatalog.yaml) ([cloudnative-pg/postgres-containers](https://github.com/cloudnative-pg/postgres-containers) repository) with:\r\n```bash\r\nkubectl apply \\\r\n  -f https://raw.githubusercontent.com/cloudnative-pg/postgres-containers/main/Debian/ClusterImageCatalog.yaml\r\n```\r\nbut because the `main/Debian/ClusterImageCatalog.yaml` is a symlink to `main/Debian/ClusterImageCatalog-bullseye.yaml` the contents of the file as render by [`raw.githubusercontent.com`](https://raw.githubusercontent.com/cloudnative-pg/postgres-containers/main/Debian/ClusterImageCatalog.yaml) are simply:\r\n```\r\nClusterImageCatalog-bullseye.yaml\r\n```\r\nYou will have to either:\r\n* remove the symlink\r\n* update the documentation\r\n### Backport?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct\r\nRelated to: #4320",
        "messages": "### Is there an existing issue already for your request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\r\n### What problem in the existing documentation this issue aims to solve?\r\nThe documentation for image catalogs says:\r\n> You can install the [latest version of the cluster catalog for the PostgreSQL Container Images](https://raw.githubusercontent.com/cloudnative-pg/postgres-containers/main/Debian/ClusterImageCatalog.yaml) ([cloudnative-pg/postgres-containers](https://github.com/cloudnative-pg/postgres-containers) repository) with:\r\n```bash\r\nkubectl apply \\\r\n  -f https://raw.githubusercontent.com/cloudnative-pg/postgres-containers/main/Debian/ClusterImageCatalog.yaml\r\n```\r\nbut because the `main/Debian/ClusterImageCatalog.yaml` is a symlink to `main/Debian/ClusterImageCatalog-bullseye.yaml` the contents of the file as render by [`raw.githubusercontent.com`](https://raw.githubusercontent.com/cloudnative-pg/postgres-containers/main/Debian/ClusterImageCatalog.yaml) are simply:\r\n```\r\nClusterImageCatalog-bullseye.yaml\r\n```\r\nYou will have to either:\r\n* remove the symlink\r\n* update the documentation\r\n### Backport?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct\r\nRelated to: #4320"
    },
    {
        "title": "Foreign data wrappers for Postgres",
        "id": 2316748262,
        "state": "open",
        "first": "Introduce declarative support for foreign data wrappers, starting with Postgres. In some cases, this might improve horizontal scalability by distributing data and queries on multiple instances (please refer to foreign tables in the Postgres docs).",
        "messages": "Introduce declarative support for foreign data wrappers, starting with Postgres. In some cases, this might improve horizontal scalability by distributing data and queries on multiple instances (please refer to foreign tables in the Postgres docs).Blocked by #6292"
    },
    {
        "title": "Declarative support for PostgreSQL major in-place upgrades",
        "id": 2316747829,
        "state": "open",
        "first": "Currently, Major upgrades of PostgreSQL can be done in an imperative way, by relying on the native logical replication capabilities and the `cnpg` plugin. See: https://www.gabrielebartolini.it/articles/2024/03/cloudnativepg-recipe-5-how-to-migrate-your-postgresql-database-in-kubernetes-with-~0-downtime-from-anywhere/\nHowever, we should provide a way to support major upgrades just by changing the `major` field like in the following example:\n```yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: cluster-example\nspec:\n  instances: 3\n  imageCatalogRef:\n    apiGroup: postgresql.cnpg.io\n    kind: ImageCatalog\n    name: postgresql\n    major: 16\n  storage:\n    size: 1Gi\n```\nTo:\n```yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: cluster-example\nspec:\n  instances: 3\n  imageCatalogRef:\n    apiGroup: postgresql.cnpg.io\n    kind: ImageCatalog\n    name: postgresql\n    major: 17\n  storage:\n    size: 1Gi\n```\nAlternative approaches are valid too, for example through CRDs that evoke imperative names (like upgrades) and that are useful too as they prepare the underlying PVCs for the new major.",
        "messages": "Currently, Major upgrades of PostgreSQL can be done in an imperative way, by relying on the native logical replication capabilities and the `cnpg` plugin. See: https://www.gabrielebartolini.it/articles/2024/03/cloudnativepg-recipe-5-how-to-migrate-your-postgresql-database-in-kubernetes-with-~0-downtime-from-anywhere/\nHowever, we should provide a way to support major upgrades just by changing the `major` field like in the following example:\n```yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: cluster-example\nspec:\n  instances: 3\n  imageCatalogRef:\n    apiGroup: postgresql.cnpg.io\n    kind: ImageCatalog\n    name: postgresql\n    major: 16\n  storage:\n    size: 1Gi\n```\nTo:\n```yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: cluster-example\nspec:\n  instances: 3\n  imageCatalogRef:\n    apiGroup: postgresql.cnpg.io\n    kind: ImageCatalog\n    name: postgresql\n    major: 17\n  storage:\n    size: 1Gi\n```\nAlternative approaches are valid too, for example through CRDs that evoke imperative names (like upgrades) and that are useful too as they prepare the underlying PVCs for the new major.Any updates on this?\r\nI can't believe that CNPG doesn't have major pg upgrades.\n---\nIt's on the roadmap for 1.26.0 https://github.com/orgs/cloudnative-pg/projects/1/views/1 - as is also noted by the events listed above your comment ;)\n---\nVersion 1.25 introduced support for logical subscriptions, which simplifies the process of performing major upgrades using logical replication. Additionally, we've added the ability to perform [parallel dump and restore](https://cloudnative-pg.io/documentation/current/database_import/#customizing-pg_dump-and-pg_restore-behavior), providing more flexibility for handling large-scale database operations. These features already cover many everyday use cases\u2014logical online upgrades were, in fact, our preferred approach for Postgres upgrades during our time at 2ndQuadrant.\nI've been reflecting on this feature and considering how the direction we're taking with operand container images and extensions might influence its evolution. Specifically, we're working towards separating PostgreSQL operand images from extensions starting with PostgreSQL 18, with a proposed patch by EDB for the `extension_control_path` in PostgreSQL (see [RFC: Extension Packaging Lookup](https://justatheory.com/2024/11/rfc-extension-packaging-lookup/). It is hard to say without a prototype, but I remain concerned about the potential for extension upgrades to lead to unpredictable or unrecoverable scenarios. \nHowever, given the strong demand for this feature, one approach for an initial implementation could be an **offline update**. If the update fails, the system would revert to the previous PostgreSQL version, resulting in an inconsistent state and requiring manual intervention to change the cluster definition. If successful, the process would promote the primary, destroy the standbys, and recreate them in sequence. Thoughts?\nGiven our current commitments, I believe we can start working on this feature for version 1.26. However, due to the uncertainties mentioned earlier, there is a possibility that its delivery might be delayed.\n---\n> one approach for an initial implementation could be an offline update. If the update fails, the system would revert to the previous PostgreSQL version, resulting in an inconsistent state and requiring manual intervention to change the cluster definition.\nThe main point of concern is extensions, right? How is the risk of failure/inconsistency different when performing offline updates?\n---\n> > one approach for an initial implementation could be an offline update. If the update fails, the system would revert to the previous PostgreSQL version, resulting in an inconsistent state and requiring manual intervention to change the cluster definition.\n> \n> The main point of concern is extensions, right? How is the risk of failure/inconsistency different when performing offline updates?\nYou work on a new cluster. You have time to prepare it in advance and test it, especially with applications. That's always been the main advantage of migrations to a different cluster, at least in my experience. You will have time to test upgrades to a different extension version as well.\n---\nAh ok that's what you mean, makes sense. The declarative in-place upgrade approach wouldn't prevent people from using the imperative approach to have more safety if preferred, right? Even clusters that don't use extensions could hit a bug in a newer PG version and (subtly) break, so  cluster upgrades would need to go through the usual QA processes anyway\n---\nI\u2019ve submitted a possible implementation for declarative support for PostgreSQL major in-place upgrades in #6664. I would greatly appreciate it if you could review and test my approach and provide feedback or suggestions for improvement."
    },
    {
        "title": "Remove Barman Cloud requirement for backup and recovery",
        "id": 2316737926,
        "state": "open",
        "first": "Currently, CloudNativePG operand images ship with Barman Cloud. However, with the new CNPG-I (#3699), we should remove Barman Cloud from the main operand images and make it part of a default available plugin, used both for WAL archiving/restore and base backup/restore.",
        "messages": "Currently, CloudNativePG operand images ship with Barman Cloud. However, with the new CNPG-I (#3699), we should remove Barman Cloud from the main operand images and make it part of a default available plugin, used both for WAL archiving/restore and base backup/restore.In version 1.25 we introduce support for Barman Cloud plugin. We will remove Barman Cloud requirement when we have given everyone the opportunity to move to the new solution."
    },
    {
        "title": "Kubernetes' `VolumeGroupSnapshot` API support",
        "id": 2316737202,
        "state": "open",
        "first": "",
        "messages": "Moving this to 1.26.0 as the feature is still alpha in Kubernetes and Leonardo is working on it."
    },
    {
        "title": "[Bug]: Request to delete unexisting volume [1.22.2]",
        "id": 2312428433,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nadessonnaz@gmail.com\n### Version\nolder in 1.22.x\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nHelm\n### What happened?\nI use the SKS service from Exoscale, a Swiss cloud provider. They wrote to me saying I was spamming their CSI controller. I originally (1 month ago) did various tests with volumes and snapshots with deletions. \r\nI'm currently using a single cluster with a single volume and no volume snapshots (backups are finally done on s3).\r\nThey told me that I'm still sending deletion requests for non-existent volumes. In the last two hours, I've sent around 2,500 such requests, with a peak every 5 minutes or so.\r\nCould you give me some ideas for a solution? I've analyzed the pods, backups and clusters, but I can't find anything wrong at the moment.\n### Cluster resource\n```shell\npiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: mrp-postgres-v3\r\n  namespace: mrpexam\r\nspec:\r\n  instances: 1\r\n  superuserSecret:\r\n    name: superuser-secret\r\n  storage:\r\n    storageClass: exoscale-sbs\r\n    size: 10Gi\r\n  bootstrap:\r\n    initdb:\r\n      database: directus\r\n      owner: directus\r\n      secret:\r\n        name: directus-secret\r\n      import:\r\n        type: microservice\r\n        databases:\r\n          - directus\r\n        source:\r\n          externalCluster: old-cluster-pg\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: \"...\"\r\n      endpointURL: \"...\"\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: aws-creds\r\n          key: ACCESS_KEY_ID\r\n        secretAccessKey:\r\n          name: aws-creds\r\n          key: ACCESS_SECRET_KEY\r\n    retentionPolicy: \"30d\"\r\n  externalClusters:\r\n    - name: old-cluster-pg\r\n      connectionParameters:\r\n        host: postgres-service\r\n        user: directus\r\n        dbname: directus\r\n      password:\r\n        name: cluster-pg96-superuser\r\n        key: password\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nadessonnaz@gmail.com\n### Version\nolder in 1.22.x\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nHelm\n### What happened?\nI use the SKS service from Exoscale, a Swiss cloud provider. They wrote to me saying I was spamming their CSI controller. I originally (1 month ago) did various tests with volumes and snapshots with deletions. \r\nI'm currently using a single cluster with a single volume and no volume snapshots (backups are finally done on s3).\r\nThey told me that I'm still sending deletion requests for non-existent volumes. In the last two hours, I've sent around 2,500 such requests, with a peak every 5 minutes or so.\r\nCould you give me some ideas for a solution? I've analyzed the pods, backups and clusters, but I can't find anything wrong at the moment.\n### Cluster resource\n```shell\npiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: mrp-postgres-v3\r\n  namespace: mrpexam\r\nspec:\r\n  instances: 1\r\n  superuserSecret:\r\n    name: superuser-secret\r\n  storage:\r\n    storageClass: exoscale-sbs\r\n    size: 10Gi\r\n  bootstrap:\r\n    initdb:\r\n      database: directus\r\n      owner: directus\r\n      secret:\r\n        name: directus-secret\r\n      import:\r\n        type: microservice\r\n        databases:\r\n          - directus\r\n        source:\r\n          externalCluster: old-cluster-pg\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: \"...\"\r\n      endpointURL: \"...\"\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: aws-creds\r\n          key: ACCESS_KEY_ID\r\n        secretAccessKey:\r\n          name: aws-creds\r\n          key: ACCESS_SECRET_KEY\r\n    retentionPolicy: \"30d\"\r\n  externalClusters:\r\n    - name: old-cluster-pg\r\n      connectionParameters:\r\n        host: postgres-service\r\n        user: directus\r\n        dbname: directus\r\n      password:\r\n        name: cluster-pg96-superuser\r\n        key: password\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI have updated the Helm chart to the latest version (1.23.1). This didn't solve the problem, there must be a bug somewhere."
    },
    {
        "title": "[Feature]: Add short names on crds",
        "id": 2308545929,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe crds are missing a shortname and if there's another CRD that exists with the same kind added after cnpg it becomes more challenging in k9s and kubectl to find the resource without the plugin. This becomes very common with the kind \"Cluster\". \n### Describe the solution you'd like\nOn the CRDS add the `shortNames:` section with some unique shortnames like:\r\n```yaml\r\napiVersion: apiextensions.k8s.io/v1\r\nkind: CustomResourceDefinition\r\nmetadata:\r\n  annotations:\r\n    controller-gen.kubebuilder.io/version: v0.15.0\r\n  name: clusters.postgresql.cnpg.io\r\nspec:\r\n  group: postgresql.cnpg.io\r\n  names:\r\n    kind: Cluster\r\n    listKind: ClusterList\r\n    plural: clusters\r\n    singular: cluster\r\n    shortNames:\r\n    - cnpgcluster  \r\n  ```\n### Describe alternatives you've considered\nNone, this was the easiest solution as it's supported in k8s. \n### Additional context\nI've added the following to my clusters.postgresql.cnpg.io CRD in my cluster to check:\r\n```yaml\r\nshortNames:\r\n- cnpgcluster\r\n```\r\nThis allowed me to use cnpgcluster in k9s and kubectl to easily find my cnpg clusters. \n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe crds are missing a shortname and if there's another CRD that exists with the same kind added after cnpg it becomes more challenging in k9s and kubectl to find the resource without the plugin. This becomes very common with the kind \"Cluster\". \n### Describe the solution you'd like\nOn the CRDS add the `shortNames:` section with some unique shortnames like:\r\n```yaml\r\napiVersion: apiextensions.k8s.io/v1\r\nkind: CustomResourceDefinition\r\nmetadata:\r\n  annotations:\r\n    controller-gen.kubebuilder.io/version: v0.15.0\r\n  name: clusters.postgresql.cnpg.io\r\nspec:\r\n  group: postgresql.cnpg.io\r\n  names:\r\n    kind: Cluster\r\n    listKind: ClusterList\r\n    plural: clusters\r\n    singular: cluster\r\n    shortNames:\r\n    - cnpgcluster  \r\n  ```\n### Describe alternatives you've considered\nNone, this was the easiest solution as it's supported in k8s. \n### Additional context\nI've added the following to my clusters.postgresql.cnpg.io CRD in my cluster to check:\r\n```yaml\r\nshortNames:\r\n- cnpgcluster\r\n```\r\nThis allowed me to use cnpgcluster in k9s and kubectl to easily find my cnpg clusters. \n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct+1"
    },
    {
        "title": "Unable to restore a VolumeSnapshot Backup ",
        "id": 2315677071,
        "state": "open",
        "first": "Hello,\r\nI've set up two namespaces, let's say source and destination, the dst is empty for now.\r\nI've created two VolumeSnapshotClass.\r\nI've created a minimal cluster in the src ns:\r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: yacine\r\n  namespace: source\r\nspec:\r\n  backup:\r\n    volumeSnapshot:\r\n      className: csi-vsc-delete\r\n      online: true\r\n      onlineConfiguration:\r\n        immediateCheckpoint: true\r\n        waitForArchive: true\r\n      snapshotOwnerReference: backup\r\n  instances: 1\r\n  storage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 2Gi\r\n      storageClassName: sc-delete\r\n      volumeMode: Filesystem\r\n  walStorage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 2Gi\r\n      storageClassName: sc-delete\r\n      volumeMode: Filesystem\r\n  switchoverDelay: 3600\r\n```\r\nThen i executed some sql commands to create tables and make some insertions in order to have some data in it to verify later that my restore work as expected.\r\nSo, i create a back up using volumesnapshot:\r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Backup\r\nmetadata:\r\n  name: yacine-snapshot\r\n  namespace: source\r\nspec:\r\n  method: volumeSnapshot\r\n  cluster:\r\n    name: yacine\r\n```\r\nI've found that two volume snapshot are created one for data and the second for wals which is the expected behavior, for now everything works fine.\r\n```\r\nkg volumesnapshot -n source\r\nNAME                  READYTOUSE   SOURCEPVC      SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS        SNAPSHOTCONTENT                                    CREATIONTIME   AGE\r\nyacine-snapshot       true         yacine-1                               2Gi           csi-vsc-delete   snapcontent-9edbca5e-1e5b-47a3-8dd8-b407b627ce3d   175m           175m\r\nyacine-snapshot-wal   true         yacine-1-wal                           2Gi           csi-vsc-delete   snapcontent-3be03e15-4272-4e0e-9a6f-e387ca980684   175m           175m\r\n```\r\nWhat i wanted to achieve is to restore this database using this backup in the **destination namespace**\r\nSearching in the official documentation of cnpg, i found out that it does not seems possible to achieve this ( directly at least)\r\nin the recovery link in the refs section below, you can find the entire example,  but what is interesting for me is this part\r\n```\r\n  bootstrap:\r\n    recovery:\r\n      volumeSnapshots:\r\n        storage:\r\n          name: <snapshot name>\r\n          kind: VolumeSnapshot\r\n          apiGroup: snapshot.storage.k8s.io\r\n```\r\ni tried to add namespace parameter directly under the storage section but it does not work, it is not supported. \r\nWhat i did next is to create two pvcs based on the volumesnapshots of the **source namespace** ant then recreate a volumesnapshot in the \"destination namespace\" which will make me able to restore the database.\r\n```\r\napiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: yacine-snapshot\r\n  namespace: destination\r\nspec:\r\n  storageClassName: sc-retain\r\n  accessModes:\r\n  - ReadWriteOnce\r\n  resources:\r\n    requests:\r\n      storage: 2Gi\r\n  dataSourceRef:\r\n    apiGroup: snapshot.storage.k8s.io\r\n    kind: VolumeSnapshot\r\n    name: yacine-snapshot\r\n    namespace: source\r\n  volumeMode: Filesystem\r\n```\r\nSame thing for the wals:\r\n```\r\napiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: yacine-snapshot-wal\r\n  namespace: destination\r\nspec:\r\n  storageClassName: sc-retain\r\n  accessModes:\r\n  - ReadWriteOnce\r\n  resources:\r\n    requests:\r\n      storage: 2Gi\r\n  dataSourceRef:\r\n    apiGroup: snapshot.storage.k8s.io\r\n    kind: VolumeSnapshot\r\n    name: yacine-snapshot-wal\r\n    namespace: source\r\n  volumeMode: Filesystem\r\n```\r\nNow VolumeSnapshot:\r\n```\r\napiVersion: snapshot.storage.k8s.io/v1\r\nkind: VolumeSnapshot\r\nmetadata:\r\n  name: yacine-snapshot\r\n  namespace: destination\r\nspec:\r\n  volumeSnapshotClassName: csi-vsc-delete\r\n  source:\r\n    persistentVolumeClaimName: yacine-snapshot \r\n```\r\n```\r\napiVersion: snapshot.storage.k8s.io/v1\r\nkind: VolumeSnapshot\r\nmetadata:\r\n  name: yacine-snapshot-wal\r\n  namespace: destination\r\nspec:\r\n  volumeSnapshotClassName: csi-vsc-delete\r\n  source:\r\n    persistentVolumeClaimName: yacine-snapshot-wal \r\n```\r\nFinally, set up a cluster that will restore from these VolumeSnapshots\r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: yacine\r\n  namespace: destination\r\nspec:\r\n  instances: 1\r\n  storage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 2Gi\r\n      storageClassName: gp2-retain-immediate\r\n      volumeMode: Filesystem\r\n  walStorage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 2Gi\r\n      storageClassName: gp2-retain-immediate\r\n      volumeMode: Filesystem\r\n  backup:\r\n    volumeSnapshot:\r\n      className: csi-osc-vsc-delete\r\n      online: true\r\n      onlineConfiguration:\r\n        immediateCheckpoint: true\r\n        waitForArchive: true\r\n      snapshotOwnerReference: backup\r\n    retentionPolicy: 3d\r\n  bootstrap:\r\n    recovery:\r\n      database: yacine\r\n      owner: yacine\r\n      volumeSnapshots:\r\n        storage:\r\n          name: yacine-snapshot\r\n          kind: VolumeSnapshot\r\n          apiGroup: snapshot.storage.k8s.io\r\n        walStorage:\r\n          name: yacine-snapshot-wal\r\n          kind: VolumeSnapshot\r\n          apiGroup: snapshot.storage.k8s.io\r\n```\r\nAt this level, i expected everything to work fine, but unfortunatelly no :) \r\n```\r\n$ kg po -n destination\r\nNAME                               READY   STATUS             RESTARTS         AGE\r\nyacine-1                           0/1     CrashLoopBackOff   10 (4m27s ago)   30m\r\nyacine-1-snapshot-recovery-lstx4   0/1     Completed          0                31m\r\n```\r\n# Logs:\r\nrecovery-job:\r\n```\r\n$ k logs yacine-1-snapshot-recovery-lstx4 -n destination\r\nDefaulted container \"snapshot-recovery\" out of: snapshot-recovery, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-05-17T12:55:45Z\",\"msg\":\"Cleaning up PGDATA from stale files\",\"logging_pod\":\"yacine-1-snapshot-recovery\"}\r\n```\r\nthe instance of the pg cluster:\r\n```\r\n$ k logs yacine-1 -n destination\r\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-05-17T13:27:20Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"yacine-1\",\"version\":\"1.23.1\",\"build\":{\"Version\":\"1.23.1\",\"Commit\":\"336ddf53\",\"Date\":\"2024-04-30\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-05-17T13:27:20Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"yacine-1\",\"version\":\"1.23.1\",\"build\":{\"Version\":\"1.23.1\",\"Commit\":\"336ddf53\",\"Date\":\"2024-04-30\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-05-17T13:27:20Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"yacine-1\",\"version\":\"1.23.1\",\"build\":{\"Version\":\"1.23.1\",\"Commit\":\"336ddf53\",\"Date\":\"2024-04-30\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-05-17T13:27:20Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"yacine-1\",\"version\":\"1.23.1\",\"build\":{\"Version\":\"1.23.1\",\"Commit\":\"336ddf53\",\"Date\":\"2024-04-30\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-05-17T13:27:20Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"yacine-1\",\"version\":\"1.23.1\",\"build\":{\"Version\":\"1.23.1\",\"Commit\":\"336ddf53\",\"Date\":\"2024-04-30\"}}\r\n**Error: lstat /var/lib/postgresql/data/pgdata/pg_wal: no such file or directory**\r\n```\r\nI've installed the cnpg plugin to see if i can find anything helpul, but i had no chance :/\r\n```\r\n$ k cnpg status yacine -n destination\r\nCluster Summary\r\nPrimary server is initializing\r\nName:              yacine\r\nNamespace:         destination\r\nPostgreSQL Image:  ghcr.io/cloudnative-pg/postgresql:16.2\r\nPrimary instance:   (switching to yacine-1)\r\nStatus:            Waiting for the instances to become active Some instances are not yet active. Please wait.\r\nInstances:         1\r\nReady instances:   0\r\nCertificates Status\r\nCertificate Name    Expiration Date                Days Left Until Expiration\r\n----------------    ---------------                --------------------------\r\nyacine-ca           2024-08-15 12:50:02 +0000 UTC  89.97\r\nyacine-replication  2024-08-15 12:50:02 +0000 UTC  89.97\r\nyacine-server       2024-08-15 12:50:02 +0000 UTC  89.97\r\nContinuous Backup status\r\nFirst Point of Recoverability:  Not Available\r\nNo Primary instance found\r\nPhysical backups\r\nPrimary instance not found\r\nStreaming Replication status\r\nNot configured\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nManaged roles status\r\nNo roles managed\r\nTablespaces status\r\nNo managed tablespaces\r\nPod Disruption Budgets status\r\nName            Role     Expected Pods  Current Healthy  Minimum Desired Healthy  Disruptions Allowed\r\n----            ----     -------------  ---------------  -----------------------  -------------------\r\nyacine-primary  primary  0              0                1                        0\r\nyacine-primary  primary  1              1                1                        0\r\nInstances status\r\nName      Database Size  Current LSN  Replication role  Status             QoS         Manager Version  Node\r\n----      -------------  -----------  ----------------  ------             ---         ---------------  ----\r\nyacine-1  -              -            -                 pod not available  BestEffort  -                worker1.compute\r\n```\r\nI've enabled two kube api server args required for crossnamespace volume snapshot\r\n```\r\n  - feature-gates=CrossNamespaceVolumeDataSource=true\r\n  - feature-gates=AnyVolumeDataSource=true\r\n```\r\nSee Ref:3 for entire docs\r\n```\r\n...\r\nKubernetes supports cross namespace volume data sources. To use cross namespace volume data sources, you must enable the AnyVolumeDataSource and CrossNamespaceVolumeDataSource [feature gates](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/) for the kube-apiserver and kube-controller-manager. Also, you must enable the CrossNamespaceVolumeDataSource feature gate for the csi-provisioner.\r\n...\r\n```\r\nAnd this parameter in args of my csi driver:\r\n```\r\n            - '--feature-gates=CrossNamespaceVolumeDataSource=true'\r\n```\r\nI'll be grateful if you can help finding why this is happening, i tried to change parameters and see if there are any changes, but nothing useful unfortunatelly. Thank you in advance \ud83d\ude04 \r\nRefs:\r\n*1 https://cloudnative-pg.io/documentation/1.22/backup_volumesnapshot/\r\n*2 https://cloudnative-pg.io/documentation/1.20/recovery/#recovery-from-volumesnapshot-objects\r\n*3 https://kubernetes.io/docs/concepts/storage/persistent-volumes/#cross-namespace-data-sources",
        "messages": "Hello,\r\nI've set up two namespaces, let's say source and destination, the dst is empty for now.\r\nI've created two VolumeSnapshotClass.\r\nI've created a minimal cluster in the src ns:\r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: yacine\r\n  namespace: source\r\nspec:\r\n  backup:\r\n    volumeSnapshot:\r\n      className: csi-vsc-delete\r\n      online: true\r\n      onlineConfiguration:\r\n        immediateCheckpoint: true\r\n        waitForArchive: true\r\n      snapshotOwnerReference: backup\r\n  instances: 1\r\n  storage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 2Gi\r\n      storageClassName: sc-delete\r\n      volumeMode: Filesystem\r\n  walStorage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 2Gi\r\n      storageClassName: sc-delete\r\n      volumeMode: Filesystem\r\n  switchoverDelay: 3600\r\n```\r\nThen i executed some sql commands to create tables and make some insertions in order to have some data in it to verify later that my restore work as expected.\r\nSo, i create a back up using volumesnapshot:\r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Backup\r\nmetadata:\r\n  name: yacine-snapshot\r\n  namespace: source\r\nspec:\r\n  method: volumeSnapshot\r\n  cluster:\r\n    name: yacine\r\n```\r\nI've found that two volume snapshot are created one for data and the second for wals which is the expected behavior, for now everything works fine.\r\n```\r\nkg volumesnapshot -n source\r\nNAME                  READYTOUSE   SOURCEPVC      SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS        SNAPSHOTCONTENT                                    CREATIONTIME   AGE\r\nyacine-snapshot       true         yacine-1                               2Gi           csi-vsc-delete   snapcontent-9edbca5e-1e5b-47a3-8dd8-b407b627ce3d   175m           175m\r\nyacine-snapshot-wal   true         yacine-1-wal                           2Gi           csi-vsc-delete   snapcontent-3be03e15-4272-4e0e-9a6f-e387ca980684   175m           175m\r\n```\r\nWhat i wanted to achieve is to restore this database using this backup in the **destination namespace**\r\nSearching in the official documentation of cnpg, i found out that it does not seems possible to achieve this ( directly at least)\r\nin the recovery link in the refs section below, you can find the entire example,  but what is interesting for me is this part\r\n```\r\n  bootstrap:\r\n    recovery:\r\n      volumeSnapshots:\r\n        storage:\r\n          name: <snapshot name>\r\n          kind: VolumeSnapshot\r\n          apiGroup: snapshot.storage.k8s.io\r\n```\r\ni tried to add namespace parameter directly under the storage section but it does not work, it is not supported. \r\nWhat i did next is to create two pvcs based on the volumesnapshots of the **source namespace** ant then recreate a volumesnapshot in the \"destination namespace\" which will make me able to restore the database.\r\n```\r\napiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: yacine-snapshot\r\n  namespace: destination\r\nspec:\r\n  storageClassName: sc-retain\r\n  accessModes:\r\n  - ReadWriteOnce\r\n  resources:\r\n    requests:\r\n      storage: 2Gi\r\n  dataSourceRef:\r\n    apiGroup: snapshot.storage.k8s.io\r\n    kind: VolumeSnapshot\r\n    name: yacine-snapshot\r\n    namespace: source\r\n  volumeMode: Filesystem\r\n```\r\nSame thing for the wals:\r\n```\r\napiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: yacine-snapshot-wal\r\n  namespace: destination\r\nspec:\r\n  storageClassName: sc-retain\r\n  accessModes:\r\n  - ReadWriteOnce\r\n  resources:\r\n    requests:\r\n      storage: 2Gi\r\n  dataSourceRef:\r\n    apiGroup: snapshot.storage.k8s.io\r\n    kind: VolumeSnapshot\r\n    name: yacine-snapshot-wal\r\n    namespace: source\r\n  volumeMode: Filesystem\r\n```\r\nNow VolumeSnapshot:\r\n```\r\napiVersion: snapshot.storage.k8s.io/v1\r\nkind: VolumeSnapshot\r\nmetadata:\r\n  name: yacine-snapshot\r\n  namespace: destination\r\nspec:\r\n  volumeSnapshotClassName: csi-vsc-delete\r\n  source:\r\n    persistentVolumeClaimName: yacine-snapshot \r\n```\r\n```\r\napiVersion: snapshot.storage.k8s.io/v1\r\nkind: VolumeSnapshot\r\nmetadata:\r\n  name: yacine-snapshot-wal\r\n  namespace: destination\r\nspec:\r\n  volumeSnapshotClassName: csi-vsc-delete\r\n  source:\r\n    persistentVolumeClaimName: yacine-snapshot-wal \r\n```\r\nFinally, set up a cluster that will restore from these VolumeSnapshots\r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: yacine\r\n  namespace: destination\r\nspec:\r\n  instances: 1\r\n  storage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 2Gi\r\n      storageClassName: gp2-retain-immediate\r\n      volumeMode: Filesystem\r\n  walStorage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 2Gi\r\n      storageClassName: gp2-retain-immediate\r\n      volumeMode: Filesystem\r\n  backup:\r\n    volumeSnapshot:\r\n      className: csi-osc-vsc-delete\r\n      online: true\r\n      onlineConfiguration:\r\n        immediateCheckpoint: true\r\n        waitForArchive: true\r\n      snapshotOwnerReference: backup\r\n    retentionPolicy: 3d\r\n  bootstrap:\r\n    recovery:\r\n      database: yacine\r\n      owner: yacine\r\n      volumeSnapshots:\r\n        storage:\r\n          name: yacine-snapshot\r\n          kind: VolumeSnapshot\r\n          apiGroup: snapshot.storage.k8s.io\r\n        walStorage:\r\n          name: yacine-snapshot-wal\r\n          kind: VolumeSnapshot\r\n          apiGroup: snapshot.storage.k8s.io\r\n```\r\nAt this level, i expected everything to work fine, but unfortunatelly no :) \r\n```\r\n$ kg po -n destination\r\nNAME                               READY   STATUS             RESTARTS         AGE\r\nyacine-1                           0/1     CrashLoopBackOff   10 (4m27s ago)   30m\r\nyacine-1-snapshot-recovery-lstx4   0/1     Completed          0                31m\r\n```\r\n# Logs:\r\nrecovery-job:\r\n```\r\n$ k logs yacine-1-snapshot-recovery-lstx4 -n destination\r\nDefaulted container \"snapshot-recovery\" out of: snapshot-recovery, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-05-17T12:55:45Z\",\"msg\":\"Cleaning up PGDATA from stale files\",\"logging_pod\":\"yacine-1-snapshot-recovery\"}\r\n```\r\nthe instance of the pg cluster:\r\n```\r\n$ k logs yacine-1 -n destination\r\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-05-17T13:27:20Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"yacine-1\",\"version\":\"1.23.1\",\"build\":{\"Version\":\"1.23.1\",\"Commit\":\"336ddf53\",\"Date\":\"2024-04-30\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-05-17T13:27:20Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"yacine-1\",\"version\":\"1.23.1\",\"build\":{\"Version\":\"1.23.1\",\"Commit\":\"336ddf53\",\"Date\":\"2024-04-30\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-05-17T13:27:20Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"yacine-1\",\"version\":\"1.23.1\",\"build\":{\"Version\":\"1.23.1\",\"Commit\":\"336ddf53\",\"Date\":\"2024-04-30\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-05-17T13:27:20Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"yacine-1\",\"version\":\"1.23.1\",\"build\":{\"Version\":\"1.23.1\",\"Commit\":\"336ddf53\",\"Date\":\"2024-04-30\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-05-17T13:27:20Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"yacine-1\",\"version\":\"1.23.1\",\"build\":{\"Version\":\"1.23.1\",\"Commit\":\"336ddf53\",\"Date\":\"2024-04-30\"}}\r\n**Error: lstat /var/lib/postgresql/data/pgdata/pg_wal: no such file or directory**\r\n```\r\nI've installed the cnpg plugin to see if i can find anything helpul, but i had no chance :/\r\n```\r\n$ k cnpg status yacine -n destination\r\nCluster Summary\r\nPrimary server is initializing\r\nName:              yacine\r\nNamespace:         destination\r\nPostgreSQL Image:  ghcr.io/cloudnative-pg/postgresql:16.2\r\nPrimary instance:   (switching to yacine-1)\r\nStatus:            Waiting for the instances to become active Some instances are not yet active. Please wait.\r\nInstances:         1\r\nReady instances:   0\r\nCertificates Status\r\nCertificate Name    Expiration Date                Days Left Until Expiration\r\n----------------    ---------------                --------------------------\r\nyacine-ca           2024-08-15 12:50:02 +0000 UTC  89.97\r\nyacine-replication  2024-08-15 12:50:02 +0000 UTC  89.97\r\nyacine-server       2024-08-15 12:50:02 +0000 UTC  89.97\r\nContinuous Backup status\r\nFirst Point of Recoverability:  Not Available\r\nNo Primary instance found\r\nPhysical backups\r\nPrimary instance not found\r\nStreaming Replication status\r\nNot configured\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nManaged roles status\r\nNo roles managed\r\nTablespaces status\r\nNo managed tablespaces\r\nPod Disruption Budgets status\r\nName            Role     Expected Pods  Current Healthy  Minimum Desired Healthy  Disruptions Allowed\r\n----            ----     -------------  ---------------  -----------------------  -------------------\r\nyacine-primary  primary  0              0                1                        0\r\nyacine-primary  primary  1              1                1                        0\r\nInstances status\r\nName      Database Size  Current LSN  Replication role  Status             QoS         Manager Version  Node\r\n----      -------------  -----------  ----------------  ------             ---         ---------------  ----\r\nyacine-1  -              -            -                 pod not available  BestEffort  -                worker1.compute\r\n```\r\nI've enabled two kube api server args required for crossnamespace volume snapshot\r\n```\r\n  - feature-gates=CrossNamespaceVolumeDataSource=true\r\n  - feature-gates=AnyVolumeDataSource=true\r\n```\r\nSee Ref:3 for entire docs\r\n```\r\n...\r\nKubernetes supports cross namespace volume data sources. To use cross namespace volume data sources, you must enable the AnyVolumeDataSource and CrossNamespaceVolumeDataSource [feature gates](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/) for the kube-apiserver and kube-controller-manager. Also, you must enable the CrossNamespaceVolumeDataSource feature gate for the csi-provisioner.\r\n...\r\n```\r\nAnd this parameter in args of my csi driver:\r\n```\r\n            - '--feature-gates=CrossNamespaceVolumeDataSource=true'\r\n```\r\nI'll be grateful if you can help finding why this is happening, i tried to change parameters and see if there are any changes, but nothing useful unfortunatelly. Thank you in advance \ud83d\ude04 \r\nRefs:\r\n*1 https://cloudnative-pg.io/documentation/1.22/backup_volumesnapshot/\r\n*2 https://cloudnative-pg.io/documentation/1.20/recovery/#recovery-from-volumesnapshot-objects\r\n*3 https://kubernetes.io/docs/concepts/storage/persistent-volumes/#cross-namespace-data-sourcesHello,\r\nI have a similar issue. I'm setting the namespace in the storage object like this:\r\n```\r\n...\r\n\"recovery\": {\r\n\t\"volumeSnapshots\": {\r\n\t\t\"storage\": {\r\n\t\t\t\"apiGroup\": \"snapshot.storage.k8s.io\",\r\n\t\t\t\"kind\": \"VolumeSnapshot\",\r\n\t\t\t\"name\": \"snapshot-name\",\r\n\t\t\t\"namespace\": \"source-namespace\",\r\n\t\t},\r\n\t},\r\n},\r\n...\r\n```\r\nBut the Cluster seems to not take the namespace into account because when I describe the Cluster I see only this:\r\n```\r\n...\r\nSpec:\r\n  Bootstrap:\r\n    Recovery:\r\n      Database:  app\r\n      Owner:     app\r\n      Volume Snapshots:\r\n        Storage:\r\n          API Group:        snapshot.storage.k8s.io\r\n          Kind:             VolumeSnapshot\r\n          Name:             snapshot-name\r\n...\r\n```"
    },
    {
        "title": "[Bug]: \"pod not available\", \"Cannot extract Pod status\", \"WAL file not found in the recovery object store\"",
        "id": 2300235604,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\nolder in 1.21.x\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nHelm\n### What happened?\nWe have a kubernetes cluster with 6 worker nodes, distributed into 3 zones with each having 2 nodes.\r\nThe CNPG cluster consists of 3 instances and a pooler, created with the CNPG Operator.\r\nI then shut down the worker node containing the primary instance of the postgres cluster forcefully to see if it heals itself properly.\r\nWithin seconds another instance was promoted as the primary and it seems to work fine.\r\nBut I assumed that after a while the operator will spawn a new standby instance on the other worker in the same zone that is still runnung. That never happened, even after waiting for 20 minutes. So the cluster never became healthy again on its own.\r\nI then started the node again which I shut down before. After a while the missing postgres instance pod got created again but now it is stuck and `kubectl cnpg status` shows `pod not available` while the pod logs show that it has restored itself from the archived WAL files in the S3 bucket except that it tries to restore from a file that is newer than the backup status shows. But see for yourself:\r\nCluster status:\r\n```\r\n$[1] kubectl cnpg -n middleware-svc-keycloak status keycloak-db-pgcluster\r\nCluster Summary\r\nName:                keycloak-db-pgcluster\r\nNamespace:           middleware-svc-keycloak\r\nSystem ID:           7343941203355570200\r\nPostgreSQL Image:    ghcr.io/cloudnative-pg/postgresql:15.3\r\nPrimary instance:    keycloak-db-pgcluster-2\r\nPrimary start time:  2024-05-15 15:35:18 +0000 UTC (uptime 19h26m11s)\r\nStatus:              Failing over Failing over from keycloak-db-pgcluster-5 to keycloak-db-pgcluster-2\r\nInstances:           3\r\nReady instances:     2\r\nCurrent Write LSN:   0/3A000000 (Timeline: 13 - WAL File: 0000000D0000000000000039)\r\nCertificates Status\r\nCertificate Name                   Expiration Date                Days Left Until Expiration\r\n----------------                   ---------------                --------------------------\r\nkeycloak-db-pgcluster-ca           2024-06-06 10:37:14 +0000 UTC  20.98\r\nkeycloak-db-pgcluster-replication  2024-06-06 10:37:14 +0000 UTC  20.98\r\nkeycloak-db-pgcluster-server       2024-06-06 10:37:14 +0000 UTC  20.98\r\nContinuous Backup status\r\nFirst Point of Recoverability:  2024-05-15T12:55:22Z\r\nWorking WAL archiving:          OK\r\nWALs waiting to be archived:    0\r\nLast Archived WAL:              0000000D0000000000000039   @   2024-05-15T15:45:20.25018Z\r\nLast Failed WAL:                00000008.history           @   2024-05-15T12:22:29.298705Z\r\nPhysical backups\r\nNo running physical backups found\r\nStreaming Replication status\r\nName                     Sent LSN    Write LSN   Flush LSN   Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority\r\n----                     --------    ---------   ---------   ----------  ---------  ---------  ----------  -----      ----------  -------------\r\nkeycloak-db-pgcluster-5  0/3A000000  0/3A000000  0/3A000000  0/3A000000  00:00:00   00:00:00   00:00:00    streaming  async       0\r\nkeycloak-db-pgcluster-6  0/3A000000  0/3A000000  0/3A000000  0/3A000000  00:00:00   00:00:00   00:00:00    streaming  quorum      1\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nManaged roles status\r\nNo roles managed\r\nTablespaces status\r\nNo managed tablespaces\r\nPod Disruption Budgets status\r\nName                           Role     Expected Pods  Current Healthy  Minimum Desired Healthy  Disruptions Allowed\r\n----                           ----     -------------  ---------------  -----------------------  -------------------\r\nkeycloak-db-pgcluster          replica  2              1                1                        0\r\nkeycloak-db-pgcluster-primary  primary  1              1                1                        0\r\nInstances status\r\nName                     Database Size  Current LSN  Replication role  Status             QoS        Manager Version  Node\r\n----                     -------------  -----------  ----------------  ------             ---        ---------------  ----\r\nkeycloak-db-pgcluster-2  34 MB          0/3A000000   Primary           OK                 Burstable  1.20.2           project-d-kubw13p\r\nkeycloak-db-pgcluster-6  34 MB          0/3A000000   Standby (sync)    OK                 Burstable  1.20.2           project-d-kubw14p\r\nkeycloak-db-pgcluster-5  -              -            -                 pod not available  Burstable  -                project-d-kubw12p\r\n```\r\nAs you can see it still thinks it is in the failing over state although the primary is ready again since 20 hours.\r\nYou can also see that the latest WAL archive is `0000000D0000000000000039`.\r\nNow let's have a look into the pod `keycloak-db-pgcluster-5` (I removed a bit of clutter from the logs so they are more easy to read):\r\n```json\r\n[\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:33Z\",\r\n    \"msg\": \"WAL file not found in the recovery object store\",\r\n    \"walName\": \"0000000E.history\",\r\n    \"options\": [\r\n      \"--endpoint-url\", \"https://s3.example.com\", \"--cloud-provider\", \"aws-s3\", \"s3://project-dev/postgres-keycloak-backup\", \"keycloak-db-pgcluster\"\r\n    ]\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:34Z\",\r\n    \"msg\": \"Restored WAL file\",\r\n    \"walName\": \"0000000D.history\"\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:34Z\",\r\n    \"msg\": \"WAL restore command completed (parallel)\",\r\n    \"walName\": \"0000000D.history\",\r\n    \"maxParallel\": 1,\r\n    \"successfulWalRestore\": 1,\r\n    \"failedWalRestore\": 0,\r\n    \"endOfWALStream\": false\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:36Z\",\r\n    \"msg\": \"Restored WAL file\",\r\n    \"walName\": \"0000000D0000000000000039\"\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:36Z\",\r\n    \"msg\": \"WAL restore command completed (parallel)\",\r\n    \"walName\": \"0000000D0000000000000039\",\r\n    \"maxParallel\": 1,\r\n    \"successfulWalRestore\": 1,\r\n    \"failedWalRestore\": 0,\r\n    \"endOfWALStream\": false\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:38Z\",\r\n    \"msg\": \"Restored WAL file\",\r\n    \"walName\": \"0000000D0000000000000038\"\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:38Z\",\r\n    \"msg\": \"WAL restore command completed (parallel)\",\r\n    \"walName\": \"0000000D0000000000000038\",\r\n    \"maxParallel\": 1,\r\n    \"successfulWalRestore\": 1,\r\n    \"failedWalRestore\": 0,\r\n    \"endOfWALStream\": false\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:40Z\",\r\n    \"msg\": \"Restored WAL file\",\r\n    \"walName\": \"0000000D0000000000000039\"\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:40Z\",\r\n    \"msg\": \"WAL restore command completed (parallel)\",\r\n    \"walName\": \"0000000D0000000000000039\",\r\n    \"maxParallel\": 1,\r\n    \"successfulWalRestore\": 1,\r\n    \"failedWalRestore\": 0,\r\n    \"endOfWALStream\": false\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:42Z\",\r\n    \"msg\": \"WAL file not found in the recovery object store\",\r\n    \"walName\": \"0000000D000000000000003A\",\r\n    \"options\": [\r\n      \"--endpoint-url\", \"https://s3.example.com\", \"--cloud-provider\", \"aws-s3\", \"s3://project-dev/postgres-keycloak-backup\", \"keycloak-db-pgcluster\"\r\n    ]\r\n  }\r\n]\r\n```\r\nWhy does it try to read `0000000D000000000000003A`? And why is `endOfWALStream` set to `false` on the last WAL file when it should be `true`? Or am I misinterpreting something here?\r\nAnd here is the log of the postgres-operator which is repeating every 2 seconds:\r\n```json\r\n{\r\n  \"ts\": \"2024-05-16T12:03:38Z\",\r\n  \"msg\": \"Cannot extract Pod status\",\r\n  \"controller\": \"cluster\",\r\n  \"controllerGroup\": \"postgresql.cnpg.io\",\r\n  \"controllerKind\": \"Cluster\",\r\n  \"Cluster\": {\r\n    \"name\": \"keycloak-db-pgcluster\",\r\n    \"namespace\": \"middleware-svc-keycloak\"\r\n  },\r\n  \"namespace\": \"middleware-svc-keycloak\",\r\n  \"name\": \"keycloak-db-pgcluster\",\r\n  \"name\": \"keycloak-db-pgcluster-5\",\r\n  \"error\": \"error status code: 500, body: failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\\n\"\r\n}\r\n{\r\n  \"level\": \"info\",\r\n  \"ts\": \"2024-05-16T12:03:39Z\",\r\n  \"msg\": \"pooler not automatically configured, manual configuration required\",\r\n  \"cluster\": \"keycloak-db-pgcluster\",\r\n  \"pooler\": \"keycloak-db-pgcluster-pooler-rw\"\r\n}\r\n```\r\nWe experience that problem quite often when we do maintenance on the kubernetes nodes and need to restart them. The already destroyed multiple clusters because of that. The only way to fix this is to destroy the instance with `kubectl cnpg destroy keycloak-db-pgcluster 5` and wait for a fresh instance to come up. Afterwards we can delete the PVC and PV of the old instance since it makes no sense to keep it.\r\nWe want to upgrade soon to the latest version but after finding this recent bug I think this still can happen in newer versions: https://github.com/cloudnative-pg/cloudnative-pg/issues/4412\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    cnpg.io/reconciliationLoop: enabled\r\n    meta.helm.sh/release-name: keycloak-db\r\n    meta.helm.sh/release-namespace: middleware-svc-keycloak\r\n  generation: 3\r\n  labels:\r\n    app.kubernetes.io/instance: keycloak-db\r\n    app.kubernetes.io/managed-by: Helm\r\n    app.kubernetes.io/name: pgcluster\r\n    app.kubernetes.io/version: '15.1'\r\n    helm.sh/chart: pgcluster-0.1.14\r\n    helm.toolkit.fluxcd.io/name: keycloak-db\r\n    helm.toolkit.fluxcd.io/namespace: middleware-svc-keycloak\r\n    k8slens-edit-resource-version: v1\r\n  name: keycloak-db-pgcluster\r\n  namespace: middleware-svc-keycloak\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: required\r\n    topologyKey: topology.kubernetes.io/zone\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3: //project-dev/postgres-keycloak-backup\r\n      endpointURL: https: //s3.example.com\r\n      s3Credentials:\r\n        accessKeyId:\r\n          key: ACCESS_KEY_ID\r\n          name: keycloak-backup-s3\r\n        inheritFromIAMRole: false\r\n        secretAccessKey:\r\n          key: ACCESS_SECRET_KEY\r\n          name: keycloak-backup-s3\r\n      wal:\r\n        compression: gzip\r\n    retentionPolicy: 30d\r\n    target: prefer-standby\r\n  bootstrap:\r\n    initdb:\r\n      database: keycloak\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: keycloak\r\n      secret:\r\n        name: postgresql-keycloak-password\r\n  description: Postgres general configuration\r\n  enableSuperuserAccess: true\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:15.3\r\n  imagePullPolicy: IfNotPresent\r\n  inheritedMetadata:\r\n    labels:\r\n      app.kubernetes.io/instance: keycloak-db\r\n      app.kubernetes.io/managed-by: Helm\r\n      app.kubernetes.io/name: pgcluster\r\n      app.kubernetes.io/version: '15.1'\r\n      helm.sh/chart: pgcluster-0.1.14\r\n  instances: 3\r\n  logLevel: warning\r\n  maxSyncReplicas: 2\r\n  minSyncReplicas: 1\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n      - key: queries\r\n        name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: false\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: 'on'\r\n      archive_timeout: 5min\r\n      auto_explain.log_min_duration: 10s\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: '0'\r\n      log_rotation_size: '0'\r\n      log_truncate_on_rotation: 'false'\r\n      logging_collector: 'on'\r\n      max_parallel_workers: '32'\r\n      max_replication_slots: '32'\r\n      max_worker_processes: '32'\r\n      pg_stat_statements.max: '10000'\r\n      pg_stat_statements.track: all\r\n      shared_buffers: 256MB\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: ''\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  resources:\r\n    requests:\r\n      cpu: 250m\r\n      memory: 512Mi\r\n  startDelay: 300\r\n  stopDelay: 300\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 1Gi\r\n    storageClass: longhorn-2-repl-retain\r\n  switchoverDelay: 2\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\nolder in 1.21.x\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nHelm\n### What happened?\nWe have a kubernetes cluster with 6 worker nodes, distributed into 3 zones with each having 2 nodes.\r\nThe CNPG cluster consists of 3 instances and a pooler, created with the CNPG Operator.\r\nI then shut down the worker node containing the primary instance of the postgres cluster forcefully to see if it heals itself properly.\r\nWithin seconds another instance was promoted as the primary and it seems to work fine.\r\nBut I assumed that after a while the operator will spawn a new standby instance on the other worker in the same zone that is still runnung. That never happened, even after waiting for 20 minutes. So the cluster never became healthy again on its own.\r\nI then started the node again which I shut down before. After a while the missing postgres instance pod got created again but now it is stuck and `kubectl cnpg status` shows `pod not available` while the pod logs show that it has restored itself from the archived WAL files in the S3 bucket except that it tries to restore from a file that is newer than the backup status shows. But see for yourself:\r\nCluster status:\r\n```\r\n$[1] kubectl cnpg -n middleware-svc-keycloak status keycloak-db-pgcluster\r\nCluster Summary\r\nName:                keycloak-db-pgcluster\r\nNamespace:           middleware-svc-keycloak\r\nSystem ID:           7343941203355570200\r\nPostgreSQL Image:    ghcr.io/cloudnative-pg/postgresql:15.3\r\nPrimary instance:    keycloak-db-pgcluster-2\r\nPrimary start time:  2024-05-15 15:35:18 +0000 UTC (uptime 19h26m11s)\r\nStatus:              Failing over Failing over from keycloak-db-pgcluster-5 to keycloak-db-pgcluster-2\r\nInstances:           3\r\nReady instances:     2\r\nCurrent Write LSN:   0/3A000000 (Timeline: 13 - WAL File: 0000000D0000000000000039)\r\nCertificates Status\r\nCertificate Name                   Expiration Date                Days Left Until Expiration\r\n----------------                   ---------------                --------------------------\r\nkeycloak-db-pgcluster-ca           2024-06-06 10:37:14 +0000 UTC  20.98\r\nkeycloak-db-pgcluster-replication  2024-06-06 10:37:14 +0000 UTC  20.98\r\nkeycloak-db-pgcluster-server       2024-06-06 10:37:14 +0000 UTC  20.98\r\nContinuous Backup status\r\nFirst Point of Recoverability:  2024-05-15T12:55:22Z\r\nWorking WAL archiving:          OK\r\nWALs waiting to be archived:    0\r\nLast Archived WAL:              0000000D0000000000000039   @   2024-05-15T15:45:20.25018Z\r\nLast Failed WAL:                00000008.history           @   2024-05-15T12:22:29.298705Z\r\nPhysical backups\r\nNo running physical backups found\r\nStreaming Replication status\r\nName                     Sent LSN    Write LSN   Flush LSN   Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority\r\n----                     --------    ---------   ---------   ----------  ---------  ---------  ----------  -----      ----------  -------------\r\nkeycloak-db-pgcluster-5  0/3A000000  0/3A000000  0/3A000000  0/3A000000  00:00:00   00:00:00   00:00:00    streaming  async       0\r\nkeycloak-db-pgcluster-6  0/3A000000  0/3A000000  0/3A000000  0/3A000000  00:00:00   00:00:00   00:00:00    streaming  quorum      1\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nManaged roles status\r\nNo roles managed\r\nTablespaces status\r\nNo managed tablespaces\r\nPod Disruption Budgets status\r\nName                           Role     Expected Pods  Current Healthy  Minimum Desired Healthy  Disruptions Allowed\r\n----                           ----     -------------  ---------------  -----------------------  -------------------\r\nkeycloak-db-pgcluster          replica  2              1                1                        0\r\nkeycloak-db-pgcluster-primary  primary  1              1                1                        0\r\nInstances status\r\nName                     Database Size  Current LSN  Replication role  Status             QoS        Manager Version  Node\r\n----                     -------------  -----------  ----------------  ------             ---        ---------------  ----\r\nkeycloak-db-pgcluster-2  34 MB          0/3A000000   Primary           OK                 Burstable  1.20.2           project-d-kubw13p\r\nkeycloak-db-pgcluster-6  34 MB          0/3A000000   Standby (sync)    OK                 Burstable  1.20.2           project-d-kubw14p\r\nkeycloak-db-pgcluster-5  -              -            -                 pod not available  Burstable  -                project-d-kubw12p\r\n```\r\nAs you can see it still thinks it is in the failing over state although the primary is ready again since 20 hours.\r\nYou can also see that the latest WAL archive is `0000000D0000000000000039`.\r\nNow let's have a look into the pod `keycloak-db-pgcluster-5` (I removed a bit of clutter from the logs so they are more easy to read):\r\n```json\r\n[\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:33Z\",\r\n    \"msg\": \"WAL file not found in the recovery object store\",\r\n    \"walName\": \"0000000E.history\",\r\n    \"options\": [\r\n      \"--endpoint-url\", \"https://s3.example.com\", \"--cloud-provider\", \"aws-s3\", \"s3://project-dev/postgres-keycloak-backup\", \"keycloak-db-pgcluster\"\r\n    ]\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:34Z\",\r\n    \"msg\": \"Restored WAL file\",\r\n    \"walName\": \"0000000D.history\"\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:34Z\",\r\n    \"msg\": \"WAL restore command completed (parallel)\",\r\n    \"walName\": \"0000000D.history\",\r\n    \"maxParallel\": 1,\r\n    \"successfulWalRestore\": 1,\r\n    \"failedWalRestore\": 0,\r\n    \"endOfWALStream\": false\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:36Z\",\r\n    \"msg\": \"Restored WAL file\",\r\n    \"walName\": \"0000000D0000000000000039\"\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:36Z\",\r\n    \"msg\": \"WAL restore command completed (parallel)\",\r\n    \"walName\": \"0000000D0000000000000039\",\r\n    \"maxParallel\": 1,\r\n    \"successfulWalRestore\": 1,\r\n    \"failedWalRestore\": 0,\r\n    \"endOfWALStream\": false\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:38Z\",\r\n    \"msg\": \"Restored WAL file\",\r\n    \"walName\": \"0000000D0000000000000038\"\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:38Z\",\r\n    \"msg\": \"WAL restore command completed (parallel)\",\r\n    \"walName\": \"0000000D0000000000000038\",\r\n    \"maxParallel\": 1,\r\n    \"successfulWalRestore\": 1,\r\n    \"failedWalRestore\": 0,\r\n    \"endOfWALStream\": false\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:40Z\",\r\n    \"msg\": \"Restored WAL file\",\r\n    \"walName\": \"0000000D0000000000000039\"\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:40Z\",\r\n    \"msg\": \"WAL restore command completed (parallel)\",\r\n    \"walName\": \"0000000D0000000000000039\",\r\n    \"maxParallel\": 1,\r\n    \"successfulWalRestore\": 1,\r\n    \"failedWalRestore\": 0,\r\n    \"endOfWALStream\": false\r\n  },\r\n  {\r\n    \"ts\": \"2024-05-16T10:41:42Z\",\r\n    \"msg\": \"WAL file not found in the recovery object store\",\r\n    \"walName\": \"0000000D000000000000003A\",\r\n    \"options\": [\r\n      \"--endpoint-url\", \"https://s3.example.com\", \"--cloud-provider\", \"aws-s3\", \"s3://project-dev/postgres-keycloak-backup\", \"keycloak-db-pgcluster\"\r\n    ]\r\n  }\r\n]\r\n```\r\nWhy does it try to read `0000000D000000000000003A`? And why is `endOfWALStream` set to `false` on the last WAL file when it should be `true`? Or am I misinterpreting something here?\r\nAnd here is the log of the postgres-operator which is repeating every 2 seconds:\r\n```json\r\n{\r\n  \"ts\": \"2024-05-16T12:03:38Z\",\r\n  \"msg\": \"Cannot extract Pod status\",\r\n  \"controller\": \"cluster\",\r\n  \"controllerGroup\": \"postgresql.cnpg.io\",\r\n  \"controllerKind\": \"Cluster\",\r\n  \"Cluster\": {\r\n    \"name\": \"keycloak-db-pgcluster\",\r\n    \"namespace\": \"middleware-svc-keycloak\"\r\n  },\r\n  \"namespace\": \"middleware-svc-keycloak\",\r\n  \"name\": \"keycloak-db-pgcluster\",\r\n  \"name\": \"keycloak-db-pgcluster-5\",\r\n  \"error\": \"error status code: 500, body: failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\\n\"\r\n}\r\n{\r\n  \"level\": \"info\",\r\n  \"ts\": \"2024-05-16T12:03:39Z\",\r\n  \"msg\": \"pooler not automatically configured, manual configuration required\",\r\n  \"cluster\": \"keycloak-db-pgcluster\",\r\n  \"pooler\": \"keycloak-db-pgcluster-pooler-rw\"\r\n}\r\n```\r\nWe experience that problem quite often when we do maintenance on the kubernetes nodes and need to restart them. The already destroyed multiple clusters because of that. The only way to fix this is to destroy the instance with `kubectl cnpg destroy keycloak-db-pgcluster 5` and wait for a fresh instance to come up. Afterwards we can delete the PVC and PV of the old instance since it makes no sense to keep it.\r\nWe want to upgrade soon to the latest version but after finding this recent bug I think this still can happen in newer versions: https://github.com/cloudnative-pg/cloudnative-pg/issues/4412\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    cnpg.io/reconciliationLoop: enabled\r\n    meta.helm.sh/release-name: keycloak-db\r\n    meta.helm.sh/release-namespace: middleware-svc-keycloak\r\n  generation: 3\r\n  labels:\r\n    app.kubernetes.io/instance: keycloak-db\r\n    app.kubernetes.io/managed-by: Helm\r\n    app.kubernetes.io/name: pgcluster\r\n    app.kubernetes.io/version: '15.1'\r\n    helm.sh/chart: pgcluster-0.1.14\r\n    helm.toolkit.fluxcd.io/name: keycloak-db\r\n    helm.toolkit.fluxcd.io/namespace: middleware-svc-keycloak\r\n    k8slens-edit-resource-version: v1\r\n  name: keycloak-db-pgcluster\r\n  namespace: middleware-svc-keycloak\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: required\r\n    topologyKey: topology.kubernetes.io/zone\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3: //project-dev/postgres-keycloak-backup\r\n      endpointURL: https: //s3.example.com\r\n      s3Credentials:\r\n        accessKeyId:\r\n          key: ACCESS_KEY_ID\r\n          name: keycloak-backup-s3\r\n        inheritFromIAMRole: false\r\n        secretAccessKey:\r\n          key: ACCESS_SECRET_KEY\r\n          name: keycloak-backup-s3\r\n      wal:\r\n        compression: gzip\r\n    retentionPolicy: 30d\r\n    target: prefer-standby\r\n  bootstrap:\r\n    initdb:\r\n      database: keycloak\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: keycloak\r\n      secret:\r\n        name: postgresql-keycloak-password\r\n  description: Postgres general configuration\r\n  enableSuperuserAccess: true\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:15.3\r\n  imagePullPolicy: IfNotPresent\r\n  inheritedMetadata:\r\n    labels:\r\n      app.kubernetes.io/instance: keycloak-db\r\n      app.kubernetes.io/managed-by: Helm\r\n      app.kubernetes.io/name: pgcluster\r\n      app.kubernetes.io/version: '15.1'\r\n      helm.sh/chart: pgcluster-0.1.14\r\n  instances: 3\r\n  logLevel: warning\r\n  maxSyncReplicas: 2\r\n  minSyncReplicas: 1\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n      - key: queries\r\n        name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: false\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: 'on'\r\n      archive_timeout: 5min\r\n      auto_explain.log_min_duration: 10s\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: '0'\r\n      log_rotation_size: '0'\r\n      log_truncate_on_rotation: 'false'\r\n      logging_collector: 'on'\r\n      max_parallel_workers: '32'\r\n      max_replication_slots: '32'\r\n      max_worker_processes: '32'\r\n      pg_stat_statements.max: '10000'\r\n      pg_stat_statements.track: all\r\n      shared_buffers: 256MB\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: ''\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  resources:\r\n    requests:\r\n      cpu: 250m\r\n      memory: 512Mi\r\n  startDelay: 300\r\n  stopDelay: 300\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 1Gi\r\n    storageClass: longhorn-2-repl-retain\r\n  switchoverDelay: 2\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI fixed the unhealthy state of the cluster by destroying the instance for now. The operator created a new pod `keycloak-db-pgcluster-7` and a temporary join job and after a minute the cluster got healthy again.\r\nHowever that should not be the solution to this kind of error. The operator should be able to heal the cluster if one worker node goes down for whatever reason. I still hope it is just a misconfiguration but I am quite sure I did nothing wrong here.\n---\nBefore I had destroyed `keycloak-db-pgcluster-5` I had a look into its filesystem, especially into the folder `/var/lib/postgresql/data/pgdata/pg_wal`:\r\n```\r\npostgres@keycloak-db-pgcluster-5:~/data/pgdata/pg_wal$ ls -la\r\ntotal 606256\r\ndrwxrws---  3 postgres tape     4096 May 16 10:41 .\r\ndrwx------ 19 postgres tape     4096 May 16 10:41 ..\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 00000005000000000000001A\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 00000005000000000000001B\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 00000005000000000000001C\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 00000005000000000000001D\r\n-rw-rw----  1 postgres tape      169 May 15 16:00 00000005.history\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 00000006000000000000001E\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 00000006000000000000001F\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 000000060000000000000020\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 000000060000000000000021\r\n-rw-rw----  1 postgres tape      212 May 15 16:00 00000006.history\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 000000070000000000000022\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 000000070000000000000023\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 000000070000000000000024\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 000000070000000000000025\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 000000070000000000000026\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 000000070000000000000027\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 000000070000000000000028\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 000000070000000000000029\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 00000007000000000000002A\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 00000007000000000000002B.partial\r\n-rw-rw----  1 postgres tape      255 May 15 16:00 00000007.history\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 00000008000000000000002B\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 00000008000000000000002C\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 00000008000000000000002D\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 00000008000000000000002E\r\n-rw-rw----  1 postgres tape      298 May 15 16:00 00000008.history\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 00000009000000000000002E\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 00000009000000000000002F\r\n-rw-rw----  1 postgres tape      341 May 15 16:00 00000009.history\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 0000000A0000000000000030.partial\r\n-rw-rw----  1 postgres tape      384 May 15 16:00 0000000A.history\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 0000000B0000000000000030\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 0000000B0000000000000031\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 0000000B0000000000000032\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 0000000B0000000000000033\r\n-rw-rw----  1 postgres tape      428 May 15 16:00 0000000B.history\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 0000000C0000000000000033\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 0000000C0000000000000034\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 0000000C0000000000000035\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 0000000C0000000000000036\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 0000000C0000000000000037\r\n-rw-rw----  1 postgres tape      472 May 15 16:00 0000000C.history\r\n-rw-------  1 postgres tape 16777216 May 16 10:41 0000000D0000000000000038\r\n-rw-------  1 postgres tape 16777216 May 16 10:41 0000000D0000000000000039\r\n-rw-rw----  1 postgres tape 16777216 May 15 16:00 0000000D000000000000003A\r\n-rw-------  1 postgres tape      516 May 16 10:41 0000000D.history\r\ndrwxrws---  2 postgres tape     4096 May 15 16:00 archive_status\r\n```\r\nI found it kind of weird that the file `0000000D000000000000003A` exists here but not in the other instances. Can this be a remnant from being the primary instance before the sudden shutdown of the worker node?\r\nI deleted that file manually and restarted the pod but it did not solve the issue. It still was a bit weird to see that file there which does not exist in the S3 backup.\n---\n> I fixed the unhealthy state of the cluster by destroying the instance for now. The operator created a new pod `keycloak-db-pgcluster-7` and a temporary join job and after a minute the cluster got healthy again.\nCan you share the way you destroyed the unhealthy instance? I am facing similar issue but when I deleted an instance pod in the past it was not recreated.\n---\n> \n> Can you share the way you destroyed the unhealthy instance? I am facing similar issue but when I deleted an instance pod in the past it was not recreated.\nI used the `kubectl-cnpg destroy <instance> <number>` command for that.\n---\nI am facing this same issue.\nLogs attached\n[postgres.txt](https://github.com/user-attachments/files/18441830/postgres.txt)\n```\nkubectl cnpg version\nBuild: {Version:1.24.0 Commit:5fe5bb6b Date:2024-08-22}\n```\nCluster CRD\n```\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: racefeed-db\n  namespace: data\nspec:\n  description: \"PostgreSQL instance\"\n  imageName: postgresql-pgcron:16.2\n  instances: 3\n  enableSuperuserAccess: true\n  startDelay: 30\n  stopDelay: 100\n  primaryUpdateStrategy: unsupervised\n  postgresql:\n    parameters:\n      pg_stat_statements.max: \"10000\"\n      pg_stat_statements.track: all\n      auto_explain.log_min_duration: \"10s\"\n      cron.database_name: \"racefeed-db\"\n      cron.use_background_workers: \"on\"\n      ssl_max_protocol_version: TLSv1.3\n      ssl_min_protocol_version: TLSv1.2\n      max_connections: \"20\"\n      shared_buffers: \"1024MB\"\n      effective_cache_size: \"3072MB\"\n      maintenance_work_mem: \"256MB\"\n      checkpoint_completion_target: \"0.9\"\n      wal_buffers: \"16MB\"\n      default_statistics_target: \"100\"\n      random_page_cost: \"1.1\"\n      effective_io_concurrency: \"300\"\n      work_mem: \"26214kB\"\n      huge_pages: \"off\"\n      min_wal_size: \"1GB\"\n      max_wal_size: \"4GB\"\n    pg_hba:\n        # host <database> <user> <address> <auth-method>\n        # 10.244.0.0/16 is for all internal pod IPs\n      - host all all 10.244.0.0/16 scram-sha-256\n    shared_preload_libraries:\n      - pg_cron\n  backup:\n    barmanObjectStore:\n      destinationPath: \"s3://data-backup/\"\n      endpointURL: \"http://rook-ceph-rgw-ceph-objectstore.rook-ceph.svc:80\"\n      s3Credentials:\n        accessKeyId:\n          name: aws-creds-local\n          key: AWS_ACCESS_KEY_ID\n        secretAccessKey:\n          name: aws-creds-local\n          key: AWS_SECRET_ACCESS_KEY\n      wal:\n        compression: bzip2\n        maxParallel: 4\n      data:\n        compression: bzip2\n        jobs: 2\n    retentionPolicy: \"7d\"\n  monitoring:\n    enablePodMonitor: false\n  bootstrap:\n    initdb:\n      database: racefeed-db\n      owner: racefeed-db-user\n      secret:\n        name: racefeed-db-app\n  superuserSecret:\n    name: racefeed-db-admin\n  storage:\n    storageClass: ceph-block\n    size: 200Gi\n    resizeInUseVolumes: false\n  resources:\n    requests:\n      memory: \"4Gi\"\n      cpu: \"2\"\n    limits:\n      memory: \"6Gi\"\n      cpu: \"4\"\n  affinity:\n    enablePodAntiAffinity: true\n    topologyKey: failure-domain.beta.kubernetes.io/zone\n  imagePullSecrets:\n  - name: regcred\n```"
    },
    {
        "title": "[Feature]: Support trusted ca certificates",
        "id": 2300059021,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIt is only supported to use self-signed server and, or client certificates.\r\nhttps://cloudnative-pg.io/documentation/1.22/certificates/\n### Describe the solution you'd like\nI would like to use trusted ca certificates.\r\nE.g.  Certificates issued by letcsencrypt (cert-manager)\n### Describe alternatives you've considered\nTerminate TLS connection on ingress controller.\n### Additional context\nApplication requires trusted ca certificates to connect to database. Self-signed certificates are not supported.\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIt is only supported to use self-signed server and, or client certificates.\r\nhttps://cloudnative-pg.io/documentation/1.22/certificates/\n### Describe the solution you'd like\nI would like to use trusted ca certificates.\r\nE.g.  Certificates issued by letcsencrypt (cert-manager)\n### Describe alternatives you've considered\nTerminate TLS connection on ingress controller.\n### Additional context\nApplication requires trusted ca certificates to connect to database. Self-signed certificates are not supported.\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conductrelated https://github.com/cloudnative-pg/cloudnative-pg/issues/3503\n---\nIndeed, #3503 is still relevant.\n---\nWould also be curious about the potential support for this? I see that there was a PR #4940 but it was closed?"
    },
    {
        "title": "[Bug]: Operator upgrade 1.22.1 -> 1.22.2 -> 1.23.1 fails due to missing CRB",
        "id": 2291754669,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.23.0\r\n### What version of Kubernetes are you using?\r\n1.27\r\n### What is your Kubernetes environment?\r\nOther\r\n### How did you install the operator?\r\nOther\r\n### What happened?\r\nWe are running CNPG on Openshift. We have updated a lab instance, Openshift 4.13.25, from 1.22.2 to 1.23.1 without issues.\r\nWe have an Openshift 4.14.17 where we had CNPG Operator 1.22.1 installed, all fine. We have upgraded the instance to 1.22.2, no issues. After the upgrade finished, all pods were up, we upgraded the operator to 1.23.1. Cluster instances were not started to be upgraded. We have checked and the controller-manager deployment complained that the cnpg-manager service account does not have permission to list ClusterImageCatalogs.\r\nThe reason was that the ClusterImageCatalogs access is a simple Role applied as a RoleBinding that allows only the namespace where the operator is installed.\r\nThe problem is that the ClusterImageCatalogs CRD instances are not created in any namespace but on the cluster level.\r\nAs a workaround, we have created a ClusterRole with the same ClusterImageCatalogs access (get, list, watch) and a ClusterRoleBinding for cnpg-manager SA. Once we applied the upgrade was completed successfully.\r\nSorry, the logs were purged when the upgrade succeeded hence I cannot attach them.\r\nCode to apply workaround:\r\n```yaml\r\nkind: ClusterRole\r\napiVersion: rbac.authorization.k8s.io/v1\r\nmetadata:\r\n  name: cnpg-clusterimagecatalogs\r\nrules:\r\n  - verbs:\r\n      - get\r\n      - list\r\n      - watch\r\n    apiGroups:\r\n      - postgresql.cnpg.io\r\n    resources:\r\n      - clusterimagecatalogs\r\n---\r\nkind: ClusterRoleBinding\r\napiVersion: rbac.authorization.k8s.io/v1\r\nmetadata:\r\n  name: cnpg-clusterimagecatalogs\r\nsubjects:\r\n  - kind: ServiceAccount\r\n    name: cnpg-manager\r\n    namespace: postgres\r\nroleRef:\r\n  apiGroup: rbac.authorization.k8s.io\r\n  kind: ClusterRole\r\n  name: cnpg-clusterimagecatalogs\r\n```\r\n### Cluster resource\r\n_No response_\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.23.0\r\n### What version of Kubernetes are you using?\r\n1.27\r\n### What is your Kubernetes environment?\r\nOther\r\n### How did you install the operator?\r\nOther\r\n### What happened?\r\nWe are running CNPG on Openshift. We have updated a lab instance, Openshift 4.13.25, from 1.22.2 to 1.23.1 without issues.\r\nWe have an Openshift 4.14.17 where we had CNPG Operator 1.22.1 installed, all fine. We have upgraded the instance to 1.22.2, no issues. After the upgrade finished, all pods were up, we upgraded the operator to 1.23.1. Cluster instances were not started to be upgraded. We have checked and the controller-manager deployment complained that the cnpg-manager service account does not have permission to list ClusterImageCatalogs.\r\nThe reason was that the ClusterImageCatalogs access is a simple Role applied as a RoleBinding that allows only the namespace where the operator is installed.\r\nThe problem is that the ClusterImageCatalogs CRD instances are not created in any namespace but on the cluster level.\r\nAs a workaround, we have created a ClusterRole with the same ClusterImageCatalogs access (get, list, watch) and a ClusterRoleBinding for cnpg-manager SA. Once we applied the upgrade was completed successfully.\r\nSorry, the logs were purged when the upgrade succeeded hence I cannot attach them.\r\nCode to apply workaround:\r\n```yaml\r\nkind: ClusterRole\r\napiVersion: rbac.authorization.k8s.io/v1\r\nmetadata:\r\n  name: cnpg-clusterimagecatalogs\r\nrules:\r\n  - verbs:\r\n      - get\r\n      - list\r\n      - watch\r\n    apiGroups:\r\n      - postgresql.cnpg.io\r\n    resources:\r\n      - clusterimagecatalogs\r\n---\r\nkind: ClusterRoleBinding\r\napiVersion: rbac.authorization.k8s.io/v1\r\nmetadata:\r\n  name: cnpg-clusterimagecatalogs\r\nsubjects:\r\n  - kind: ServiceAccount\r\n    name: cnpg-manager\r\n    namespace: postgres\r\nroleRef:\r\n  apiGroup: rbac.authorization.k8s.io\r\n  kind: ClusterRole\r\n  name: cnpg-clusterimagecatalogs\r\n```\r\n### Cluster resource\r\n_No response_\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductWe also had to add the `imagecatalogs` resource in the clusterRole before it worked.\r\nThanks for sharing!"
    },
    {
        "title": "[Bug]: There is no problem with backups, but I still see \"Last Failed Backup Time\" alert.",
        "id": 2290766741,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.21.5\n### What version of Kubernetes are you using?\n1.27\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nHelm\n### What happened?\n\"Last Failed Backup Time\" is firing.\r\ncnpg_collector_last_failed_backup_timestamp > 1 -> show \"1715294339\"\n### Cluster resource\n_No response_\n### Relevant log output\n```shell\nContinuous Backup status\r\nFirst Point of Recoverability:  2024-03-24T01:51:29Z\r\nWorking WAL archiving:          OK\r\nWALs waiting to be archived:    0\r\nLast Archived WAL:              000000150000016000000075   @   2024-05-11T07:42:43.778192Z\r\nLast Failed WAL:                -\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.21.5\n### What version of Kubernetes are you using?\n1.27\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nHelm\n### What happened?\n\"Last Failed Backup Time\" is firing.\r\ncnpg_collector_last_failed_backup_timestamp > 1 -> show \"1715294339\"\n### Cluster resource\n_No response_\n### Relevant log output\n```shell\nContinuous Backup status\r\nFirst Point of Recoverability:  2024-03-24T01:51:29Z\r\nWorking WAL archiving:          OK\r\nWALs waiting to be archived:    0\r\nLast Archived WAL:              000000150000016000000075   @   2024-05-11T07:42:43.778192Z\r\nLast Failed WAL:                -\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI have the same problem. I was expecting that cnpg_collector_last_failed_backup_timestamp will be reset after a successfull backup. I have 5 successful backups and one failed a month ago and I still see alerts on the failed backup. How can I get rid of this?\n---\nSame issue for me\n---\nCan you confirm that this still being the case in the latest version @kfkawalec  ?\n---\ntaking a cursory look at the `main` branch of the code, it seems to me like pg_collector is getting this from `cluster.Status.LastFailedBackup`\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/a0800ca858b4ad260c8a9fc965b482f445090a6a/pkg/management/postgres/webserver/metricserver/pg_collector.go#L498-L503\nfrom looking around, i see that value being set in `backup.go`\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/a0800ca858b4ad260c8a9fc965b482f445090a6a/pkg/management/postgres/backup.go#L197\nand also in `plugin_backup.go`\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/a0800ca858b4ad260c8a9fc965b482f445090a6a/pkg/management/postgres/webserver/plugin_backup.go#L189\nbut i don't see any place where the value is reset\n---\nNow I am using 1.24.1 and there is still a problem.\nCommands like \"SELECT * FROM pg_stat_archiver\\gx\" show no problem.\nOf course all previous backups have been done without any problems."
    },
    {
        "title": "[Feature]: Change Grafana dashboard default range from 7d to 24h",
        "id": 2290030484,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\n7 day range in Grafana Dashboard looks too big. \n### Describe the solution you'd like\nPlease change default to 24h\n### Describe alternatives you've considered\nnone\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\n7 day range in Grafana Dashboard looks too big. \n### Describe the solution you'd like\nPlease change default to 24h\n### Describe alternatives you've considered\nnone\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: Bootstrapping from a backup fails if `ssl = on` is present in the backup's `postgresql.auto.conf`",
        "id": 2288408738,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\nolder in 1.21.x\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Amazon EKS\n### How did you install the operator?\nYAML manifest\n### What happened?\nThe backup stored in S3 has `postgresql.auto.conf` with `ssl = on` in it. The bootstrapping fails after successfully restoring the backup with this error message:\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-05-09T19:47:12Z\",\"logger\":\"pg_ctl\",\"msg\":\"waiting for server to start....2024-05-09 19:47:12.449 UTC [33] FATAL:  could not load server certificate file \\\"/controller/certificates/server.crt\\\": No such file or directory\",\"pipe\":\"stdout\",\"logging_pod\":\"test-restore-1-full-recovery\"}\r\n```\r\nLooking at the [code](https://github.com/cloudnative-pg/cloudnative-pg/blob/d30c09e23d4f2409a7d0e0fd2d6b6e67484794b3/pkg/management/postgres/restore.go#L750C1-L754C1):\r\n```\r\n\t// Disable SSL as we still don't have the required certificates\r\n\terr = fileutils.AppendStringToFile(\r\n\t\tpath.Join(info.PgData, constants.PostgresqlCustomConfigurationFile),\r\n\t\t\"ssl = 'off'\\n\")\r\n```\r\nwe do attempt to override the `ssl` setting, but `postgresql.auto.conf` has the final say.\r\nPerhaps it would be better to just remove it when bootstrapping from a backup, because who knows what else might be there messing up with the new cluster's config.\n### Cluster resource\n```shell\nkind: Cluster\r\nmetadata:\r\n  name: test-restore\r\nspec:\r\n  instances: 1\r\n  primaryUpdateStrategy: unsupervised\r\n  storage:\r\n    size: 1Gi\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:12.18\r\n  # imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  bootstrap:\r\n    recovery:\r\n      source: clusterBackup\r\n  externalClusters:\r\n    - name: clusterBackup\r\n      barmanObjectStore:\r\n        serverName: cluster-example\r\n        # serverName: m1-1\r\n        destinationPath: \"s3://m1-1\"\r\n        endpointURL: \"https://minio.ns-minio.svc.cluster.local\"\r\n        endpointCA:\r\n          name: kube-root-ca.crt\r\n          key: ca.crt\r\n        s3Credentials:\r\n          accessKeyId:\r\n            key: ACCESS_KEY_ID\r\n            name: demo-s3-auth\r\n          secretAccessKey:\r\n            key: ACCESS_SECRET_KEY\r\n            name: demo-s3-auth\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-05-09T19:47:12Z\",\"logger\":\"pg_ctl\",\"msg\":\"waiting for server to start....2024-05-09 19:47:12.449 UTC [33] FATAL:  could not load server certificate file \\\"/controller/certificates/server.crt\\\": No such file or directory\",\"pipe\":\"stdout\",\"logging_pod\":\"test-restore-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-09T19:47:12Z\",\"logger\":\"pg_ctl\",\"msg\":\"2024-05-09 19:47:12.449 UTC [33] LOG:  database system is shut down\",\"pipe\":\"stdout\",\"logging_pod\":\"test-restore-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-09T19:47:12Z\",\"logger\":\"pg_ctl\",\"msg\":\" stopped waiting\",\"pipe\":\"stdout\",\"logging_pod\":\"test-restore-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-09T19:47:12Z\",\"logger\":\"pg_ctl\",\"msg\":\"pg_ctl: could not start server\",\"pipe\":\"stderr\",\"logging_pod\":\"test-restore-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-09T19:47:12Z\",\"logger\":\"pg_ctl\",\"msg\":\"Examine the log output.\",\"pipe\":\"stderr\",\"logging_pod\":\"test-restore-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-09T19:47:12Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"test-restore-1-full-recovery\"}\r\n{\"level\":\"error\",\"ts\":\"2024-05-09T19:47:12Z\",\"msg\":\"Error while restoring a backup\",\"logging_pod\":\"test-restore-1-full-recovery\",\"error\":\"while activating instance: error starting PostgreSQL instance: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/cmd.go:89\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.NewCmd.func2\\n\\tinternal/cmd/manager/instance/restore/cmd.go:60\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.6/x64/src/runtime/proc.go:267\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\nolder in 1.21.x\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Amazon EKS\n### How did you install the operator?\nYAML manifest\n### What happened?\nThe backup stored in S3 has `postgresql.auto.conf` with `ssl = on` in it. The bootstrapping fails after successfully restoring the backup with this error message:\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-05-09T19:47:12Z\",\"logger\":\"pg_ctl\",\"msg\":\"waiting for server to start....2024-05-09 19:47:12.449 UTC [33] FATAL:  could not load server certificate file \\\"/controller/certificates/server.crt\\\": No such file or directory\",\"pipe\":\"stdout\",\"logging_pod\":\"test-restore-1-full-recovery\"}\r\n```\r\nLooking at the [code](https://github.com/cloudnative-pg/cloudnative-pg/blob/d30c09e23d4f2409a7d0e0fd2d6b6e67484794b3/pkg/management/postgres/restore.go#L750C1-L754C1):\r\n```\r\n\t// Disable SSL as we still don't have the required certificates\r\n\terr = fileutils.AppendStringToFile(\r\n\t\tpath.Join(info.PgData, constants.PostgresqlCustomConfigurationFile),\r\n\t\t\"ssl = 'off'\\n\")\r\n```\r\nwe do attempt to override the `ssl` setting, but `postgresql.auto.conf` has the final say.\r\nPerhaps it would be better to just remove it when bootstrapping from a backup, because who knows what else might be there messing up with the new cluster's config.\n### Cluster resource\n```shell\nkind: Cluster\r\nmetadata:\r\n  name: test-restore\r\nspec:\r\n  instances: 1\r\n  primaryUpdateStrategy: unsupervised\r\n  storage:\r\n    size: 1Gi\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:12.18\r\n  # imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  bootstrap:\r\n    recovery:\r\n      source: clusterBackup\r\n  externalClusters:\r\n    - name: clusterBackup\r\n      barmanObjectStore:\r\n        serverName: cluster-example\r\n        # serverName: m1-1\r\n        destinationPath: \"s3://m1-1\"\r\n        endpointURL: \"https://minio.ns-minio.svc.cluster.local\"\r\n        endpointCA:\r\n          name: kube-root-ca.crt\r\n          key: ca.crt\r\n        s3Credentials:\r\n          accessKeyId:\r\n            key: ACCESS_KEY_ID\r\n            name: demo-s3-auth\r\n          secretAccessKey:\r\n            key: ACCESS_SECRET_KEY\r\n            name: demo-s3-auth\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-05-09T19:47:12Z\",\"logger\":\"pg_ctl\",\"msg\":\"waiting for server to start....2024-05-09 19:47:12.449 UTC [33] FATAL:  could not load server certificate file \\\"/controller/certificates/server.crt\\\": No such file or directory\",\"pipe\":\"stdout\",\"logging_pod\":\"test-restore-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-09T19:47:12Z\",\"logger\":\"pg_ctl\",\"msg\":\"2024-05-09 19:47:12.449 UTC [33] LOG:  database system is shut down\",\"pipe\":\"stdout\",\"logging_pod\":\"test-restore-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-09T19:47:12Z\",\"logger\":\"pg_ctl\",\"msg\":\" stopped waiting\",\"pipe\":\"stdout\",\"logging_pod\":\"test-restore-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-09T19:47:12Z\",\"logger\":\"pg_ctl\",\"msg\":\"pg_ctl: could not start server\",\"pipe\":\"stderr\",\"logging_pod\":\"test-restore-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-09T19:47:12Z\",\"logger\":\"pg_ctl\",\"msg\":\"Examine the log output.\",\"pipe\":\"stderr\",\"logging_pod\":\"test-restore-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-09T19:47:12Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"test-restore-1-full-recovery\"}\r\n{\"level\":\"error\",\"ts\":\"2024-05-09T19:47:12Z\",\"msg\":\"Error while restoring a backup\",\"logging_pod\":\"test-restore-1-full-recovery\",\"error\":\"while activating instance: error starting PostgreSQL instance: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/cmd.go:89\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.NewCmd.func2\\n\\tinternal/cmd/manager/instance/restore/cmd.go:60\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.6/x64/src/runtime/proc.go:267\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductIs there currently any workaround for this issue yet?\n---\n> Is there currently any workaround for this issue yet?\r\nRemove `postgresql.auto.conf` from the backup.\n---\nCan you suggest how to remove postgresql.auto.conf or set ssl=off ? From the documentation, it looks like you cannot disable ssl as its fixed in operator.\n---\nCan we do PITR if we have both volumesnapshots and wal backups?\n---\nSame issue is observed when using volume snapshots and PITR following steps in https://cloudnative-pg.io/documentation/current/recovery/#point-in-time-recovery-pitr. I was using targetTime.\n---\nhi @sgojexcaliber , did you able to disable the ssl? i'm facing the same issue while migrating the zalando postgres cluster to cloudnative-pg."
    },
    {
        "title": "[Feature]: Is there any way we can have sidecar container with PG container in same pod ? ",
        "id": 2279452572,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWe are trying to add one of the helper container for backup solution(this is standard backup solution across our organization). Now when we checked with backup vendor, this container needs to be added as sidecar container(within same pod where PG is running). I believe CloudNativePG does not support this. Do we have a way to perform this kind of configuration ? \r\nIf not, are we planning to get this kind of feature in future releases? \n### Describe the solution you'd like\nShould have configuration in place to mention another helper image container. And it should run with Primary PG instance.\n### Describe alternatives you've considered\nI havent tried any alternatives , as I believe CloudNativePG does not support this feature.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWe are trying to add one of the helper container for backup solution(this is standard backup solution across our organization). Now when we checked with backup vendor, this container needs to be added as sidecar container(within same pod where PG is running). I believe CloudNativePG does not support this. Do we have a way to perform this kind of configuration ? \r\nIf not, are we planning to get this kind of feature in future releases? \n### Describe the solution you'd like\nShould have configuration in place to mention another helper image container. And it should run with Primary PG instance.\n### Describe alternatives you've considered\nI havent tried any alternatives , as I believe CloudNativePG does not support this feature.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI have the same question. I want to add a Yandex Odyssey connection pooler instead of the built-in pgbouncer and without Deployment+Service (it's not efficient because connections aren't reused), but I want to use sidecar containers instead (like in Stackgres).\r\n<img width=\"534\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9bcfe9d6-5816-4033-9cc3-516f7e8291d2\">"
    },
    {
        "title": "feat: add additional labels for pod monitor in Cluster",
        "id": 2279343682,
        "state": "open",
        "first": "Adds the possibility to add additional labels to just the pod monitor. Currently, this is already possible by setting them on the Cluster object, but then a significant amount of resources get labeled with labels that are not relevant there. This is to support multi-tenant Prometheus situations where there might be separate Prometheus scrapers\r\nFixes: #4435",
        "messages": "Adds the possibility to add additional labels to just the pod monitor. Currently, this is already possible by setting them on the Cluster object, but then a significant amount of resources get labeled with labels that are not relevant there. This is to support multi-tenant Prometheus situations where there might be separate Prometheus scrapers\r\nFixes: #4435Any chance this one get approval ? :-)\r\nWe can't use  `.spec.monitoring.enablePodMonitor: true` for this exact same reason (mutiple prometheus instances in the k8s cluster) and have to manage PodMonitor by ourselves because of the inability to specify a custom `prometheus` label.\n---\nWe also have to manage our PodMonitors because of this.\r\nIs there something blocking the approval?\r\nWould be great if this could be merged :pray:\n---\nThis feature to add additional labels to the PodMonitor resource is going to be super helpful, especially for multi-tenant Prometheus setups. The ability to fine-tune labels directly on the PodMonitor will make things so much cleaner and avoid unnecessary labeling across resources.\r\nLooking forward to seeing this land \ud83d\ude80\ud83d\ude0a\n---\n## Hello!\r\n---\r\n### Is anyone home?\r\n@mnencia @gbartolini @sxd @leonardoce @fcanovai @sxd @phisco\n---\nAny updates on this?\r\nI have installed Prometheus with Kube Prometheus and it does not capture any metrics when cloudnative-pg deployment has monitor = true.\r\nWhat should be wrong?\n---\nHi all! \r\nAny updates on this?\r\nWe deployed cloudnativePg and to integrate it with our prometheus, I need manually label the podmonitors with needed labels, anyways it does not picks up the podmonitors into the target list. When will this be able to do from the cluster values? \r\nThanks in advance, \r\nChris\n---\n> Hi all! \n> \n> \n> \n> Any updates on this?\n> \n> \n> \n> We deployed cloudnativePg and to integrate it with our prometheus, I need manually label the podmonitors with needed labels, anyways it does not picks up the podmonitors into the target list. When will this be able to do from the cluster values? \n> \n> \n> \n> Thanks in advance, \n> \n> Chris\nI believe I was able to make it work. I had to add the operator namespace with a label release: monitoring.\nI thought by default Prometheus would consider all namespaces."
    },
    {
        "title": "[Feature]: Specify addtional labels for PodMonitor",
        "id": 2278835605,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI create a PG cluster with monitoring enabled. A PodMonitor resource is created:\r\n```yaml\r\n---\r\napiVersion: monitoring.coreos.com/v1\r\nkind: PodMonitor\r\nmetadata:\r\n  annotations:\r\n    cnpg.io/operatorVersion: 1.23.1\r\n  creationTimestamp: \"2024-05-03T16:12:34Z\"\r\n  generation: 1\r\n  labels:\r\n    cnpg.io/cluster: authentik\r\n  name: authentik\r\n  namespace: identity\r\n  ownerReferences:\r\n  - apiVersion: postgresql.cnpg.io/v1\r\n    controller: true\r\n    kind: Cluster\r\n    name: authentik\r\n    uid: 54d94e3b-a590-4dc4-b8fd-7925ff3699e4\r\n  resourceVersion: \"117090107\"\r\n  uid: 00755fb7-c757-40ba-bb72-68472d98834a\r\nspec:\r\n  namespaceSelector: {}\r\n  podMetricsEndpoints:\r\n  - bearerTokenSecret:\r\n      key: \"\"\r\n    port: metrics\r\n  selector:\r\n    matchLabels:\r\n      cnpg.io/cluster: authentik\r\n```\r\nMy Prometheus Operator load only PodMonitor/ServiceMonitor with a specific label like `monitoring: infra`.\r\nSo this PodMonitor for my PG cluster isn't loaded and I've got no monitoring.\n### Describe the solution you'd like\nPossibility to add additional labels to PodMonitor resource.\n### Describe alternatives you've considered\nThe cloudnative-pg chart is setup using these values:\r\n```yaml\r\ncloudnative-pg:\r\n  monitoring:\r\n    podMonitorEnabled: true\r\n    podMonitorAdditionalLabels:\r\n      monitoring: infra\r\n```\r\nIt is correctly monitoring:\r\n```\r\n\u279c kubectl -n database get podmonitors.monitoring.coreos.com cloudnativepg-cloudnative-pg -o yaml\r\napiVersion: monitoring.coreos.com/v1\r\nkind: PodMonitor\r\nmetadata:\r\n  labels:\r\n    app.kubernetes.io/instance: cloudnativepg\r\n    app.kubernetes.io/managed-by: Helm\r\n    app.kubernetes.io/name: cloudnative-pg\r\n    app.kubernetes.io/version: 1.23.1\r\n    argocd.argoproj.io/instance: cloudnativepg\r\n    helm.sh/chart: cloudnative-pg-0.21.2\r\n    monitoring: infra\r\n  name: cloudnativepg-cloudnative-pg\r\n  namespace: database\r\nspec:\r\n  podMetricsEndpoints:\r\n  - port: metrics\r\n  selector:\r\n    matchLabels:\r\n      app.kubernetes.io/instance: cloudnativepg\r\n      app.kubernetes.io/name: cloudnative-pg\r\n```\r\nIt could be fixed with `additionalLabels` for the cluster. Like that:\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: authentik\r\nspec:\r\n  ....\r\n  monitoring:\r\n    enablePodMonitor: true\r\n    additionalLabels:\r\n      monitoring: infra\r\n```\n### Additional context\nNo.\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI create a PG cluster with monitoring enabled. A PodMonitor resource is created:\r\n```yaml\r\n---\r\napiVersion: monitoring.coreos.com/v1\r\nkind: PodMonitor\r\nmetadata:\r\n  annotations:\r\n    cnpg.io/operatorVersion: 1.23.1\r\n  creationTimestamp: \"2024-05-03T16:12:34Z\"\r\n  generation: 1\r\n  labels:\r\n    cnpg.io/cluster: authentik\r\n  name: authentik\r\n  namespace: identity\r\n  ownerReferences:\r\n  - apiVersion: postgresql.cnpg.io/v1\r\n    controller: true\r\n    kind: Cluster\r\n    name: authentik\r\n    uid: 54d94e3b-a590-4dc4-b8fd-7925ff3699e4\r\n  resourceVersion: \"117090107\"\r\n  uid: 00755fb7-c757-40ba-bb72-68472d98834a\r\nspec:\r\n  namespaceSelector: {}\r\n  podMetricsEndpoints:\r\n  - bearerTokenSecret:\r\n      key: \"\"\r\n    port: metrics\r\n  selector:\r\n    matchLabels:\r\n      cnpg.io/cluster: authentik\r\n```\r\nMy Prometheus Operator load only PodMonitor/ServiceMonitor with a specific label like `monitoring: infra`.\r\nSo this PodMonitor for my PG cluster isn't loaded and I've got no monitoring.\n### Describe the solution you'd like\nPossibility to add additional labels to PodMonitor resource.\n### Describe alternatives you've considered\nThe cloudnative-pg chart is setup using these values:\r\n```yaml\r\ncloudnative-pg:\r\n  monitoring:\r\n    podMonitorEnabled: true\r\n    podMonitorAdditionalLabels:\r\n      monitoring: infra\r\n```\r\nIt is correctly monitoring:\r\n```\r\n\u279c kubectl -n database get podmonitors.monitoring.coreos.com cloudnativepg-cloudnative-pg -o yaml\r\napiVersion: monitoring.coreos.com/v1\r\nkind: PodMonitor\r\nmetadata:\r\n  labels:\r\n    app.kubernetes.io/instance: cloudnativepg\r\n    app.kubernetes.io/managed-by: Helm\r\n    app.kubernetes.io/name: cloudnative-pg\r\n    app.kubernetes.io/version: 1.23.1\r\n    argocd.argoproj.io/instance: cloudnativepg\r\n    helm.sh/chart: cloudnative-pg-0.21.2\r\n    monitoring: infra\r\n  name: cloudnativepg-cloudnative-pg\r\n  namespace: database\r\nspec:\r\n  podMetricsEndpoints:\r\n  - port: metrics\r\n  selector:\r\n    matchLabels:\r\n      app.kubernetes.io/instance: cloudnativepg\r\n      app.kubernetes.io/name: cloudnative-pg\r\n```\r\nIt could be fixed with `additionalLabels` for the cluster. Like that:\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: authentik\r\nspec:\r\n  ....\r\n  monitoring:\r\n    enablePodMonitor: true\r\n    additionalLabels:\r\n      monitoring: infra\r\n```\n### Additional context\nNo.\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "docs: add a Production Readiness guide",
        "id": 2277454841,
        "state": "open",
        "first": "Closes #4428",
        "messages": "Closes #4428"
    },
    {
        "title": "[Docs]: guide on production readiness for CNPG",
        "id": 2277410937,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThere is no discussion on the minimum checklist for what would be\r\na \"production ready\" CNPG installation.\r\nWe see people using CNPG in production that don't have backups.\r\nWe see some PostgreSQL newbies running CNPG clusters that don't\r\nhave any monitoring....\n### Describe what additions need to be done to the documentation\nWe should have a new page guiding users to production readiness.\n### Describe what pages need to change in the documentation, if any\n_No response_\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nThere is no discussion on the minimum checklist for what would be\r\na \"production ready\" CNPG installation.\r\nWe see people using CNPG in production that don't have backups.\r\nWe see some PostgreSQL newbies running CNPG clusters that don't\r\nhave any monitoring....\n### Describe what additions need to be done to the documentation\nWe should have a new page guiding users to production readiness.\n### Describe what pages need to change in the documentation, if any\n_No response_\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI think we should be careful here as \"production readiness\" might vary from environment to environment. I am hesitant to provide any sort of checklist at Community level, as that might be interpreted as an exhaustive list of suggestions or recommendations.\r\nWhat if we forget something? I'd hate in 12 months having to deal with somebody saying: \"Well, I followed all your recommendations\".\r\nAlso, to what extent it will relate to running Postgres? or to just Kubernetes? I think we should encourage third-party organizations or individuals to take the initiative here, as I don't think as a Community we have the strength and the resources to then respond to enquiries and discussions. For example, not even Postgres has that information in the documentation.\r\nJust my two cents. For me this is -1.\n---\nThose are good points. Yes, probably right, better left to individuals or third parties."
    },
    {
        "title": "[Bug]: Operator restarting due to DetectAvailableArchitectures()",
        "id": 2275402168,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.0\n### What version of Kubernetes are you using?\n1.30 (unsuppprted)\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nThis issue seems to be present overall, but so far I\u2019ve only been able to reproduce it in certain cloud providers (mainly EKS, but also AKS).\r\nWhat\u2019s happening is that utils.DetectAvailableArchitectures() is slowing down RunController() enough so that the ReadinessProbePeriod 10 seconds are not respected anymore, and the pod gets killed by the Kubelet (and thus restart).\r\nDetectAvailableArchitectures() should be calculating each architecture\u2019s sha256 hash asynchronously, so it shouldn\u2019t lock the startup of the manager. \r\nNeeds more investigation to understand if the function is not working properly or if we are just hitting the timeout. \n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.23.0\n### What version of Kubernetes are you using?\n1.30 (unsuppprted)\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nThis issue seems to be present overall, but so far I\u2019ve only been able to reproduce it in certain cloud providers (mainly EKS, but also AKS).\r\nWhat\u2019s happening is that utils.DetectAvailableArchitectures() is slowing down RunController() enough so that the ReadinessProbePeriod 10 seconds are not respected anymore, and the pod gets killed by the Kubelet (and thus restart).\r\nDetectAvailableArchitectures() should be calculating each architecture\u2019s sha256 hash asynchronously, so it shouldn\u2019t lock the startup of the manager. \r\nNeeds more investigation to understand if the function is not working properly or if we are just hitting the timeout. \n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: New replica searches for WAL archive that doesn't exist",
        "id": 2272419431,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nstephen.corcoran@telus.com\r\n### Version\r\n1.23.0\r\n### What version of Kubernetes are you using?\r\n1.29\r\n### What is your Kubernetes environment?\r\nCloud: Amazon EKS\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\n- Cluster experienced an issue and failed over (Node resources issue)\r\n- Original primary does not come back online. End up destroying it\r\n- Make a volume snapshot before starting new replica so the recovery doesn't have to go through large WAL archive\r\n- When attempting to spin up a new replica the logs suggest the new replica is looking for a WAL archive that doesn't exist.\r\nStatus of cluster at time the new replica is requested (scaling up)\r\nFirst Point of Recoverability:  2024-04-30T14:01:25Z\r\nWorking WAL archiving:          OK\r\nWALs waiting to be archived:    0\r\nLast Archived WAL:              0000001100000682000000E2   @   2024-04-30T20:36:13.111958Z\r\nLast Failed WAL:                00000011.history           @   2024-04-30T18:52:16.746355Z\r\nWhen the replica pod starts up, the logs suggest its looking for a file that doesn't exist yet:\r\n\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"pgv2-dev-11\",\"walName\":\"00000012.history\"\r\nI can also verify that the wal archive does not exist in the pg_wal directory on the primary so it wasn't a flushing issue.\r\nThe cnpg status will indicate the replica is in Standby (file based) recovery, but it does not fully synchronize.\r\nInstances status\r\nName         Database Size  Current LSN   Replication role      Status  QoS         Manager Version\r\n----         -------------  -----------   ----------------      ------  ---         ---------------  ----\r\npgv2-dev-1   719 GB         682/E3002900  Primary               OK      Guaranteed  1.23.0          \r\npgv2-dev-11  719 GB         682/E40000A0  Standby (file based)  OK      Guaranteed  1.23.0   \r\npgv2-dev-3   719 GB         682/E3002900  Standby (sync)        OK      Guaranteed  1.23.0       \r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: pgv2-dev\r\n  namespace: pgdb\r\nspec:\r\n  description: V2 Database Cluster DEV\r\n  imageName: ghcr.io/cloudnative-pg/postgis:16-3.4\r\n  instances: 2\r\n  maxSyncReplicas: 1\r\n  minSyncReplicas: 1\r\n  logLevel: debug\r\n  enableSuperuserAccess: true\r\n  enablePDB: true\r\n  superuserSecret:\r\n    name: superuser-auth\r\n  smartShutdownTimeout: 180\r\n  startDelay: 300\r\n  stopDelay: 300\r\n  failoverDelay: 0\r\n  switchoverDelay: 300\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  serviceAccountTemplate:\r\n    metadata:\r\n      annotations:\r\n        eks.amazonaws.com/role-arn: <AWS ARN>\r\n  bootstrap:\r\n    recovery:\r\n      source: v2\r\n      database: v2dev\r\n      owner: app\r\n      secret:\r\n        name: app-auth\r\n      volumeSnapshots:\r\n        #Make a volume snapshot backup JUST BEFORE creating this cluster\r\n        storage:\r\n          apiGroup: snapshot.storage.k8s.io\r\n          kind: VolumeSnapshot\r\n          name: v2-yyyyMMddhhmmss\r\n        walStorage:\r\n          apiGroup: snapshot.storage.k8s.io\r\n          kind: VolumeSnapshot\r\n          name: v2-yyyyMMddhhmmss-wal\r\n  externalClusters:\r\n  - name: v2\r\n    barmanObjectStore:\r\n      data:\r\n        compression: gzip\r\n        jobs: 2\r\n      destinationPath: s3://<BUCKET DIR>\r\n      s3Credentials:\r\n        inheritFromIAMRole: true\r\n      wal:\r\n        compression: gzip\r\n        maxParallel: 20\r\n    connectionParameters:\r\n      dbname: postgres\r\n      host: v2-rw.default.svc\r\n      user: postgres\r\n    password:\r\n      key: password\r\n      name: superuser-auth\r\n  replica:\r\n    enabled: true\r\n    source: v2\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    synchronizeReplicas:\r\n      enabled: true\r\n    updateInterval: 300\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      auto_explain.log_min_duration: 10s\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      maintenance_work_mem: 1GB\r\n      max_connections: \"600\"\r\n      max_parallel_workers: \"32\"\r\n      max_parallel_workers_per_gather: \"4\"\r\n      max_replication_slots: \"32\"\r\n      max_wal_size: 2048MB\r\n      max_worker_processes: \"32\"\r\n      pg_stat_statements.max: \"10000\"\r\n      pg_stat_statements.track: all\r\n      shared_buffers: 1GB\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 1024MB\r\n      wal_level: logical\r\n      wal_log_hints: \"on\"\r\n      wal_receiver_timeout: 10s\r\n      wal_sender_timeout: 10s\r\n      work_mem: 256MB\r\n    pg_hba:\r\n    - host all app all password\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  backup:\r\n    barmanObjectStore:\r\n      data:\r\n        compression: gzip\r\n        jobs: 2\r\n      destinationPath: s3://<BUCKET DIR>/\r\n      s3Credentials:\r\n        inheritFromIAMRole: true\r\n      wal:\r\n        compression: gzip\r\n        maxParallel: 20\r\n    retentionPolicy: 30d\r\n    target: prefer-standby\r\n    volumeSnapshot:\r\n      className: v2-vsc\r\n      online: true\r\n      onlineConfiguration:\r\n        immediateCheckpoint: true\r\n        waitForArchive: false\r\n      snapshotOwnerReference: none\r\n      walClassName: v2-vsc-wal\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: false\r\n  resources:\r\n    limits:\r\n      cpu: \"3\"\r\n      memory: 4Gi\r\n    requests:\r\n      cpu: \"3\"\r\n      memory: 4Gi\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 3000Gi\r\n    storageClass: gp3\r\n  walStorage:\r\n    resizeInUseVolumes: true\r\n    size: 50Gi\r\n    storageClass: io2-unencrypted\r\n  affinity:\r\n    enablePodAntiAffinity: true\r\n    nodeAffinity:\r\n      requiredDuringSchedulingIgnoredDuringExecution:\r\n        nodeSelectorTerms:\r\n        - matchExpressions:\r\n          - key: eks.amazonaws.com/nodegroup\r\n            operator: In\r\n            values:\r\n            - pgdb-server\r\n    podAntiAffinityType: preferred\r\n    topologyKey: failure-domain.beta.kubernetes.io/zone\r\n```\r\n### Relevant log output\r\n```shell\r\nstream logs failed container \"postgres\" in pod \"pgv2-dev-11\" is waiting to start: PodInitializing for pgdb/pgv2-dev-11 (postgres)\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"pgv2-dev-11\",\"version\":\"1.23.0\",\"build\":{\"Version\":\"1.23.0\",\"Commit\":\"f6292625\",\"Date\":\"2024-04-25\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"setup\",\"msg\":\"starting tablespace manager\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"setup\",\"msg\":\"starting external server manager\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"pgv2-dev-11\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"starting the postgres loop\",\"caller\":\"internal/cmd/manager/instance/run/lifecycle/lifecycle.go:74\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"starting signal loop\",\"caller\":\"internal/cmd/manager/instance/run/lifecycle/lifecycle.go:81\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Checking PGDATA coherence\",\"caller\":\"pkg/management/postgres/instance.go:296\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"pgv2-dev-11\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"pgv2-dev-11\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"tbs_reconciler\",\"msg\":\"no tablespaces to create\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"98b7c93f-91d2-4d99-9245-72823280b28b\",\"caller\":\"internal/management/controller/tablespaces/reconciler.go:68\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Reconciling Cluster\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"caller\":\"internal/management/controller/instance_controller.go:111\",\"logging_pod\":\"pgv2-dev-11\",\"cluster\":{\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"external_servers_reconciler\",\"msg\":\"starting up the external servers reconciler\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"6f865827-ab7c-4a90-b4b6-321a879e262a\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:56\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Reconciling custom monitoring queries\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"caller\":\"internal/management/controller/instance_controller.go:753\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"External server connection string\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"6f865827-ab7c-4a90-b4b6-321a879e262a\",\"serverName\":\"v2\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:80\",\"logging_pod\":\"pgv2-dev-11\",\"connectionString\":\"dbname='postgres' host='v2-rw.default.svc' passfile='/controller/external/v2/pgpass' user='postgres'\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"/controller/certificates/server.crt\",\"secret\":\"pgv2-dev-server\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"/controller/certificates/server.key\",\"secret\":\"pgv2-dev-server\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"/controller/certificates/streaming_replica.crt\",\"secret\":\"pgv2-dev-replication\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"/controller/certificates/streaming_replica.key\",\"secret\":\"pgv2-dev-replication\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"/controller/certificates/client-ca.crt\",\"secret\":\"pgv2-dev-ca\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"/controller/certificates/server-ca.crt\",\"secret\":\"pgv2-dev-ca\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Installed configuration file\",\"logging_pod\":\"pgv2-dev-11\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"filename\":\"pg_ident.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Cluster status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"currentPrimary\":\"pgv2-dev-1\",\"targetPrimary\":\"pgv2-dev-1\",\"isReplicaCluster\":false}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"This is an old primary instance, waiting for the switchover to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"currentPrimary\":\"pgv2-dev-1\",\"targetPrimary\":\"pgv2-dev-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Switchover completed\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"targetPrimary\":\"pgv2-dev-1\",\"currentPrimary\":\"pgv2-dev-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Waiting for the new primary to be available\",\"logging_pod\":\"pgv2-dev-11\",\"primaryConnInfo\":\"host=pgv2-dev-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=pgv2-dev-11 sslmode=verify-ca dbname=postgres connect_timeout=5\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Extracting pg_controldata information\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"reason\":\"before pg_rewind\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7345552334997741588\\nDatabase cluster state:               in archive recovery\\npg_control last modified:             Tue 30 Apr 2024 08:37:38 PM UTC\\nLatest checkpoint location:           682/E251A0C8\\nLatest checkpoint's REDO location:    682/E251A040\\nLatest checkpoint's REDO WAL file:    0000001100000682000000E2\\nLatest checkpoint's TimeLineID:       17\\nLatest checkpoint's PrevTimeLineID:   17\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:181815604\\nLatest checkpoint's NextOID:          149694561\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  181815603\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Tue 30 Apr 2024 08:35:11 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     682/E3000000\\nMin recovery ending loc's timeline:   17\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              600\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            b2ae8cbe39949b76539a9f04f6903695bba7311148df10284169aa832cc898ae\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting up pg_rewind\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"options\":[\"-P\",\"--source-server\",\"host=pgv2-dev-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=pgv2-dev-11 sslmode=verify-ca dbname=postgres\",\"--target-pgdata\",\"/var/lib/postgresql/data/pgdata\",\"--restore-target-wal\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: connected to server\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: executing \\\"/usr/lib/postgresql/16/bin/postgres\\\" for target server to complete crash recovery\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:25.801 UTC [29] LOG:  database system was interrupted while in recovery at log time 2024-04-30 20:35:11 UTC\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:25.801 UTC [29] HINT:  If this has occurred more than once some data might be corrupted and you might need to choose an earlier recovery target.\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:25.905 UTC [29] LOG:  database system was not properly shut down; automatic recovery in progress\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:25.969 UTC [29] LOG:  redo starts at 682/E251A040\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:25.999 UTC [29] LOG:  unexpected pageaddr 680/60000000 in WAL segment 0000001100000682000000E3, LSN 682/E3000000, offset 0\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:25.999 UTC [29] LOG:  redo done at 682/E251A1C8 system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.03 s\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:26Z\",\"logger\":\"external_servers_reconciler\",\"msg\":\"starting up the external servers reconciler\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"0d35dfe0-84ee-4a1e-bbe0-0debe5696f32\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:56\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:26Z\",\"logger\":\"tbs_reconciler\",\"msg\":\"no tablespaces to create\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"37020710-021e-430c-b564-eae2cb5f6b96\",\"caller\":\"internal/management/controller/tablespaces/reconciler.go:68\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:26Z\",\"msg\":\"External server connection string\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"0d35dfe0-84ee-4a1e-bbe0-0debe5696f32\",\"serverName\":\"v2\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:80\",\"logging_pod\":\"pgv2-dev-11\",\"connectionString\":\"dbname='postgres' host='v2-rw.default.svc' passfile='/controller/external/v2/pgpass' user='postgres'\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:26Z\",\"logger\":\"tbs_reconciler\",\"msg\":\"no tablespaces to create\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"c507c153-4fa1-4523-933d-9de54f7b372f\",\"caller\":\"internal/management/controller/tablespaces/reconciler.go:68\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:26Z\",\"logger\":\"external_servers_reconciler\",\"msg\":\"starting up the external servers reconciler\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"70c90437-ecc1-4b01-8ff7-11f78af99451\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:56\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:26Z\",\"msg\":\"External server connection string\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"70c90437-ecc1-4b01-8ff7-11f78af99451\",\"serverName\":\"v2\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:80\",\"logging_pod\":\"pgv2-dev-11\",\"connectionString\":\"dbname='postgres' host='v2-rw.default.svc' passfile='/controller/external/v2/pgpass' user='postgres'\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:26Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:26.221 UTC [29] LOG:  checkpoint starting: end-of-recovery immediate wait\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:26Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:26.223 UTC [29] LOG:  checkpoint complete: wrote 3 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.002 s, sync=0.001 s, total=0.002 s; sync files=0, longest=0.000 s, average=0.000 s; distance=11159 kB, estimate=11159 kB; lsn=682/E3000028, redo lsn=682/E3000028\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"pg_rewind\",\"msg\":\"PostgreSQL stand-alone backend 16.2 (Debian 16.2-1.pgdg110+2)\",\"pipe\":\"stdout\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:27.103 UTC [29] LOG:  checkpoint starting: shutdown immediate\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:27.123 UTC [29] LOG:  checkpoint complete: wrote 1 buffers (0.0%); 0 WAL file(s) added, 1 removed, 0 recycled; write=0.002 s, sync=0.001 s, total=0.020 s; sync files=0, longest=0.000 s, average=0.000 s; distance=16384 kB, estimate=16384 kB; lsn=682/E4000028, redo lsn=682/E4000028\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: source and target cluster are on the same timeline\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: no rewind required\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"pg_rewind\",\"msg\":\"backend> \",\"pipe\":\"stdout\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Demoting instance\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"pgpdata\":\"/var/lib/postgresql/data/pgdata\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"override.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"pgv2-dev-11\",\"reason\":\"postmaster start up\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7345552334997741588\\nDatabase cluster state:               shut down\\npg_control last modified:             Tue 30 Apr 2024 08:40:27 PM UTC\\nLatest checkpoint location:           682/E4000028\\nLatest checkpoint's REDO location:    682/E4000028\\nLatest checkpoint's REDO WAL file:    0000001100000682000000E4\\nLatest checkpoint's TimeLineID:       17\\nLatest checkpoint's PrevTimeLineID:   17\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:181815604\\nLatest checkpoint's NextOID:          149694561\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  0\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Tue 30 Apr 2024 08:40:27 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              600\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            b2ae8cbe39949b76539a9f04f6903695bba7311148df10284169aa832cc898ae\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Reconciling Cluster\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"320fac48-6f15-4133-940f-8df5cc0f128d\",\"caller\":\"internal/management/controller/instance_controller.go:111\",\"logging_pod\":\"pgv2-dev-11\",\"cluster\":{\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Reconciling custom monitoring queries\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"320fac48-6f15-4133-940f-8df5cc0f128d\",\"caller\":\"internal/management/controller/instance_controller.go:753\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"2024-04-30 20:40:27.180 UTC [32] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"2024-04-30 20:40:27.180 UTC [32] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.180 UTC\",\"process_id\":\"32\",\"session_id\":\"6631573b.20\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.180 UTC\",\"process_id\":\"32\",\"session_id\":\"6631573b.20\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 16.2 (Debian 16.2-1.pgdg110+2) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.180 UTC\",\"process_id\":\"32\",\"session_id\":\"6631573b.20\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.180 UTC\",\"process_id\":\"32\",\"session_id\":\"6631573b.20\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"2024-04-30 20:40:27.180 UTC [32] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"2024-04-30 20:40:27.180 UTC [32] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.183 UTC\",\"process_id\":\"32\",\"session_id\":\"6631573b.20\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.188 UTC\",\"process_id\":\"36\",\"session_id\":\"6631573b.24\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was shut down at 2024-04-30 20:40:27 UTC\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"override.conf\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"override.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.290 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"46\",\"connection_from\":\"[local]\",\"session_id\":\"6631573b.2e\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"320fac48-6f15-4133-940f-8df5cc0f128d\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.292 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"47\",\"connection_from\":\"[local]\",\"session_id\":\"6631573b.2f\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"pgv2-dev-11\",\"walName\":\"00000012.history\",\"options\":[\"--cloud-provider\",\"aws-s3\",\"s3://x319-cnpg-barman-bucket/\",\"pgv2-dev\"],\"startTime\":\"2024-04-30T20:40:27Z\",\"endTime\":\"2024-04-30T20:40:27Z\",\"elapsedWalTime\":0.362220191}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.757 UTC\",\"process_id\":\"36\",\"session_id\":\"6631573b.24\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"entering standby mode\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.781 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"57\",\"connection_from\":\"[local]\",\"session_id\":\"6631573b.39\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.872 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"59\",\"connection_from\":\"[local]\",\"session_id\":\"6631573b.3b\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.927 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"60\",\"connection_from\":\"[local]\",\"session_id\":\"6631573b.3c\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:28Z\",\"msg\":\"Reconciling Cluster\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"2bdb660e-46b5-4c6a-a18e-bc17bdffd380\",\"caller\":\"internal/management/controller/instance_controller.go:111\",\"logging_pod\":\"pgv2-dev-11\",\"cluster\":{\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:28Z\",\"msg\":\"Reconciling custom monitoring queries\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"2bdb660e-46b5-4c6a-a18e-bc17bdffd380\",\"caller\":\"internal/management/controller/instance_controller.go:753\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:28.190 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"62\",\"connection_from\":\"[local]\",\"session_id\":\"6631573c.3e\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:28Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:28.212 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"63\",\"connection_from\":\"[local]\",\"session_id\":\"6631573c.3f\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:28.214 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"64\",\"connection_from\":\"[local]\",\"session_id\":\"6631573c.40\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:28Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"2bdb660e-46b5-4c6a-a18e-bc17bdffd380\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:28Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"pgv2-dev-11\",\"walName\":\"00000011.history\",\"startTime\":\"2024-04-30T20:40:27Z\",\"endTime\":\"2024-04-30T20:40:28Z\",\"elapsedWalTime\":0.3778327}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:28Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"pgv2-dev-11\",\"walName\":\"00000011.history\",\"maxParallel\":20,\"successfulWalRestore\":1,\"failedWalRestore\":19,\"endOfWALStream\":false,\"startTime\":\"2024-04-30T20:40:27Z\",\"downloadStartTime\":\"2024-04-30T20:40:27Z\",\"downloadTotalTime\":0.378065225,\"totalTime\":0.456196816}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:28.237 UTC\",\"process_id\":\"36\",\"session_id\":\"6631573b.24\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000011.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:28Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:28Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:29Z\",\"msg\":\"Reconciling Cluster\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"0dccee4c-a55c-4ce4-9eab-b69d2bf40ce9\",\"caller\":\"internal/management/controller/instance_controller.go:111\",\"logging_pod\":\"pgv2-dev-11\",\"cluster\":{\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:29Z\",\"msg\":\"Reconciling custom monitoring queries\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"0dccee4c-a55c-4ce4-9eab-b69d2bf40ce9\",\"caller\":\"internal/management/controller/instance_controller.go:753\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:29Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:29.512 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"111\",\"connection_from\":\"[local]\",\"session_id\":\"6631573d.6f\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:29Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:29Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:29.532 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"112\",\"connection_from\":\"[local]\",\"session_id\":\"6631573d.70\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:29Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:29.588 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"113\",\"connection_from\":\"[local]\",\"session_id\":\"6631573d.71\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:29Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"0dccee4c-a55c-4ce4-9eab-b69d2bf40ce9\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:30Z\",\"msg\":\"Reconciling Cluster\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"598035a4-2e04-44b5-83a7-7514d47b62f6\",\"caller\":\"internal/management/controller/instance_controller.go:111\",\"logging_pod\":\"pgv2-dev-11\",\"cluster\":{\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:30Z\",\"msg\":\"Reconciling custom monitoring queries\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"598035a4-2e04-44b5-83a7-7514d47b62f6\",\"caller\":\"internal/management/controller/instance_controller.go:753\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:30Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:30.708 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"115\",\"connection_from\":\"[local]\",\"session_id\":\"6631573e.73\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:30 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:30Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:30Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:30.740 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"116\",\"connection_from\":\"[local]\",\"session_id\":\"6631573e.74\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:30 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:30Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:30Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:30.809 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"117\",\"connection_from\":\"[local]\",\"session_id\":\"6631573e.75\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:30 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:30Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:31.014 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"118\",\"connection_from\":\"[local]\",\"session_id\":\"6631573f.76\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:31 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:31.032 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"119\",\"connection_from\":\"[local]\",\"session_id\":\"6631573f.77\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:31 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"598035a4-2e04-44b5-83a7-7514d47b62f6\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:31.124 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"120\",\"connection_from\":\"[local]\",\"session_id\":\"6631573f.78\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:31 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:31Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"pgv2-dev-11\",\"walName\":\"0000001100000682000000E4\",\"options\":[\"--cloud-provider\",\"aws-s3\",\"s3://x319-cnpg-barman-bucket/\",\"pgv2-dev\"],\"startTime\":\"2024-04-30T20:40:28Z\",\"endTime\":\"2024-04-30T20:40:31Z\",\"elapsedWalTime\":3.152753172}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:31.604 UTC\",\"process_id\":\"36\",\"session_id\":\"6631573b.24\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"consistent recovery state reached at 682/E40000A0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:31.604 UTC\",\"process_id\":\"36\",\"session_id\":\"6631573b.24\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"invalid record length at 682/E40000A0: expected at least 24, got 0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:31.604 UTC\",\"process_id\":\"32\",\"session_id\":\"6631573b.20\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is ready to accept read-only connections\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:31.614 UTC\",\"process_id\":\"121\",\"session_id\":\"6631573f.79\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:31 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"started streaming WAL from primary at 682/E4000000 on timeline 17\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:31.614 UTC\",\"process_id\":\"121\",\"session_id\":\"6631573f.79\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-04-30 20:40:31 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"08P01\",\"message\":\"could not receive data from WAL stream: ERROR:  requested starting point 682/E4000000 is ahead of the WAL flush position of this server 682/E3000000\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:31Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:31Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:32Z\",\"msg\":\"Reconciling Cluster\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"00a69c30-b361-4731-81d6-d61d3b8fd1a0\",\"caller\":\"internal/management/controller/instance_controller.go:111\",\"logging_pod\":\"pgv2-dev-11\",\"cluster\":{\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:32Z\",\"msg\":\"Reconciling custom monitoring queries\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"00a69c30-b361-4731-81d6-d61d3b8fd1a0\",\"caller\":\"internal/management/controller/instance_controller.go:753\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:32Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"pgv2-dev-11\",\"walName\":\"00000012.history\",\"options\":[\"--cloud-provider\",\"aws-s3\",\"s3://x319-cnpg-barman-bucket/\",\"pgv2-dev\"],\"startTime\":\"2024-04-30T20:40:31Z\",\"endTime\":\"2024-04-30T20:40:32Z\",\"elapsedWalTime\":0.344877257}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:32Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:32Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nstephen.corcoran@telus.com\r\n### Version\r\n1.23.0\r\n### What version of Kubernetes are you using?\r\n1.29\r\n### What is your Kubernetes environment?\r\nCloud: Amazon EKS\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\n- Cluster experienced an issue and failed over (Node resources issue)\r\n- Original primary does not come back online. End up destroying it\r\n- Make a volume snapshot before starting new replica so the recovery doesn't have to go through large WAL archive\r\n- When attempting to spin up a new replica the logs suggest the new replica is looking for a WAL archive that doesn't exist.\r\nStatus of cluster at time the new replica is requested (scaling up)\r\nFirst Point of Recoverability:  2024-04-30T14:01:25Z\r\nWorking WAL archiving:          OK\r\nWALs waiting to be archived:    0\r\nLast Archived WAL:              0000001100000682000000E2   @   2024-04-30T20:36:13.111958Z\r\nLast Failed WAL:                00000011.history           @   2024-04-30T18:52:16.746355Z\r\nWhen the replica pod starts up, the logs suggest its looking for a file that doesn't exist yet:\r\n\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"pgv2-dev-11\",\"walName\":\"00000012.history\"\r\nI can also verify that the wal archive does not exist in the pg_wal directory on the primary so it wasn't a flushing issue.\r\nThe cnpg status will indicate the replica is in Standby (file based) recovery, but it does not fully synchronize.\r\nInstances status\r\nName         Database Size  Current LSN   Replication role      Status  QoS         Manager Version\r\n----         -------------  -----------   ----------------      ------  ---         ---------------  ----\r\npgv2-dev-1   719 GB         682/E3002900  Primary               OK      Guaranteed  1.23.0          \r\npgv2-dev-11  719 GB         682/E40000A0  Standby (file based)  OK      Guaranteed  1.23.0   \r\npgv2-dev-3   719 GB         682/E3002900  Standby (sync)        OK      Guaranteed  1.23.0       \r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: pgv2-dev\r\n  namespace: pgdb\r\nspec:\r\n  description: V2 Database Cluster DEV\r\n  imageName: ghcr.io/cloudnative-pg/postgis:16-3.4\r\n  instances: 2\r\n  maxSyncReplicas: 1\r\n  minSyncReplicas: 1\r\n  logLevel: debug\r\n  enableSuperuserAccess: true\r\n  enablePDB: true\r\n  superuserSecret:\r\n    name: superuser-auth\r\n  smartShutdownTimeout: 180\r\n  startDelay: 300\r\n  stopDelay: 300\r\n  failoverDelay: 0\r\n  switchoverDelay: 300\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  serviceAccountTemplate:\r\n    metadata:\r\n      annotations:\r\n        eks.amazonaws.com/role-arn: <AWS ARN>\r\n  bootstrap:\r\n    recovery:\r\n      source: v2\r\n      database: v2dev\r\n      owner: app\r\n      secret:\r\n        name: app-auth\r\n      volumeSnapshots:\r\n        #Make a volume snapshot backup JUST BEFORE creating this cluster\r\n        storage:\r\n          apiGroup: snapshot.storage.k8s.io\r\n          kind: VolumeSnapshot\r\n          name: v2-yyyyMMddhhmmss\r\n        walStorage:\r\n          apiGroup: snapshot.storage.k8s.io\r\n          kind: VolumeSnapshot\r\n          name: v2-yyyyMMddhhmmss-wal\r\n  externalClusters:\r\n  - name: v2\r\n    barmanObjectStore:\r\n      data:\r\n        compression: gzip\r\n        jobs: 2\r\n      destinationPath: s3://<BUCKET DIR>\r\n      s3Credentials:\r\n        inheritFromIAMRole: true\r\n      wal:\r\n        compression: gzip\r\n        maxParallel: 20\r\n    connectionParameters:\r\n      dbname: postgres\r\n      host: v2-rw.default.svc\r\n      user: postgres\r\n    password:\r\n      key: password\r\n      name: superuser-auth\r\n  replica:\r\n    enabled: true\r\n    source: v2\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    synchronizeReplicas:\r\n      enabled: true\r\n    updateInterval: 300\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      auto_explain.log_min_duration: 10s\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      maintenance_work_mem: 1GB\r\n      max_connections: \"600\"\r\n      max_parallel_workers: \"32\"\r\n      max_parallel_workers_per_gather: \"4\"\r\n      max_replication_slots: \"32\"\r\n      max_wal_size: 2048MB\r\n      max_worker_processes: \"32\"\r\n      pg_stat_statements.max: \"10000\"\r\n      pg_stat_statements.track: all\r\n      shared_buffers: 1GB\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 1024MB\r\n      wal_level: logical\r\n      wal_log_hints: \"on\"\r\n      wal_receiver_timeout: 10s\r\n      wal_sender_timeout: 10s\r\n      work_mem: 256MB\r\n    pg_hba:\r\n    - host all app all password\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  backup:\r\n    barmanObjectStore:\r\n      data:\r\n        compression: gzip\r\n        jobs: 2\r\n      destinationPath: s3://<BUCKET DIR>/\r\n      s3Credentials:\r\n        inheritFromIAMRole: true\r\n      wal:\r\n        compression: gzip\r\n        maxParallel: 20\r\n    retentionPolicy: 30d\r\n    target: prefer-standby\r\n    volumeSnapshot:\r\n      className: v2-vsc\r\n      online: true\r\n      onlineConfiguration:\r\n        immediateCheckpoint: true\r\n        waitForArchive: false\r\n      snapshotOwnerReference: none\r\n      walClassName: v2-vsc-wal\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: false\r\n  resources:\r\n    limits:\r\n      cpu: \"3\"\r\n      memory: 4Gi\r\n    requests:\r\n      cpu: \"3\"\r\n      memory: 4Gi\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 3000Gi\r\n    storageClass: gp3\r\n  walStorage:\r\n    resizeInUseVolumes: true\r\n    size: 50Gi\r\n    storageClass: io2-unencrypted\r\n  affinity:\r\n    enablePodAntiAffinity: true\r\n    nodeAffinity:\r\n      requiredDuringSchedulingIgnoredDuringExecution:\r\n        nodeSelectorTerms:\r\n        - matchExpressions:\r\n          - key: eks.amazonaws.com/nodegroup\r\n            operator: In\r\n            values:\r\n            - pgdb-server\r\n    podAntiAffinityType: preferred\r\n    topologyKey: failure-domain.beta.kubernetes.io/zone\r\n```\r\n### Relevant log output\r\n```shell\r\nstream logs failed container \"postgres\" in pod \"pgv2-dev-11\" is waiting to start: PodInitializing for pgdb/pgv2-dev-11 (postgres)\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"pgv2-dev-11\",\"version\":\"1.23.0\",\"build\":{\"Version\":\"1.23.0\",\"Commit\":\"f6292625\",\"Date\":\"2024-04-25\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"setup\",\"msg\":\"starting tablespace manager\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"setup\",\"msg\":\"starting external server manager\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"pgv2-dev-11\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"starting the postgres loop\",\"caller\":\"internal/cmd/manager/instance/run/lifecycle/lifecycle.go:74\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"starting signal loop\",\"caller\":\"internal/cmd/manager/instance/run/lifecycle/lifecycle.go:81\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Checking PGDATA coherence\",\"caller\":\"pkg/management/postgres/instance.go:296\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"pgv2-dev-11\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"pgv2-dev-11\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"tbs_reconciler\",\"msg\":\"no tablespaces to create\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"98b7c93f-91d2-4d99-9245-72823280b28b\",\"caller\":\"internal/management/controller/tablespaces/reconciler.go:68\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Reconciling Cluster\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"caller\":\"internal/management/controller/instance_controller.go:111\",\"logging_pod\":\"pgv2-dev-11\",\"cluster\":{\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"external_servers_reconciler\",\"msg\":\"starting up the external servers reconciler\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"6f865827-ab7c-4a90-b4b6-321a879e262a\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:56\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Reconciling custom monitoring queries\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"caller\":\"internal/management/controller/instance_controller.go:753\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"External server connection string\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"6f865827-ab7c-4a90-b4b6-321a879e262a\",\"serverName\":\"v2\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:80\",\"logging_pod\":\"pgv2-dev-11\",\"connectionString\":\"dbname='postgres' host='v2-rw.default.svc' passfile='/controller/external/v2/pgpass' user='postgres'\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"/controller/certificates/server.crt\",\"secret\":\"pgv2-dev-server\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"/controller/certificates/server.key\",\"secret\":\"pgv2-dev-server\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"/controller/certificates/streaming_replica.crt\",\"secret\":\"pgv2-dev-replication\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"/controller/certificates/streaming_replica.key\",\"secret\":\"pgv2-dev-replication\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"/controller/certificates/client-ca.crt\",\"secret\":\"pgv2-dev-ca\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"/controller/certificates/server-ca.crt\",\"secret\":\"pgv2-dev-ca\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Installed configuration file\",\"logging_pod\":\"pgv2-dev-11\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"filename\":\"pg_ident.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Cluster status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"currentPrimary\":\"pgv2-dev-1\",\"targetPrimary\":\"pgv2-dev-1\",\"isReplicaCluster\":false}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"This is an old primary instance, waiting for the switchover to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"currentPrimary\":\"pgv2-dev-1\",\"targetPrimary\":\"pgv2-dev-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Switchover completed\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"targetPrimary\":\"pgv2-dev-1\",\"currentPrimary\":\"pgv2-dev-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Waiting for the new primary to be available\",\"logging_pod\":\"pgv2-dev-11\",\"primaryConnInfo\":\"host=pgv2-dev-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=pgv2-dev-11 sslmode=verify-ca dbname=postgres connect_timeout=5\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Extracting pg_controldata information\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"reason\":\"before pg_rewind\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7345552334997741588\\nDatabase cluster state:               in archive recovery\\npg_control last modified:             Tue 30 Apr 2024 08:37:38 PM UTC\\nLatest checkpoint location:           682/E251A0C8\\nLatest checkpoint's REDO location:    682/E251A040\\nLatest checkpoint's REDO WAL file:    0000001100000682000000E2\\nLatest checkpoint's TimeLineID:       17\\nLatest checkpoint's PrevTimeLineID:   17\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:181815604\\nLatest checkpoint's NextOID:          149694561\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  181815603\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Tue 30 Apr 2024 08:35:11 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     682/E3000000\\nMin recovery ending loc's timeline:   17\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              600\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            b2ae8cbe39949b76539a9f04f6903695bba7311148df10284169aa832cc898ae\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"msg\":\"Starting up pg_rewind\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"options\":[\"-P\",\"--source-server\",\"host=pgv2-dev-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=pgv2-dev-11 sslmode=verify-ca dbname=postgres\",\"--target-pgdata\",\"/var/lib/postgresql/data/pgdata\",\"--restore-target-wal\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: connected to server\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: executing \\\"/usr/lib/postgresql/16/bin/postgres\\\" for target server to complete crash recovery\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:25.801 UTC [29] LOG:  database system was interrupted while in recovery at log time 2024-04-30 20:35:11 UTC\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:25.801 UTC [29] HINT:  If this has occurred more than once some data might be corrupted and you might need to choose an earlier recovery target.\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:25.905 UTC [29] LOG:  database system was not properly shut down; automatic recovery in progress\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:25.969 UTC [29] LOG:  redo starts at 682/E251A040\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:25.999 UTC [29] LOG:  unexpected pageaddr 680/60000000 in WAL segment 0000001100000682000000E3, LSN 682/E3000000, offset 0\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:25Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:25.999 UTC [29] LOG:  redo done at 682/E251A1C8 system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.03 s\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:26Z\",\"logger\":\"external_servers_reconciler\",\"msg\":\"starting up the external servers reconciler\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"0d35dfe0-84ee-4a1e-bbe0-0debe5696f32\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:56\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:26Z\",\"logger\":\"tbs_reconciler\",\"msg\":\"no tablespaces to create\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"37020710-021e-430c-b564-eae2cb5f6b96\",\"caller\":\"internal/management/controller/tablespaces/reconciler.go:68\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:26Z\",\"msg\":\"External server connection string\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"0d35dfe0-84ee-4a1e-bbe0-0debe5696f32\",\"serverName\":\"v2\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:80\",\"logging_pod\":\"pgv2-dev-11\",\"connectionString\":\"dbname='postgres' host='v2-rw.default.svc' passfile='/controller/external/v2/pgpass' user='postgres'\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:26Z\",\"logger\":\"tbs_reconciler\",\"msg\":\"no tablespaces to create\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"c507c153-4fa1-4523-933d-9de54f7b372f\",\"caller\":\"internal/management/controller/tablespaces/reconciler.go:68\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:26Z\",\"logger\":\"external_servers_reconciler\",\"msg\":\"starting up the external servers reconciler\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"70c90437-ecc1-4b01-8ff7-11f78af99451\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:56\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:26Z\",\"msg\":\"External server connection string\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"70c90437-ecc1-4b01-8ff7-11f78af99451\",\"serverName\":\"v2\",\"caller\":\"internal/management/controller/externalservers/reconciler.go:80\",\"logging_pod\":\"pgv2-dev-11\",\"connectionString\":\"dbname='postgres' host='v2-rw.default.svc' passfile='/controller/external/v2/pgpass' user='postgres'\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:26Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:26.221 UTC [29] LOG:  checkpoint starting: end-of-recovery immediate wait\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:26Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:26.223 UTC [29] LOG:  checkpoint complete: wrote 3 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.002 s, sync=0.001 s, total=0.002 s; sync files=0, longest=0.000 s, average=0.000 s; distance=11159 kB, estimate=11159 kB; lsn=682/E3000028, redo lsn=682/E3000028\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"pg_rewind\",\"msg\":\"PostgreSQL stand-alone backend 16.2 (Debian 16.2-1.pgdg110+2)\",\"pipe\":\"stdout\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:27.103 UTC [29] LOG:  checkpoint starting: shutdown immediate\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-04-30 20:40:27.123 UTC [29] LOG:  checkpoint complete: wrote 1 buffers (0.0%); 0 WAL file(s) added, 1 removed, 0 recycled; write=0.002 s, sync=0.001 s, total=0.020 s; sync files=0, longest=0.000 s, average=0.000 s; distance=16384 kB, estimate=16384 kB; lsn=682/E4000028, redo lsn=682/E4000028\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: source and target cluster are on the same timeline\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: no rewind required\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"pg_rewind\",\"msg\":\"backend> \",\"pipe\":\"stdout\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Demoting instance\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\",\"pgpdata\":\"/var/lib/postgresql/data/pgdata\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"override.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"pgv2-dev-11\",\"reason\":\"postmaster start up\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7345552334997741588\\nDatabase cluster state:               shut down\\npg_control last modified:             Tue 30 Apr 2024 08:40:27 PM UTC\\nLatest checkpoint location:           682/E4000028\\nLatest checkpoint's REDO location:    682/E4000028\\nLatest checkpoint's REDO WAL file:    0000001100000682000000E4\\nLatest checkpoint's TimeLineID:       17\\nLatest checkpoint's PrevTimeLineID:   17\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:181815604\\nLatest checkpoint's NextOID:          149694561\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  0\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Tue 30 Apr 2024 08:40:27 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              600\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            b2ae8cbe39949b76539a9f04f6903695bba7311148df10284169aa832cc898ae\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"15648365-92da-4ecc-8e11-968e7875036a\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Reconciling Cluster\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"320fac48-6f15-4133-940f-8df5cc0f128d\",\"caller\":\"internal/management/controller/instance_controller.go:111\",\"logging_pod\":\"pgv2-dev-11\",\"cluster\":{\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Reconciling custom monitoring queries\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"320fac48-6f15-4133-940f-8df5cc0f128d\",\"caller\":\"internal/management/controller/instance_controller.go:753\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"2024-04-30 20:40:27.180 UTC [32] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"2024-04-30 20:40:27.180 UTC [32] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.180 UTC\",\"process_id\":\"32\",\"session_id\":\"6631573b.20\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.180 UTC\",\"process_id\":\"32\",\"session_id\":\"6631573b.20\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 16.2 (Debian 16.2-1.pgdg110+2) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.180 UTC\",\"process_id\":\"32\",\"session_id\":\"6631573b.20\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.180 UTC\",\"process_id\":\"32\",\"session_id\":\"6631573b.20\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"2024-04-30 20:40:27.180 UTC [32] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"2024-04-30 20:40:27.180 UTC [32] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.183 UTC\",\"process_id\":\"32\",\"session_id\":\"6631573b.20\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.188 UTC\",\"process_id\":\"36\",\"session_id\":\"6631573b.24\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was shut down at 2024-04-30 20:40:27 UTC\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"override.conf\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"pgv2-dev-11\",\"filename\":\"override.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.290 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"46\",\"connection_from\":\"[local]\",\"session_id\":\"6631573b.2e\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"320fac48-6f15-4133-940f-8df5cc0f128d\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.292 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"47\",\"connection_from\":\"[local]\",\"session_id\":\"6631573b.2f\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"pgv2-dev-11\",\"walName\":\"00000012.history\",\"options\":[\"--cloud-provider\",\"aws-s3\",\"s3://x319-cnpg-barman-bucket/\",\"pgv2-dev\"],\"startTime\":\"2024-04-30T20:40:27Z\",\"endTime\":\"2024-04-30T20:40:27Z\",\"elapsedWalTime\":0.362220191}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.757 UTC\",\"process_id\":\"36\",\"session_id\":\"6631573b.24\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"entering standby mode\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.781 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"57\",\"connection_from\":\"[local]\",\"session_id\":\"6631573b.39\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.872 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"59\",\"connection_from\":\"[local]\",\"session_id\":\"6631573b.3b\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:27Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:27Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:27.927 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"60\",\"connection_from\":\"[local]\",\"session_id\":\"6631573b.3c\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:28Z\",\"msg\":\"Reconciling Cluster\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"2bdb660e-46b5-4c6a-a18e-bc17bdffd380\",\"caller\":\"internal/management/controller/instance_controller.go:111\",\"logging_pod\":\"pgv2-dev-11\",\"cluster\":{\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:28Z\",\"msg\":\"Reconciling custom monitoring queries\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"2bdb660e-46b5-4c6a-a18e-bc17bdffd380\",\"caller\":\"internal/management/controller/instance_controller.go:753\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:28.190 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"62\",\"connection_from\":\"[local]\",\"session_id\":\"6631573c.3e\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:28Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:28.212 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"63\",\"connection_from\":\"[local]\",\"session_id\":\"6631573c.3f\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:28.214 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"64\",\"connection_from\":\"[local]\",\"session_id\":\"6631573c.40\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:28Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"2bdb660e-46b5-4c6a-a18e-bc17bdffd380\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:28Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"pgv2-dev-11\",\"walName\":\"00000011.history\",\"startTime\":\"2024-04-30T20:40:27Z\",\"endTime\":\"2024-04-30T20:40:28Z\",\"elapsedWalTime\":0.3778327}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:28Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"pgv2-dev-11\",\"walName\":\"00000011.history\",\"maxParallel\":20,\"successfulWalRestore\":1,\"failedWalRestore\":19,\"endOfWALStream\":false,\"startTime\":\"2024-04-30T20:40:27Z\",\"downloadStartTime\":\"2024-04-30T20:40:27Z\",\"downloadTotalTime\":0.378065225,\"totalTime\":0.456196816}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:28.237 UTC\",\"process_id\":\"36\",\"session_id\":\"6631573b.24\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"00000011.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:28Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:28Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:29Z\",\"msg\":\"Reconciling Cluster\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"0dccee4c-a55c-4ce4-9eab-b69d2bf40ce9\",\"caller\":\"internal/management/controller/instance_controller.go:111\",\"logging_pod\":\"pgv2-dev-11\",\"cluster\":{\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:29Z\",\"msg\":\"Reconciling custom monitoring queries\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"0dccee4c-a55c-4ce4-9eab-b69d2bf40ce9\",\"caller\":\"internal/management/controller/instance_controller.go:753\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:29Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:29.512 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"111\",\"connection_from\":\"[local]\",\"session_id\":\"6631573d.6f\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:29Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:29Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:29.532 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"112\",\"connection_from\":\"[local]\",\"session_id\":\"6631573d.70\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:29Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:29.588 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"113\",\"connection_from\":\"[local]\",\"session_id\":\"6631573d.71\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:29Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"0dccee4c-a55c-4ce4-9eab-b69d2bf40ce9\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:30Z\",\"msg\":\"Reconciling Cluster\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"598035a4-2e04-44b5-83a7-7514d47b62f6\",\"caller\":\"internal/management/controller/instance_controller.go:111\",\"logging_pod\":\"pgv2-dev-11\",\"cluster\":{\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:30Z\",\"msg\":\"Reconciling custom monitoring queries\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"598035a4-2e04-44b5-83a7-7514d47b62f6\",\"caller\":\"internal/management/controller/instance_controller.go:753\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:30Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:30.708 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"115\",\"connection_from\":\"[local]\",\"session_id\":\"6631573e.73\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:30 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:30Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:30Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:30.740 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"116\",\"connection_from\":\"[local]\",\"session_id\":\"6631573e.74\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:30 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:30Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:30Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:30.809 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"117\",\"connection_from\":\"[local]\",\"session_id\":\"6631573e.75\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:30 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:30Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:31.014 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"118\",\"connection_from\":\"[local]\",\"session_id\":\"6631573f.76\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:31 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:31.032 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"119\",\"connection_from\":\"[local]\",\"session_id\":\"6631573f.77\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:31 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"598035a4-2e04-44b5-83a7-7514d47b62f6\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:31.124 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"120\",\"connection_from\":\"[local]\",\"session_id\":\"6631573f.78\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:31 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:31Z\",\"msg\":\"Instance status probe failing\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"pgv2-dev-11\",\"walName\":\"0000001100000682000000E4\",\"options\":[\"--cloud-provider\",\"aws-s3\",\"s3://x319-cnpg-barman-bucket/\",\"pgv2-dev\"],\"startTime\":\"2024-04-30T20:40:28Z\",\"endTime\":\"2024-04-30T20:40:31Z\",\"elapsedWalTime\":3.152753172}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:31.604 UTC\",\"process_id\":\"36\",\"session_id\":\"6631573b.24\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"consistent recovery state reached at 682/E40000A0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:31.604 UTC\",\"process_id\":\"36\",\"session_id\":\"6631573b.24\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"invalid record length at 682/E40000A0: expected at least 24, got 0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:31.604 UTC\",\"process_id\":\"32\",\"session_id\":\"6631573b.20\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-04-30 20:40:27 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is ready to accept read-only connections\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:31.614 UTC\",\"process_id\":\"121\",\"session_id\":\"6631573f.79\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-04-30 20:40:31 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"started streaming WAL from primary at 682/E4000000 on timeline 17\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgv2-dev-11\",\"record\":{\"log_time\":\"2024-04-30 20:40:31.614 UTC\",\"process_id\":\"121\",\"session_id\":\"6631573f.79\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-04-30 20:40:31 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"08P01\",\"message\":\"could not receive data from WAL stream: ERROR:  requested starting point 682/E4000000 is ahead of the WAL flush position of this server 682/E3000000\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:31Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:31Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:32Z\",\"msg\":\"Reconciling Cluster\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"00a69c30-b361-4731-81d6-d61d3b8fd1a0\",\"caller\":\"internal/management/controller/instance_controller.go:111\",\"logging_pod\":\"pgv2-dev-11\",\"cluster\":{\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\"}}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:32Z\",\"msg\":\"Reconciling custom monitoring queries\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgv2-dev\",\"namespace\":\"pgdb\"},\"namespace\":\"pgdb\",\"name\":\"pgv2-dev\",\"reconcileID\":\"00a69c30-b361-4731-81d6-d61d3b8fd1a0\",\"caller\":\"internal/management/controller/instance_controller.go:753\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-30T20:40:32Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"pgv2-dev-11\",\"walName\":\"00000012.history\",\"options\":[\"--cloud-provider\",\"aws-s3\",\"s3://x319-cnpg-barman-bucket/\",\"pgv2-dev\"],\"startTime\":\"2024-04-30T20:40:31Z\",\"endTime\":\"2024-04-30T20:40:32Z\",\"elapsedWalTime\":0.344877257}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:32Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n{\"level\":\"debug\",\"ts\":\"2024-04-30T20:40:32Z\",\"msg\":\"Cached object request received\",\"caller\":\"pkg/management/log/log.go:178\",\"logging_pod\":\"pgv2-dev-11\"}\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductWe are experiencing this same issue. @stevec-skyhawk did you manage to find a solution?\n---\n> We are experiencing this same issue. @stevec-skyhawk did you manage to find a solution?\r\nNo, we did not find a solution. We ended up having to create a new cluster through the recovery process.\n---\nWe are also seeing the same issue."
    },
    {
        "title": "[Bug]: Recovery from WAL fails when tablespace was added",
        "id": 2270632850,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n(I'll watch this issue)\r\n### Version\r\n1.23.0\r\n### What version of Kubernetes are you using?\r\n1.26\r\n### What is your Kubernetes environment?\r\nCloud: Other (provider using Gardener on OpenStack)\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\n1. Start with a database with backup setup (base + WAL) and make sure a base backup is present.\r\n2. Add a (temporary) tablespace by updating the resource, and make sure WALs are backed up. I also deleted the cluster afterwards.\r\n3. Create a new cluster by bootstrapping from recovery, from the backup.\r\n4. The recovery process has a fatal error on the redo of the `Tablespace/CREATE` record.\r\nI was able to recover to just before affected WAL record without data loss, but I can imagine that this could bring unexpected downtime during recovery in certain situations, hence this report.\r\nAt the very least, I would recommend to mention something in the [tablespaces limitations](https://cloudnative-pg.io/documentation/current/tablespaces/#limitations), e.g. make sure to create a base backup right after changing tablespaces (which I assume would work fine) and/or noting that changing tablespaces of an existing cluster can introduce recovery issues.\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: db2\r\n  labels:\r\n    cnpg.io/reload: 'true'\r\nspec:\r\n  instances: 1\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.2\r\n  storage:\r\n    size: 80Gi\r\n  # tried restoring both with and without this tablespace\r\n  # in both cases, restore failed because the tablespace was not setup\r\n  # ideally, I would be able to restore with a different temporary tablespace setup\r\n  #tablespaces:\r\n  #  - name: temporary\r\n  #    storage:\r\n  #      size: 40Gi\r\n  #    temporary: true\r\n  env:\r\n    - name: PSQL_HISTORY\r\n      value: /run/psql_history\r\n  bootstrap:\r\n    recovery:\r\n      source: clusterBackup\r\n      database: db\r\n      owner: db\r\n      secret:\r\n        name: db-secret\r\n  resources:\r\n    limits:\r\n      memory: 6000Mi\r\n    requests:\r\n      memory: 3000Mi\r\n  postgresql:\r\n    parameters:\r\n      checkpoint_completion_target: \"0.9\"\r\n      default_statistics_target: \"100\"\r\n      effective_cache_size: 4608MB\r\n      effective_io_concurrency: \"300\"\r\n      maintenance_work_mem: 384MB\r\n      max_connections: \"20\"\r\n      max_standby_streaming_delay: \"300000\"\r\n      max_wal_size: 4GB\r\n      min_wal_size: 1GB\r\n      pg_stat_statements.max: \"10000\"\r\n      pg_stat_statements.track: all\r\n      random_page_cost: \"1.1\"\r\n      shared_buffers: 1536MB\r\n      timezone: Europe/Amsterdam\r\n      wal_buffers: 16MB\r\n      work_mem: 19660kB\r\n  externalClusters:\r\n    - name: clusterBackup\r\n      barmanObjectStore:\r\n        destinationPath: \"s3://dbbackup/pg16\"\r\n        endpointURL: \"https://s3.example.com\"\r\n        serverName: db\r\n        s3Credentials:\r\n          accessKeyId:\r\n            name: db-s3-secret\r\n            key: key\r\n          secretAccessKey:\r\n            name: db-s3-secret\r\n            key: secret\r\n        wal:\r\n          maxParallel: 4\r\n```\r\n### Relevant log output\r\nThis is the log output for when the tablespace was not present in the cluster definition. When the tablespace was present, restore failed with the error _\"PGData already exists, can't overwrite\"_ (instead of here that the directory is absent).\r\n```shell\r\n...\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:43.892 UTC\",\"process_id\":\"37\",\"session_id\":\"662fe84e.25\",\"session_line_num\":\"18117\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000C00000277000000AE\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:44.368 UTC\",\"process_id\":\"37\",\"session_id\":\"662fe84e.25\",\"session_line_num\":\"18118\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000C00000277000000AF\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:44Z\",\"msg\":\"Checking if the server is still in recovery\",\"logging_pod\":\"db2-1-full-recovery\",\"recovery\":true}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:44.824 UTC\",\"process_id\":\"37\",\"session_id\":\"662fe84e.25\",\"session_line_num\":\"18119\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000C00000277000000B0\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:45.296 UTC\",\"process_id\":\"37\",\"session_id\":\"662fe84e.25\",\"session_line_num\":\"18120\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000C00000277000000B1\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:45.871 UTC\",\"process_id\":\"37\",\"session_id\":\"662fe84e.25\",\"session_line_num\":\"18121\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000C00000277000000B2\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:45.895 UTC\",\"process_id\":\"37\",\"session_id\":\"662fe84e.25\",\"session_line_num\":\"18122\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"58P01\",\"message\":\"directory \\\"/var/lib/postgresql/tablespaces/temporary/data\\\" does not exist\",\"hint\":\"Create this directory for the tablespace before restarting the server.\",\"context\":\"WAL redo at 277/AE0031F0 for Tablespace/CREATE: 3383535 \\\"/var/lib/postgresql/tablespaces/temporary/data\\\"\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:45.953 UTC\",\"process_id\":\"33\",\"session_id\":\"662fe84e.21\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 37) exited with exit code 1\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:45.953 UTC\",\"process_id\":\"33\",\"session_id\":\"662fe84e.21\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"terminating any other active server processes\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:46Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:46.141 UTC\",\"process_id\":\"33\",\"session_id\":\"662fe84e.21\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"shutting down due to startup process failure\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:46Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:46.310 UTC\",\"process_id\":\"33\",\"session_id\":\"662fe84e.21\",\"session_line_num\":\"9\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:49Z\",\"logger\":\"pg_ctl\",\"msg\":\"pg_ctl: no server running\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"db2-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:49Z\",\"msg\":\"Error while deactivating instance\",\"logging_pod\":\"db2-1-full-recovery\",\"err\":\"instance is not running\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:49Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"db2-1-full-recovery\"}\r\n{\"level\":\"error\",\"ts\":\"2024-04-29T21:28:49Z\",\"msg\":\"Error while restoring a backup\",\"logging_pod\":\"db2-1-full-recovery\",\"error\":\"while waiting for PostgreSQL to stop recovery mode: error while reading results of pg_is_in_recovery: failed to connect to `host=/controller/run user=postgres database=postgres`: dial error (dial unix /controller/run/.s.PGSQL.5432: connect: no such file or directory)\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/cmd.go:92\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.NewCmd.func2\\n\\tinternal/cmd/manager/instance/restore/cmd.go:63\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.22.2/x64/src/runtime/proc.go:271\"}\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n(I'll watch this issue)\r\n### Version\r\n1.23.0\r\n### What version of Kubernetes are you using?\r\n1.26\r\n### What is your Kubernetes environment?\r\nCloud: Other (provider using Gardener on OpenStack)\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\n1. Start with a database with backup setup (base + WAL) and make sure a base backup is present.\r\n2. Add a (temporary) tablespace by updating the resource, and make sure WALs are backed up. I also deleted the cluster afterwards.\r\n3. Create a new cluster by bootstrapping from recovery, from the backup.\r\n4. The recovery process has a fatal error on the redo of the `Tablespace/CREATE` record.\r\nI was able to recover to just before affected WAL record without data loss, but I can imagine that this could bring unexpected downtime during recovery in certain situations, hence this report.\r\nAt the very least, I would recommend to mention something in the [tablespaces limitations](https://cloudnative-pg.io/documentation/current/tablespaces/#limitations), e.g. make sure to create a base backup right after changing tablespaces (which I assume would work fine) and/or noting that changing tablespaces of an existing cluster can introduce recovery issues.\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: db2\r\n  labels:\r\n    cnpg.io/reload: 'true'\r\nspec:\r\n  instances: 1\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.2\r\n  storage:\r\n    size: 80Gi\r\n  # tried restoring both with and without this tablespace\r\n  # in both cases, restore failed because the tablespace was not setup\r\n  # ideally, I would be able to restore with a different temporary tablespace setup\r\n  #tablespaces:\r\n  #  - name: temporary\r\n  #    storage:\r\n  #      size: 40Gi\r\n  #    temporary: true\r\n  env:\r\n    - name: PSQL_HISTORY\r\n      value: /run/psql_history\r\n  bootstrap:\r\n    recovery:\r\n      source: clusterBackup\r\n      database: db\r\n      owner: db\r\n      secret:\r\n        name: db-secret\r\n  resources:\r\n    limits:\r\n      memory: 6000Mi\r\n    requests:\r\n      memory: 3000Mi\r\n  postgresql:\r\n    parameters:\r\n      checkpoint_completion_target: \"0.9\"\r\n      default_statistics_target: \"100\"\r\n      effective_cache_size: 4608MB\r\n      effective_io_concurrency: \"300\"\r\n      maintenance_work_mem: 384MB\r\n      max_connections: \"20\"\r\n      max_standby_streaming_delay: \"300000\"\r\n      max_wal_size: 4GB\r\n      min_wal_size: 1GB\r\n      pg_stat_statements.max: \"10000\"\r\n      pg_stat_statements.track: all\r\n      random_page_cost: \"1.1\"\r\n      shared_buffers: 1536MB\r\n      timezone: Europe/Amsterdam\r\n      wal_buffers: 16MB\r\n      work_mem: 19660kB\r\n  externalClusters:\r\n    - name: clusterBackup\r\n      barmanObjectStore:\r\n        destinationPath: \"s3://dbbackup/pg16\"\r\n        endpointURL: \"https://s3.example.com\"\r\n        serverName: db\r\n        s3Credentials:\r\n          accessKeyId:\r\n            name: db-s3-secret\r\n            key: key\r\n          secretAccessKey:\r\n            name: db-s3-secret\r\n            key: secret\r\n        wal:\r\n          maxParallel: 4\r\n```\r\n### Relevant log output\r\nThis is the log output for when the tablespace was not present in the cluster definition. When the tablespace was present, restore failed with the error _\"PGData already exists, can't overwrite\"_ (instead of here that the directory is absent).\r\n```shell\r\n...\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:43.892 UTC\",\"process_id\":\"37\",\"session_id\":\"662fe84e.25\",\"session_line_num\":\"18117\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000C00000277000000AE\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:44.368 UTC\",\"process_id\":\"37\",\"session_id\":\"662fe84e.25\",\"session_line_num\":\"18118\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000C00000277000000AF\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:44Z\",\"msg\":\"Checking if the server is still in recovery\",\"logging_pod\":\"db2-1-full-recovery\",\"recovery\":true}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:44.824 UTC\",\"process_id\":\"37\",\"session_id\":\"662fe84e.25\",\"session_line_num\":\"18119\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000C00000277000000B0\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:45.296 UTC\",\"process_id\":\"37\",\"session_id\":\"662fe84e.25\",\"session_line_num\":\"18120\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000C00000277000000B1\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:45.871 UTC\",\"process_id\":\"37\",\"session_id\":\"662fe84e.25\",\"session_line_num\":\"18121\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000C00000277000000B2\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:45.895 UTC\",\"process_id\":\"37\",\"session_id\":\"662fe84e.25\",\"session_line_num\":\"18122\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"58P01\",\"message\":\"directory \\\"/var/lib/postgresql/tablespaces/temporary/data\\\" does not exist\",\"hint\":\"Create this directory for the tablespace before restarting the server.\",\"context\":\"WAL redo at 277/AE0031F0 for Tablespace/CREATE: 3383535 \\\"/var/lib/postgresql/tablespaces/temporary/data\\\"\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:45.953 UTC\",\"process_id\":\"33\",\"session_id\":\"662fe84e.21\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 37) exited with exit code 1\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:45.953 UTC\",\"process_id\":\"33\",\"session_id\":\"662fe84e.21\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"terminating any other active server processes\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:46Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:46.141 UTC\",\"process_id\":\"33\",\"session_id\":\"662fe84e.21\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"shutting down due to startup process failure\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:46Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"db2-1-full-recovery\",\"record\":{\"log_time\":\"2024-04-29 21:28:46.310 UTC\",\"process_id\":\"33\",\"session_id\":\"662fe84e.21\",\"session_line_num\":\"9\",\"session_start_time\":\"2024-04-29 18:34:54 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:49Z\",\"logger\":\"pg_ctl\",\"msg\":\"pg_ctl: no server running\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"db2-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:49Z\",\"msg\":\"Error while deactivating instance\",\"logging_pod\":\"db2-1-full-recovery\",\"err\":\"instance is not running\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-29T21:28:49Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"db2-1-full-recovery\"}\r\n{\"level\":\"error\",\"ts\":\"2024-04-29T21:28:49Z\",\"msg\":\"Error while restoring a backup\",\"logging_pod\":\"db2-1-full-recovery\",\"error\":\"while waiting for PostgreSQL to stop recovery mode: error while reading results of pg_is_in_recovery: failed to connect to `host=/controller/run user=postgres database=postgres`: dial error (dial unix /controller/run/.s.PGSQL.5432: connect: no such file or directory)\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:163\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/cmd.go:92\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.NewCmd.func2\\n\\tinternal/cmd/manager/instance/restore/cmd.go:63\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.22.2/x64/src/runtime/proc.go:271\"}\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductHi @wvengen,\r\nYou are right. This is something that needs to be documented, at the very least. IMO, if you set the 'future' state (i.e. with), we should be able to support it."
    },
    {
        "title": "feat: Adding container lifecycle fields to be able to set preStop and postStart hooks",
        "id": 2270574772,
        "state": "open",
        "first": "Closes https://github.com/cloudnative-pg/cloudnative-pg/issues/4405",
        "messages": "Closes https://github.com/cloudnative-pg/cloudnative-pg/issues/4405"
    },
    {
        "title": "feat: allow injecting schema into lifecycle hook",
        "id": 2269059242,
        "state": "open",
        "first": "An attempt at fixing the root cause of #4286, mostly a POC, I feel it could be improved.",
        "messages": "An attempt at fixing the root cause of #4286, mostly a POC, I feel it could be improved."
    },
    {
        "title": "Can cloudnative-pg support longhorn to make data sync instead of pg sync data itself in pg cluster, to support infinity disk capacity",
        "id": 2264828825,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nsuggest data sync use pg ability\uff0cbut data size can not big than host disk\n### Describe what additions need to be done to the documentation\n_No response_\n### Describe what pages need to change in the documentation, if any\n_No response_\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nsuggest data sync use pg ability\uff0cbut data size can not big than host disk\n### Describe what additions need to be done to the documentation\n_No response_\n### Describe what pages need to change in the documentation, if any\n_No response_\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Give the possibility to automatically clean pgbench jobs - ttlSecondsAfterFinished option",
        "id": 2263023591,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\npgbench jobs are not cleaned by default. It would be great to have to possibility to clean them automatically after a certain time.\r\n### Describe the solution you'd like\r\nThe **ttlSecondsAfterFinished** option of a Kubernetes Job could be implemented. See [https://kubernetes.io/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically]( https://kubernetes.io/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically)\r\nIt could be configured in the YAML file, in a job or pgbench / section . For example for one hour TTL : \r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: perf-cluster\r\nspec:\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.2\r\n  instances: 1\r\n  pgbench:\r\n    ttlSecondsAfterFinished: '3600'\r\n...\r\n```\r\nBy default, we can keep it disabled.\r\n### Describe alternatives you've considered\r\nIt could be done as a parameter given at the pgbench plugin.\r\n``\r\nkubectl cnpg pgbench --ttl 300 .....\r\n``\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nNo\r\n### Are you willing to actively contribute to this feature?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\npgbench jobs are not cleaned by default. It would be great to have to possibility to clean them automatically after a certain time.\r\n### Describe the solution you'd like\r\nThe **ttlSecondsAfterFinished** option of a Kubernetes Job could be implemented. See [https://kubernetes.io/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically]( https://kubernetes.io/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically)\r\nIt could be configured in the YAML file, in a job or pgbench / section . For example for one hour TTL : \r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: perf-cluster\r\nspec:\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.2\r\n  instances: 1\r\n  pgbench:\r\n    ttlSecondsAfterFinished: '3600'\r\n...\r\n```\r\nBy default, we can keep it disabled.\r\n### Describe alternatives you've considered\r\nIt could be done as a parameter given at the pgbench plugin.\r\n``\r\nkubectl cnpg pgbench --ttl 300 .....\r\n``\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nNo\r\n### Are you willing to actively contribute to this feature?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "CRDs to access CNPG from vCluster [Feature]: ",
        "id": 2262894239,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently, Postgres inside a vCluster is not able to talk to CNPG of the parent cluster and we get below error on deploying Postgres in such scenario:\r\n```\r\nresource mapping not found for name: \"pgcluster\" namespace: \"pgcluster\" from \"pgcluster.yaml\": no matches for kind \"Cluster\" in version \"postgresql.cnpg.io/v1\"\r\nensure CRDs are installed first\r\n``` \r\nTherefore, this new feature will allow a Postgres in a vCluster to access CNPG on the parent cluster. Adding this feature will allow end-users to deploy only Postgres in the vCluster without having to deploy CNPG. It also saves extra workload for the whole cluster (parent and vCluster).\n### Describe the solution you'd like\nI want CNPG be able to monitor and manage Postgres clusters inside vCluster(s) as it watches Postgres' in each namespace. \r\n### Describe alternatives you've considered\nIf this feature is not provided by CNPG, then sysadmins who wants to have above mentioned facility need to create their own CRDs which would be more difficult to maintain than creating - especially with each upgrade etc. \n### Additional context\nIn addition to this section above: \"_I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated_\", I have also posted this question on slacks, but haven't got any helpful answer.\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently, Postgres inside a vCluster is not able to talk to CNPG of the parent cluster and we get below error on deploying Postgres in such scenario:\r\n```\r\nresource mapping not found for name: \"pgcluster\" namespace: \"pgcluster\" from \"pgcluster.yaml\": no matches for kind \"Cluster\" in version \"postgresql.cnpg.io/v1\"\r\nensure CRDs are installed first\r\n``` \r\nTherefore, this new feature will allow a Postgres in a vCluster to access CNPG on the parent cluster. Adding this feature will allow end-users to deploy only Postgres in the vCluster without having to deploy CNPG. It also saves extra workload for the whole cluster (parent and vCluster).\n### Describe the solution you'd like\nI want CNPG be able to monitor and manage Postgres clusters inside vCluster(s) as it watches Postgres' in each namespace. \r\n### Describe alternatives you've considered\nIf this feature is not provided by CNPG, then sysadmins who wants to have above mentioned facility need to create their own CRDs which would be more difficult to maintain than creating - especially with each upgrade etc. \n### Additional context\nIn addition to this section above: \"_I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated_\", I have also posted this question on slacks, but haven't got any helpful answer.\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductHi @retrogaming457 \r\nIsn't part of the whole idea here that the vCluster is completed isolated from the parent one?Therefore, there's any documentation about vCluster allowing this? because I wasn't able to find one.\r\nRegards,"
    },
    {
        "title": "[Feature]: Disperse Primary Pods equally across Kubernetes Compute Nodes ",
        "id": 2262294479,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWhile running multiple CNPG clusters on a given kubernetes cluster, the operator tends to lump the primary CNPG pods on a given kubernetes node.   This can impact overall kubernetes cluster performance as all the PG read/write traffic is directed at a specific compute node. \r\nThis behavior also occurs when you perform kubernetes node maintenance and life cycle out older nodes with newer ones, the primary's tend to end up on the same node. \r\nThis can be problematic when you run dedicated kubernetes clusters with dozens of CNPG clusters.  \n### Describe the solution you'd like\nAs a CNPG administrator, I would like the CNPG operator to place the primary CNPG pods across the kubernetes compute nodes in a round-robin fashion.  \r\nAlternatively, \r\nAs a CNPG administrator, I would like to be able to issue a single kubectl CNPG operator  command to request that the CNPG operator to disperse the primary CNPG pods across the kubernetes compute nodes in a round-robin fashion.  For example: `kubectl cnpg disperse primary --all-namespaces`\n### Describe alternatives you've considered\nManually promoting primary's using the `kubectl cnpg promote` command.  This works fine if you have 4-5 clusters, but becomes difficult if you have 50+. \n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWhile running multiple CNPG clusters on a given kubernetes cluster, the operator tends to lump the primary CNPG pods on a given kubernetes node.   This can impact overall kubernetes cluster performance as all the PG read/write traffic is directed at a specific compute node. \r\nThis behavior also occurs when you perform kubernetes node maintenance and life cycle out older nodes with newer ones, the primary's tend to end up on the same node. \r\nThis can be problematic when you run dedicated kubernetes clusters with dozens of CNPG clusters.  \n### Describe the solution you'd like\nAs a CNPG administrator, I would like the CNPG operator to place the primary CNPG pods across the kubernetes compute nodes in a round-robin fashion.  \r\nAlternatively, \r\nAs a CNPG administrator, I would like to be able to issue a single kubectl CNPG operator  command to request that the CNPG operator to disperse the primary CNPG pods across the kubernetes compute nodes in a round-robin fashion.  For example: `kubectl cnpg disperse primary --all-namespaces`\n### Describe alternatives you've considered\nManually promoting primary's using the `kubectl cnpg promote` command.  This works fine if you have 4-5 clusters, but becomes difficult if you have 50+. \n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductTo get somewhat closer to this, you can set topologyspreadconstraints to prefer nodes that do not run a CNPG pod yet."
    },
    {
        "title": "[Feature]: Options for more stable PVC/pod naming",
        "id": 2258618133,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nPod and, more importantly, PVC naming can vary a lot:\r\nAs PVC-1 and PVC-2, can start becoming PVC-2 and PVC-3 after a pod is destroyed.\r\n(for example in cases where PVC-1 was on a now-unavailable node)\r\nWhich means we never can rely on being able to connect to any PVC or pod directly with tooling.\r\n### Describe the solution you'd like\nGive users an option not-to reschedule on a new node or as a new pod-name\r\n---\r\nThis might mean that pods become uncreatable however, as pod-1 might be bound to storage on an unavailable node. Hence such a thing should be optional.\n### Describe alternatives you've considered\nnot much really\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nPod and, more importantly, PVC naming can vary a lot:\r\nAs PVC-1 and PVC-2, can start becoming PVC-2 and PVC-3 after a pod is destroyed.\r\n(for example in cases where PVC-1 was on a now-unavailable node)\r\nWhich means we never can rely on being able to connect to any PVC or pod directly with tooling.\r\n### Describe the solution you'd like\nGive users an option not-to reschedule on a new node or as a new pod-name\r\n---\r\nThis might mean that pods become uncreatable however, as pod-1 might be bound to storage on an unavailable node. Hence such a thing should be optional.\n### Describe alternatives you've considered\nnot much really\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: Bootstrap from PersistentVolumeClaim not accepted by validateVolumeSnapshotSource",
        "id": 2245278082,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nrobin.goots@dynapps.be\n### Version\n1.22.2\n### What version of Kubernetes are you using?\n1.27\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nTrying to create a new cluster bootstrapped from an existing PVC fails validation. This should work according to the doc string of the validation function.\r\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/3e8fea06a0da792b12e6a09b585d7cf5a8c5dcfb/api/v1/cluster_webhook.go#L971\r\nHowever, we get the response that only `VolumeSnapshots` are supported.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: restore-db\r\n  namespace: restore\r\nspec:\r\n  bootstrap:\r\n    recovery:\r\n      volumeSnapshots:\r\n        storage:\r\n          name: restore-db-1\r\n          kind: PersistentVolumeClaim\r\n          apiGroup: \"\"\r\n  imageName: 'ghcr.io/cloudnative-pg/postgresql:16.1'\r\n  instances: 1\r\n  primaryUpdateStrategy: unsupervised\r\n  resources:\r\n    limits:\r\n      cpu: '1'\r\n      memory: 2Gi\r\n    requests:\r\n      cpu: 100m\r\n      memory: 1Gi\r\n  storage:\r\n    size: 5Gi\n```\n### Relevant log output\n```shell\nThe Cluster \"restore-db\" is invalid: spec.bootstrap.recovery.storage: Invalid value: v1.TypedLocalObjectReference{APIGroup:(*string)(0xc004117380), Kind:\"PersistentVolumeClaim\", Name:\"restore-db-1\"}: Only VolumeSnapshots are supported\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nrobin.goots@dynapps.be\n### Version\n1.22.2\n### What version of Kubernetes are you using?\n1.27\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nTrying to create a new cluster bootstrapped from an existing PVC fails validation. This should work according to the doc string of the validation function.\r\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/3e8fea06a0da792b12e6a09b585d7cf5a8c5dcfb/api/v1/cluster_webhook.go#L971\r\nHowever, we get the response that only `VolumeSnapshots` are supported.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: restore-db\r\n  namespace: restore\r\nspec:\r\n  bootstrap:\r\n    recovery:\r\n      volumeSnapshots:\r\n        storage:\r\n          name: restore-db-1\r\n          kind: PersistentVolumeClaim\r\n          apiGroup: \"\"\r\n  imageName: 'ghcr.io/cloudnative-pg/postgresql:16.1'\r\n  instances: 1\r\n  primaryUpdateStrategy: unsupervised\r\n  resources:\r\n    limits:\r\n      cpu: '1'\r\n      memory: 2Gi\r\n    requests:\r\n      cpu: 100m\r\n      memory: 1Gi\r\n  storage:\r\n    size: 5Gi\n```\n### Relevant log output\n```shell\nThe Cluster \"restore-db\" is invalid: spec.bootstrap.recovery.storage: Invalid value: v1.TypedLocalObjectReference{APIGroup:(*string)(0xc004117380), Kind:\"PersistentVolumeClaim\", Name:\"restore-db-1\"}: Only VolumeSnapshots are supported\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Chore]: avoid logging the complete Pod definition when triggering a failover/switchover",
        "id": 2243921014,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nleonardo.cecchi@gmail.com\n### Version\n1.22.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nCNPG logs the Pod definition of every instance when triggering a switchover or a failover. Perhaps having just the replication status is enough, and we can reserve the full Pod logging to a tracing level.\r\nThis would help to enhance the readability of our logs.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nleonardo.cecchi@gmail.com\n### Version\n1.22.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nCNPG logs the Pod definition of every instance when triggering a switchover or a failover. Perhaps having just the replication status is enough, and we can reserve the full Pod logging to a tracing level.\r\nThis would help to enhance the readability of our logs.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "feat: Improve PodMonitor spec",
        "id": 2237366002,
        "state": "open",
        "first": "# Summary\r\nThis PR introduces new PodMonitor specs to customise even more the monitor capabilities, I propose a new spec that is intended to allow more and more integration between Prometheus CRD and CNPG monitoring support. \r\nThis in done **deprecating** the old Monitoring field - relabing configs - in favour of a new `PodMonitoringConfiguration` dictionary that implement the same fields plus more. This allow us to reduce the complexity of reconciling the PodMonitor with a unified interface+struct and to allow more and more customisation in the upcoming future.\r\n# Bug Fixes\r\nThis pr fix a [bug](https://github.com/cloudnative-pg/cloudnative-pg/blob/6e423eb50b71ced1fe78aaecdb1cf0411aefdac5/internal/cnpi/plugin/client/lifecycle.go#L61) introduced by the Plugin API which caused the reconcile failure for object not recognized by the operator, such as PodMonitor. The lifecycle hook now stop if the resource is not known but returns the original object so the reconcile can continue.\r\n# Description\r\nThis pr introduce new specs for PodMonitor functionality with the plan to implement 1:1 functionality as PodMonitor spec by also guard-railing the creation. This has been done by completely rewrite the PodMonitor logic implementing a new kind of Controller that will implement all the current and future logic. We choose to **not** use the prometheus CRD spec to prevent accidental issue with some fields calculated by the Cluster spec - such as PodMonitor endpoint port name. \r\nThis pr also propose the deprecation of the original PodMonitor field in favour of a more standardized sub-spec which can be improved more easily in the future; finally e2e test for PodMonitor are implemented to intercept eventual regression more easily.\r\nSome other quirks and fixes have been implemented:\r\n- Add commodities for testing\r\n- Implement Prometheus CRD installation for e2e testing\r\n# Testing Done\r\n- Implemented monitoring_test.go\r\n- Implemented e2e tests",
        "messages": "# Summary\r\nThis PR introduces new PodMonitor specs to customise even more the monitor capabilities, I propose a new spec that is intended to allow more and more integration between Prometheus CRD and CNPG monitoring support. \r\nThis in done **deprecating** the old Monitoring field - relabing configs - in favour of a new `PodMonitoringConfiguration` dictionary that implement the same fields plus more. This allow us to reduce the complexity of reconciling the PodMonitor with a unified interface+struct and to allow more and more customisation in the upcoming future.\r\n# Bug Fixes\r\nThis pr fix a [bug](https://github.com/cloudnative-pg/cloudnative-pg/blob/6e423eb50b71ced1fe78aaecdb1cf0411aefdac5/internal/cnpi/plugin/client/lifecycle.go#L61) introduced by the Plugin API which caused the reconcile failure for object not recognized by the operator, such as PodMonitor. The lifecycle hook now stop if the resource is not known but returns the original object so the reconcile can continue.\r\n# Description\r\nThis pr introduce new specs for PodMonitor functionality with the plan to implement 1:1 functionality as PodMonitor spec by also guard-railing the creation. This has been done by completely rewrite the PodMonitor logic implementing a new kind of Controller that will implement all the current and future logic. We choose to **not** use the prometheus CRD spec to prevent accidental issue with some fields calculated by the Cluster spec - such as PodMonitor endpoint port name. \r\nThis pr also propose the deprecation of the original PodMonitor field in favour of a more standardized sub-spec which can be improved more easily in the future; finally e2e test for PodMonitor are implemented to intercept eventual regression more easily.\r\nSome other quirks and fixes have been implemented:\r\n- Add commodities for testing\r\n- Implement Prometheus CRD installation for e2e testing\r\n# Testing Done\r\n- Implemented monitoring_test.go\r\n- Implemented e2e testsI think the approach so far has been to avoid providing too many knobs for podmonitors, you can always define your own separately if you need more control, you just need to match the labels on the pods, which you can control from the Cluster.\n---\nIt would be nice that managing the PodMonitor would be done by the operator; if I recall correctly a discussion about 1:1 functionality is present somewhere in this repo. Can you describe why you would prefer letting the user manage the PodMonitor?\n---\nBecause we have to draw the line somewhere \ud83d\ude05 and this operator's job is not to handle Prometheus operator's resources, we do that for simple scenarios because it might be convenient for less experienced users, but we should not try to offer full customization. Otherwise we risk having to keep up to date with all the features they might introduce there, just to let users with more complex requirements deploy a single resource instead of two.\n---\n> Because we have to draw the line somewhere \ud83d\ude05 and this operator's job is not to handle Prometheus operator's resources, we do that for simple scenarios because it might be convenient for less experienced users, but we should not try to offer full customization. Otherwise we risk having to keep up to date with all the features they might introduce there, just to let users with more complex requirements deploy a single resource instead of two.\r\nI agree that a line should be drawn and the pod monitor should not be parameterized too much, but I think that in a simple scenario and for a novice user, it is useful to be able to change the frequency of data scraping without having to re-implement it from scratch. I also do not see any big changes that require more effort to keep them up to date.\r\nIn addition, tests on the pod monitor have also been added with this PR, which identified a bug on the operator caused by the API Plugin implementation.\n---\nEither way, could you please split the fix to its own PR?\n---\nHi. Will split the PR ASAP, I will also refer this PR in the other so we can continue the discussion. \r\nThanks"
    },
    {
        "title": "[Bug]: Error while checking barman-cloud-wal-archive version restoring backup from GCS in GKE cluster",
        "id": 2233259731,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nmmoncadaisla@gmail.com\r\n### Version\r\n1.22.2\r\n### What version of Kubernetes are you using?\r\n1.27\r\n### What is your Kubernetes environment?\r\nCloud: Google GKE\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\n### Context \r\n- [Slack thread](https://cloudnativepg.slack.com/archives/C03AX0J5P29/p1710934244917939)\r\n- [Possibly related PR](https://github.com/cloudnative-pg/cloudnative-pg/pull/3931)\r\nI'm hitting an error when attempting to restore from a backup in GCS for a cluster in a different namespace than the production cluster (same GKE cluster) \"while checking barman-cloud-wal-archive version\". Like if the `barman-cloud-wal-archive` CLI was not present.\r\nCluster status:\r\n```\r\nk cnpg status db-restore-debug -n cambium-recovery-a5\r\nCluster Summary\r\nPrimary server is initializing\r\nName:              db-restore-debug\r\nNamespace:         cambium-recovery-a5\r\nPostgreSQL Image:  cambiumearth/base-image-cnpg:14-3.4-v0.3\r\nPrimary instance:   (switching to db-restore-debug-1)\r\nStatus:            Setting up primary Creating primary instance db-restore-debug-1\r\nInstances:         3\r\nReady instances:   0\r\nCertificates Status\r\nCertificate Name              Expiration Date                Days Left Until Expiration\r\n----------------              ---------------                --------------------------\r\ndb-restore-debug-ca           2024-07-08 11:37:16 +0000 UTC  90.00\r\ndb-restore-debug-replication  2024-07-08 11:37:16 +0000 UTC  90.00\r\ndb-restore-debug-server       2024-07-08 11:37:16 +0000 UTC  90.00\r\nContinuous Backup status\r\nFirst Point of Recoverability:  Not Available\r\nNo Primary instance found\r\nStreaming Replication status\r\nPrimary instance not found\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nInstances status\r\nName  Database Size  Current LSN  Replication role  Status  QoS  Manager Version  Node\r\n----  -------------  -----------  ----------------  ------  ---  ---------------  ----\r\n```\r\nThe recovery job description is:\r\n```\r\nk describe job -n cambium-recovery-a5 db-restore-debug-1-full-recovery\r\nName:             db-restore-debug-1-full-recovery\r\nNamespace:        cambium-recovery-a5\r\nSelector:         batch.kubernetes.io/controller-uid=50cd68f3-7053-4b56-b5ad-fecdddd76a21\r\nLabels:           cnpg.io/cluster=db-restore-debug\r\n                  cnpg.io/instanceName=db-restore-debug-1\r\nAnnotations:      batch.kubernetes.io/job-tracking:\r\n                  cnpg.io/operatorVersion: 1.22.2\r\nControlled By:    Cluster/db-restore-debug\r\nParallelism:      1\r\nCompletions:      1\r\nCompletion Mode:  NonIndexed\r\nStart Time:       Tue, 09 Apr 2024 13:42:17 +0200\r\nPods Statuses:    0 Active (0 Ready) / 0 Succeeded / 5 Failed\r\nPod Template:\r\n  Labels:           batch.kubernetes.io/controller-uid=50cd68f3-7053-4b56-b5ad-fecdddd76a21\r\n                    batch.kubernetes.io/job-name=db-restore-debug-1-full-recovery\r\n                    cnpg.io/cluster=db-restore-debug\r\n                    cnpg.io/instanceName=db-restore-debug-1\r\n                    cnpg.io/jobRole=full-recovery\r\n                    controller-uid=50cd68f3-7053-4b56-b5ad-fecdddd76a21\r\n                    job-name=db-restore-debug-1-full-recovery\r\n  Service Account:  db-restore-debug\r\n  Init Containers:\r\n   bootstrap-controller:\r\n    Image:           ghcr.io/cloudnative-pg/cloudnative-pg:1.22.2\r\n    Port:            <none>\r\n    Host Port:       <none>\r\n    SeccompProfile:  RuntimeDefault\r\n    Command:\r\n      /manager\r\n      bootstrap\r\n      /controller/manager\r\n      --log-level=info\r\n    Environment:  <none>\r\n    Mounts:\r\n      /controller from scratch-data (rw)\r\n      /dev/shm from shm (rw)\r\n      /etc/app-secret from app-secret (rw)\r\n      /run from scratch-data (rw)\r\n      /var/lib/postgresql/data from pgdata (rw)\r\n  Containers:\r\n   full-recovery:\r\n    Image:           cambiumearth/experimental-image-cnpg:14-3.4-v0.5\r\n    Port:            <none>\r\n    Host Port:       <none>\r\n    SeccompProfile:  RuntimeDefault\r\n    Command:\r\n      /controller/manager\r\n      instance\r\n      restore\r\n      --log-level=info\r\n    Environment Variables from:\r\n      cambium-restore-config  ConfigMap  Optional: false\r\n    Environment:\r\n      PGDATA:        /var/lib/postgresql/data/pgdata\r\n      POD_NAME:      db-restore-debug-1-full-recovery\r\n      NAMESPACE:     cambium-recovery-a5\r\n      CLUSTER_NAME:  db-restore-debug\r\n      PGPORT:        5432\r\n      PGHOST:        /controller/run\r\n    Mounts:\r\n      /controller from scratch-data (rw)\r\n      /dev/shm from shm (rw)\r\n      /etc/app-secret from app-secret (rw)\r\n      /run from scratch-data (rw)\r\n      /var/lib/postgresql/data from pgdata (rw)\r\n  Volumes:\r\n   pgdata:\r\n    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\r\n    ClaimName:  db-restore-debug-1\r\n    ReadOnly:   false\r\n   scratch-data:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:\r\n    SizeLimit:  <unset>\r\n   shm:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:     Memory\r\n    SizeLimit:  <unset>\r\n   app-secret:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  db-restore-debug-app\r\n    Optional:    false\r\nEvents:\r\n  Type    Reason            Age    From            Message\r\n  ----    ------            ----   ----            -------\r\n  Normal  SuccessfulCreate  5m1s   job-controller  Created pod: db-restore-debug-1-full-recovery-6zgf6\r\n  Normal  SuccessfulCreate  4m28s  job-controller  Created pod: db-restore-debug-1-full-recovery-h8v25\r\n  Normal  SuccessfulCreate  3m49s  job-controller  Created pod: db-restore-debug-1-full-recovery-6rptj\r\n  Normal  SuccessfulCreate  3m     job-controller  Created pod: db-restore-debug-1-full-recovery-5mz4w\r\n  Normal  SuccessfulCreate  87s    job-controller  Created pod: db-restore-debug-1-full-recovery-p8k9f\r\n```\r\nAnd the job associated attempts pods result in error, raising the mentioned error message.\r\n```\r\nk get pods -n cambium-recovery-a5\r\nNAME                                     READY   STATUS   RESTARTS   AGE\r\ndb-restore-debug-1-full-recovery-5mz4w   0/1     Error    0          2m1s\r\ndb-restore-debug-1-full-recovery-6rptj   0/1     Error    0          2m50s\r\ndb-restore-debug-1-full-recovery-6zgf6   0/1     Error    0          4m2s\r\ndb-restore-debug-1-full-recovery-h8v25   0/1     Error    0          3m29s\r\ndb-restore-debug-1-full-recovery-p8k9f   0/1     Error    0          28s\r\n```\r\nThe operator logs do not show much more info apparently only showing the job creation:\r\n``` \r\n{\"level\":\"info\",\"ts\":\"2024-04-09T09:45:29Z\",\"msg\":\"Creating new Job\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"db-restore-debug\",\"namespace\":\"cambium-recovery-a5\"},\"namespace\":\"cambium-recovery-a5\",\"name\":\"db-restore-debug\",\"reconcileID\":\"9200cae6-97d5-4835-a90c-5d86f6b514f5\",\"uuid\":\"e66dc461-f655-11ee-aa41-5abbf5b78419\",\"name\":\"db-restore-debug-1-full-recovery\",\"primary\":true}\r\n```\r\nThe same source image is running in my production cluster, this is the corresponding Dockerfile, the image is publicly available [in Docker Hub](https://hub.docker.com/layers/cambiumearth/base-image-cnpg/14-3.4-v0.3/images/sha256-2f7fc18239114ac4e9be08120149eec56fa4aace3615ae3e468cd27c711f6796?context=explore):\r\nI've verified that `barman-cloud-check-wal-archive --version` command runs in the production cluster without issue, returning:\r\n```bash\r\npostgres@cambium-db-5:/$ barman-cloud-check-wal-archive --version\r\nbarman-cloud-check-wal-archive 3.4.0\r\n```\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: db-restore-debug\r\nspec:\r\n  instances: 3\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: \"gs://cnpg-recovery-new-backups\"\r\n      googleCredentials:\r\n        applicationCredentials:\r\n          name: backup-creds\r\n          key: gcsCredentials\r\n    retentionPolicy: \"1d\"\r\n  externalClusters:\r\n    - name: cambium-db\r\n      barmanObjectStore:\r\n        destinationPath: \"gs://cnpg-recovery-debug\"\r\n        googleCredentials:\r\n          applicationCredentials:\r\n            name: backup-creds\r\n            key: gcsCredentials\r\n  postgresql:\r\n    shared_preload_libraries:\r\n      - timescaledb\r\n    parameters:\r\n      track_commit_timestamp: \"on\"\r\n      shared_buffers: \"2GB\"\r\n      max_wal_size: \"14GB\"\r\n      wal_compression: \"on\"\r\n      effective_cache_size: \"6GB\"\r\n      maintenance_work_mem: \"1GB\"\r\n      work_mem: \"13107kB\"\r\n      huge_pages: \"off\"\r\n      max_worker_processes: \"4\"\r\n      max_parallel_workers_per_gather: \"2\"\r\n      max_parallel_workers: \"4\"\r\n      wal_buffers: \"16MB\"\r\n      min_wal_size: \"4GB\"\r\n      default_statistics_target: \"500\"\r\n      random_page_cost: \"1.1\"\r\n      checkpoint_completion_target: \"0.9\"\r\n      max_locks_per_transaction: \"256\"\r\n      autovacuum_max_workers: \"10\"\r\n      autovacuum_naptime: \"10\"\r\n      effective_io_concurrency: \"200\"\r\n      max_files_per_process: \"65536\"\r\n  imageName: cambiumearth/base-image-cnpg:14-3.4-v0.3\r\n  bootstrap:\r\n    recovery:\r\n      source: cambium-db\r\n  storage:\r\n    storageClass: standard\r\n    size: 60Gi\r\n  # resources:\r\n  #   requests:\r\n  #     cpu: \"200m\"\r\n  #     memory: \"300Mi\"\r\n  #   limits:\r\n  #     cpu: \"2\"\r\n  #     memory: \"5Gi\"\r\n  envFrom:\r\n    - configMapRef:\r\n        name: cambium-restore-config\r\n```\r\n### Relevant log output\r\n```shell\r\n{\"level\":\"error\",\"ts\":\"2024-04-09T11:42:40Z\",\"msg\":\"while getting barman-cloud-wal-archive options\",\"logging_pod\":\"db-restore-debug-1-full-recovery\",\"error\":\"while checking barman-cloud-wal-archive version: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*InitInfo).checkBackupDestination\\n\\tpkg/management/postgres/restore.go:862\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.InitInfo.Restore\\n\\tpkg/management/postgres/restore.go:236\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/cmd.go:87\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.NewCmd.func2\\n\\tinternal/cmd/manager/instance/restore/cmd.go:60\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.8/x64/src/runtime/proc.go:267\"}\r\n{\"level\":\"error\",\"ts\":\"2024-04-09T11:42:40Z\",\"msg\":\"Error while restoring a backup\",\"logging_pod\":\"db-restore-debug-1-full-recovery\",\"error\":\"while checking barman-cloud-wal-archive version: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/cmd.go:89\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.NewCmd.func2\\n\\tinternal/cmd/manager/instance/restore/cmd.go:60\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.8/x64/src/runtime/proc.go:267\"}\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nmmoncadaisla@gmail.com\r\n### Version\r\n1.22.2\r\n### What version of Kubernetes are you using?\r\n1.27\r\n### What is your Kubernetes environment?\r\nCloud: Google GKE\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\n### Context \r\n- [Slack thread](https://cloudnativepg.slack.com/archives/C03AX0J5P29/p1710934244917939)\r\n- [Possibly related PR](https://github.com/cloudnative-pg/cloudnative-pg/pull/3931)\r\nI'm hitting an error when attempting to restore from a backup in GCS for a cluster in a different namespace than the production cluster (same GKE cluster) \"while checking barman-cloud-wal-archive version\". Like if the `barman-cloud-wal-archive` CLI was not present.\r\nCluster status:\r\n```\r\nk cnpg status db-restore-debug -n cambium-recovery-a5\r\nCluster Summary\r\nPrimary server is initializing\r\nName:              db-restore-debug\r\nNamespace:         cambium-recovery-a5\r\nPostgreSQL Image:  cambiumearth/base-image-cnpg:14-3.4-v0.3\r\nPrimary instance:   (switching to db-restore-debug-1)\r\nStatus:            Setting up primary Creating primary instance db-restore-debug-1\r\nInstances:         3\r\nReady instances:   0\r\nCertificates Status\r\nCertificate Name              Expiration Date                Days Left Until Expiration\r\n----------------              ---------------                --------------------------\r\ndb-restore-debug-ca           2024-07-08 11:37:16 +0000 UTC  90.00\r\ndb-restore-debug-replication  2024-07-08 11:37:16 +0000 UTC  90.00\r\ndb-restore-debug-server       2024-07-08 11:37:16 +0000 UTC  90.00\r\nContinuous Backup status\r\nFirst Point of Recoverability:  Not Available\r\nNo Primary instance found\r\nStreaming Replication status\r\nPrimary instance not found\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nInstances status\r\nName  Database Size  Current LSN  Replication role  Status  QoS  Manager Version  Node\r\n----  -------------  -----------  ----------------  ------  ---  ---------------  ----\r\n```\r\nThe recovery job description is:\r\n```\r\nk describe job -n cambium-recovery-a5 db-restore-debug-1-full-recovery\r\nName:             db-restore-debug-1-full-recovery\r\nNamespace:        cambium-recovery-a5\r\nSelector:         batch.kubernetes.io/controller-uid=50cd68f3-7053-4b56-b5ad-fecdddd76a21\r\nLabels:           cnpg.io/cluster=db-restore-debug\r\n                  cnpg.io/instanceName=db-restore-debug-1\r\nAnnotations:      batch.kubernetes.io/job-tracking:\r\n                  cnpg.io/operatorVersion: 1.22.2\r\nControlled By:    Cluster/db-restore-debug\r\nParallelism:      1\r\nCompletions:      1\r\nCompletion Mode:  NonIndexed\r\nStart Time:       Tue, 09 Apr 2024 13:42:17 +0200\r\nPods Statuses:    0 Active (0 Ready) / 0 Succeeded / 5 Failed\r\nPod Template:\r\n  Labels:           batch.kubernetes.io/controller-uid=50cd68f3-7053-4b56-b5ad-fecdddd76a21\r\n                    batch.kubernetes.io/job-name=db-restore-debug-1-full-recovery\r\n                    cnpg.io/cluster=db-restore-debug\r\n                    cnpg.io/instanceName=db-restore-debug-1\r\n                    cnpg.io/jobRole=full-recovery\r\n                    controller-uid=50cd68f3-7053-4b56-b5ad-fecdddd76a21\r\n                    job-name=db-restore-debug-1-full-recovery\r\n  Service Account:  db-restore-debug\r\n  Init Containers:\r\n   bootstrap-controller:\r\n    Image:           ghcr.io/cloudnative-pg/cloudnative-pg:1.22.2\r\n    Port:            <none>\r\n    Host Port:       <none>\r\n    SeccompProfile:  RuntimeDefault\r\n    Command:\r\n      /manager\r\n      bootstrap\r\n      /controller/manager\r\n      --log-level=info\r\n    Environment:  <none>\r\n    Mounts:\r\n      /controller from scratch-data (rw)\r\n      /dev/shm from shm (rw)\r\n      /etc/app-secret from app-secret (rw)\r\n      /run from scratch-data (rw)\r\n      /var/lib/postgresql/data from pgdata (rw)\r\n  Containers:\r\n   full-recovery:\r\n    Image:           cambiumearth/experimental-image-cnpg:14-3.4-v0.5\r\n    Port:            <none>\r\n    Host Port:       <none>\r\n    SeccompProfile:  RuntimeDefault\r\n    Command:\r\n      /controller/manager\r\n      instance\r\n      restore\r\n      --log-level=info\r\n    Environment Variables from:\r\n      cambium-restore-config  ConfigMap  Optional: false\r\n    Environment:\r\n      PGDATA:        /var/lib/postgresql/data/pgdata\r\n      POD_NAME:      db-restore-debug-1-full-recovery\r\n      NAMESPACE:     cambium-recovery-a5\r\n      CLUSTER_NAME:  db-restore-debug\r\n      PGPORT:        5432\r\n      PGHOST:        /controller/run\r\n    Mounts:\r\n      /controller from scratch-data (rw)\r\n      /dev/shm from shm (rw)\r\n      /etc/app-secret from app-secret (rw)\r\n      /run from scratch-data (rw)\r\n      /var/lib/postgresql/data from pgdata (rw)\r\n  Volumes:\r\n   pgdata:\r\n    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\r\n    ClaimName:  db-restore-debug-1\r\n    ReadOnly:   false\r\n   scratch-data:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:\r\n    SizeLimit:  <unset>\r\n   shm:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:     Memory\r\n    SizeLimit:  <unset>\r\n   app-secret:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  db-restore-debug-app\r\n    Optional:    false\r\nEvents:\r\n  Type    Reason            Age    From            Message\r\n  ----    ------            ----   ----            -------\r\n  Normal  SuccessfulCreate  5m1s   job-controller  Created pod: db-restore-debug-1-full-recovery-6zgf6\r\n  Normal  SuccessfulCreate  4m28s  job-controller  Created pod: db-restore-debug-1-full-recovery-h8v25\r\n  Normal  SuccessfulCreate  3m49s  job-controller  Created pod: db-restore-debug-1-full-recovery-6rptj\r\n  Normal  SuccessfulCreate  3m     job-controller  Created pod: db-restore-debug-1-full-recovery-5mz4w\r\n  Normal  SuccessfulCreate  87s    job-controller  Created pod: db-restore-debug-1-full-recovery-p8k9f\r\n```\r\nAnd the job associated attempts pods result in error, raising the mentioned error message.\r\n```\r\nk get pods -n cambium-recovery-a5\r\nNAME                                     READY   STATUS   RESTARTS   AGE\r\ndb-restore-debug-1-full-recovery-5mz4w   0/1     Error    0          2m1s\r\ndb-restore-debug-1-full-recovery-6rptj   0/1     Error    0          2m50s\r\ndb-restore-debug-1-full-recovery-6zgf6   0/1     Error    0          4m2s\r\ndb-restore-debug-1-full-recovery-h8v25   0/1     Error    0          3m29s\r\ndb-restore-debug-1-full-recovery-p8k9f   0/1     Error    0          28s\r\n```\r\nThe operator logs do not show much more info apparently only showing the job creation:\r\n``` \r\n{\"level\":\"info\",\"ts\":\"2024-04-09T09:45:29Z\",\"msg\":\"Creating new Job\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"db-restore-debug\",\"namespace\":\"cambium-recovery-a5\"},\"namespace\":\"cambium-recovery-a5\",\"name\":\"db-restore-debug\",\"reconcileID\":\"9200cae6-97d5-4835-a90c-5d86f6b514f5\",\"uuid\":\"e66dc461-f655-11ee-aa41-5abbf5b78419\",\"name\":\"db-restore-debug-1-full-recovery\",\"primary\":true}\r\n```\r\nThe same source image is running in my production cluster, this is the corresponding Dockerfile, the image is publicly available [in Docker Hub](https://hub.docker.com/layers/cambiumearth/base-image-cnpg/14-3.4-v0.3/images/sha256-2f7fc18239114ac4e9be08120149eec56fa4aace3615ae3e468cd27c711f6796?context=explore):\r\nI've verified that `barman-cloud-check-wal-archive --version` command runs in the production cluster without issue, returning:\r\n```bash\r\npostgres@cambium-db-5:/$ barman-cloud-check-wal-archive --version\r\nbarman-cloud-check-wal-archive 3.4.0\r\n```\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: db-restore-debug\r\nspec:\r\n  instances: 3\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: \"gs://cnpg-recovery-new-backups\"\r\n      googleCredentials:\r\n        applicationCredentials:\r\n          name: backup-creds\r\n          key: gcsCredentials\r\n    retentionPolicy: \"1d\"\r\n  externalClusters:\r\n    - name: cambium-db\r\n      barmanObjectStore:\r\n        destinationPath: \"gs://cnpg-recovery-debug\"\r\n        googleCredentials:\r\n          applicationCredentials:\r\n            name: backup-creds\r\n            key: gcsCredentials\r\n  postgresql:\r\n    shared_preload_libraries:\r\n      - timescaledb\r\n    parameters:\r\n      track_commit_timestamp: \"on\"\r\n      shared_buffers: \"2GB\"\r\n      max_wal_size: \"14GB\"\r\n      wal_compression: \"on\"\r\n      effective_cache_size: \"6GB\"\r\n      maintenance_work_mem: \"1GB\"\r\n      work_mem: \"13107kB\"\r\n      huge_pages: \"off\"\r\n      max_worker_processes: \"4\"\r\n      max_parallel_workers_per_gather: \"2\"\r\n      max_parallel_workers: \"4\"\r\n      wal_buffers: \"16MB\"\r\n      min_wal_size: \"4GB\"\r\n      default_statistics_target: \"500\"\r\n      random_page_cost: \"1.1\"\r\n      checkpoint_completion_target: \"0.9\"\r\n      max_locks_per_transaction: \"256\"\r\n      autovacuum_max_workers: \"10\"\r\n      autovacuum_naptime: \"10\"\r\n      effective_io_concurrency: \"200\"\r\n      max_files_per_process: \"65536\"\r\n  imageName: cambiumearth/base-image-cnpg:14-3.4-v0.3\r\n  bootstrap:\r\n    recovery:\r\n      source: cambium-db\r\n  storage:\r\n    storageClass: standard\r\n    size: 60Gi\r\n  # resources:\r\n  #   requests:\r\n  #     cpu: \"200m\"\r\n  #     memory: \"300Mi\"\r\n  #   limits:\r\n  #     cpu: \"2\"\r\n  #     memory: \"5Gi\"\r\n  envFrom:\r\n    - configMapRef:\r\n        name: cambium-restore-config\r\n```\r\n### Relevant log output\r\n```shell\r\n{\"level\":\"error\",\"ts\":\"2024-04-09T11:42:40Z\",\"msg\":\"while getting barman-cloud-wal-archive options\",\"logging_pod\":\"db-restore-debug-1-full-recovery\",\"error\":\"while checking barman-cloud-wal-archive version: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*InitInfo).checkBackupDestination\\n\\tpkg/management/postgres/restore.go:862\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.InitInfo.Restore\\n\\tpkg/management/postgres/restore.go:236\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/cmd.go:87\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.NewCmd.func2\\n\\tinternal/cmd/manager/instance/restore/cmd.go:60\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.8/x64/src/runtime/proc.go:267\"}\r\n{\"level\":\"error\",\"ts\":\"2024-04-09T11:42:40Z\",\"msg\":\"Error while restoring a backup\",\"logging_pod\":\"db-restore-debug-1-full-recovery\",\"error\":\"while checking barman-cloud-wal-archive version: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/cmd.go:89\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.NewCmd.func2\\n\\tinternal/cmd/manager/instance/restore/cmd.go:60\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.8/x64/src/runtime/proc.go:267\"}\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductUPDATE: Apologies, just figured out what was happening, the backup-creds secret was apparently not well created, re-creating it from scratch and recreating the cluster did the trick, it's now running the recovery:\r\n```shell\r\n{\"level\":\"info\",\"ts\":\"2024-04-09T11:57:16Z\",\"msg\":\"barman-cloud-check-wal-archive checking the first wal\",\"logging_pod\":\"db-restore-debug-1-full-recovery\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-09T11:57:18Z\",\"msg\":\"Recovering from external cluster\",\"logging_pod\":\"db-restore-debug-1-full-recovery\",\"sourceName\":\"cambium-db\"}\r\n{\"level\":\"info\",\"ts\":\"2024-04-09T11:57:20Z\",\"msg\":\"Target backup found\",\"logging_pod\":\"db-restore-debug-1-full-recovery\",\"backup\":{\"backup_name\":\"backup-20240409000001\",\"backup_label\":\"'START WAL LOCATION: 30/5201CD60 (file 0000002B0000003000000052)\\\\nCHECKPOINT LOCATION: 30/53000060\\\\nBACKUP METHOD: streamed\\\\nBACKUP FROM: standby\\\\nSTART TIME: 2024-04-09 00:00:02 UTC\\\\nLABEL: Barman backup cloud 20240409T000002\\\\nSTART TIMELINE: 43\\\\n'\",\"begin_time\":\"Tue Apr  9 00:00:02 2024\",\"end_time\":\"Tue Apr  9 00:13:35 2024\",\"BeginTime\":\"2024-04-09T00:00:02Z\",\"EndTime\":\"2024-04-09T00:13:35Z\",\"begin_wal\":\"0000002B0000003000000052\",\"end_wal\":\"0000002B0000003000000054\",\"begin_xlog\":\"30/5201CD60\",\"end_xlog\":\"30/54000000\",\"systemid\":\"7283048068991815698\",\"backup_id\":\"20240409T000002\",\"error\":\"\",\"timeline\":43}}\r\n{\"level\":\"info\",\"ts\":\"2024-04-09T11:57:21Z\",\"msg\":\"Starting barman-cloud-restore\",\"logging_pod\":\"db-restore-debug-1-full-recovery\",\"options\":[\"gs://cnpg-recovery-debug\",\"cambium-db\",\"20240409T000002\",\"--cloud-provider\",\"google-cloud-storage\",\"/var/lib/postgresql/data/pgdata\"]}\r\n```\r\nPerhaps there could be a more insightful error message here? Otherwise let's please close this issue."
    },
    {
        "title": "[Feature]: Add the labelSelector to the scale subresource of the Pooler CRD to support the vertical-pod-autoscaler",
        "id": 2226148818,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe vertical-pod-autoscaler cannot operate on the pooler deployments due to the Pooler resource lacking a scale subresource on its CRD.\r\nThis results in error messages in the VPA such as: `The targetRef controller has a parent but it should point to a topmost well-known or scalable controller`\n### Describe the solution you'd like\nImplement the [scale subresource](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource). \r\nThis should not require any additional code as all of the data needed is already tracked in the CRD. The CRD only needs add a few additional fields to be extended in this backwards compatible way.\n### Describe alternatives you've considered\nOnly possibility is to implement this.\n### Additional context\nThis is similarly related to #2574 which is a request to implement the scale subresource for Cluster\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe vertical-pod-autoscaler cannot operate on the pooler deployments due to the Pooler resource lacking a scale subresource on its CRD.\r\nThis results in error messages in the VPA such as: `The targetRef controller has a parent but it should point to a topmost well-known or scalable controller`\n### Describe the solution you'd like\nImplement the [scale subresource](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource). \r\nThis should not require any additional code as all of the data needed is already tracked in the CRD. The CRD only needs add a few additional fields to be extended in this backwards compatible way.\n### Describe alternatives you've considered\nOnly possibility is to implement this.\n### Additional context\nThis is similarly related to #2574 which is a request to implement the scale subresource for Cluster\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductThe pooler should already have a scale subresource, see https://github.com/cloudnative-pg/cloudnative-pg/blob/fa30f88fc867cc43bb280f990e2ceea3fd2710b4/api/v1/pooler_types.go#L246.\nCan you share your hpa? The error suggests you are actually pointing it to the wrong thing, you should point it at the pooler, not the deployment it creates.\n---\nI was pointing the vpa at the Deployment, so let me switch it to the Pooler and get back to you.\n---\nI do see the scale subresource on the CRD. However, it is missing a field required by the VPA: [docs](https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/FAQ.md#how-can-i-apply-vpa-to-my-custom-resource).\r\nIn particular, it is missing the `labelSelector` field which is used by the VPA to identify the managed pods. I will update the issue title to reflect the actual problem.\r\nI believe the HPA will work fine, but the VPA will not.\r\nDeployed VPA (notice the status error):\r\n```yaml\r\napiVersion: autoscaling.k8s.io/v1\r\nkind: VerticalPodAutoscaler\r\nmetadata:\r\n  creationTimestamp: \"2024-04-05T20:11:38Z\"\r\n  generation: 1\r\n  labels:\r\n    environment: production\r\n    id: pg-pooler-rw-dd15\r\n    panfactum.com/environment: production\r\n    panfactum.com/local: \"false\"\r\n    panfactum.com/module: kube_pg_cluster\r\n    panfactum.com/region: us-east-2\r\n    panfactum.com/root-module: kube_authentik\r\n    region: us-east-2\r\n    terraform: \"true\"\r\n  name: pg-6284-pooler-rw\r\n  namespace: authentik\r\n  resourceVersion: \"111354\"\r\n  uid: c52d5346-e587-4684-b927-c0b9c1d67c19\r\nspec:\r\n  targetRef:\r\n    apiVersion: postgresql.cnpg.io/v1\r\n    kind: Pooler\r\n    name: pg-6284-pooler-rw\r\n  updatePolicy:\r\n    updateMode: Auto\r\nstatus:\r\n  conditions:\r\n  - lastTransitionTime: \"2024-04-05T20:11:58Z\"\r\n    message: 'Cannot read targetRef. Reason: Unhandled targetRef postgresql.cnpg.io/v1\r\n      / Pooler / pg-6284-pooler-rw, last error Resource authentik/pg-6284-pooler-rw\r\n      has an empty selector for scale sub-resource'\r\n    status: \"True\"\r\n    type: ConfigUnsupported\r\n  - lastTransitionTime: \"2024-04-05T20:11:58Z\"\r\n    message: No pods match this VPA object\r\n    reason: NoPodsMatched\r\n    status: \"True\"\r\n    type: NoPodsMatched\r\n  - lastTransitionTime: \"2024-04-05T20:11:58Z\"\r\n    message: No pods match this VPA object\r\n    reason: NoPodsMatched\r\n    status: \"False\"\r\n    type: RecommendationProvided\r\n  recommendation: {}\r\n```"
    },
    {
        "title": "[Bug]: Incorrect calculation of cronjobs in scheduled backups",
        "id": 2223291347,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\npatrik.zotj@gmail.com\n### Version\n1.22.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Amazon EKS\n### How did you install the operator?\nYAML manifest\n### What happened?\nAfter creating a cluster, i tried attaching a scheduled backup resource to it using the yaml below.\r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: ScheduledBackup\r\nmetadata:\r\n  name: backup-test\r\nspec:\r\n  schedule: \"0 1 * * *\"\r\n  backupOwnerReference: self\r\n  immediate: true\r\n  cluster:\r\n    name: test-db\r\n```\r\nThe backups created by it are fine, the issue is the frequency of the backups does not match the given cron in the yaml file.\r\nFor examle if i give it a cronjob to backup every day at 1 am, even though it says the schedule is correct when i use kubectl describe, the backups are instead done every 1 hour. \r\n<img width=\"513\" alt=\"image\" src=\"https://github.com/cloudnative-pg/cloudnative-pg/assets/90926812/19a16033-9445-4202-a5d0-7eafd515098e\">\r\n<img width=\"160\" alt=\"image\" src=\"https://github.com/cloudnative-pg/cloudnative-pg/assets/90926812/5181fae4-92fc-4bae-b2d4-c94e2b5a1ce8\">\r\nI have tried this with other formats for example every 8 hours and that makes the backups occur every 8 minutes instead.\r\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\npatrik.zotj@gmail.com\n### Version\n1.22.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Amazon EKS\n### How did you install the operator?\nYAML manifest\n### What happened?\nAfter creating a cluster, i tried attaching a scheduled backup resource to it using the yaml below.\r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: ScheduledBackup\r\nmetadata:\r\n  name: backup-test\r\nspec:\r\n  schedule: \"0 1 * * *\"\r\n  backupOwnerReference: self\r\n  immediate: true\r\n  cluster:\r\n    name: test-db\r\n```\r\nThe backups created by it are fine, the issue is the frequency of the backups does not match the given cron in the yaml file.\r\nFor examle if i give it a cronjob to backup every day at 1 am, even though it says the schedule is correct when i use kubectl describe, the backups are instead done every 1 hour. \r\n<img width=\"513\" alt=\"image\" src=\"https://github.com/cloudnative-pg/cloudnative-pg/assets/90926812/19a16033-9445-4202-a5d0-7eafd515098e\">\r\n<img width=\"160\" alt=\"image\" src=\"https://github.com/cloudnative-pg/cloudnative-pg/assets/90926812/5181fae4-92fc-4bae-b2d4-c94e2b5a1ce8\">\r\nI have tried this with other formats for example every 8 hours and that makes the backups occur every 8 minutes instead.\r\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductGot same problem with cron error\n---\nI am facing same problem\n---\n@gbartolini is that targeting in next release ?\n---\nMay be this will help: https://pkg.go.dev/github.com/robfig/cron\r\nI used this doc to configure\n---\n> May be this will help: https://pkg.go.dev/github.com/robfig/cron I used this doc to configure\r\nthank you, will check and share update\n---\nThe issue seems to be that CNPG uses a 6 string cron as defined on the link on the docs (https://pkg.go.dev/github.com/robfig/cron). However, the docs mention:\r\n > This field is a cron schedule specification, which follows the same [format used in Kubernetes CronJobs](https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format)\r\nAnd when looking at [k8s CronJob docs here](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/) It says that is a 5 digit string cron.\r\n> The .spec.schedule field is required. The value of that field follows the [Cron](https://en.wikipedia.org/wiki/Cron) syntax:\r\nSo the issue doesn't seem to be that is calculating it wrong, but rather that the docs/links/logic are inconsistent.\n---\n+1, I had the same problem and it caused error. \nIt would be better to have the same syntax as K8s.\n---\n+1, because of this, my s3 grew to incredible sizes, since I did not immediately notice the problem\n---\nI think we should either throw an error if the user passes a 5-part format, or add 0 to the beginning. Or always use 5-part and add 0 to the beginning (since seconds part is useless in backup schedule)"
    },
    {
        "title": "[Feature]: Add SBOMs for operator images and kubectl plugin",
        "id": 2222880618,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently we don't generate any kind of SBOMS\n### Describe the solution you'd like\nWe should generate SBOMs for the operator images and the kubectl plugin, for both of them it should be during release time. These SBOMs should be published alongside with the other packages published per release\n### Describe alternatives you've considered\nWe could also generate these SBOMs during the CI/CD process\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently we don't generate any kind of SBOMS\n### Describe the solution you'd like\nWe should generate SBOMs for the operator images and the kubectl plugin, for both of them it should be during release time. These SBOMs should be published alongside with the other packages published per release\n### Describe alternatives you've considered\nWe could also generate these SBOMs during the CI/CD process\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductThe patch has been reverted because it was causing problems in the 1.23 release.\n---\nThe new PR reuse the old branch and install syft before call `goreleaser release`"
    },
    {
        "title": "[Bug]: backup resource should report a clear error when cluster not found ",
        "id": 2220654431,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\nolder in 1.22.x\r\n### What version of Kubernetes are you using?\r\n1.27\r\n### What is your Kubernetes environment?\r\nOther\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nWe accidentally created a backup resource for a not existing cluster and got the following output\r\n```\r\nNAME                                                   AGE     CLUSTER                METHOD              PHASE       ERROR\r\nexample-scheduled-backup-20240327030000                7h50m   example                barmanObjectStore   \r\n```\r\nNo `PHASE` or `ERROR` is shown\r\nOnly the event section within the backup resource shows a warning \r\n```\r\nWarning  FindingCluster  19s (x942 over 7h50m)  cloudnative-pg-backup  Unknown cluster example, will retry in 30 seconds\r\n```\r\nAs [discussed on slack](https://cloudnativepg.slack.com/archives/C03AX0J5P29/p1711463617097579) there should be a clearer error message on the resource in this case.\r\n### Cluster resource\r\n_No response_\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\nolder in 1.22.x\r\n### What version of Kubernetes are you using?\r\n1.27\r\n### What is your Kubernetes environment?\r\nOther\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nWe accidentally created a backup resource for a not existing cluster and got the following output\r\n```\r\nNAME                                                   AGE     CLUSTER                METHOD              PHASE       ERROR\r\nexample-scheduled-backup-20240327030000                7h50m   example                barmanObjectStore   \r\n```\r\nNo `PHASE` or `ERROR` is shown\r\nOnly the event section within the backup resource shows a warning \r\n```\r\nWarning  FindingCluster  19s (x942 over 7h50m)  cloudnative-pg-backup  Unknown cluster example, will retry in 30 seconds\r\n```\r\nAs [discussed on slack](https://cloudnativepg.slack.com/archives/C03AX0J5P29/p1711463617097579) there should be a clearer error message on the resource in this case.\r\n### Cluster resource\r\n_No response_\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Allow using existing pgdata when found and skip initdb",
        "id": 2219183239,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThere are a **LOT** of niche usecases where people have to recreate the `cluster` object and just want to reuse their existing PV.\r\nEven when recreated from scratch as a 100% duplicate of the previous cluster object, this would not work and throw the following error: `PGDATA already exists`\r\nMe, and many other users, are slowly starting to get more annoyed by the enforcement of a database wipe when existing PGDATA is detected.\r\nIt's understandable that Cloudnative-PG views a new cluster as a new cluster, but when deployed cluster objects are literally identical, it should not act like they are not.\r\nA redeployment of the `mycluster` object, pointed to PVCs of a previous instance of `mycluster` should at least *be able to* reuse that data\n### Describe the solution you'd like\nEasiest solution, is to allow overriding the `PGDATA` check and accept to use existing PGDATA.\r\nIt should be optional and disabled by default, documented as an advanced option etc.\r\nIt should not be the default, but it should be an available *option* for niche usecases.\r\nProposed key:\r\n`initdb.useExisting: true`\r\nDefault set to: `false`\n### Describe alternatives you've considered\nFor all the many cases running into this, there isn't really a clean different solution.\r\nAs all of those cases are slightly different\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThere are a **LOT** of niche usecases where people have to recreate the `cluster` object and just want to reuse their existing PV.\r\nEven when recreated from scratch as a 100% duplicate of the previous cluster object, this would not work and throw the following error: `PGDATA already exists`\r\nMe, and many other users, are slowly starting to get more annoyed by the enforcement of a database wipe when existing PGDATA is detected.\r\nIt's understandable that Cloudnative-PG views a new cluster as a new cluster, but when deployed cluster objects are literally identical, it should not act like they are not.\r\nA redeployment of the `mycluster` object, pointed to PVCs of a previous instance of `mycluster` should at least *be able to* reuse that data\n### Describe the solution you'd like\nEasiest solution, is to allow overriding the `PGDATA` check and accept to use existing PGDATA.\r\nIt should be optional and disabled by default, documented as an advanced option etc.\r\nIt should not be the default, but it should be an available *option* for niche usecases.\r\nProposed key:\r\n`initdb.useExisting: true`\r\nDefault set to: `false`\n### Describe alternatives you've considered\nFor all the many cases running into this, there isn't really a clean different solution.\r\nAs all of those cases are slightly different\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductThis issue has been very difficult to resolve when trying to work with cnpg cluster during development, such as when doing local development with minikube where I may <strike>start and stop</strike> teardown/destroy the cluster including cnpg cluster a few times a day. The only reliable solution I've found so far to get past \"PGDATA already exists\" is to stop everything including minikube and delete local data/volumes. It'd be great to have this feature. Thanks!\n---\n> This issue has been very difficult to resolve when trying to work with cnpg cluster during development, such as when doing local development with minikube where I may start and stop services including cnpg cluster a few times an hour. The only reliable solution I've found so far to get past \"PGDATA already exists\" is to stop everything including minikube and delete local data/volumes. It'd be great to have this feature. Thanks!\r\nstart en stop isn't the issue, the problem is recreating the cluster object.\r\nIf you need to stop something, you should be using the `hibernation` system, this stops all pods.\r\nTHe issue here, is only a thing when recreating the cluster object.\n---\n> > This issue has been very difficult to resolve when trying to work with cnpg cluster during development, such as when doing local development with minikube where I may start and stop services including cnpg cluster a few times an hour. The only reliable solution I've found so far to get past \"PGDATA already exists\" is to stop everything including minikube and delete local data/volumes. It'd be great to have this feature. Thanks!\r\n> \r\n> start en stop isn't the issue, the problem is recreating the cluster object. If you need to stop something, you should be using the `hibernation` system, this stops all pods.\r\n> \r\n> THe issue here, is only a thing when recreating the cluster object.\r\nGood point, I should have written that it was when doing teardown/recreating the cluster (I wasn't really doing much start/stop at that point). \r\nI was also running two cnpg clusters on my single minikube node, so perhaps they didn't like having to share the same node. Once I progressed with testing locally and moved to a remote cluster instead of minikube, I came across [this section in docs](https://cloudnative-pg.io/documentation/1.22/scheduling/#pod-affinity-and-anti-affinity) about scheduling and pod affinity, then node affinity, and found that by editing my PVs for both clusters to have affinity towards one node each, it seemed to further reduce occurrence of PGData already exists, and of multi-attach error.\n---\ndo we have any progress status for this issue regarding the reusing of PGDATA when redeploying ?"
    },
    {
        "title": "[Bug]: Running a new replica is constantly failing",
        "id": 2215588188,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n### Version\r\n1.22.2\r\n### What version of Kubernetes are you using?\r\n1.26\r\n### What is your Kubernetes environment?\r\nCloud: Amazon EKS\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nWe are experiencing an issue with creating cluster replicas by scaling up the cluster resource. After finishing the phase of creating a backup with pg_basebackup it failed with the following error:\r\n`requested timeline 5 does not contain minimum recovery point 466/FD7555E8 on timeline 4`.  I have been attempting to create other replicas but it always failed on the same phase. \r\nThe cluster is currently on the following timeline: \r\n`Current Write LSN:   481/769AC000 (Timeline: 4 - WAL File: 000000040000048100000076)`\r\nPlease let me know what additional information are required to help solve that issue. \r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    cnpg.io/reloadedAt: \"2024-03-29T11:07:33.020678+01:00\"\r\n    kubectl.kubernetes.io/restartedAt: \"2024-03-29T11:52:20+01:00\"\r\n  creationTimestamp: \"2024-03-29T11:17:22Z\"\r\n  generation: 7\r\n  labels:\r\n    argocd.argoproj.io/instance: prd-infra-eks-cnpg0-pg\r\n  name: pg-database0\r\n  namespace: cnpg1\r\n  resourceVersion: \"223159316\"\r\n  uid: cda6c776-0499-42bd-a7f2-14ac204ab15a\r\nspec:\r\n  affinity:\r\n    nodeAffinity:\r\n      requiredDuringSchedulingIgnoredDuringExecution:\r\n        nodeSelectorTerms:\r\n        - matchExpressions:\r\n          - key: cnpg-workload-32\r\n            operator: In\r\n            values:\r\n            - enabled\r\n    podAntiAffinityType: preferred\r\n    tolerations:\r\n    - effect: NoSchedule\r\n      key: cnpg-workload-32\r\n      operator: Equal\r\n      value: enabled\r\n    topologyKey: topology.kubernetes.io/zone\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3://prd-cnpg-backups/backups\r\n      s3Credentials:\r\n        inheritFromIAMRole: true\r\n      wal:\r\n        compression: gzip\r\n        encryption: AES256\r\n        maxParallel: 4\r\n    retentionPolicy: 30d\r\n    target: prefer-standby\r\n    volumeSnapshot:\r\n      className: pg-data-snapshot-class\r\n      labels:\r\n        app.kubernetes.io/managed-by: cloudnative-pg\r\n      online: true\r\n      onlineConfiguration:\r\n        immediateCheckpoint: true\r\n        waitForArchive: true\r\n      snapshotOwnerReference: backup\r\n      walClassName: pg-wal-snapshot-class\r\n  bootstrap:\r\n    initdb:\r\n      dataChecksums: false\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enableSuperuserAccess: true\r\n  failoverDelay: 0\r\n  imageName: ecr-private-repo/infra/postgresql:13.11-10-202308231244\r\n  imagePullPolicy: Always\r\n  instances: 1\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: false\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      auto_explain.log_min_duration: 10s\r\n      autovacuum_analyze_scale_factor: \"0.05\"\r\n      autovacuum_max_workers: \"6\"\r\n      autovacuum_naptime: \"30\"\r\n      autovacuum_vacuum_cost_delay: \"20\"\r\n      autovacuum_vacuum_scale_factor: \"0.1\"\r\n      checkpoint_completion_target: \"0.9\"\r\n      checkpoint_timeout: \"300\"\r\n      client_encoding: UTF8\r\n      dynamic_shared_memory_type: posix\r\n      effective_cache_size: 16GB\r\n      huge_pages: \"on\"\r\n      idle_in_transaction_session_timeout: \"300000\"\r\n      log_checkpoints: \"1\"\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_hostname: \"1\"\r\n      log_min_duration_statement: \"5000\"\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      maintenance_work_mem: 1024MB\r\n      max_connections: \"2000\"\r\n      max_locks_per_transaction: \"64\"\r\n      max_logical_replication_workers: \"10\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"10\"\r\n      max_stack_depth: \"6144\"\r\n      max_sync_workers_per_subscription: \"4\"\r\n      max_wal_senders: \"20\"\r\n      max_wal_size: \"2048\"\r\n      max_worker_processes: \"15\"\r\n      min_wal_size: \"192\"\r\n      pg_stat_statements.max: \"10000\"\r\n      pg_stat_statements.track: all\r\n      pgaudit.log: all, -misc\r\n      pgaudit.log_catalog: \"off\"\r\n      pgaudit.log_parameter: \"on\"\r\n      pgaudit.log_relation: \"on\"\r\n      shared_buffers: 4000MB\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      tcp_keepalives_count: \"2\"\r\n      tcp_keepalives_idle: \"300\"\r\n      tcp_keepalives_interval: \"30\"\r\n      timezone: UTC\r\n      track_activity_query_size: \"2048\"\r\n      track_commit_timestamp: \"on\"\r\n      track_io_timing: \"on\"\r\n      wal_keep_size: 512MB\r\n      wal_level: logical\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n      work_mem: 32MB\r\n    pg_hba:\r\n    - hostssl app all all cert\r\n    shared_preload_libraries:\r\n    - pg_stat_statements\r\n    - pg_stat_monitor\r\n    - pg_qualstats\r\n    - pg_stat_kcache\r\n    - pg_wait_sampling\r\n    - pgaudit\r\n    - auto_explain\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  priorityClassName: infra-cluster-high\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      hugepages-2Mi: 5Gi\r\n      memory: 25Gi\r\n    requests:\r\n      cpu: \"7\"\r\n      hugepages-2Mi: 5Gi\r\n      memory: 16Gi\r\n  serviceAccountTemplate:\r\n    metadata:\r\n      annotations:\r\n        eks.amazonaws.com/role-arn: arn:aws:iam::1234:role/eks-cnpg0-cloudnative-pg\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 220Gi\r\n    storageClass: io2-pg-data\r\n  switchoverDelay: 3600\r\n  walStorage:\r\n    resizeInUseVolumes: true\r\n    size: 200Gi\r\n    storageClass: io2\r\nstatus:\r\n  availableArchitectures:\r\n  - goArch: amd64\r\n    hash: 72462f8f498e9a1e31eeb819b74038375e02b5bf3d5f9adb6016ab713c96d9dc\r\n  - goArch: arm64\r\n    hash: 14ec408750588c000a891253608b5f6a84cc436f607b99a6c15611b106136e44\r\n  certificates:\r\n    clientCASecret: pg-database0-ca\r\n    expirations:\r\n      pg-database0-ca: 2024-06-27 11:12:23 +0000 UTC\r\n      pg-database0-replication: 2024-06-27 11:12:23 +0000 UTC\r\n      pg-database0-server: 2024-06-27 11:12:23 +0000 UTC\r\n    replicationTLSSecret: pg-database0-replication\r\n    serverAltDNSNames:\r\n    - pg-database0-rw\r\n    - pg-database0-rw.cnpg1\r\n    - pg-database0-rw.cnpg1.svc\r\n    - pg-database0-r\r\n    - pg-database0-r.cnpg1\r\n    - pg-database0-r.cnpg1.svc\r\n    - pg-database0-ro\r\n    - pg-database0-ro.cnpg1\r\n    - pg-database0-ro.cnpg1.svc\r\n    serverCASecret: pg-database0-ca\r\n    serverTLSSecret: pg-database0-server\r\n  cloudNativePGCommitHash: bcdcd885\r\n  cloudNativePGOperatorHash: 72462f8f498e9a1e31eeb819b74038375e02b5bf3d5f9adb6016ab713c96d9dc\r\n  conditions:\r\n  - lastTransitionTime: \"2024-03-29T13:06:28Z\"\r\n    message: Cluster is Ready\r\n    reason: ClusterIsReady\r\n    status: \"True\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-03-29T11:18:08Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  - lastTransitionTime: \"2024-03-29T11:28:32Z\"\r\n    message: Backup was successful\r\n    reason: LastBackupSucceeded\r\n    status: \"True\"\r\n    type: LastBackupSucceeded\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"174342296\"\r\n  currentPrimary: pg-database0-2\r\n  currentPrimaryTimestamp: \"2024-03-29T11:18:03.180909Z\"\r\n  firstRecoverabilityPoint: \"2024-03-29T11:28:32Z\"\r\n  firstRecoverabilityPointByMethod:\r\n    volumeSnapshot: \"2024-03-29T11:28:32Z\"\r\n  healthyPVC:\r\n  - pg-database0-2\r\n  - pg-database0-2-wal\r\n  instanceNames:\r\n  - pg-database0-2\r\n  instances: 1\r\n  instancesReportedState:\r\n    pg-database0-2:\r\n      isPrimary: true\r\n      timeLineID: 4\r\n  instancesStatus:\r\n    healthy:\r\n    - pg-database0-2\r\n  lastSuccessfulBackup: \"2024-03-29T11:28:32Z\"\r\n  lastSuccessfulBackupByMethod:\r\n    volumeSnapshot: \"2024-03-29T11:28:32Z\"\r\n  latestGeneratedNode: 5\r\n  managedRolesStatus: {}\r\n  phase: Cluster in healthy state\r\n  poolerIntegrations:\r\n    pgBouncerIntegration:\r\n      secrets:\r\n      - pg-database0-pooler\r\n  pvcCount: 2\r\n  readService: pg-database0-r\r\n  readyInstances: 1\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"223091672\"\r\n    clientCaSecretVersion: \"223091668\"\r\n    replicationSecretVersion: \"223091670\"\r\n    serverCaSecretVersion: \"223091668\"\r\n    serverSecretVersion: \"223091669\"\r\n    superuserSecretVersion: \"223091671\"\r\n  targetPrimary: pg-database0-2\r\n  timelineID: 4\r\n  topology:\r\n    instances:\r\n      pg-database0-2: {}\r\n    nodesUsed: 1\r\n    successfullyExtracted: true\r\n  writeService: pg-database0-rw\r\n```\r\n### Relevant log output\r\n```shell\r\n{\"level\":\"info\",\"ts\":\"2024-03-29T11:58:29Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg-database0-4\",\"record\":{\"log_time\":\"2024-03-29 11:58:29.032 UTC\",\"process_id\":\"27\",\"session_id\":\"6606ace0.1b\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-03-29 11:58:24 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"XX000\",\"message\":\"requested timeline 5 is not a child of this server's history\",\"detail\":\"Latest checkpoint is at 478/18379B60 on timeline 4, but in the history of the requested timeline, the server forked off from that timeline at 466/EB0000A0.\",\"backend_type\":\"startup\"}}\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n### Version\r\n1.22.2\r\n### What version of Kubernetes are you using?\r\n1.26\r\n### What is your Kubernetes environment?\r\nCloud: Amazon EKS\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nWe are experiencing an issue with creating cluster replicas by scaling up the cluster resource. After finishing the phase of creating a backup with pg_basebackup it failed with the following error:\r\n`requested timeline 5 does not contain minimum recovery point 466/FD7555E8 on timeline 4`.  I have been attempting to create other replicas but it always failed on the same phase. \r\nThe cluster is currently on the following timeline: \r\n`Current Write LSN:   481/769AC000 (Timeline: 4 - WAL File: 000000040000048100000076)`\r\nPlease let me know what additional information are required to help solve that issue. \r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    cnpg.io/reloadedAt: \"2024-03-29T11:07:33.020678+01:00\"\r\n    kubectl.kubernetes.io/restartedAt: \"2024-03-29T11:52:20+01:00\"\r\n  creationTimestamp: \"2024-03-29T11:17:22Z\"\r\n  generation: 7\r\n  labels:\r\n    argocd.argoproj.io/instance: prd-infra-eks-cnpg0-pg\r\n  name: pg-database0\r\n  namespace: cnpg1\r\n  resourceVersion: \"223159316\"\r\n  uid: cda6c776-0499-42bd-a7f2-14ac204ab15a\r\nspec:\r\n  affinity:\r\n    nodeAffinity:\r\n      requiredDuringSchedulingIgnoredDuringExecution:\r\n        nodeSelectorTerms:\r\n        - matchExpressions:\r\n          - key: cnpg-workload-32\r\n            operator: In\r\n            values:\r\n            - enabled\r\n    podAntiAffinityType: preferred\r\n    tolerations:\r\n    - effect: NoSchedule\r\n      key: cnpg-workload-32\r\n      operator: Equal\r\n      value: enabled\r\n    topologyKey: topology.kubernetes.io/zone\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3://prd-cnpg-backups/backups\r\n      s3Credentials:\r\n        inheritFromIAMRole: true\r\n      wal:\r\n        compression: gzip\r\n        encryption: AES256\r\n        maxParallel: 4\r\n    retentionPolicy: 30d\r\n    target: prefer-standby\r\n    volumeSnapshot:\r\n      className: pg-data-snapshot-class\r\n      labels:\r\n        app.kubernetes.io/managed-by: cloudnative-pg\r\n      online: true\r\n      onlineConfiguration:\r\n        immediateCheckpoint: true\r\n        waitForArchive: true\r\n      snapshotOwnerReference: backup\r\n      walClassName: pg-wal-snapshot-class\r\n  bootstrap:\r\n    initdb:\r\n      dataChecksums: false\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enableSuperuserAccess: true\r\n  failoverDelay: 0\r\n  imageName: ecr-private-repo/infra/postgresql:13.11-10-202308231244\r\n  imagePullPolicy: Always\r\n  instances: 1\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: false\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      auto_explain.log_min_duration: 10s\r\n      autovacuum_analyze_scale_factor: \"0.05\"\r\n      autovacuum_max_workers: \"6\"\r\n      autovacuum_naptime: \"30\"\r\n      autovacuum_vacuum_cost_delay: \"20\"\r\n      autovacuum_vacuum_scale_factor: \"0.1\"\r\n      checkpoint_completion_target: \"0.9\"\r\n      checkpoint_timeout: \"300\"\r\n      client_encoding: UTF8\r\n      dynamic_shared_memory_type: posix\r\n      effective_cache_size: 16GB\r\n      huge_pages: \"on\"\r\n      idle_in_transaction_session_timeout: \"300000\"\r\n      log_checkpoints: \"1\"\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_hostname: \"1\"\r\n      log_min_duration_statement: \"5000\"\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      maintenance_work_mem: 1024MB\r\n      max_connections: \"2000\"\r\n      max_locks_per_transaction: \"64\"\r\n      max_logical_replication_workers: \"10\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"10\"\r\n      max_stack_depth: \"6144\"\r\n      max_sync_workers_per_subscription: \"4\"\r\n      max_wal_senders: \"20\"\r\n      max_wal_size: \"2048\"\r\n      max_worker_processes: \"15\"\r\n      min_wal_size: \"192\"\r\n      pg_stat_statements.max: \"10000\"\r\n      pg_stat_statements.track: all\r\n      pgaudit.log: all, -misc\r\n      pgaudit.log_catalog: \"off\"\r\n      pgaudit.log_parameter: \"on\"\r\n      pgaudit.log_relation: \"on\"\r\n      shared_buffers: 4000MB\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      tcp_keepalives_count: \"2\"\r\n      tcp_keepalives_idle: \"300\"\r\n      tcp_keepalives_interval: \"30\"\r\n      timezone: UTC\r\n      track_activity_query_size: \"2048\"\r\n      track_commit_timestamp: \"on\"\r\n      track_io_timing: \"on\"\r\n      wal_keep_size: 512MB\r\n      wal_level: logical\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n      work_mem: 32MB\r\n    pg_hba:\r\n    - hostssl app all all cert\r\n    shared_preload_libraries:\r\n    - pg_stat_statements\r\n    - pg_stat_monitor\r\n    - pg_qualstats\r\n    - pg_stat_kcache\r\n    - pg_wait_sampling\r\n    - pgaudit\r\n    - auto_explain\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  priorityClassName: infra-cluster-high\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      hugepages-2Mi: 5Gi\r\n      memory: 25Gi\r\n    requests:\r\n      cpu: \"7\"\r\n      hugepages-2Mi: 5Gi\r\n      memory: 16Gi\r\n  serviceAccountTemplate:\r\n    metadata:\r\n      annotations:\r\n        eks.amazonaws.com/role-arn: arn:aws:iam::1234:role/eks-cnpg0-cloudnative-pg\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 220Gi\r\n    storageClass: io2-pg-data\r\n  switchoverDelay: 3600\r\n  walStorage:\r\n    resizeInUseVolumes: true\r\n    size: 200Gi\r\n    storageClass: io2\r\nstatus:\r\n  availableArchitectures:\r\n  - goArch: amd64\r\n    hash: 72462f8f498e9a1e31eeb819b74038375e02b5bf3d5f9adb6016ab713c96d9dc\r\n  - goArch: arm64\r\n    hash: 14ec408750588c000a891253608b5f6a84cc436f607b99a6c15611b106136e44\r\n  certificates:\r\n    clientCASecret: pg-database0-ca\r\n    expirations:\r\n      pg-database0-ca: 2024-06-27 11:12:23 +0000 UTC\r\n      pg-database0-replication: 2024-06-27 11:12:23 +0000 UTC\r\n      pg-database0-server: 2024-06-27 11:12:23 +0000 UTC\r\n    replicationTLSSecret: pg-database0-replication\r\n    serverAltDNSNames:\r\n    - pg-database0-rw\r\n    - pg-database0-rw.cnpg1\r\n    - pg-database0-rw.cnpg1.svc\r\n    - pg-database0-r\r\n    - pg-database0-r.cnpg1\r\n    - pg-database0-r.cnpg1.svc\r\n    - pg-database0-ro\r\n    - pg-database0-ro.cnpg1\r\n    - pg-database0-ro.cnpg1.svc\r\n    serverCASecret: pg-database0-ca\r\n    serverTLSSecret: pg-database0-server\r\n  cloudNativePGCommitHash: bcdcd885\r\n  cloudNativePGOperatorHash: 72462f8f498e9a1e31eeb819b74038375e02b5bf3d5f9adb6016ab713c96d9dc\r\n  conditions:\r\n  - lastTransitionTime: \"2024-03-29T13:06:28Z\"\r\n    message: Cluster is Ready\r\n    reason: ClusterIsReady\r\n    status: \"True\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-03-29T11:18:08Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  - lastTransitionTime: \"2024-03-29T11:28:32Z\"\r\n    message: Backup was successful\r\n    reason: LastBackupSucceeded\r\n    status: \"True\"\r\n    type: LastBackupSucceeded\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"174342296\"\r\n  currentPrimary: pg-database0-2\r\n  currentPrimaryTimestamp: \"2024-03-29T11:18:03.180909Z\"\r\n  firstRecoverabilityPoint: \"2024-03-29T11:28:32Z\"\r\n  firstRecoverabilityPointByMethod:\r\n    volumeSnapshot: \"2024-03-29T11:28:32Z\"\r\n  healthyPVC:\r\n  - pg-database0-2\r\n  - pg-database0-2-wal\r\n  instanceNames:\r\n  - pg-database0-2\r\n  instances: 1\r\n  instancesReportedState:\r\n    pg-database0-2:\r\n      isPrimary: true\r\n      timeLineID: 4\r\n  instancesStatus:\r\n    healthy:\r\n    - pg-database0-2\r\n  lastSuccessfulBackup: \"2024-03-29T11:28:32Z\"\r\n  lastSuccessfulBackupByMethod:\r\n    volumeSnapshot: \"2024-03-29T11:28:32Z\"\r\n  latestGeneratedNode: 5\r\n  managedRolesStatus: {}\r\n  phase: Cluster in healthy state\r\n  poolerIntegrations:\r\n    pgBouncerIntegration:\r\n      secrets:\r\n      - pg-database0-pooler\r\n  pvcCount: 2\r\n  readService: pg-database0-r\r\n  readyInstances: 1\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"223091672\"\r\n    clientCaSecretVersion: \"223091668\"\r\n    replicationSecretVersion: \"223091670\"\r\n    serverCaSecretVersion: \"223091668\"\r\n    serverSecretVersion: \"223091669\"\r\n    superuserSecretVersion: \"223091671\"\r\n  targetPrimary: pg-database0-2\r\n  timelineID: 4\r\n  topology:\r\n    instances:\r\n      pg-database0-2: {}\r\n    nodesUsed: 1\r\n    successfullyExtracted: true\r\n  writeService: pg-database0-rw\r\n```\r\n### Relevant log output\r\n```shell\r\n{\"level\":\"info\",\"ts\":\"2024-03-29T11:58:29Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg-database0-4\",\"record\":{\"log_time\":\"2024-03-29 11:58:29.032 UTC\",\"process_id\":\"27\",\"session_id\":\"6606ace0.1b\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-03-29 11:58:24 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"XX000\",\"message\":\"requested timeline 5 is not a child of this server's history\",\"detail\":\"Latest checkpoint is at 478/18379B60 on timeline 4, but in the history of the requested timeline, the server forked off from that timeline at 466/EB0000A0.\",\"backend_type\":\"startup\"}}\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductIt seems I have the same issue, with the same error. The restore starts well but at some point, the WAL may be corrupt or something:\r\n```json\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"grafana-db-6\",\"walName\":\"000000290000014B000000DE\",\"startTime\":\"2024-05-22T13:23:35Z\",\"endTime\":\"2024-05-22T13:23:41Z\",\"elapsedWalTime\":6.624312737}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"grafana-db-6\",\"walName\":\"000000290000014B000000DE\",\"maxParallel\":1,\"successfulWalRestore\":1,\"failedWalRestore\":0,\"endOfWALStream\":false,\"startTime\":\"2024-05-22T13:23:35Z\",\"downloadStartTime\":\"2024-05-22T13:23:35Z\",\"downloadTotalTime\":6.624637657,\"totalTime\":6.73175157}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"grafana-db-6\",\"record\":{\"log_time\":\"2024-05-22 13:23:41.900 UTC\",\"process_id\":\"29\",\"session_id\":\"664df1d0.1d\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-05-22 13:23:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"000000290000014B000000DE\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"grafana-db-6\",\"record\":{\"log_time\":\"2024-05-22 13:23:41.957 UTC\",\"process_id\":\"29\",\"session_id\":\"664df1d0.1d\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-05-22 13:23:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"XX000\",\"message\":\"requested timeline 42 is not a child of this server's history\",\"detail\":\"Latest checkpoint is at 14B/DE002C40 on timeline 41, but in the history of the requested timeline, the server forked off from that timeline at 148/22081808.\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"grafana-db-6\",\"record\":{\"log_time\":\"2024-05-22 13:23:41.960 UTC\",\"process_id\":\"25\",\"session_id\":\"664df1d0.19\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-05-22 13:23:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 29) exited with exit code 1\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"grafana-db-6\",\"record\":{\"log_time\":\"2024-05-22 13:23:41.960 UTC\",\"process_id\":\"25\",\"session_id\":\"664df1d0.19\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-05-22 13:23:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"aborting startup due to startup process failure\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"grafana-db-6\",\"record\":{\"log_time\":\"2024-05-22 13:23:41.963 UTC\",\"process_id\":\"25\",\"session_id\":\"664df1d0.19\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-05-22 13:23:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"grafana-db-6\",\"reason\":\"postmaster has exited\"}\r\n{\"level\":\"error\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"PostgreSQL process exited with errors\",\"logging_pod\":\"grafana-db-6\",\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:98\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.17.3/pkg/manager/runnable_group.go:223\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"Stopping and waiting for leader election runnables\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"error\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"error received after stop sequence was engaged\",\"error\":\"exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).engageStopProcedure.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.17.3/pkg/manager/internal.go:490\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"grafana-db-6\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"grafana-db-6\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"grafana-db-6\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"grafana-db-6\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"grafana-db-6\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"grafana-db-6\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"grafana-db-6\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"Stopping and waiting for caches\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"Stopping and waiting for webhooks\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"pkg/mod/k8s.io/client-go@v0.29.4/tools/cache/reflector.go:229: watch of *v1.Cluster ended with: an error on the server (\\\"unable to decode an event from the watch stream: context canceled\\\") has prevented the request from succeeding\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"Stopping and waiting for HTTP servers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-05-22T13:23:41Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\r\n```\n---\nI'm having the same issue, but not after replica rescaling. Instead, it started happening after scaling down all worker nodes to 0 and then back up to 3. 2 out of 3 replicas are running fine, but the third is unable to come up with the same logs as @sambonbonne.\n---\n+1, having the same problem in some of the clusters\n---\nThe only reasonable workaround I've found for now is to run clusters with a maximum of 1 instance. This, of course, sacrifices availability. But as is evidenced by this issue, CNPG currently has *severe* correctness issues managing Postgres clusters with >0 replicas :(\n---\nSo it's somehow related to backups.\r\nSince new instance is being recreated using backups, I tried to remove/comment backup setup from cluster manifest, set the number of instances to one and applied. Deleted PVC of second instance. Then i scaled up to two replicas. New instance was replicated from primary node and both instances were fine. Then i re-enabled backup again and tried to restart second instance and it started failing again. So, i deleted the backups (it was just a test cluster) from S3 bucket and restarted second instance. Cluster is now running fine.\n---\nI experienced this on 1.24.0 today and can confirm that removing the backup config temporary enables to boot more replicas.\n---\nI encountered this issue on version 1.25.0 as well. To allow new replicas to run, I initially disabled the backup configuration. Interestingly, I have another cluster with backup enabled that has been running in production for over eight months without any issues.  \nThe only difference in the backup configuration is:  \n```yaml\nwal:\n  compression: gzip\n  maxParallel: 8\n  encryption: AES256\n```  \nAfter noticing this, I re-enabled backups but removed these specific properties from the configuration. Since then, the cluster has remained stable for two consecutive days, successfully creating two daily scheduled backups and storing WALs in S3.  \nHope this helps in diagnosing the issue! Let me know if I can provide any further details."
    },
    {
        "title": "[Feature]: unified pipeline for the CNPG codebase",
        "id": 2208455274,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe build/test scripts for CloudNativePG are a heterogenous mixture of shell scripts, Makefiles, python, with a very convoluted calling pattern between them.\r\nThis makes it difficult to understand, and error prone.\n### Describe the solution you'd like\nRewrite as much as possible of the pipeline code in Go. Have a clear and simple dependency graph.\n### Describe alternatives you've considered\ndagger.io\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe build/test scripts for CloudNativePG are a heterogenous mixture of shell scripts, Makefiles, python, with a very convoluted calling pattern between them.\r\nThis makes it difficult to understand, and error prone.\n### Describe the solution you'd like\nRewrite as much as possible of the pipeline code in Go. Have a clear and simple dependency graph.\n### Describe alternatives you've considered\ndagger.io\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: retentionPolicy does not limit backups",
        "id": 2207962263,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nkamikaze.is.waiting.you@gmail.com\n### Version\n1.22.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nFor some reason base backups are not being dropped after retentionPolicy: 7d (another cluster for 2d as well). I have such \"extra\" base backups since 2024-03-18, today is 2024-03-26. Worked before correctly.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: pg16\r\n  namespace: pg\r\nspec:\r\n  description: \"PROD cluster\"\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.2\r\n  startDelay: 300\r\n  stopDelay: 300\r\n  instances: 3\r\n  storage:\r\n    size: 20Gi\r\n    resizeInUseVolumes: False\r\n  minSyncReplicas: 1\r\n  maxSyncReplicas: 1\r\n  primaryUpdateStrategy: unsupervised\r\n  primaryUpdateMethod: switchover\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  enableSuperuserAccess: true\r\n  superuserSecret:\r\n    name: db-postgres-creds\r\n  externalClusters:\r\n    - name: prod-db-backup\r\n      barmanObjectStore:\r\n        destinationPath: \"s3://prod-db-backup/\"\r\n        serverName: \"pg-0\"\r\n        s3Credentials:\r\n          accessKeyId:\r\n            name: s3-backuper-creds\r\n            key: ACCESS_KEY_ID\r\n          secretAccessKey:\r\n            name: s3-backuper-creds\r\n            key: ACCESS_SECRET_KEY\r\n        wal:\r\n          maxParallel: 8\r\n  bootstrap:\r\n    initdb:\r\n      database: db\r\n      owner: owner\r\n      secret:\r\n        name: db-creds\r\n      localeCollate: \"en_US.UTF-8\"\r\n      localeCType: \"en_US.UTF-8\"\r\n      postInitApplicationSQLRefs:\r\n        secretRefs:\r\n          - name: post-init-sql\r\n            key: post-init.sql\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: \"s3://prod-db-backup/\"\r\n      serverName: \"pg16-0\"\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: s3-backuper-creds\r\n          key: ACCESS_KEY_ID\r\n        secretAccessKey:\r\n          name: s3-backuper-creds\r\n          key: ACCESS_SECRET_KEY\r\n      wal:\r\n        compression: bzip2\r\n        maxParallel: 8\r\n      data:\r\n        compression: bzip2\r\n    retentionPolicy: 7d\r\n  postgresql:\r\n    parameters:\r\n      work_mem: \"32MB\"\r\n      shared_buffers: \"1GB\"\r\n  resources:\r\n    requests:\r\n      memory: \"1.3Gi\"\r\n      cpu: \"0.5\"\r\n    limits:\r\n      memory: \"2.6Gi\"\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nkamikaze.is.waiting.you@gmail.com\n### Version\n1.22.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nFor some reason base backups are not being dropped after retentionPolicy: 7d (another cluster for 2d as well). I have such \"extra\" base backups since 2024-03-18, today is 2024-03-26. Worked before correctly.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: pg16\r\n  namespace: pg\r\nspec:\r\n  description: \"PROD cluster\"\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.2\r\n  startDelay: 300\r\n  stopDelay: 300\r\n  instances: 3\r\n  storage:\r\n    size: 20Gi\r\n    resizeInUseVolumes: False\r\n  minSyncReplicas: 1\r\n  maxSyncReplicas: 1\r\n  primaryUpdateStrategy: unsupervised\r\n  primaryUpdateMethod: switchover\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  enableSuperuserAccess: true\r\n  superuserSecret:\r\n    name: db-postgres-creds\r\n  externalClusters:\r\n    - name: prod-db-backup\r\n      barmanObjectStore:\r\n        destinationPath: \"s3://prod-db-backup/\"\r\n        serverName: \"pg-0\"\r\n        s3Credentials:\r\n          accessKeyId:\r\n            name: s3-backuper-creds\r\n            key: ACCESS_KEY_ID\r\n          secretAccessKey:\r\n            name: s3-backuper-creds\r\n            key: ACCESS_SECRET_KEY\r\n        wal:\r\n          maxParallel: 8\r\n  bootstrap:\r\n    initdb:\r\n      database: db\r\n      owner: owner\r\n      secret:\r\n        name: db-creds\r\n      localeCollate: \"en_US.UTF-8\"\r\n      localeCType: \"en_US.UTF-8\"\r\n      postInitApplicationSQLRefs:\r\n        secretRefs:\r\n          - name: post-init-sql\r\n            key: post-init.sql\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: \"s3://prod-db-backup/\"\r\n      serverName: \"pg16-0\"\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: s3-backuper-creds\r\n          key: ACCESS_KEY_ID\r\n        secretAccessKey:\r\n          name: s3-backuper-creds\r\n          key: ACCESS_SECRET_KEY\r\n      wal:\r\n        compression: bzip2\r\n        maxParallel: 8\r\n      data:\r\n        compression: bzip2\r\n    retentionPolicy: 7d\r\n  postgresql:\r\n    parameters:\r\n      work_mem: \"32MB\"\r\n      shared_buffers: \"1GB\"\r\n  resources:\r\n    requests:\r\n      memory: \"1.3Gi\"\r\n      cpu: \"0.5\"\r\n    limits:\r\n      memory: \"2.6Gi\"\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductAlso I have a daily backup set\n---\nmight be related to 1.22.2 release?\n---\nI have the same issue for more than ~2 months now, but not for all my clusters \ud83e\udd37"
    },
    {
        "title": "[Bug]: K8 worker run out of resources and died, leaving couple of CNPG clusters not working. What means danglingPVC?",
        "id": 2207804991,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ngmatej@gmail.com\n### Version\nolder in 1.21.x\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Amazon EKS\n### How did you install the operator?\nHelm\n### What happened?\nK8 worker run out of resources and died, leaving couple of CNPG clusters not working. The problem is that primary pods disappeared. When looking the report it says:\r\n  danglingPVC:\r\n  - cluster-coordinator-2\r\n  healthyPVC:\r\n  - cluster-coordinator-1\r\n  instanceNames:\r\n  - cluster-coordinator-1\r\n  - cluster-coordinator-2\r\nI know that maybe the latest CNPG version would fix this. But I would like to know what danglingPVC actually means. On k8 level I can't see anything worng with the PVC or volume.\r\nI would say that because this dangling flag the pod dosn't even start. Anyway I could force it to do so. \n### Cluster resource\n```shell\nmetadata:\r\n  annotations:\r\n    cnpg.io/reloadedAt: \"2024-03-25T14:13:45.196615+01:00\"\r\n    kubectl.kubernetes.io/restartedAt: \"2024-03-25T14:11:47+01:00\"\r\n    meta.helm.sh/release-name: cluster\r\n    meta.helm.sh/release-namespace: transit-platform\r\n  creationTimestamp: \"2024-03-18T07:16:42Z\"\r\n  generation: 1\r\n  labels:\r\n    app.kubernetes.io/managed-by: Helm\r\n  managedFields:\r\n  - apiVersion: postgresql.cnpg.io/v1\r\n    fieldsType: FieldsV1\r\n    fieldsV1:\r\n      f:metadata:\r\n        f:annotations:\r\n          .: {}\r\n          f:meta.helm.sh/release-name: {}\r\n          f:meta.helm.sh/release-namespace: {}\r\n        f:labels:\r\n          .: {}\r\n          f:app.kubernetes.io/managed-by: {}\r\n      f:spec:\r\n        .: {}\r\n        f:bootstrap:\r\n          .: {}\r\n          f:initdb:\r\n            .: {}\r\n            f:dataChecksums: {}\r\n            f:database: {}\r\n            f:owner: {}\r\n            f:postInitApplicationSQL: {}\r\n        f:enableSuperuserAccess: {}\r\n        f:failoverDelay: {}\r\n        f:imageName: {}\r\n        f:imagePullSecrets: {}\r\n        f:instances: {}\r\n        f:logLevel: {}\r\n        f:maxSyncReplicas: {}\r\n        f:minSyncReplicas: {}\r\n        f:monitoring:\r\n          .: {}\r\n          f:disableDefaultQueries: {}\r\n          f:enablePodMonitor: {}\r\n        f:postgresGID: {}\r\n        f:postgresUID: {}\r\n        f:postgresql:\r\n          .: {}\r\n          f:parameters:\r\n            .: {}\r\n            f:pg_partman_bgw.dbname: {}\r\n          f:pg_hba: {}\r\n          f:shared_preload_libraries: {}\r\n        f:primaryUpdateMethod: {}\r\n        f:primaryUpdateStrategy: {}\r\n        f:replicationSlots:\r\n          .: {}\r\n          f:highAvailability:\r\n            .: {}\r\n            f:enabled: {}\r\n            f:slotPrefix: {}\r\n          f:updateInterval: {}\r\n        f:smartShutdownTimeout: {}\r\n        f:startDelay: {}\r\n        f:stopDelay: {}\r\n        f:storage:\r\n          .: {}\r\n          f:resizeInUseVolumes: {}\r\n          f:size: {}\r\n        f:switchoverDelay: {}\r\n    manager: helm\r\n    operation: Update\r\n    time: \"2024-03-18T07:16:42Z\"\r\n  - apiVersion: postgresql.cnpg.io/v1\r\n    fieldsType: FieldsV1\r\n    fieldsV1:\r\n      f:status:\r\n        .: {}\r\n        f:certificates:\r\n          .: {}\r\n          f:clientCASecret: {}\r\n          f:expirations:\r\n            .: {}\r\n            f:cluster-coordinator-ca: {}\r\n            f:cluster-coordinator-replication: {}\r\n            f:cluster-coordinator-server: {}\r\n          f:replicationTLSSecret: {}\r\n          f:serverAltDNSNames: {}\r\n          f:serverCASecret: {}\r\n          f:serverTLSSecret: {}\r\n        f:cloudNativePGCommitHash: {}\r\n        f:cloudNativePGOperatorHash: {}\r\n        f:conditions: {}\r\n        f:configMapResourceVersion:\r\n          .: {}\r\n          f:metrics:\r\n            .: {}\r\n            f:cnpg-default-monitoring: {}\r\n        f:currentPrimary: {}\r\n        f:currentPrimaryTimestamp: {}\r\n        f:danglingPVC: {}\r\n        f:healthyPVC: {}\r\n        f:instanceNames: {}\r\n        f:instances: {}\r\n        f:instancesReportedState:\r\n          .: {}\r\n          f:cluster-coordinator-1:\r\n            .: {}\r\n            f:isPrimary: {}\r\n        f:instancesStatus:\r\n          .: {}\r\n          f:replicating: {}\r\n        f:latestGeneratedNode: {}\r\n        f:managedRolesStatus: {}\r\n        f:phase: {}\r\n        f:phaseReason: {}\r\n        f:poolerIntegrations:\r\n          .: {}\r\n          f:pgBouncerIntegration: {}\r\n        f:pvcCount: {}\r\n        f:readService: {}\r\n        f:secretsResourceVersion:\r\n          .: {}\r\n          f:applicationSecretVersion: {}\r\n          f:clientCaSecretVersion: {}\r\n          f:replicationSecretVersion: {}\r\n          f:serverCaSecretVersion: {}\r\n          f:serverSecretVersion: {}\r\n        f:targetPrimary: {}\r\n        f:targetPrimaryTimestamp: {}\r\n        f:timelineID: {}\r\n        f:topology:\r\n          .: {}\r\n          f:instances:\r\n            .: {}\r\n            f:cluster-coordinator-1: {}\r\n          f:nodesUsed: {}\r\n          f:successfullyExtracted: {}\r\n        f:writeService: {}\r\n    manager: manager\r\n    operation: Update\r\n    subresource: status\r\n    time: \"2024-03-25T12:43:12Z\"\r\n  - apiVersion: postgresql.cnpg.io/v1\r\n    fieldsType: FieldsV1\r\n    fieldsV1:\r\n      f:metadata:\r\n        f:annotations:\r\n          f:cnpg.io/reloadedAt: {}\r\n          f:kubectl.kubernetes.io/restartedAt: {}\r\n    manager: kubectl-cnpg\r\n    operation: Update\r\n    time: \"2024-03-25T13:13:45Z\"\r\n  name: cluster-coordinator\r\n  namespace: transit-platform\r\n  resourceVersion: \"42962432\"\r\n  uid: 6eb5329b-7596-4890-94c8-1c1c3aa7f5fa\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n  bootstrap:\r\n    initdb:\r\n      dataChecksums: true\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enableSuperuserAccess: false\r\n  imageName: harbor.ridango.io/infra/cloudnative-pg/postgresql-citus:16.2-1\r\n  imagePullSecrets:\r\n  - name: harbor-cred\r\n  instances: 2\r\n  logLevel: info\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      pg_partman_bgw.dbname: app\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    pg_hba:\r\n    - host all all all trust\r\n    shared_preload_libraries:\r\n    - citus\r\n    - pg_partman_bgw\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources: {}\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 20Gi\r\n  switchoverDelay: 3600\r\nstatus:\r\n  certificates:\r\n    clientCASecret: cluster-coordinator-ca\r\n    expirations:\r\n      cluster-coordinator-ca: 2024-06-16 07:11:43 +0000 UTC\r\n      cluster-coordinator-replication: 2024-06-16 07:11:43 +0000 UTC\r\n      cluster-coordinator-server: 2024-06-16 07:11:43 +0000 UTC\r\n    replicationTLSSecret: cluster-coordinator-replication\r\n    serverAltDNSNames:\r\n    - cluster-coordinator-rw\r\n    - cluster-coordinator-rw.transit-platform\r\n    - cluster-coordinator-rw.transit-platform.svc\r\n    - cluster-coordinator-r\r\n    - cluster-coordinator-r.transit-platform\r\n    - cluster-coordinator-r.transit-platform.svc\r\n    - cluster-coordinator-ro\r\n    - cluster-coordinator-ro.transit-platform\r\n    - cluster-coordinator-ro.transit-platform.svc\r\n    serverCASecret: cluster-coordinator-ca\r\n    serverTLSSecret: cluster-coordinator-server\r\n  cloudNativePGCommitHash: 27f62cac\r\n  cloudNativePGOperatorHash: 4912e5eb808aca3bf134923625f2346d404a3c860cf24aca4266493846f8fc3b\r\n  conditions:\r\n  - lastTransitionTime: \"2024-03-22T16:01:27Z\"\r\n    message: Cluster Is Not Ready\r\n    reason: ClusterIsNotReady\r\n    status: \"False\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-03-18T07:18:07Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"3872787\"\r\n  currentPrimary: cluster-coordinator-2\r\n  currentPrimaryTimestamp: \"2024-03-22T16:06:41.914856Z\"\r\n  danglingPVC:\r\n  - cluster-coordinator-2\r\n  healthyPVC:\r\n  - cluster-coordinator-1\r\n  instanceNames:\r\n  - cluster-coordinator-1\r\n  - cluster-coordinator-2\r\n  instances: 2\r\n  instancesReportedState:\r\n    cluster-coordinator-1:\r\n      isPrimary: false\r\n  instancesStatus:\r\n    replicating:\r\n    - cluster-coordinator-1\r\n  latestGeneratedNode: 2\r\n  managedRolesStatus: {}\r\n  phase: Waiting for the instances to become active\r\n  phaseReason: Some instances are not yet active. Please wait.\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 2\r\n  readService: cluster-coordinator-r\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"38676142\"\r\n    clientCaSecretVersion: \"38676135\"\r\n    replicationSecretVersion: \"38676140\"\r\n    serverCaSecretVersion: \"38676135\"\r\n    serverSecretVersion: \"38676137\"\r\n  targetPrimary: cluster-coordinator-2\r\n  targetPrimaryTimestamp: \"2024-03-25T12:39:33.230797Z\"\r\n  timelineID: 2\r\n  topology:\r\n    instances:\r\n      cluster-coordinator-1: {}\r\n    nodesUsed: 1\r\n    successfullyExtracted: true\r\n  writeService: cluster-coordinator-rw\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ngmatej@gmail.com\n### Version\nolder in 1.21.x\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Amazon EKS\n### How did you install the operator?\nHelm\n### What happened?\nK8 worker run out of resources and died, leaving couple of CNPG clusters not working. The problem is that primary pods disappeared. When looking the report it says:\r\n  danglingPVC:\r\n  - cluster-coordinator-2\r\n  healthyPVC:\r\n  - cluster-coordinator-1\r\n  instanceNames:\r\n  - cluster-coordinator-1\r\n  - cluster-coordinator-2\r\nI know that maybe the latest CNPG version would fix this. But I would like to know what danglingPVC actually means. On k8 level I can't see anything worng with the PVC or volume.\r\nI would say that because this dangling flag the pod dosn't even start. Anyway I could force it to do so. \n### Cluster resource\n```shell\nmetadata:\r\n  annotations:\r\n    cnpg.io/reloadedAt: \"2024-03-25T14:13:45.196615+01:00\"\r\n    kubectl.kubernetes.io/restartedAt: \"2024-03-25T14:11:47+01:00\"\r\n    meta.helm.sh/release-name: cluster\r\n    meta.helm.sh/release-namespace: transit-platform\r\n  creationTimestamp: \"2024-03-18T07:16:42Z\"\r\n  generation: 1\r\n  labels:\r\n    app.kubernetes.io/managed-by: Helm\r\n  managedFields:\r\n  - apiVersion: postgresql.cnpg.io/v1\r\n    fieldsType: FieldsV1\r\n    fieldsV1:\r\n      f:metadata:\r\n        f:annotations:\r\n          .: {}\r\n          f:meta.helm.sh/release-name: {}\r\n          f:meta.helm.sh/release-namespace: {}\r\n        f:labels:\r\n          .: {}\r\n          f:app.kubernetes.io/managed-by: {}\r\n      f:spec:\r\n        .: {}\r\n        f:bootstrap:\r\n          .: {}\r\n          f:initdb:\r\n            .: {}\r\n            f:dataChecksums: {}\r\n            f:database: {}\r\n            f:owner: {}\r\n            f:postInitApplicationSQL: {}\r\n        f:enableSuperuserAccess: {}\r\n        f:failoverDelay: {}\r\n        f:imageName: {}\r\n        f:imagePullSecrets: {}\r\n        f:instances: {}\r\n        f:logLevel: {}\r\n        f:maxSyncReplicas: {}\r\n        f:minSyncReplicas: {}\r\n        f:monitoring:\r\n          .: {}\r\n          f:disableDefaultQueries: {}\r\n          f:enablePodMonitor: {}\r\n        f:postgresGID: {}\r\n        f:postgresUID: {}\r\n        f:postgresql:\r\n          .: {}\r\n          f:parameters:\r\n            .: {}\r\n            f:pg_partman_bgw.dbname: {}\r\n          f:pg_hba: {}\r\n          f:shared_preload_libraries: {}\r\n        f:primaryUpdateMethod: {}\r\n        f:primaryUpdateStrategy: {}\r\n        f:replicationSlots:\r\n          .: {}\r\n          f:highAvailability:\r\n            .: {}\r\n            f:enabled: {}\r\n            f:slotPrefix: {}\r\n          f:updateInterval: {}\r\n        f:smartShutdownTimeout: {}\r\n        f:startDelay: {}\r\n        f:stopDelay: {}\r\n        f:storage:\r\n          .: {}\r\n          f:resizeInUseVolumes: {}\r\n          f:size: {}\r\n        f:switchoverDelay: {}\r\n    manager: helm\r\n    operation: Update\r\n    time: \"2024-03-18T07:16:42Z\"\r\n  - apiVersion: postgresql.cnpg.io/v1\r\n    fieldsType: FieldsV1\r\n    fieldsV1:\r\n      f:status:\r\n        .: {}\r\n        f:certificates:\r\n          .: {}\r\n          f:clientCASecret: {}\r\n          f:expirations:\r\n            .: {}\r\n            f:cluster-coordinator-ca: {}\r\n            f:cluster-coordinator-replication: {}\r\n            f:cluster-coordinator-server: {}\r\n          f:replicationTLSSecret: {}\r\n          f:serverAltDNSNames: {}\r\n          f:serverCASecret: {}\r\n          f:serverTLSSecret: {}\r\n        f:cloudNativePGCommitHash: {}\r\n        f:cloudNativePGOperatorHash: {}\r\n        f:conditions: {}\r\n        f:configMapResourceVersion:\r\n          .: {}\r\n          f:metrics:\r\n            .: {}\r\n            f:cnpg-default-monitoring: {}\r\n        f:currentPrimary: {}\r\n        f:currentPrimaryTimestamp: {}\r\n        f:danglingPVC: {}\r\n        f:healthyPVC: {}\r\n        f:instanceNames: {}\r\n        f:instances: {}\r\n        f:instancesReportedState:\r\n          .: {}\r\n          f:cluster-coordinator-1:\r\n            .: {}\r\n            f:isPrimary: {}\r\n        f:instancesStatus:\r\n          .: {}\r\n          f:replicating: {}\r\n        f:latestGeneratedNode: {}\r\n        f:managedRolesStatus: {}\r\n        f:phase: {}\r\n        f:phaseReason: {}\r\n        f:poolerIntegrations:\r\n          .: {}\r\n          f:pgBouncerIntegration: {}\r\n        f:pvcCount: {}\r\n        f:readService: {}\r\n        f:secretsResourceVersion:\r\n          .: {}\r\n          f:applicationSecretVersion: {}\r\n          f:clientCaSecretVersion: {}\r\n          f:replicationSecretVersion: {}\r\n          f:serverCaSecretVersion: {}\r\n          f:serverSecretVersion: {}\r\n        f:targetPrimary: {}\r\n        f:targetPrimaryTimestamp: {}\r\n        f:timelineID: {}\r\n        f:topology:\r\n          .: {}\r\n          f:instances:\r\n            .: {}\r\n            f:cluster-coordinator-1: {}\r\n          f:nodesUsed: {}\r\n          f:successfullyExtracted: {}\r\n        f:writeService: {}\r\n    manager: manager\r\n    operation: Update\r\n    subresource: status\r\n    time: \"2024-03-25T12:43:12Z\"\r\n  - apiVersion: postgresql.cnpg.io/v1\r\n    fieldsType: FieldsV1\r\n    fieldsV1:\r\n      f:metadata:\r\n        f:annotations:\r\n          f:cnpg.io/reloadedAt: {}\r\n          f:kubectl.kubernetes.io/restartedAt: {}\r\n    manager: kubectl-cnpg\r\n    operation: Update\r\n    time: \"2024-03-25T13:13:45Z\"\r\n  name: cluster-coordinator\r\n  namespace: transit-platform\r\n  resourceVersion: \"42962432\"\r\n  uid: 6eb5329b-7596-4890-94c8-1c1c3aa7f5fa\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n  bootstrap:\r\n    initdb:\r\n      dataChecksums: true\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enableSuperuserAccess: false\r\n  imageName: harbor.ridango.io/infra/cloudnative-pg/postgresql-citus:16.2-1\r\n  imagePullSecrets:\r\n  - name: harbor-cred\r\n  instances: 2\r\n  logLevel: info\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      pg_partman_bgw.dbname: app\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    pg_hba:\r\n    - host all all all trust\r\n    shared_preload_libraries:\r\n    - citus\r\n    - pg_partman_bgw\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources: {}\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 20Gi\r\n  switchoverDelay: 3600\r\nstatus:\r\n  certificates:\r\n    clientCASecret: cluster-coordinator-ca\r\n    expirations:\r\n      cluster-coordinator-ca: 2024-06-16 07:11:43 +0000 UTC\r\n      cluster-coordinator-replication: 2024-06-16 07:11:43 +0000 UTC\r\n      cluster-coordinator-server: 2024-06-16 07:11:43 +0000 UTC\r\n    replicationTLSSecret: cluster-coordinator-replication\r\n    serverAltDNSNames:\r\n    - cluster-coordinator-rw\r\n    - cluster-coordinator-rw.transit-platform\r\n    - cluster-coordinator-rw.transit-platform.svc\r\n    - cluster-coordinator-r\r\n    - cluster-coordinator-r.transit-platform\r\n    - cluster-coordinator-r.transit-platform.svc\r\n    - cluster-coordinator-ro\r\n    - cluster-coordinator-ro.transit-platform\r\n    - cluster-coordinator-ro.transit-platform.svc\r\n    serverCASecret: cluster-coordinator-ca\r\n    serverTLSSecret: cluster-coordinator-server\r\n  cloudNativePGCommitHash: 27f62cac\r\n  cloudNativePGOperatorHash: 4912e5eb808aca3bf134923625f2346d404a3c860cf24aca4266493846f8fc3b\r\n  conditions:\r\n  - lastTransitionTime: \"2024-03-22T16:01:27Z\"\r\n    message: Cluster Is Not Ready\r\n    reason: ClusterIsNotReady\r\n    status: \"False\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-03-18T07:18:07Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"3872787\"\r\n  currentPrimary: cluster-coordinator-2\r\n  currentPrimaryTimestamp: \"2024-03-22T16:06:41.914856Z\"\r\n  danglingPVC:\r\n  - cluster-coordinator-2\r\n  healthyPVC:\r\n  - cluster-coordinator-1\r\n  instanceNames:\r\n  - cluster-coordinator-1\r\n  - cluster-coordinator-2\r\n  instances: 2\r\n  instancesReportedState:\r\n    cluster-coordinator-1:\r\n      isPrimary: false\r\n  instancesStatus:\r\n    replicating:\r\n    - cluster-coordinator-1\r\n  latestGeneratedNode: 2\r\n  managedRolesStatus: {}\r\n  phase: Waiting for the instances to become active\r\n  phaseReason: Some instances are not yet active. Please wait.\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 2\r\n  readService: cluster-coordinator-r\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"38676142\"\r\n    clientCaSecretVersion: \"38676135\"\r\n    replicationSecretVersion: \"38676140\"\r\n    serverCaSecretVersion: \"38676135\"\r\n    serverSecretVersion: \"38676137\"\r\n  targetPrimary: cluster-coordinator-2\r\n  targetPrimaryTimestamp: \"2024-03-25T12:39:33.230797Z\"\r\n  timelineID: 2\r\n  topology:\r\n    instances:\r\n      cluster-coordinator-1: {}\r\n    nodesUsed: 1\r\n    successfullyExtracted: true\r\n  writeService: cluster-coordinator-rw\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI have the same state for my cluster. My cluster isn't that critical so I haven't looked at the problem until today, but it has been like this for 6 weeks. Today I have tried upgrade the operator to version 1.22.1 and the instance image to 15.6-10 without any difference. Would you mind uploading the logs from the operator and from the instance number 1?\n---\nI solved my problem by scaling down to 1 instance and deleted the pvc for the instance 2. Then I used the `kubectl` plugin `edit-status` and changed the `currentPrimary` and `targetPrimary` in the status section to refer to the instance 1. After saving the changes the instance 1 started up. After scaling up to 2 instances I got an instance number 3. After a while of syncing the data I now have two instances and everything seems to work fine.\r\nTo check the status I use the kubectl plugin `cnpg` by running the command `kubectl cnpg -n postgresql-db status db`."
    },
    {
        "title": "[Bug]: Error while bootstrapping data directory",
        "id": 2205626293,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.22.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nRunning the following manifest:\r\n```yaml\r\napiVersion: v1\r\nkind: Secret\r\nmetadata:\r\n  name: plausible-postgres-credentials\r\n  namespace: plausible\r\ntype: kubernetes.io/basic-auth\r\nstringData:\r\n  username: plausible\r\n  password: redacted\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: postgres\r\n  namespace: plausible\r\nspec:\r\n  instances: 2\r\n  imageName: postgres:14.11\r\n  primaryUpdateStrategy: unsupervised\r\n  storage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 1Gi\r\n      storageClassName: standard-rwo\r\n  enableSuperuserAccess: false\r\n  superuserSecret:\r\n    name: redacted\r\n  bootstrap:\r\n    initdb:\r\n      database: plausible\r\n      owner: plausible\r\n      secret:\r\n        name: plausible-postgres-credentials\r\n  postgresql:\r\n    parameters:\r\n      timezone: \"Europe/London\"\r\n```\r\nresults in this error:\r\n```bash\r\npod/pg-cluster-1-initdb-dr2t8   0/1     Error      0              31s\r\npod/pg-cluster-1-initdb-mtcs8   0/1     Error      0              72s\r\n```\r\nwith logs for the container:\r\n```bash\r\nDefaulted container \"initdb\" out of: initdb, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T10:31:57Z\",\"msg\":\"Creating new data directory\",\"logging_pod\":\"pg-cluster-1-initdb\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"initDbOptions\":[\"--username\",\"postgres\",\"-D\",\"/var/lib/postgresql/data/pgdata\",\"--encoding=UTF8\",\"--lc-collate=C\",\"--lc-ctype=C\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T10:31:57Z\",\"logger\":\"initdb\",\"msg\":\"initdb: could not look up effective user ID 26: user does not exist\\n\",\"pipe\":\"stderr\",\"logging_pod\":\"pg-cluster-1-initdb\"}\r\n{\"level\":\"error\",\"ts\":\"2024-03-25T10:31:57Z\",\"msg\":\"Error while bootstrapping data directory\",\"logging_pod\":\"pg-cluster-1-initdb\",\"error\":\"error while creating the PostgreSQL instance: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.initSubCommand\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:151\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.NewCmd.func2\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:104\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.8/x64/src/runtime/proc.go:267\"}\r\nError: error while creating the PostgreSQL instance: exit status 1\r\n```\n### Cluster resource\n```shell\napiVersion: v1\r\nkind: Secret\r\nmetadata:\r\n  name: plausible-postgres-credentials\r\n  namespace: plausible\r\ntype: kubernetes.io/basic-auth\r\nstringData:\r\n  username: plausible\r\n  password: redacted\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: postgres\r\n  namespace: plausible\r\nspec:\r\n  instances: 2\r\n  imageName: postgres:14.11\r\n  primaryUpdateStrategy: unsupervised\r\n  storage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 1Gi\r\n      storageClassName: standard-rwo\r\n  enableSuperuserAccess: false\r\n  superuserSecret:\r\n    name: redacted\r\n  bootstrap:\r\n    initdb:\r\n      database: plausible\r\n      owner: plausible\r\n      secret:\r\n        name: plausible-postgres-credentials\r\n  postgresql:\r\n    parameters:\r\n      timezone: \"Europe/London\"\n```\n### Relevant log output\n```shell\nDefaulted container \"initdb\" out of: initdb, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T10:31:57Z\",\"msg\":\"Creating new data directory\",\"logging_pod\":\"pg-cluster-1-initdb\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"initDbOptions\":[\"--username\",\"postgres\",\"-D\",\"/var/lib/postgresql/data/pgdata\",\"--encoding=UTF8\",\"--lc-collate=C\",\"--lc-ctype=C\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T10:31:57Z\",\"logger\":\"initdb\",\"msg\":\"initdb: could not look up effective user ID 26: user does not exist\\n\",\"pipe\":\"stderr\",\"logging_pod\":\"pg-cluster-1-initdb\"}\r\n{\"level\":\"error\",\"ts\":\"2024-03-25T10:31:57Z\",\"msg\":\"Error while bootstrapping data directory\",\"logging_pod\":\"pg-cluster-1-initdb\",\"error\":\"error while creating the PostgreSQL instance: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.initSubCommand\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:151\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.NewCmd.func2\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:104\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.8/x64/src/runtime/proc.go:267\"}\r\nError: error while creating the PostgreSQL instance: exit status 1\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.22.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nCloud: Google GKE\n### How did you install the operator?\nYAML manifest\n### What happened?\nRunning the following manifest:\r\n```yaml\r\napiVersion: v1\r\nkind: Secret\r\nmetadata:\r\n  name: plausible-postgres-credentials\r\n  namespace: plausible\r\ntype: kubernetes.io/basic-auth\r\nstringData:\r\n  username: plausible\r\n  password: redacted\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: postgres\r\n  namespace: plausible\r\nspec:\r\n  instances: 2\r\n  imageName: postgres:14.11\r\n  primaryUpdateStrategy: unsupervised\r\n  storage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 1Gi\r\n      storageClassName: standard-rwo\r\n  enableSuperuserAccess: false\r\n  superuserSecret:\r\n    name: redacted\r\n  bootstrap:\r\n    initdb:\r\n      database: plausible\r\n      owner: plausible\r\n      secret:\r\n        name: plausible-postgres-credentials\r\n  postgresql:\r\n    parameters:\r\n      timezone: \"Europe/London\"\r\n```\r\nresults in this error:\r\n```bash\r\npod/pg-cluster-1-initdb-dr2t8   0/1     Error      0              31s\r\npod/pg-cluster-1-initdb-mtcs8   0/1     Error      0              72s\r\n```\r\nwith logs for the container:\r\n```bash\r\nDefaulted container \"initdb\" out of: initdb, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T10:31:57Z\",\"msg\":\"Creating new data directory\",\"logging_pod\":\"pg-cluster-1-initdb\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"initDbOptions\":[\"--username\",\"postgres\",\"-D\",\"/var/lib/postgresql/data/pgdata\",\"--encoding=UTF8\",\"--lc-collate=C\",\"--lc-ctype=C\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T10:31:57Z\",\"logger\":\"initdb\",\"msg\":\"initdb: could not look up effective user ID 26: user does not exist\\n\",\"pipe\":\"stderr\",\"logging_pod\":\"pg-cluster-1-initdb\"}\r\n{\"level\":\"error\",\"ts\":\"2024-03-25T10:31:57Z\",\"msg\":\"Error while bootstrapping data directory\",\"logging_pod\":\"pg-cluster-1-initdb\",\"error\":\"error while creating the PostgreSQL instance: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.initSubCommand\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:151\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.NewCmd.func2\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:104\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.8/x64/src/runtime/proc.go:267\"}\r\nError: error while creating the PostgreSQL instance: exit status 1\r\n```\n### Cluster resource\n```shell\napiVersion: v1\r\nkind: Secret\r\nmetadata:\r\n  name: plausible-postgres-credentials\r\n  namespace: plausible\r\ntype: kubernetes.io/basic-auth\r\nstringData:\r\n  username: plausible\r\n  password: redacted\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: postgres\r\n  namespace: plausible\r\nspec:\r\n  instances: 2\r\n  imageName: postgres:14.11\r\n  primaryUpdateStrategy: unsupervised\r\n  storage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 1Gi\r\n      storageClassName: standard-rwo\r\n  enableSuperuserAccess: false\r\n  superuserSecret:\r\n    name: redacted\r\n  bootstrap:\r\n    initdb:\r\n      database: plausible\r\n      owner: plausible\r\n      secret:\r\n        name: plausible-postgres-credentials\r\n  postgresql:\r\n    parameters:\r\n      timezone: \"Europe/London\"\n```\n### Relevant log output\n```shell\nDefaulted container \"initdb\" out of: initdb, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T10:31:57Z\",\"msg\":\"Creating new data directory\",\"logging_pod\":\"pg-cluster-1-initdb\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"initDbOptions\":[\"--username\",\"postgres\",\"-D\",\"/var/lib/postgresql/data/pgdata\",\"--encoding=UTF8\",\"--lc-collate=C\",\"--lc-ctype=C\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T10:31:57Z\",\"logger\":\"initdb\",\"msg\":\"initdb: could not look up effective user ID 26: user does not exist\\n\",\"pipe\":\"stderr\",\"logging_pod\":\"pg-cluster-1-initdb\"}\r\n{\"level\":\"error\",\"ts\":\"2024-03-25T10:31:57Z\",\"msg\":\"Error while bootstrapping data directory\",\"logging_pod\":\"pg-cluster-1-initdb\",\"error\":\"error while creating the PostgreSQL instance: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.initSubCommand\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:151\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.NewCmd.func2\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:104\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:66\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.8/x64/src/runtime/proc.go:267\"}\r\nError: error while creating the PostgreSQL instance: exit status 1\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductIf I understook well, you just have to use :\r\n```yaml\r\nimageName: ghcr.io/cloudnative-pg/postgresql:14.11\r\n```\r\ninstead of :\r\n```yaml\r\nimageName: postgres:14.11\r\n```\n---\nI was getting the same error, but in my case I was using\r\n```yaml\r\nimageName: ghcr.io/cloudnative-pg/postgresql:17rc1\r\n```\r\nso I updated it to the release one\r\n```yaml\r\nimageName: ghcr.io/cloudnative-pg/postgresql:17\r\n```\r\nand now the bootstrap works."
    },
    {
        "title": "[Bug]: Silent failures in controller",
        "id": 2205576981,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.22.2\r\n### What version of Kubernetes are you using?\r\n1.26\r\n### What is your Kubernetes environment?\r\nSelf-managed: RKE\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\nAfter a fresh deployment of the operator, the contoller pod is crashlooping without any indication why:\r\n```\r\n    Last State:     Terminated\r\n      Reason:       Error\r\n      Exit Code:    2\r\n      Started:      Mon, 25 Mar 2024 11:54:08 +0000\r\n      Finished:     Mon, 25 Mar 2024 11:54:37 +0000\r\n``` \r\nI have installed with \r\n```\r\nkubectl apply --server-side -f \\\r\n  https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.22/releases/cnpg-1.22.2.yaml\r\n```\r\naccording to https://cloudnative-pg.io/documentation/1.22/installation_upgrade/\r\nI think you need to print the error before os.Exit() here : https://github.com/cloudnative-pg/cloudnative-pg/blob/fa3ed188f8fa230b9f447b9b3c3886a4a1f7dbbd/cmd/manager/main.go#L67\r\nOtherwise the relevant error is silently forgotten. Although, if I read GO right, exit code 2 is an unofficial panic symptom, so probably needs panic handling first.\r\n### Cluster resource\r\n```shell\r\nN/A\r\n```\r\n### Relevant log output\r\n```shell\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T11:17:06Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Operator\",\"version\":\"1.22.2\",\"build\":{\"Version\":\"1.22.2\",\"Commit\":\"bcdcd885\",\"Date\":\"2024-03-14\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T11:17:06Z\",\"logger\":\"setup\",\"msg\":\"Listening for changes on all namespaces\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T11:17:06Z\",\"logger\":\"setup\",\"msg\":\"Loading configuration from ConfigMap\",\"namespace\":\"cnpg-system\",\"name\":\"cnpg-controller-manager-config\"}\r\nThe cnpg-controller-manager-config does not exist!\r\nAfter removing the --config-map-name and --secret-name args we get further:\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T11:25:58Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Operator\",\"version\":\"1.22.2\",\"build\":{\"Version\":\"1.22.2\",\"Commit\":\"bcdcd885\",\"Date\":\"2024-03-14\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T11:25:58Z\",\"logger\":\"setup\",\"msg\":\"Listening for changes on all namespaces\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T11:25:58Z\",\"logger\":\"setup\",\"msg\":\"Operator configuration loaded\",\"configuration\":{\"webhookCertDir\":\"\",\"watchNamespace\":\"\",\"operatorNamespace\":\"cnpg-system\",\"operatorPullSecretName\":\"cnpg-pull-secret\",\"operatorImageName\":\"ghcr.io/cloudnative-pg/cloudnative-pg:1.22.2\",\"postgresImageName\":\"ghcr.io/cloudnative-pg/postgresql:16.2\",\"inheritedAnnotations\":null,\"inheritedLabels\":null,\"monitoringQueriesConfigmap\":\"cnpg-default-monitoring\",\"monitoringQueriesSecret\":\"\",\"enableInstanceManagerInplaceUpdates\":false,\"enableAzurePVCUpdates\":false,\"enablePodDebugging\":false,\"certificateDuration\":90,\"expiringCheckThreshold\":7,\"createAnyService\":false}}\r\nBut again pod terminated, no explanation.\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.22.2\r\n### What version of Kubernetes are you using?\r\n1.26\r\n### What is your Kubernetes environment?\r\nSelf-managed: RKE\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\nAfter a fresh deployment of the operator, the contoller pod is crashlooping without any indication why:\r\n```\r\n    Last State:     Terminated\r\n      Reason:       Error\r\n      Exit Code:    2\r\n      Started:      Mon, 25 Mar 2024 11:54:08 +0000\r\n      Finished:     Mon, 25 Mar 2024 11:54:37 +0000\r\n``` \r\nI have installed with \r\n```\r\nkubectl apply --server-side -f \\\r\n  https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.22/releases/cnpg-1.22.2.yaml\r\n```\r\naccording to https://cloudnative-pg.io/documentation/1.22/installation_upgrade/\r\nI think you need to print the error before os.Exit() here : https://github.com/cloudnative-pg/cloudnative-pg/blob/fa3ed188f8fa230b9f447b9b3c3886a4a1f7dbbd/cmd/manager/main.go#L67\r\nOtherwise the relevant error is silently forgotten. Although, if I read GO right, exit code 2 is an unofficial panic symptom, so probably needs panic handling first.\r\n### Cluster resource\r\n```shell\r\nN/A\r\n```\r\n### Relevant log output\r\n```shell\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T11:17:06Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Operator\",\"version\":\"1.22.2\",\"build\":{\"Version\":\"1.22.2\",\"Commit\":\"bcdcd885\",\"Date\":\"2024-03-14\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T11:17:06Z\",\"logger\":\"setup\",\"msg\":\"Listening for changes on all namespaces\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T11:17:06Z\",\"logger\":\"setup\",\"msg\":\"Loading configuration from ConfigMap\",\"namespace\":\"cnpg-system\",\"name\":\"cnpg-controller-manager-config\"}\r\nThe cnpg-controller-manager-config does not exist!\r\nAfter removing the --config-map-name and --secret-name args we get further:\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T11:25:58Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Operator\",\"version\":\"1.22.2\",\"build\":{\"Version\":\"1.22.2\",\"Commit\":\"bcdcd885\",\"Date\":\"2024-03-14\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T11:25:58Z\",\"logger\":\"setup\",\"msg\":\"Listening for changes on all namespaces\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-25T11:25:58Z\",\"logger\":\"setup\",\"msg\":\"Operator configuration loaded\",\"configuration\":{\"webhookCertDir\":\"\",\"watchNamespace\":\"\",\"operatorNamespace\":\"cnpg-system\",\"operatorPullSecretName\":\"cnpg-pull-secret\",\"operatorImageName\":\"ghcr.io/cloudnative-pg/cloudnative-pg:1.22.2\",\"postgresImageName\":\"ghcr.io/cloudnative-pg/postgresql:16.2\",\"inheritedAnnotations\":null,\"inheritedLabels\":null,\"monitoringQueriesConfigmap\":\"cnpg-default-monitoring\",\"monitoringQueriesSecret\":\"\",\"enableInstanceManagerInplaceUpdates\":false,\"enableAzurePVCUpdates\":false,\"enablePodDebugging\":false,\"certificateDuration\":90,\"expiringCheckThreshold\":7,\"createAnyService\":false}}\r\nBut again pod terminated, no explanation.\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Allow to specify roles when using the \"Cluster\" chart",
        "id": 2204293157,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\n[The cluster chart](https://github.com/cloudnative-pg/charts/blob/main/charts/cluster/README.md) is helpful for getting started with cnpg. However, it does not offer a way to modify the database roles (via Cluster's [`spec.managed.roles`](https://cloudnative-pg.io/documentation/1.22/cloudnative-pg.v1/#postgresql-cnpg-io-v1-RoleConfiguration)).\r\n### Describe the solution you'd like\nI would be helpful if `spec.managed.roles` could be configured via the `cluster` option in `values.yaml`.\n### Describe alternatives you've considered\nCurrently, as a workaround, I've pre-rendered the chart files and then modified them manually. to add the managed roles.\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\n[The cluster chart](https://github.com/cloudnative-pg/charts/blob/main/charts/cluster/README.md) is helpful for getting started with cnpg. However, it does not offer a way to modify the database roles (via Cluster's [`spec.managed.roles`](https://cloudnative-pg.io/documentation/1.22/cloudnative-pg.v1/#postgresql-cnpg-io-v1-RoleConfiguration)).\r\n### Describe the solution you'd like\nI would be helpful if `spec.managed.roles` could be configured via the `cluster` option in `values.yaml`.\n### Describe alternatives you've considered\nCurrently, as a workaround, I've pre-rendered the chart files and then modified them manually. to add the managed roles.\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductHi, there's actually a PR for that: https://github.com/cloudnative-pg/charts/pull/247"
    },
    {
        "title": "[Bug]: Fenced instance on the status even when pod doesn't exist",
        "id": 2199941676,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.22.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nAfter fencing the node number `cluster-example-3` using the cnpg plugin and the pod cluster-example-3 was destroyed using the cnpg plugin the pod still being show in the status as fenced\r\n```Cluster Summary\r\nName:                cluster-example\r\nNamespace:           default\r\nSystem ID:           7348774010380202014\r\nPostgreSQL Image:    ghcr.io/cloudnative-pg/postgresql:16.2\r\nPrimary instance:    cluster-example-2\r\nPrimary start time:  2024-03-21 11:24:41 +0000 UTC (uptime 9m12s)\r\nStatus:              Cluster in healthy state \r\nInstances:           3\r\nReady instances:     3\r\nFenced instances:    cluster-example-3\r\nCurrent Write LSN:   0/9019A20 (Timeline: 2 - WAL File: 000000020000000000000009)\r\nCertificates Status\r\nCertificate Name             Expiration Date                Days Left Until Expiration\r\n----------------             ---------------                --------------------------\r\ncluster-example-ca           2024-06-19 11:07:55 +0000 UTC  89.98\r\ncluster-example-replication  2024-06-19 11:07:55 +0000 UTC  89.98\r\ncluster-example-server       2024-06-19 11:07:55 +0000 UTC  89.98\r\nContinuous Backup status\r\nNot configured\r\nPhysical backups\r\nNo running physical backups found\r\nStreaming Replication status\r\nReplication Slots Enabled\r\nName               Sent LSN   Write LSN  Flush LSN  Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority  Replication Slot\r\n----               --------   ---------  ---------  ----------  ---------  ---------  ----------  -----      ----------  -------------  ----------------\r\ncluster-example-1  0/9019A20  0/9019A20  0/9019A20  0/9019A20   00:00:00   00:00:00   00:00:00    streaming  async       0              active\r\ncluster-example-4  0/9019A20  0/9019A20  0/9019A20  0/9019A20   00:00:00   00:00:00   00:00:00    streaming  async       0              active\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nManaged roles status\r\nNo roles managed\r\nTablespaces status\r\nNo managed tablespaces\r\nInstances status\r\nName               Database Size  Current LSN  Replication role  Status  QoS         Manager Version  Node\r\n----               -------------  -----------  ----------------  ------  ---         ---------------  ----\r\ncluster-example-2  29 MB          0/9019A20    Primary           OK      BestEffort  1.22.2           pg-operator-e2e-v1-29-2-worker3\r\ncluster-example-1  29 MB          0/9019A20    Standby (async)   OK      BestEffort  1.22.2           pg-operator-e2e-v1-29-2-worker\r\ncluster-example-4  29 MB          0/9019A20    Standby (async)   OK      BestEffort  1.22.2           pg-operator-e2e-v1-29-2-worker2\r\n```\r\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.22.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nAfter fencing the node number `cluster-example-3` using the cnpg plugin and the pod cluster-example-3 was destroyed using the cnpg plugin the pod still being show in the status as fenced\r\n```Cluster Summary\r\nName:                cluster-example\r\nNamespace:           default\r\nSystem ID:           7348774010380202014\r\nPostgreSQL Image:    ghcr.io/cloudnative-pg/postgresql:16.2\r\nPrimary instance:    cluster-example-2\r\nPrimary start time:  2024-03-21 11:24:41 +0000 UTC (uptime 9m12s)\r\nStatus:              Cluster in healthy state \r\nInstances:           3\r\nReady instances:     3\r\nFenced instances:    cluster-example-3\r\nCurrent Write LSN:   0/9019A20 (Timeline: 2 - WAL File: 000000020000000000000009)\r\nCertificates Status\r\nCertificate Name             Expiration Date                Days Left Until Expiration\r\n----------------             ---------------                --------------------------\r\ncluster-example-ca           2024-06-19 11:07:55 +0000 UTC  89.98\r\ncluster-example-replication  2024-06-19 11:07:55 +0000 UTC  89.98\r\ncluster-example-server       2024-06-19 11:07:55 +0000 UTC  89.98\r\nContinuous Backup status\r\nNot configured\r\nPhysical backups\r\nNo running physical backups found\r\nStreaming Replication status\r\nReplication Slots Enabled\r\nName               Sent LSN   Write LSN  Flush LSN  Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority  Replication Slot\r\n----               --------   ---------  ---------  ----------  ---------  ---------  ----------  -----      ----------  -------------  ----------------\r\ncluster-example-1  0/9019A20  0/9019A20  0/9019A20  0/9019A20   00:00:00   00:00:00   00:00:00    streaming  async       0              active\r\ncluster-example-4  0/9019A20  0/9019A20  0/9019A20  0/9019A20   00:00:00   00:00:00   00:00:00    streaming  async       0              active\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nManaged roles status\r\nNo roles managed\r\nTablespaces status\r\nNo managed tablespaces\r\nInstances status\r\nName               Database Size  Current LSN  Replication role  Status  QoS         Manager Version  Node\r\n----               -------------  -----------  ----------------  ------  ---         ---------------  ----\r\ncluster-example-2  29 MB          0/9019A20    Primary           OK      BestEffort  1.22.2           pg-operator-e2e-v1-29-2-worker3\r\ncluster-example-1  29 MB          0/9019A20    Standby (async)   OK      BestEffort  1.22.2           pg-operator-e2e-v1-29-2-worker\r\ncluster-example-4  29 MB          0/9019A20    Standby (async)   OK      BestEffort  1.22.2           pg-operator-e2e-v1-29-2-worker2\r\n```\r\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: cluster creation fails with error \"Selected PVC is not ready yet, waiting for 1 second\" ",
        "id": 2199385601,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nhai.he@enterprisedb.com\n### Version\n1.22.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\n1. Create a Cluster like below:\r\n```\r\napiVersion: postgresql.k8s.enterprisedb.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-example\r\nspec:\r\n  instances: 1\r\n  storage:\r\n    size: 1Gi\r\n  walStorage:\r\n    size: 1Gi\r\n```\r\n2. Wait a short time (2s or so) before the `cluster-example-1-initdb`  job to finish off (which will, in the end, add the label of `k8s.enterprisedb.io/pvcStatus: ready to the PVC`) and scale down the operator deployment to 0 replicas immediately: `kubectl -n postgresql-operator-system scale deployment postgresql-operator-controller-manager --replicas=0`\r\n3. Delete the initdb job: \r\n   `kubectl delete jobs.batch cluster-example-1-initdb`\r\n4. Scale up the operator deployment to 1 replica:\r\n    `kubectl -n postgresql-operator-system scale deployment postgresql-operator-controller-manager --replicas=1`\r\n   Then the cluster will be stuck in `Setting up primary` and the the operator logs will keep emitting messages of `Selected PVC is not ready yet, waiting for 1 second`\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nhai.he@enterprisedb.com\n### Version\n1.22.2\n### What version of Kubernetes are you using?\n1.29\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\n1. Create a Cluster like below:\r\n```\r\napiVersion: postgresql.k8s.enterprisedb.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-example\r\nspec:\r\n  instances: 1\r\n  storage:\r\n    size: 1Gi\r\n  walStorage:\r\n    size: 1Gi\r\n```\r\n2. Wait a short time (2s or so) before the `cluster-example-1-initdb`  job to finish off (which will, in the end, add the label of `k8s.enterprisedb.io/pvcStatus: ready to the PVC`) and scale down the operator deployment to 0 replicas immediately: `kubectl -n postgresql-operator-system scale deployment postgresql-operator-controller-manager --replicas=0`\r\n3. Delete the initdb job: \r\n   `kubectl delete jobs.batch cluster-example-1-initdb`\r\n4. Scale up the operator deployment to 1 replica:\r\n    `kubectl -n postgresql-operator-system scale deployment postgresql-operator-controller-manager --replicas=1`\r\n   Then the cluster will be stuck in `Setting up primary` and the the operator logs will keep emitting messages of `Selected PVC is not ready yet, waiting for 1 second`\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductHi\r\nHaving the same problem with Longhorn (V2 Engine)\r\nVolume is READY but I'm getting `Selected PVC is not ready yet, waiting for 1 second` is operator logs\n---\nI am seeing this also. In my case, a validating admission web hook is preventing the controller manager from being able to create the cluster's initdb job. The controller manager never retries creating the job and the cluster gets stuck in the \"Setting up primary\" state.\r\nController manager logs:\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-09-26T12:53:19Z\",\"msg\":\"Creating new Job\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"sam2\",\"namespace\":\"sam-sandbox\"},\"namespace\":\"sam-sandbox\",\"name\":\"sam2\",\"reconcileID\":\"5d0ccee9-74bc-4ed2-8dcf-1a03da82675e\",\"name\":\"sam2-1-initdb\",\"primary\":true}\r\n{\"level\":\"info\",\"ts\":\"2024-09-26T12:53:21Z\",\"logger\":\"cluster-resource\",\"msg\":\"default\",\"version\":\"v1\",\"name\":\"sam\",\"namespace\":\"sam-sandbox\"}\r\n{\"level\":\"info\",\"ts\":\"2024-09-26T12:53:21Z\",\"logger\":\"cluster-resource\",\"msg\":\"validate create\",\"version\":\"v1\",\"name\":\"sam\",\"namespace\":\"sam-sandbox\"}\r\n{\"level\":\"error\",\"ts\":\"2024-09-26T12:53:32Z\",\"msg\":\"Unable to create job\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"sam2\",\"namespace\":\"sam-sandbox\"},\"namespace\":\"sam-sandbox\",\"name\":\"sam2\",\"reconcileID\":\"5d0ccee9-74bc-4ed2-8dcf-1a03da82675e\",\"job\":{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"namespace\":\"sam-sandbox\",\"name\":\"sam2-1-initdb\"},\"error\":\"Internal error occurred: admission plugin \\\"ValidatingAdmissionWebhook\\\" failed to complete validation in 13s\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:125\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/controller.(*ClusterReconciler).createPrimaryInstance\\n\\tinternal/controller/cluster_create.go:1096\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/controller.(*ClusterReconciler).reconcilePods\\n\\tinternal/controller/cluster_controller.go:732\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/controller.(*ClusterReconciler).reconcileResources\\n\\tinternal/controller/cluster_controller.go:572\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/controller.(*ClusterReconciler).reconcile\\n\\tinternal/controller/cluster_controller.go:386\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/controller.(*ClusterReconciler).Reconcile\\n\\tinternal/controller/cluster_controller.go:149\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/internal/controller/controller.go:114\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/internal/controller/controller.go:311\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/internal/controller/controller.go:261\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/internal/controller/controller.go:222\"}\r\n{\"level\":\"error\",\"ts\":\"2024-09-26T12:53:32Z\",\"msg\":\"Reconciler error\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"sam2\",\"namespace\":\"sam-sandbox\"},\"namespace\":\"sam-sandbox\",\"name\":\"sam2\",\"reconcileID\":\"5d0ccee9-74bc-4ed2-8dcf-1a03da82675e\",\"error\":\"Internal error occurred: admission plugin \\\"ValidatingAdmissionWebhook\\\" failed to complete validation in 13s\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/internal/controller/controller.go:324\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/internal/controller/controller.go:261\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.18.4/pkg/internal/controller/controller.go:222\"}\r\n```\r\nIn my case this is an OpenShift cluster with `cloudnative-pg.v1.23.3` installed from the marketplace. The exact reason for the failing to create the job doesn't really matter, the problem is that the cnpg controller manager never seems to retry creating the job again.\n---\n@gbartolini I also encountered this. Here is some information:\n```\nCNPG 1.25.0\n```\nOperator Log Message:\n```\n{\"level\":\"info\",\"ts\":\"2025-01-25T23:34:15.712760882Z\",\"msg\":\"Selected PVC is not ready yet, waiting for 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"paradedb\",\"namespace\":\"paradedb\"},\"namespace\":\"paradedb\",\"name\":\"paradedb\",\"reconcileID\":\"c8db0729-d46e-4286-9391-ae7f3cdd34ec\",\"pvc\":\"paradedb-1\",\"status\":\"initializing\",\"instance\":\"paradedb-1\"}\n```\nPVC:\n```\nName:          paradedb-1\nNamespace:     paradedb\nStorageClass:  gp3\nStatus:        Pending\nVolume:        \nLabels:        cnpg.io/cluster=paradedb\n               cnpg.io/instanceName=paradedb-1\n               cnpg.io/pvcRole=PG_DATA\nAnnotations:   cnpg.io/nodeSerial: 1\n               cnpg.io/operatorVersion: 1.24.1\n               cnpg.io/pvcStatus: initializing\nFinalizers:    [kubernetes.io/pvc-protection]\nCapacity:      \nAccess Modes:  \nVolumeMode:    Filesystem\nUsed By:       <none>\nEvents:\n  Type    Reason                Age                   From                         Message\n  ----    ------                ----                  ----                         -------\n  Normal  WaitForFirstConsumer  93s (x26 over 7m35s)  persistentvolume-controller  waiting for first consumer to be created before binding\n```\n---\nHello @hh24k \nCan you take a look here and tell us if this still an issue?\nRegards,\n---\nSpecial thanks to @sxd for allocating some time on a Sunday to help me diagnose this. \nCheck your CSI driver logs for information:\n```bash\nkubectl logs --follow --namespace kube-system deployments/ebs-csi-controller --all-containers --max-log-requests=10\n```\nI noticed the following error there:\n```txt\n1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243: Failed to watch *v1.VolumeSnapshotClass: failed to list *v1.VolumeSnapshotClass: the server could not find the requested resource (get volumesnapshotclasses.snapshot.storage.k8s.io)\" logger=\"UnhandledError\"\n```\n---\nWe found a typo in our case. We accidentally set in the `Cluster` spec:\n```yaml\nresources:\n  limits:\n    cpu: \"1\"\n    mem: \"1Gi\"\n  requests:\n    cpu: \"1\"\n    mem: \"1Gi\"\n```\nNote that it says `mem` instead of `memory`. This passed validation and resulted in no Pods created by the operator and the following message with `info` level repeatedly reported by the operator:\n```txt\nSelected PVC is not ready yet, waiting for 1 second\n``` \nThis was quite difficult to diagnose as the effect of the error was quite unexpected. If you encounter this, look for a similar kind of issue, don't obsess around the PV, PVC and CSI drivers."
    },
    {
        "title": "[Feature]:  Use Workload Identity for Backup from GKE",
        "id": 2195837289,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nCurrently for Google Bucket backup to work it is mandatory to provide a service account JSON key. It is considered legacy way and WorkLoad identity should be used instead. \r\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity \r\nAt the moment it fails if no credentials are provided:\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-03-18T22:05:44Z\",\"logger\":\"barman-cloud-check-wal-archive\",\"msg\":\"2024-03-18 22:05:44,562 [1245] ERROR: Barman cloud WAL archive check exception: ('File /controller/.application_credentials.json is not a valid json file.', JSONDecodeError('Expecting value: line 1 column 1 (char 0)'))\",\"pipe\":\"stderr\",\"logging_pod\":\"psql-0-cluster-1\"}\r\n```\r\n### Describe the solution you'd like\r\n1. Add an option to use custom annotations for Kubernetes service accounts so it would be possible to add:\r\n      iam.gke.io/gcp-service-account: postgres-sa@<project>.iam.gserviceaccount.com\r\n2. \"Teach\" the backup mechanism when \"google\" is used, it should use native authentication mechanisms. \r\nMore details here: https://cloud.google.com/docs/authentication/client-libraries  \r\n### Describe alternatives you've considered\r\nUse the service account JSON file created manually. \r\n### Additional context\r\n_No response_\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nCurrently for Google Bucket backup to work it is mandatory to provide a service account JSON key. It is considered legacy way and WorkLoad identity should be used instead. \r\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity \r\nAt the moment it fails if no credentials are provided:\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-03-18T22:05:44Z\",\"logger\":\"barman-cloud-check-wal-archive\",\"msg\":\"2024-03-18 22:05:44,562 [1245] ERROR: Barman cloud WAL archive check exception: ('File /controller/.application_credentials.json is not a valid json file.', JSONDecodeError('Expecting value: line 1 column 1 (char 0)'))\",\"pipe\":\"stderr\",\"logging_pod\":\"psql-0-cluster-1\"}\r\n```\r\n### Describe the solution you'd like\r\n1. Add an option to use custom annotations for Kubernetes service accounts so it would be possible to add:\r\n      iam.gke.io/gcp-service-account: postgres-sa@<project>.iam.gserviceaccount.com\r\n2. \"Teach\" the backup mechanism when \"google\" is used, it should use native authentication mechanisms. \r\nMore details here: https://cloud.google.com/docs/authentication/client-libraries  \r\n### Describe alternatives you've considered\r\nUse the service account JSON file created manually. \r\n### Additional context\r\n_No response_\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductI have to add clarification here and close the issue. \r\n1. I was confused because I tried to create a \"cluster\" using the helm chart.  It is a 0.0.3 version and doesn't have all the power of raw manifest yet. \r\n2. In order to make WI work you should use \"Cluster\" manifest \"as is\" and add the following field to the configuration:\r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: psql-0-cluster\r\n  namespace: p01\r\n  labels:\r\n    helm.sh/chart: cluster-0.0.3\r\n    app.kubernetes.io/name: cluster\r\nspec:\r\n  instances: 3\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:15.2\r\n  imagePullPolicy: IfNotPresent\r\n...\r\n  backup:\r\n      googleCredentials:\r\n        gkeEnvironment: true\r\n  ...\r\n  serviceAccountTemplate:\r\n    metadata:\r\n      annotations:\r\n        iam.gke.io/gcp-service-account: postgres-sa@<project>.iam.gserviceaccount.com\r\n```\r\nDocs: https://cloudnative-pg.io/documentation/1.18/api_reference/#serviceaccounttemplate"
    },
    {
        "title": "[Bug]: unable to mutate instance pod image path",
        "id": 2193810447,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nsimon.shi@ibm.com\r\n### Version\r\nolder in 1.22.x\r\n### What version of Kubernetes are you using?\r\n1.26\r\n### What is your Kubernetes environment?\r\nSelf-managed: kind (evaluation)\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\nour application needs to mutate image path of the instance pod to point to our private registry, and we're mutating only the image path, leaving the actual image name and digest part intacted, this however, resulted in restart of the instance pods and change reverted back to original image path \r\n### Cluster resource\r\n```shell\r\nNA\r\n```\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nsimon.shi@ibm.com\r\n### Version\r\nolder in 1.22.x\r\n### What version of Kubernetes are you using?\r\n1.26\r\n### What is your Kubernetes environment?\r\nSelf-managed: kind (evaluation)\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\nour application needs to mutate image path of the instance pod to point to our private registry, and we're mutating only the image path, leaving the actual image name and digest part intacted, this however, resulted in restart of the instance pods and change reverted back to original image path \r\n### Cluster resource\r\n```shell\r\nNA\r\n```\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductI also encountered this issue.\r\nI use https://github.com/indeedeng/harbor-container-webhook which conveniently rewrites upstream image URLs to point to an internal Harbor registry (which proxies + caches the images).\r\nI suspect [checkPodImageIsOutdated](https://github.com/cloudnative-pg/cloudnative-pg/blob/e6b16b665e7a2c84ce081a214ba289c249bad967/internal/controller/cluster_upgrade.go#L449) is to blame here.\n---\nI am also hitting this issue, so this is just another example of where the problem might pop up. I'm using [zarf](https://zarf.dev) to do some air-gapped installations. In this case, the original image reference is available from the pod annotation `zarf.dev/original-image-manager`.\n---\nI hit the same issue as @samcday ! I want to update the image path of all pods running in my air-gapped environment to point to my private repository using Kyverno. I can work around the [checkPodImageIsOutdated](https://github.com/cloudnative-pg/cloudnative-pg/blob/e6b16b665e7a2c84ce081a214ba289c249bad967/internal/controller/cluster_upgrade.go#L449) by providing the expected image path directly in the cluster config, but there is no way to provide the image path of the initContainer! [checkPodInitContainerIsOutdated](https://github.com/cloudnative-pg/cloudnative-pg/blob/e6b16b665e7a2c84ce081a214ba289c249bad967/internal/controller/cluster_upgrade.go#L471)\nWould it be possible to make this image check optional via a flag in the operator or at least allow to specify the initContainer image path as well in the cluster configuration?\n---\nHello guys, any update?\n---\nAny update on this? We are using zarf and like @tschlaepfer said I am able to patch the cluster so it gets the expected postgres image but I cannot patch the bootsrap (init) image so now it is failing for reason\n`the instance is using an old init container image:`\n---\n@JoeButler7 we are using zarf and we manage to fix it (quick and dirty) by editing on the the operator yaml file (it includes also the deployment and the crds..), we replace the \"OPERATOR_IMAGE_NAME\" with the image full path that we have in our zarf registry and we redeploy the operator, and on the the \"cluster\" we put the pg image that we have also on the zarf registry, and it works like charm:\noperator yaml file:\n![Image](https://github.com/user-attachments/assets/75ba79a4-d47e-43e1-806c-f5f9063e80af)\ncluster yaml file:\n![Image](https://github.com/user-attachments/assets/87f59710-25cd-402d-9ae1-b71c01751930)\n---\n> I hit the same issue as [@samcday](https://github.com/samcday) ! I want to update the image path of all pods running in my air-gapped environment to point to my private repository using Kyverno. I can work around the [checkPodImageIsOutdated](https://github.com/cloudnative-pg/cloudnative-pg/blob/e6b16b665e7a2c84ce081a214ba289c249bad967/internal/controller/cluster_upgrade.go#L449) by providing the expected image path directly in the cluster config, but there is no way to provide the image path of the initContainer! [checkPodInitContainerIsOutdated](https://github.com/cloudnative-pg/cloudnative-pg/blob/e6b16b665e7a2c84ce081a214ba289c249bad967/internal/controller/cluster_upgrade.go#L471)\n> \n> Would it be possible to make this image check optional via a flag in the operator or at least allow to specify the initContainer image path as well in the cluster configuration?\nI agree that this is the best path forward"
    },
    {
        "title": "[Bug]: VolumeSnapshot support needs manual restart of operator pod",
        "id": 2192332209,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nlukas.topiarz@embedit.com\n### Version\nolder in 1.22.x\n### What version of Kubernetes are you using?\n1.24 (unsupported)\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nHi,\r\nwhen we provision a new cluster argocd deploys cloudnative-pg operator before the snapshot-controller operator. Hence volumeSnapshot resource doesn't exist before the cnpg operator is started. Manual intervention (deleting cnpg pod) is required. \r\n```bash\r\nFailed sync attempt to ee35e66328a66533b5554e1 9c3a4c70b05aef831: one or more objects failed to apply, reason: admission webhook \"vscheduledbackup.cnpg.io\" denied the request: Backup.scheduledbackup.cnpg.io \"grafana-db\u2019 is invalid:\r\nspec.method: Invalid value: \"volumeSnapshot\": Cannot use volumeSnapshot backup method due to missing volumeSnapshot CRD. If you installed the CRD after having started the operator, please restart it to enable VolumeSnapshot support\r\n```\r\nThis little bug breaks our deployment pipeline as it is fully automatic. We are unable to order the deployment of each operator due to some undisclosed limitation. We think that cnpg operator should periodically check if volumeSnapshot CRD exists and align to that. To honor Kubernetes declarative way of resource handling rather than bringing imperative dependency.\r\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nlukas.topiarz@embedit.com\n### Version\nolder in 1.22.x\n### What version of Kubernetes are you using?\n1.24 (unsupported)\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nHi,\r\nwhen we provision a new cluster argocd deploys cloudnative-pg operator before the snapshot-controller operator. Hence volumeSnapshot resource doesn't exist before the cnpg operator is started. Manual intervention (deleting cnpg pod) is required. \r\n```bash\r\nFailed sync attempt to ee35e66328a66533b5554e1 9c3a4c70b05aef831: one or more objects failed to apply, reason: admission webhook \"vscheduledbackup.cnpg.io\" denied the request: Backup.scheduledbackup.cnpg.io \"grafana-db\u2019 is invalid:\r\nspec.method: Invalid value: \"volumeSnapshot\": Cannot use volumeSnapshot backup method due to missing volumeSnapshot CRD. If you installed the CRD after having started the operator, please restart it to enable VolumeSnapshot support\r\n```\r\nThis little bug breaks our deployment pipeline as it is fully automatic. We are unable to order the deployment of each operator due to some undisclosed limitation. We think that cnpg operator should periodically check if volumeSnapshot CRD exists and align to that. To honor Kubernetes declarative way of resource handling rather than bringing imperative dependency.\r\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductHi, any update please?"
    },
    {
        "title": "[Bug]: Add read replica with snapshot enabled fails to start",
        "id": 2190122312,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nnick.hudson@gmail.com\r\n### Version\r\n1.22.1\r\n### What version of Kubernetes are you using?\r\n1.25 (unsupported)\r\n### What is your Kubernetes environment?\r\nCloud: Amazon EKS\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nWhen adding a read replica (`spec.instances: 2`) to a `Cluster`, unless the `VolumeSnapshot` has very recently been taken, adding the new replica will fail to start.\r\nI would expect the new replica to be added and catch up the new replica using WAL.\r\nI understand that currently the only way this works is through the `pg_basebackup` or streaming replica method.  The current method assumes that you would take a brand new snapshot before the replica comes online.  What I am proposing is that this needs to be changed to not use the `--immediate` flag when running the `restoresnapshot` command and basically come up like a normal restore.  This would allow the new replica to start Postgres in the restore job and then stream WAL from objectStore to catch the new replica up with the primary instance.\r\nI would be interested to help contribute this change to CNPG. Please let me know about the direction, and I am happy to give it a shot! Thank you\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    cnpg.io/fencedInstances: '[]'\r\n  name: org-tembo-test-inst-nick-test-cnpg\r\n  namespace: org-tembo-test-inst-nick-test-cnpg\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n    topologyKey: topology.kubernetes.io/zone\r\n  backup:\r\n    barmanObjectStore:\r\n      data:\r\n        compression: snappy\r\n        encryption: AES256\r\n        immediateCheckpoint: true\r\n      destinationPath: s3://cdb-plat-use1-dev-instance-backups/coredb/tembo-test/org-tembo-test-inst-nick-test-cnpg\r\n      s3Credentials:\r\n        inheritFromIAMRole: true\r\n      wal:\r\n        compression: snappy\r\n        encryption: AES256\r\n        maxParallel: 8\r\n    retentionPolicy: 30d\r\n    target: prefer-standby\r\n    volumeSnapshot:\r\n      className: cnpg-snapshot-class\r\n      online: true\r\n      onlineConfiguration:\r\n        immediateCheckpoint: true\r\n        waitForArchive: true\r\n      snapshotOwnerReference: cluster\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  certificates:\r\n    clientCASecret: org-tembo-test-inst-nick-test-cnpg-ca1\r\n    replicationTLSSecret: org-tembo-test-inst-nick-test-cnpg-replication1\r\n    serverCASecret: org-tembo-test-inst-nick-test-cnpg-ca1\r\n    serverTLSSecret: org-tembo-test-inst-nick-test-cnpg-server1\r\n  enableSuperuserAccess: true\r\n  externalClusters:\r\n  - connectionParameters:\r\n      host: org-tembo-test-inst-nick-test-cnpg\r\n      user: postgres\r\n    name: coredb\r\n    password:\r\n      key: password\r\n      name: org-tembo-test-inst-nick-test-cnpg-connection\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  instances: 2\r\n  logLevel: info\r\n  managed:\r\n    roles:\r\n    - connectionLimit: -1\r\n      ensure: present\r\n      inRoles:\r\n      - pg_read_all_data\r\n      inherit: true\r\n      login: true\r\n      name: readonly\r\n      passwordSecret:\r\n        name: org-tembo-test-inst-nick-test-cnpg-ro\r\n    - connectionLimit: -1\r\n      ensure: present\r\n      inRoles:\r\n      - pg_read_all_stats\r\n      - pg_monitor\r\n      inherit: true\r\n      login: true\r\n      name: postgres_exporter\r\n      passwordSecret:\r\n        name: org-tembo-test-inst-nick-test-cnpg-exporter\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  nodeMaintenanceWindow:\r\n    inProgress: true\r\n    reusePVC: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    enableAlterSystem: true\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      autovacuum_analyze_scale_factor: \"0.05\"\r\n      autovacuum_vacuum_cost_limit: \"-1\"\r\n      autovacuum_vacuum_insert_scale_factor: \"0.05\"\r\n      autovacuum_vacuum_scale_factor: \"0.05\"\r\n      bgwriter_delay: 10ms\r\n      checkpoint_timeout: 10min\r\n      dynamic_shared_memory_type: posix\r\n      effective_cache_size: 5734MB\r\n      effective_io_concurrency: \"100\"\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_min_duration_statement: \"1000\"\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      maintenance_work_mem: 409MB\r\n      max_connections: \"862\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_wal_size: 2GB\r\n      max_worker_processes: \"32\"\r\n      pg_stat_statements.track: all\r\n      shared_buffers: 2048MB\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.2\r\n      track_activity_query_size: \"2048\"\r\n      track_io_timing: \"on\"\r\n      wal_compression: \"on\"\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n      work_mem: 5MB\r\n    shared_preload_libraries:\r\n    - pg_stat_statements\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: false\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      cpu: \"2\"\r\n      memory: 8Gi\r\n    requests:\r\n      cpu: \"1\"\r\n      memory: 8Gi\r\n  serviceAccountTemplate:\r\n    metadata:\r\n      annotations:\r\n        eks.amazonaws.com/role-arn: arn:aws:iam::484221059514:role/org-tembo-test-inst-nick-test-cnpg-iam\r\n  smartShutdownTimeout: 15\r\n  startDelay: 30\r\n  stopDelay: 30\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 10Gi\r\n  superuserSecret:\r\n    name: org-tembo-test-inst-nick-test-cnpg-connection\r\n  switchoverDelay: 60\r\n  walStorage:\r\n    resizeInUseVolumes: true\r\n    size: 10Gi\r\n```\r\n### Relevant log output\r\n```shell\r\nDefaulted container \"snapshot-recovery\" out of: snapshot-recovery, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:20:49Z\",\"msg\":\"Cleaning up PGDATA from stale files\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2-snapshot-recovery\"}\r\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"version\":\"1.22.1\",\"build\":{\"Version\":\"1.22.1\",\"Commit\":\"c7be872e\",\"Date\":\"2024-02-02\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"setup\",\"msg\":\"starting tablespace manager\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"setup\",\"msg\":\"starting external server manager\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"/controller/certificates/server.crt\",\"secret\":\"org-tembo-test-inst-nick-test-cnpg-server1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"/controller/certificates/server.key\",\"secret\":\"org-tembo-test-inst-nick-test-cnpg-server1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"/controller/certificates/streaming_replica.crt\",\"secret\":\"org-tembo-test-inst-nick-test-cnpg-replication1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"/controller/certificates/streaming_replica.key\",\"secret\":\"org-tembo-test-inst-nick-test-cnpg-replication1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"/controller/certificates/client-ca.crt\",\"secret\":\"org-tembo-test-inst-nick-test-cnpg-ca1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"/controller/certificates/server-ca.crt\",\"secret\":\"org-tembo-test-inst-nick-test-cnpg-ca1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Installed configuration file\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"filename\":\"pg_ident.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Cluster status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"currentPrimary\":\"org-tembo-test-inst-nick-test-cnpg-1\",\"targetPrimary\":\"org-tembo-test-inst-nick-test-cnpg-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"This is an old primary instance, waiting for the switchover to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"currentPrimary\":\"org-tembo-test-inst-nick-test-cnpg-1\",\"targetPrimary\":\"org-tembo-test-inst-nick-test-cnpg-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Switchover completed\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"targetPrimary\":\"org-tembo-test-inst-nick-test-cnpg-1\",\"currentPrimary\":\"org-tembo-test-inst-nick-test-cnpg-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Waiting for the new primary to be available\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"primaryConnInfo\":\"host=org-tembo-test-inst-nick-test-cnpg-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=org-tembo-test-inst-nick-test-cnpg-2 sslmode=verify-ca dbname=postgres connect_timeout=5\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Extracting pg_controldata information\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"reason\":\"before pg_rewind\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7346702466234134552\\nDatabase cluster state:               in production\\npg_control last modified:             Fri 15 Mar 2024 09:25:18 PM UTC\\nLatest checkpoint location:           0/5000060\\nLatest checkpoint's REDO location:    0/5000028\\nLatest checkpoint's REDO WAL file:    000000010000000000000005\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:754\\nLatest checkpoint's NextOID:          24578\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  754\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Fri 15 Mar 2024 09:25:18 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              862\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            94be80c477a3ac40963b9c8747d4e0475a5678474cf995571c98831a7a4e35af\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting up pg_rewind\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"options\":[\"-P\",\"--source-server\",\"host=org-tembo-test-inst-nick-test-cnpg-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=org-tembo-test-inst-nick-test-cnpg-2 sslmode=verify-ca dbname=postgres\",\"--target-pgdata\",\"/var/lib/postgresql/data/pgdata\",\"--restore-target-wal\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: connected to server\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: executing \\\"/usr/lib/postgresql/16/bin/postgres\\\" for target server to complete crash recovery\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.444 UTC [29] LOG:  database system was interrupted; last known up at 2024-03-15 21:25:18 UTC\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.444 UTC [29] LOG:  database system was not properly shut down; automatic recovery in progress\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.445 UTC [29] LOG:  redo starts at 0/5000028\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.445 UTC [29] LOG:  invalid record length at 0/50000D8: expected at least 24, got 0\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.445 UTC [29] LOG:  redo done at 0/5000060 system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.447 UTC [29] LOG:  checkpoint starting: end-of-recovery immediate wait\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.449 UTC [29] LOG:  checkpoint complete: wrote 3 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.002 s, sync=0.001 s, total=0.002 s; sync files=0, longest=0.000 s, average=0.000 s; distance=0 kB, estimate=0 kB; lsn=0/50000D8, redo lsn=0/50000D8\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"PostgreSQL stand-alone backend 16.2 (Debian 16.2-1.pgdg110+2)\",\"pipe\":\"stdout\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.469 UTC [29] LOG:  checkpoint starting: shutdown immediate\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.478 UTC [29] LOG:  checkpoint complete: wrote 0 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.002 s, sync=0.001 s, total=0.009 s; sync files=0, longest=0.000 s, average=0.000 s; distance=16383 kB, estimate=16383 kB; lsn=0/6000028, redo lsn=0/6000028\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: source and target cluster are on the same timeline\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: no rewind required\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"backend> \",\"pipe\":\"stdout\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Demoting instance\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"pgpdata\":\"/var/lib/postgresql/data/pgdata\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"override.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"reason\":\"postmaster start up\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7346702466234134552\\nDatabase cluster state:               shut down\\npg_control last modified:             Sat 16 Mar 2024 04:21:01 PM UTC\\nLatest checkpoint location:           0/6000028\\nLatest checkpoint's REDO location:    0/6000028\\nLatest checkpoint's REDO WAL file:    000000010000000000000006\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:754\\nLatest checkpoint's NextOID:          24578\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  0\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Sat 16 Mar 2024 04:21:01 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              862\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            94be80c477a3ac40963b9c8747d4e0475a5678474cf995571c98831a7a4e35af\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"2024-03-16 16:21:01.542 UTC [33] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"2024-03-16 16:21:01.542 UTC [33] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"2024-03-16 16:21:01.542 UTC [33] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"2024-03-16 16:21:01.542 UTC [33] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:01.542 UTC\",\"process_id\":\"33\",\"session_id\":\"65f5c6ed.21\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:01.542 UTC\",\"process_id\":\"33\",\"session_id\":\"65f5c6ed.21\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 16.2 (Debian 16.2-1.pgdg110+2) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:01.543 UTC\",\"process_id\":\"33\",\"session_id\":\"65f5c6ed.21\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:01.543 UTC\",\"process_id\":\"33\",\"session_id\":\"65f5c6ed.21\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:01.548 UTC\",\"process_id\":\"33\",\"session_id\":\"65f5c6ed.21\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:01.559 UTC\",\"process_id\":\"37\",\"session_id\":\"65f5c6ed.25\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was shut down at 2024-03-16 16:21:01 UTC\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:02Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"walName\":\"00000002.history\",\"options\":[\"--cloud-provider\",\"aws-s3\",\"s3://cdb-plat-use1-dev-instance-backups/coredb/tembo-test/org-tembo-test-inst-nick-test-cnpg\",\"org-tembo-test-inst-nick-test-cnpg\"],\"startTime\":\"2024-03-16T16:21:01Z\",\"endTime\":\"2024-03-16T16:21:02Z\",\"elapsedWalTime\":0.398964517}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:02.168 UTC\",\"process_id\":\"37\",\"session_id\":\"65f5c6ed.25\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"entering standby mode\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:02Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"override.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:02Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"override.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:02.760 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"70\",\"connection_from\":\"[local]\",\"session_id\":\"65f5c6ee.46\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-03-16 16:21:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:02Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"5da81f45-a6d8-40b8-a908-38d7f9834b0f\",\"uuid\":\"2e95125c-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:02.840 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"71\",\"connection_from\":\"[local]\",\"session_id\":\"65f5c6ee.47\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-03-16 16:21:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:03Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"walName\":\"000000010000000000000008\",\"startTime\":\"2024-03-16T16:21:02Z\",\"endTime\":\"2024-03-16T16:21:03Z\",\"elapsedWalTime\":1.565887147}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:04.037 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"74\",\"connection_from\":\"[local]\",\"session_id\":\"65f5c6f0.4a\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-03-16 16:21:04 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:04.040 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"75\",\"connection_from\":\"[local]\",\"session_id\":\"65f5c6f0.4b\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-03-16 16:21:04 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"1835a9bf-3ea7-4757-a0df-e1347382bdbc\",\"uuid\":\"2f5d1ed4-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"walName\":\"000000010000000000000009\",\"startTime\":\"2024-03-16T16:21:02Z\",\"endTime\":\"2024-03-16T16:21:04Z\",\"elapsedWalTime\":1.900110461}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"walName\":\"000000010000000000000007\",\"startTime\":\"2024-03-16T16:21:02Z\",\"endTime\":\"2024-03-16T16:21:04Z\",\"elapsedWalTime\":1.904205925}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"walName\":\"00000001000000000000000A\",\"startTime\":\"2024-03-16T16:21:02Z\",\"endTime\":\"2024-03-16T16:21:04Z\",\"elapsedWalTime\":2.064930029}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"walName\":\"000000010000000000000006\",\"startTime\":\"2024-03-16T16:21:02Z\",\"endTime\":\"2024-03-16T16:21:04Z\",\"elapsedWalTime\":2.082237892}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"wal-restore\",\"msg\":\"Set end-of-wal-stream flag as one of the WAL files to be prefetched was not found\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"walName\":\"000000010000000000000006\",\"maxParallel\":8,\"successfulWalRestore\":5,\"failedWalRestore\":3,\"endOfWALStream\":true,\"startTime\":\"2024-03-16T16:21:02Z\",\"downloadStartTime\":\"2024-03-16T16:21:02Z\",\"downloadTotalTime\":2.087463371,\"totalTime\":2.166260384}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:04.355 UTC\",\"process_id\":\"37\",\"session_id\":\"65f5c6ed.25\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"000000010000000000000006\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:04.360 UTC\",\"process_id\":\"37\",\"session_id\":\"65f5c6ed.25\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"invalid resource manager ID in checkpoint record\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:04.360 UTC\",\"process_id\":\"37\",\"session_id\":\"65f5c6ed.25\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"PANIC\",\"sql_state_code\":\"XX000\",\"message\":\"could not locate a valid checkpoint record\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:04.368 UTC\",\"process_id\":\"33\",\"session_id\":\"65f5c6ed.21\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 37) was terminated by signal 6: Aborted\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:04.368 UTC\",\"process_id\":\"33\",\"session_id\":\"65f5c6ed.21\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"aborting startup due to startup process failure\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:04.371 UTC\",\"process_id\":\"33\",\"session_id\":\"65f5c6ed.21\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"reason\":\"postmaster has exited\"}\r\n{\"level\":\"error\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"PostgreSQL process exited with errors\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:98\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/manager/runnable_group.go:223\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\r\n{\"level\":\"error\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"error received after stop sequence was engaged\",\"error\":\"exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).engageStopProcedure.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/manager/internal.go:490\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Stopping and waiting for leader election runnables\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Stopping and waiting for caches\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Stopping and waiting for webhooks\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Stopping and waiting for HTTP servers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7346702466234134552\\nDatabase cluster state:               shut down\\npg_control last modified:             Sat 16 Mar 2024 04:21:01 PM UTC\\nLatest checkpoint location:           0/6000028\\nLatest checkpoint's REDO location:    0/6000028\\nLatest checkpoint's REDO WAL file:    000000010000000000000006\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:754\\nLatest checkpoint's NextOID:          24578\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  0\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Sat 16 Mar 2024 04:21:01 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              862\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            94be80c477a3ac40963b9c8747d4e0475a5678474cf995571c98831a7a4e35af\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\nNAME                                                           READY   STATUS             RESTARTS     AGE\r\norg-tembo-test-inst-nick-test-cnpg-1                           1/1     Running            0            19h\r\norg-tembo-test-inst-nick-test-cnpg-2                           0/1     CrashLoopBackOff   1 (5s ago)   16s\r\norg-tembo-test-inst-nick-test-cnpg-2-snapshot-recovery-d79mk   0/1     Completed          0            42s\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nnick.hudson@gmail.com\r\n### Version\r\n1.22.1\r\n### What version of Kubernetes are you using?\r\n1.25 (unsupported)\r\n### What is your Kubernetes environment?\r\nCloud: Amazon EKS\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nWhen adding a read replica (`spec.instances: 2`) to a `Cluster`, unless the `VolumeSnapshot` has very recently been taken, adding the new replica will fail to start.\r\nI would expect the new replica to be added and catch up the new replica using WAL.\r\nI understand that currently the only way this works is through the `pg_basebackup` or streaming replica method.  The current method assumes that you would take a brand new snapshot before the replica comes online.  What I am proposing is that this needs to be changed to not use the `--immediate` flag when running the `restoresnapshot` command and basically come up like a normal restore.  This would allow the new replica to start Postgres in the restore job and then stream WAL from objectStore to catch the new replica up with the primary instance.\r\nI would be interested to help contribute this change to CNPG. Please let me know about the direction, and I am happy to give it a shot! Thank you\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    cnpg.io/fencedInstances: '[]'\r\n  name: org-tembo-test-inst-nick-test-cnpg\r\n  namespace: org-tembo-test-inst-nick-test-cnpg\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n    topologyKey: topology.kubernetes.io/zone\r\n  backup:\r\n    barmanObjectStore:\r\n      data:\r\n        compression: snappy\r\n        encryption: AES256\r\n        immediateCheckpoint: true\r\n      destinationPath: s3://cdb-plat-use1-dev-instance-backups/coredb/tembo-test/org-tembo-test-inst-nick-test-cnpg\r\n      s3Credentials:\r\n        inheritFromIAMRole: true\r\n      wal:\r\n        compression: snappy\r\n        encryption: AES256\r\n        maxParallel: 8\r\n    retentionPolicy: 30d\r\n    target: prefer-standby\r\n    volumeSnapshot:\r\n      className: cnpg-snapshot-class\r\n      online: true\r\n      onlineConfiguration:\r\n        immediateCheckpoint: true\r\n        waitForArchive: true\r\n      snapshotOwnerReference: cluster\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  certificates:\r\n    clientCASecret: org-tembo-test-inst-nick-test-cnpg-ca1\r\n    replicationTLSSecret: org-tembo-test-inst-nick-test-cnpg-replication1\r\n    serverCASecret: org-tembo-test-inst-nick-test-cnpg-ca1\r\n    serverTLSSecret: org-tembo-test-inst-nick-test-cnpg-server1\r\n  enableSuperuserAccess: true\r\n  externalClusters:\r\n  - connectionParameters:\r\n      host: org-tembo-test-inst-nick-test-cnpg\r\n      user: postgres\r\n    name: coredb\r\n    password:\r\n      key: password\r\n      name: org-tembo-test-inst-nick-test-cnpg-connection\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  instances: 2\r\n  logLevel: info\r\n  managed:\r\n    roles:\r\n    - connectionLimit: -1\r\n      ensure: present\r\n      inRoles:\r\n      - pg_read_all_data\r\n      inherit: true\r\n      login: true\r\n      name: readonly\r\n      passwordSecret:\r\n        name: org-tembo-test-inst-nick-test-cnpg-ro\r\n    - connectionLimit: -1\r\n      ensure: present\r\n      inRoles:\r\n      - pg_read_all_stats\r\n      - pg_monitor\r\n      inherit: true\r\n      login: true\r\n      name: postgres_exporter\r\n      passwordSecret:\r\n        name: org-tembo-test-inst-nick-test-cnpg-exporter\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  nodeMaintenanceWindow:\r\n    inProgress: true\r\n    reusePVC: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    enableAlterSystem: true\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      autovacuum_analyze_scale_factor: \"0.05\"\r\n      autovacuum_vacuum_cost_limit: \"-1\"\r\n      autovacuum_vacuum_insert_scale_factor: \"0.05\"\r\n      autovacuum_vacuum_scale_factor: \"0.05\"\r\n      bgwriter_delay: 10ms\r\n      checkpoint_timeout: 10min\r\n      dynamic_shared_memory_type: posix\r\n      effective_cache_size: 5734MB\r\n      effective_io_concurrency: \"100\"\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_min_duration_statement: \"1000\"\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      maintenance_work_mem: 409MB\r\n      max_connections: \"862\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_wal_size: 2GB\r\n      max_worker_processes: \"32\"\r\n      pg_stat_statements.track: all\r\n      shared_buffers: 2048MB\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.2\r\n      track_activity_query_size: \"2048\"\r\n      track_io_timing: \"on\"\r\n      wal_compression: \"on\"\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n      work_mem: 5MB\r\n    shared_preload_libraries:\r\n    - pg_stat_statements\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: false\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      cpu: \"2\"\r\n      memory: 8Gi\r\n    requests:\r\n      cpu: \"1\"\r\n      memory: 8Gi\r\n  serviceAccountTemplate:\r\n    metadata:\r\n      annotations:\r\n        eks.amazonaws.com/role-arn: arn:aws:iam::484221059514:role/org-tembo-test-inst-nick-test-cnpg-iam\r\n  smartShutdownTimeout: 15\r\n  startDelay: 30\r\n  stopDelay: 30\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 10Gi\r\n  superuserSecret:\r\n    name: org-tembo-test-inst-nick-test-cnpg-connection\r\n  switchoverDelay: 60\r\n  walStorage:\r\n    resizeInUseVolumes: true\r\n    size: 10Gi\r\n```\r\n### Relevant log output\r\n```shell\r\nDefaulted container \"snapshot-recovery\" out of: snapshot-recovery, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:20:49Z\",\"msg\":\"Cleaning up PGDATA from stale files\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2-snapshot-recovery\"}\r\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"version\":\"1.22.1\",\"build\":{\"Version\":\"1.22.1\",\"Commit\":\"c7be872e\",\"Date\":\"2024-02-02\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"setup\",\"msg\":\"starting tablespace manager\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"setup\",\"msg\":\"starting external server manager\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"/controller/certificates/server.crt\",\"secret\":\"org-tembo-test-inst-nick-test-cnpg-server1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"/controller/certificates/server.key\",\"secret\":\"org-tembo-test-inst-nick-test-cnpg-server1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"/controller/certificates/streaming_replica.crt\",\"secret\":\"org-tembo-test-inst-nick-test-cnpg-replication1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"/controller/certificates/streaming_replica.key\",\"secret\":\"org-tembo-test-inst-nick-test-cnpg-replication1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"/controller/certificates/client-ca.crt\",\"secret\":\"org-tembo-test-inst-nick-test-cnpg-ca1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"/controller/certificates/server-ca.crt\",\"secret\":\"org-tembo-test-inst-nick-test-cnpg-ca1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Installed configuration file\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"filename\":\"pg_ident.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Cluster status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"currentPrimary\":\"org-tembo-test-inst-nick-test-cnpg-1\",\"targetPrimary\":\"org-tembo-test-inst-nick-test-cnpg-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"This is an old primary instance, waiting for the switchover to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"currentPrimary\":\"org-tembo-test-inst-nick-test-cnpg-1\",\"targetPrimary\":\"org-tembo-test-inst-nick-test-cnpg-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Switchover completed\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"targetPrimary\":\"org-tembo-test-inst-nick-test-cnpg-1\",\"currentPrimary\":\"org-tembo-test-inst-nick-test-cnpg-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Waiting for the new primary to be available\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"primaryConnInfo\":\"host=org-tembo-test-inst-nick-test-cnpg-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=org-tembo-test-inst-nick-test-cnpg-2 sslmode=verify-ca dbname=postgres connect_timeout=5\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Extracting pg_controldata information\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"reason\":\"before pg_rewind\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7346702466234134552\\nDatabase cluster state:               in production\\npg_control last modified:             Fri 15 Mar 2024 09:25:18 PM UTC\\nLatest checkpoint location:           0/5000060\\nLatest checkpoint's REDO location:    0/5000028\\nLatest checkpoint's REDO WAL file:    000000010000000000000005\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:754\\nLatest checkpoint's NextOID:          24578\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  754\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Fri 15 Mar 2024 09:25:18 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              862\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            94be80c477a3ac40963b9c8747d4e0475a5678474cf995571c98831a7a4e35af\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Starting up pg_rewind\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"options\":[\"-P\",\"--source-server\",\"host=org-tembo-test-inst-nick-test-cnpg-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=org-tembo-test-inst-nick-test-cnpg-2 sslmode=verify-ca dbname=postgres\",\"--target-pgdata\",\"/var/lib/postgresql/data/pgdata\",\"--restore-target-wal\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: connected to server\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: executing \\\"/usr/lib/postgresql/16/bin/postgres\\\" for target server to complete crash recovery\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.444 UTC [29] LOG:  database system was interrupted; last known up at 2024-03-15 21:25:18 UTC\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.444 UTC [29] LOG:  database system was not properly shut down; automatic recovery in progress\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.445 UTC [29] LOG:  redo starts at 0/5000028\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.445 UTC [29] LOG:  invalid record length at 0/50000D8: expected at least 24, got 0\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.445 UTC [29] LOG:  redo done at 0/5000060 system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.447 UTC [29] LOG:  checkpoint starting: end-of-recovery immediate wait\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.449 UTC [29] LOG:  checkpoint complete: wrote 3 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.002 s, sync=0.001 s, total=0.002 s; sync files=0, longest=0.000 s, average=0.000 s; distance=0 kB, estimate=0 kB; lsn=0/50000D8, redo lsn=0/50000D8\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"PostgreSQL stand-alone backend 16.2 (Debian 16.2-1.pgdg110+2)\",\"pipe\":\"stdout\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.469 UTC [29] LOG:  checkpoint starting: shutdown immediate\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"2024-03-16 16:21:01.478 UTC [29] LOG:  checkpoint complete: wrote 0 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.002 s, sync=0.001 s, total=0.009 s; sync files=0, longest=0.000 s, average=0.000 s; distance=16383 kB, estimate=16383 kB; lsn=0/6000028, redo lsn=0/6000028\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: source and target cluster are on the same timeline\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: no rewind required\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_rewind\",\"msg\":\"backend> \",\"pipe\":\"stdout\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Demoting instance\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"pgpdata\":\"/var/lib/postgresql/data/pgdata\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"override.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"reason\":\"postmaster start up\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7346702466234134552\\nDatabase cluster state:               shut down\\npg_control last modified:             Sat 16 Mar 2024 04:21:01 PM UTC\\nLatest checkpoint location:           0/6000028\\nLatest checkpoint's REDO location:    0/6000028\\nLatest checkpoint's REDO WAL file:    000000010000000000000006\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:754\\nLatest checkpoint's NextOID:          24578\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  0\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Sat 16 Mar 2024 04:21:01 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              862\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            94be80c477a3ac40963b9c8747d4e0475a5678474cf995571c98831a7a4e35af\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"989b9cda-4f1e-45a3-92ce-92d6e7bce8e6\",\"uuid\":\"2dd9caeb-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"2024-03-16 16:21:01.542 UTC [33] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"2024-03-16 16:21:01.542 UTC [33] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"2024-03-16 16:21:01.542 UTC [33] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"2024-03-16 16:21:01.542 UTC [33] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:01.542 UTC\",\"process_id\":\"33\",\"session_id\":\"65f5c6ed.21\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:01.542 UTC\",\"process_id\":\"33\",\"session_id\":\"65f5c6ed.21\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 16.2 (Debian 16.2-1.pgdg110+2) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:01.543 UTC\",\"process_id\":\"33\",\"session_id\":\"65f5c6ed.21\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:01.543 UTC\",\"process_id\":\"33\",\"session_id\":\"65f5c6ed.21\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:01.548 UTC\",\"process_id\":\"33\",\"session_id\":\"65f5c6ed.21\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:01Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:01.559 UTC\",\"process_id\":\"37\",\"session_id\":\"65f5c6ed.25\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was shut down at 2024-03-16 16:21:01 UTC\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:02Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"walName\":\"00000002.history\",\"options\":[\"--cloud-provider\",\"aws-s3\",\"s3://cdb-plat-use1-dev-instance-backups/coredb/tembo-test/org-tembo-test-inst-nick-test-cnpg\",\"org-tembo-test-inst-nick-test-cnpg\"],\"startTime\":\"2024-03-16T16:21:01Z\",\"endTime\":\"2024-03-16T16:21:02Z\",\"elapsedWalTime\":0.398964517}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:02.168 UTC\",\"process_id\":\"37\",\"session_id\":\"65f5c6ed.25\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"entering standby mode\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:02Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"override.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:02Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"filename\":\"override.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:02.760 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"70\",\"connection_from\":\"[local]\",\"session_id\":\"65f5c6ee.46\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-03-16 16:21:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:02Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"5da81f45-a6d8-40b8-a908-38d7f9834b0f\",\"uuid\":\"2e95125c-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:02.840 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"71\",\"connection_from\":\"[local]\",\"session_id\":\"65f5c6ee.47\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-03-16 16:21:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:03Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"walName\":\"000000010000000000000008\",\"startTime\":\"2024-03-16T16:21:02Z\",\"endTime\":\"2024-03-16T16:21:03Z\",\"elapsedWalTime\":1.565887147}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:04.037 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"74\",\"connection_from\":\"[local]\",\"session_id\":\"65f5c6f0.4a\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-03-16 16:21:04 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:04.040 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"75\",\"connection_from\":\"[local]\",\"session_id\":\"65f5c6f0.4b\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-03-16 16:21:04 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\"},\"namespace\":\"org-tembo-test-inst-nick-test-cnpg\",\"name\":\"org-tembo-test-inst-nick-test-cnpg\",\"reconcileID\":\"1835a9bf-3ea7-4757-a0df-e1347382bdbc\",\"uuid\":\"2f5d1ed4-e3b1-11ee-9255-9ade2a1c5f02\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"walName\":\"000000010000000000000009\",\"startTime\":\"2024-03-16T16:21:02Z\",\"endTime\":\"2024-03-16T16:21:04Z\",\"elapsedWalTime\":1.900110461}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"walName\":\"000000010000000000000007\",\"startTime\":\"2024-03-16T16:21:02Z\",\"endTime\":\"2024-03-16T16:21:04Z\",\"elapsedWalTime\":1.904205925}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"walName\":\"00000001000000000000000A\",\"startTime\":\"2024-03-16T16:21:02Z\",\"endTime\":\"2024-03-16T16:21:04Z\",\"elapsedWalTime\":2.064930029}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"walName\":\"000000010000000000000006\",\"startTime\":\"2024-03-16T16:21:02Z\",\"endTime\":\"2024-03-16T16:21:04Z\",\"elapsedWalTime\":2.082237892}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"wal-restore\",\"msg\":\"Set end-of-wal-stream flag as one of the WAL files to be prefetched was not found\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"walName\":\"000000010000000000000006\",\"maxParallel\":8,\"successfulWalRestore\":5,\"failedWalRestore\":3,\"endOfWALStream\":true,\"startTime\":\"2024-03-16T16:21:02Z\",\"downloadStartTime\":\"2024-03-16T16:21:02Z\",\"downloadTotalTime\":2.087463371,\"totalTime\":2.166260384}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:04.355 UTC\",\"process_id\":\"37\",\"session_id\":\"65f5c6ed.25\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"000000010000000000000006\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:04.360 UTC\",\"process_id\":\"37\",\"session_id\":\"65f5c6ed.25\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"invalid resource manager ID in checkpoint record\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:04.360 UTC\",\"process_id\":\"37\",\"session_id\":\"65f5c6ed.25\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"PANIC\",\"sql_state_code\":\"XX000\",\"message\":\"could not locate a valid checkpoint record\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:04.368 UTC\",\"process_id\":\"33\",\"session_id\":\"65f5c6ed.21\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 37) was terminated by signal 6: Aborted\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:04.368 UTC\",\"process_id\":\"33\",\"session_id\":\"65f5c6ed.21\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"aborting startup due to startup process failure\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"record\":{\"log_time\":\"2024-03-16 16:21:04.371 UTC\",\"process_id\":\"33\",\"session_id\":\"65f5c6ed.21\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-03-16 16:21:01 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"reason\":\"postmaster has exited\"}\r\n{\"level\":\"error\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"PostgreSQL process exited with errors\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:98\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/manager/runnable_group.go:223\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\r\n{\"level\":\"error\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"error received after stop sequence was engaged\",\"error\":\"exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).engageStopProcedure.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/manager/internal.go:490\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Stopping and waiting for leader election runnables\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Stopping and waiting for caches\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Stopping and waiting for webhooks\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Stopping and waiting for HTTP servers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\r\n{\"level\":\"info\",\"ts\":\"2024-03-16T16:21:04Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7346702466234134552\\nDatabase cluster state:               shut down\\npg_control last modified:             Sat 16 Mar 2024 04:21:01 PM UTC\\nLatest checkpoint location:           0/6000028\\nLatest checkpoint's REDO location:    0/6000028\\nLatest checkpoint's REDO WAL file:    000000010000000000000006\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:754\\nLatest checkpoint's NextOID:          24578\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  0\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Sat 16 Mar 2024 04:21:01 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              862\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            94be80c477a3ac40963b9c8747d4e0475a5678474cf995571c98831a7a4e35af\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"org-tembo-test-inst-nick-test-cnpg-2\"}\r\nNAME                                                           READY   STATUS             RESTARTS     AGE\r\norg-tembo-test-inst-nick-test-cnpg-1                           1/1     Running            0            19h\r\norg-tembo-test-inst-nick-test-cnpg-2                           0/1     CrashLoopBackOff   1 (5s ago)   16s\r\norg-tembo-test-inst-nick-test-cnpg-2-snapshot-recovery-d79mk   0/1     Completed          0            42s\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductHi @nhudson \r\nI see you created the issue here but there's no PR here, are you planning to create one? \r\nRegards,\n---\n> Hi @nhudson\r\n> \r\n> I see you created the issue here but there's no PR here, are you planning to create one?\r\n> \r\n> Regards,\r\n@sxd yes!  I am hoping either myself or someone from our team will be creating a PR to fix this in the near future.\n---\nFixed with #5080.  This can be closed."
    },
    {
        "title": "[Feature]: Add a new `ClientCertificate` resource type",
        "id": 2185181086,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nUsing client certificate authentication requires to create client certificates with the `kubectl cnpg certificate` cmd - at least this is the only know method to me :)\r\nThe corresponding created secret can be mounted into a container for authentication to a respective cnpg cluster. \r\nThe client certificates expire after 90 days, meaning the need to be renewed by:\r\n1. deleting the old client cert secret\r\n2. running `kubectl cnpg certificate` cmd again\r\n3. restarting the corresponding container\r\n### Describe the solution you'd like\r\nIt would be cool to have cnpg resource (CRD) which one could create and the cnpg operator would create a corresponding client-cert kubernetes secret, e.g. like this:\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: ClientCertificate\r\nmetadata:\r\n  name: my-client-certificate\r\n  namespace: default\r\n  labels:\r\n    k8s-sidecar/watch: \"true\"\r\n  annotations:\r\n    k8s-sidecar-target-directory: \"certs\"\r\nspec:\r\n  cluster: \"cnpg-cluster\"\r\n  certificateDuration: \"30d\"\r\n  user: \"app\"\r\n  revokeOnRenewal: \"true\"\r\n  secretName: \"my-client-certificate\"\r\n```\r\nAs you can see I also would like to assign custom labels/annotations to the resource, so that I could e.g. `kiwigrid/k8s-sidecar` to dynamically inject an updated `secret` into a running container.\r\nMaybe the operator could also revoke old client certificates when a new one has been created.\r\n### Describe alternatives you've considered\r\nI created a kubernetes `CronJob` for this task, but in my opinion such a task is better suited in the cnpg-operator:\r\n```yaml\r\napiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  name: scripts-krew-plugin\r\n  namespace: cloudnative-pg\r\ndata:\r\n  download-krew.sh: |\r\n    #!/bin/sh\r\n    (\r\n      set -x; cd \"$(mktemp -d)\" &&\r\n      OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &&\r\n      ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &&\r\n      KREW=\"krew-${OS}_${ARCH}\" &&\r\n      curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &&\r\n      tar zxvf \"${KREW}.tar.gz\" -C /krew\r\n    )\r\n  install-krew.sh: |\r\n    #!/bin/bash\r\n    (\r\n      set -x; OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &&\r\n      ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &&\r\n      KREW=\"krew-${OS}_${ARCH}\" &&\r\n      /krew/\"${KREW}\" install krew\r\n    )\r\n```\r\n```yaml\r\napiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  name: create-client-cert-cnpg-ejabberd-testing-01\r\n  namespace: cloudnative-pg\r\ndata:\r\n  create-client-cert.sh: |\r\n    #!/bin/bash\r\n    echo \"> Install krew plugin ...\"\r\n    export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\r\n    kubectl krew install cnpg\r\n    echo \"> Check if cnpg client secret exists, delete old ...\"\r\n    if [ \"$(kubectl get secret $CNPG_CLUSTER_CLIENT_SECRET_NAME --namespace $CNPG_CLUSTER_NAMESPACE)\" ]\r\n    then kubectl delete secret $CNPG_CLUSTER_CLIENT_SECRET_NAME --namespace $CNPG_CLUSTER_NAMESPACE\r\n    fi\r\n    echo \"> Create cnpg client secret for $CNPG_CLUSTER_USERNAME ...\"\r\n    kubectl cnpg certificate $CNPG_CLUSTER_CLIENT_SECRET_NAME \\\r\n      --namespace $CNPG_CLUSTER_NAMESPACE \\\r\n      --cnpg-cluster $CNPG_CLUSTER_NAME \\\r\n      --cnpg-user $CNPG_CLUSTER_USERNAME\r\n    echo \"> Label and annotate secret for k8s-watcher ...\"\r\n    kubectl label secret $CNPG_CLUSTER_CLIENT_SECRET_NAME \\\r\n      --namespace $CNPG_CLUSTER_NAMESPACE \\\r\n      \"helm-ejabberd/testing=true\"\r\n    kubectl label secret $CNPG_CLUSTER_CLIENT_SECRET_NAME \\\r\n      --namespace $CNPG_CLUSTER_NAMESPACE \\\r\n      \"cnpg.io/reload=\"\r\n    kubectl annotate secret $CNPG_CLUSTER_CLIENT_SECRET_NAME \\\r\n      --namespace $CNPG_CLUSTER_NAMESPACE \\\r\n      \"k8s-sidecar-target-directory=certs/cnpg-tls\"\r\n    echo \"> Label and annotate $CNPG_CLUSTER_NAME-ca for k8s-watcher ...\"\r\n    kubectl label secret $CNPG_CLUSTER_NAME-ca \\\r\n      --namespace $CNPG_CLUSTER_NAMESPACE \\\r\n      \"helm-ejabberd/testing=true\"\r\n    kubectl annotate secret $CNPG_CLUSTER_NAME-ca \\\r\n      --namespace $CNPG_CLUSTER_NAMESPACE \\\r\n      \"k8s-sidecar-target-directory=certs/cnpg-tls\"\r\n---\r\napiVersion: batch/v1\r\nkind: CronJob\r\nmetadata:\r\n  name: client-cert-cnpg-ejabberd-testing-01\r\n  namespace: cloudnative-pg\r\nspec:\r\n  schedule: '@weekly'\r\n  concurrencyPolicy: Forbid\r\n  jobTemplate:\r\n    spec:\r\n      template:\r\n        metadata:\r\n          labels:\r\n            app: client-cert-cnpg-ejabberd-testing-01\r\n        spec:\r\n          restartPolicy: Never\r\n          serviceAccountName: cloudnative-pg\r\n          initContainers:\r\n            - image: docker.io/curlimages/curl:8.6.0\r\n              name: download-krew\r\n              command: [\"/bin/sh\", \"-c\", \"/script/download-krew.sh\"]\r\n              volumeMounts:\r\n              - name: scripts-krew-plugin\r\n                mountPath: /script/download-krew.sh\r\n                subPath: download-krew.sh\r\n                readOnly: true\r\n              - name: tmpfs\r\n                mountPath: /krew\r\n                subPath: krew\r\n          containers:\r\n            - image: docker.io/bitnami/kubectl:1.29.1\r\n              name: create-client-certs\r\n              command:\r\n                - \"bin/bash\"\r\n                - \"-c\"\r\n                - \"/script/install-krew.sh && /script/create-client-cert.sh\"\r\n              env:\r\n              - name: CNPG_CLUSTER_NAME\r\n                value: \"cnpg-ejabberd-testing-01\"\r\n              - name: CNPG_CLUSTER_NAMESPACE\r\n                value: \"ejabberd\"\r\n              - name: CNPG_CLUSTER_USERNAME\r\n                value: \"ejabberd\"\r\n              - name: CNPG_CLUSTER_CLIENT_SECRET_NAME\r\n                value: \"cnpg-ejabberd-testing-user\"\r\n              volumeMounts:\r\n              - name: scripts-krew-plugin\r\n                mountPath: /script/install-krew.sh\r\n                subPath: install-krew.sh\r\n                readOnly: true\r\n              - name: create-cnpg-client-cert\r\n                mountPath: /script/create-client-cert.sh\r\n                subPath: create-client-cert.sh\r\n                readOnly: true\r\n              - name: tmpfs\r\n                mountPath: /krew\r\n                subPath: krew\r\n              - name: tmpfs\r\n                mountPath: /.krew\r\n                subPath: krew-home\r\n          volumes:\r\n          - name: tmpfs\r\n            emptyDir:\r\n              defaultMode: 777\r\n          - name: scripts-krew-plugin\r\n            configMap:\r\n              name: scripts-krew-plugin # part of cloudnative-pg kustomization\r\n              defaultMode: 0755\r\n              items:\r\n              - key: download-krew.sh\r\n                path: download-krew.sh\r\n              - key: install-krew.sh\r\n                path: install-krew.sh\r\n          - name: create-cnpg-client-cert\r\n            configMap:\r\n              name: create-client-cert-cnpg-ejabberd-testing-01\r\n              defaultMode: 0755\r\n              items:\r\n              - key: create-client-cert.sh\r\n                path: create-client-cert.sh\r\n---\r\napiVersion: batch/v1\r\nkind: Job\r\nmetadata:\r\n  name: client-cert-cnpg-ejabberd-testing-01\r\n  namespace: cloudnative-pg\r\n  labels:\r\n    app: client-cert-cnpg-ejabberd-testing-01\r\nspec:\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: client-cert-cnpg-ejabberd-testing-01\r\n    spec:\r\n      restartPolicy: Never\r\n      serviceAccountName: cloudnative-pg\r\n      initContainers:\r\n        - name: download-krew\r\n          image: docker.io/curlimages/curl:8.6.0\r\n          command:\r\n            - /bin/sh\r\n            - '-c'\r\n            - /script/download-krew.sh\r\n          volumeMounts:\r\n            - name: scripts-krew-plugin\r\n              readOnly: true\r\n              mountPath: /script/download-krew.sh\r\n              subPath: download-krew.sh\r\n            - name: tmpfs\r\n              mountPath: /krew\r\n              subPath: krew\r\n      containers:\r\n        - name: create-client-certs\r\n          image: docker.io/bitnami/kubectl\r\n          command:\r\n            - bin/bash\r\n            - '-c'\r\n            - \"/script/install-krew.sh && /script/create-client-cert.sh\"\r\n          env:\r\n          - name: CNPG_CLUSTER_NAME\r\n            value: \"cnpg-ejabberd-testing-01\"\r\n          - name: CNPG_CLUSTER_NAMESPACE\r\n            value: \"ejabberd\"\r\n          - name: CNPG_CLUSTER_USERNAME\r\n            value: \"ejabberd\"\r\n          - name: CNPG_CLUSTER_CLIENT_SECRET_NAME\r\n            value: \"cnpg-ejabberd-testing-user\"\r\n          volumeMounts:\r\n            - name: scripts-krew-plugin\r\n              readOnly: true\r\n              mountPath: /script/install-krew.sh\r\n              subPath: install-krew.sh\r\n            - name: create-cnpg-client-cert\r\n              readOnly: true\r\n              mountPath: /script/create-client-cert.sh\r\n              subPath: create-client-cert.sh\r\n            - name: tmpfs\r\n              mountPath: /krew\r\n              subPath: krew\r\n            - name: tmpfs\r\n              mountPath: /.krew\r\n              subPath: krew-home\r\n      volumes:\r\n        - name: tmpfs\r\n          emptyDir:\r\n            defaultMode: 777\r\n        - name: scripts-krew-plugin\r\n          configMap:\r\n            name: scripts-krew-plugin\r\n            items:\r\n              - key: download-krew.sh\r\n                path: download-krew.sh\r\n              - key: install-krew.sh\r\n                path: install-krew.sh\r\n            defaultMode: 0755\r\n        - name: create-cnpg-client-cert\r\n          configMap:\r\n            name: create-client-cert-cnpg-ejabberd-testing-01\r\n            items:\r\n              - key: create-client-cert.sh\r\n                path: create-client-cert.sh\r\n            defaultMode: 0755\r\n```\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nNo (but I am happy to test)\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nUsing client certificate authentication requires to create client certificates with the `kubectl cnpg certificate` cmd - at least this is the only know method to me :)\r\nThe corresponding created secret can be mounted into a container for authentication to a respective cnpg cluster. \r\nThe client certificates expire after 90 days, meaning the need to be renewed by:\r\n1. deleting the old client cert secret\r\n2. running `kubectl cnpg certificate` cmd again\r\n3. restarting the corresponding container\r\n### Describe the solution you'd like\r\nIt would be cool to have cnpg resource (CRD) which one could create and the cnpg operator would create a corresponding client-cert kubernetes secret, e.g. like this:\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: ClientCertificate\r\nmetadata:\r\n  name: my-client-certificate\r\n  namespace: default\r\n  labels:\r\n    k8s-sidecar/watch: \"true\"\r\n  annotations:\r\n    k8s-sidecar-target-directory: \"certs\"\r\nspec:\r\n  cluster: \"cnpg-cluster\"\r\n  certificateDuration: \"30d\"\r\n  user: \"app\"\r\n  revokeOnRenewal: \"true\"\r\n  secretName: \"my-client-certificate\"\r\n```\r\nAs you can see I also would like to assign custom labels/annotations to the resource, so that I could e.g. `kiwigrid/k8s-sidecar` to dynamically inject an updated `secret` into a running container.\r\nMaybe the operator could also revoke old client certificates when a new one has been created.\r\n### Describe alternatives you've considered\r\nI created a kubernetes `CronJob` for this task, but in my opinion such a task is better suited in the cnpg-operator:\r\n```yaml\r\napiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  name: scripts-krew-plugin\r\n  namespace: cloudnative-pg\r\ndata:\r\n  download-krew.sh: |\r\n    #!/bin/sh\r\n    (\r\n      set -x; cd \"$(mktemp -d)\" &&\r\n      OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &&\r\n      ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &&\r\n      KREW=\"krew-${OS}_${ARCH}\" &&\r\n      curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &&\r\n      tar zxvf \"${KREW}.tar.gz\" -C /krew\r\n    )\r\n  install-krew.sh: |\r\n    #!/bin/bash\r\n    (\r\n      set -x; OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &&\r\n      ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &&\r\n      KREW=\"krew-${OS}_${ARCH}\" &&\r\n      /krew/\"${KREW}\" install krew\r\n    )\r\n```\r\n```yaml\r\napiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  name: create-client-cert-cnpg-ejabberd-testing-01\r\n  namespace: cloudnative-pg\r\ndata:\r\n  create-client-cert.sh: |\r\n    #!/bin/bash\r\n    echo \"> Install krew plugin ...\"\r\n    export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\r\n    kubectl krew install cnpg\r\n    echo \"> Check if cnpg client secret exists, delete old ...\"\r\n    if [ \"$(kubectl get secret $CNPG_CLUSTER_CLIENT_SECRET_NAME --namespace $CNPG_CLUSTER_NAMESPACE)\" ]\r\n    then kubectl delete secret $CNPG_CLUSTER_CLIENT_SECRET_NAME --namespace $CNPG_CLUSTER_NAMESPACE\r\n    fi\r\n    echo \"> Create cnpg client secret for $CNPG_CLUSTER_USERNAME ...\"\r\n    kubectl cnpg certificate $CNPG_CLUSTER_CLIENT_SECRET_NAME \\\r\n      --namespace $CNPG_CLUSTER_NAMESPACE \\\r\n      --cnpg-cluster $CNPG_CLUSTER_NAME \\\r\n      --cnpg-user $CNPG_CLUSTER_USERNAME\r\n    echo \"> Label and annotate secret for k8s-watcher ...\"\r\n    kubectl label secret $CNPG_CLUSTER_CLIENT_SECRET_NAME \\\r\n      --namespace $CNPG_CLUSTER_NAMESPACE \\\r\n      \"helm-ejabberd/testing=true\"\r\n    kubectl label secret $CNPG_CLUSTER_CLIENT_SECRET_NAME \\\r\n      --namespace $CNPG_CLUSTER_NAMESPACE \\\r\n      \"cnpg.io/reload=\"\r\n    kubectl annotate secret $CNPG_CLUSTER_CLIENT_SECRET_NAME \\\r\n      --namespace $CNPG_CLUSTER_NAMESPACE \\\r\n      \"k8s-sidecar-target-directory=certs/cnpg-tls\"\r\n    echo \"> Label and annotate $CNPG_CLUSTER_NAME-ca for k8s-watcher ...\"\r\n    kubectl label secret $CNPG_CLUSTER_NAME-ca \\\r\n      --namespace $CNPG_CLUSTER_NAMESPACE \\\r\n      \"helm-ejabberd/testing=true\"\r\n    kubectl annotate secret $CNPG_CLUSTER_NAME-ca \\\r\n      --namespace $CNPG_CLUSTER_NAMESPACE \\\r\n      \"k8s-sidecar-target-directory=certs/cnpg-tls\"\r\n---\r\napiVersion: batch/v1\r\nkind: CronJob\r\nmetadata:\r\n  name: client-cert-cnpg-ejabberd-testing-01\r\n  namespace: cloudnative-pg\r\nspec:\r\n  schedule: '@weekly'\r\n  concurrencyPolicy: Forbid\r\n  jobTemplate:\r\n    spec:\r\n      template:\r\n        metadata:\r\n          labels:\r\n            app: client-cert-cnpg-ejabberd-testing-01\r\n        spec:\r\n          restartPolicy: Never\r\n          serviceAccountName: cloudnative-pg\r\n          initContainers:\r\n            - image: docker.io/curlimages/curl:8.6.0\r\n              name: download-krew\r\n              command: [\"/bin/sh\", \"-c\", \"/script/download-krew.sh\"]\r\n              volumeMounts:\r\n              - name: scripts-krew-plugin\r\n                mountPath: /script/download-krew.sh\r\n                subPath: download-krew.sh\r\n                readOnly: true\r\n              - name: tmpfs\r\n                mountPath: /krew\r\n                subPath: krew\r\n          containers:\r\n            - image: docker.io/bitnami/kubectl:1.29.1\r\n              name: create-client-certs\r\n              command:\r\n                - \"bin/bash\"\r\n                - \"-c\"\r\n                - \"/script/install-krew.sh && /script/create-client-cert.sh\"\r\n              env:\r\n              - name: CNPG_CLUSTER_NAME\r\n                value: \"cnpg-ejabberd-testing-01\"\r\n              - name: CNPG_CLUSTER_NAMESPACE\r\n                value: \"ejabberd\"\r\n              - name: CNPG_CLUSTER_USERNAME\r\n                value: \"ejabberd\"\r\n              - name: CNPG_CLUSTER_CLIENT_SECRET_NAME\r\n                value: \"cnpg-ejabberd-testing-user\"\r\n              volumeMounts:\r\n              - name: scripts-krew-plugin\r\n                mountPath: /script/install-krew.sh\r\n                subPath: install-krew.sh\r\n                readOnly: true\r\n              - name: create-cnpg-client-cert\r\n                mountPath: /script/create-client-cert.sh\r\n                subPath: create-client-cert.sh\r\n                readOnly: true\r\n              - name: tmpfs\r\n                mountPath: /krew\r\n                subPath: krew\r\n              - name: tmpfs\r\n                mountPath: /.krew\r\n                subPath: krew-home\r\n          volumes:\r\n          - name: tmpfs\r\n            emptyDir:\r\n              defaultMode: 777\r\n          - name: scripts-krew-plugin\r\n            configMap:\r\n              name: scripts-krew-plugin # part of cloudnative-pg kustomization\r\n              defaultMode: 0755\r\n              items:\r\n              - key: download-krew.sh\r\n                path: download-krew.sh\r\n              - key: install-krew.sh\r\n                path: install-krew.sh\r\n          - name: create-cnpg-client-cert\r\n            configMap:\r\n              name: create-client-cert-cnpg-ejabberd-testing-01\r\n              defaultMode: 0755\r\n              items:\r\n              - key: create-client-cert.sh\r\n                path: create-client-cert.sh\r\n---\r\napiVersion: batch/v1\r\nkind: Job\r\nmetadata:\r\n  name: client-cert-cnpg-ejabberd-testing-01\r\n  namespace: cloudnative-pg\r\n  labels:\r\n    app: client-cert-cnpg-ejabberd-testing-01\r\nspec:\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: client-cert-cnpg-ejabberd-testing-01\r\n    spec:\r\n      restartPolicy: Never\r\n      serviceAccountName: cloudnative-pg\r\n      initContainers:\r\n        - name: download-krew\r\n          image: docker.io/curlimages/curl:8.6.0\r\n          command:\r\n            - /bin/sh\r\n            - '-c'\r\n            - /script/download-krew.sh\r\n          volumeMounts:\r\n            - name: scripts-krew-plugin\r\n              readOnly: true\r\n              mountPath: /script/download-krew.sh\r\n              subPath: download-krew.sh\r\n            - name: tmpfs\r\n              mountPath: /krew\r\n              subPath: krew\r\n      containers:\r\n        - name: create-client-certs\r\n          image: docker.io/bitnami/kubectl\r\n          command:\r\n            - bin/bash\r\n            - '-c'\r\n            - \"/script/install-krew.sh && /script/create-client-cert.sh\"\r\n          env:\r\n          - name: CNPG_CLUSTER_NAME\r\n            value: \"cnpg-ejabberd-testing-01\"\r\n          - name: CNPG_CLUSTER_NAMESPACE\r\n            value: \"ejabberd\"\r\n          - name: CNPG_CLUSTER_USERNAME\r\n            value: \"ejabberd\"\r\n          - name: CNPG_CLUSTER_CLIENT_SECRET_NAME\r\n            value: \"cnpg-ejabberd-testing-user\"\r\n          volumeMounts:\r\n            - name: scripts-krew-plugin\r\n              readOnly: true\r\n              mountPath: /script/install-krew.sh\r\n              subPath: install-krew.sh\r\n            - name: create-cnpg-client-cert\r\n              readOnly: true\r\n              mountPath: /script/create-client-cert.sh\r\n              subPath: create-client-cert.sh\r\n            - name: tmpfs\r\n              mountPath: /krew\r\n              subPath: krew\r\n            - name: tmpfs\r\n              mountPath: /.krew\r\n              subPath: krew-home\r\n      volumes:\r\n        - name: tmpfs\r\n          emptyDir:\r\n            defaultMode: 777\r\n        - name: scripts-krew-plugin\r\n          configMap:\r\n            name: scripts-krew-plugin\r\n            items:\r\n              - key: download-krew.sh\r\n                path: download-krew.sh\r\n              - key: install-krew.sh\r\n                path: install-krew.sh\r\n            defaultMode: 0755\r\n        - name: create-cnpg-client-cert\r\n          configMap:\r\n            name: create-client-cert-cnpg-ejabberd-testing-01\r\n            items:\r\n              - key: create-client-cert.sh\r\n                path: create-client-cert.sh\r\n            defaultMode: 0755\r\n```\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nNo (but I am happy to test)\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Add the pprof service to the instance manager",
        "id": 2177816351,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently we don't know the amount of resources consumed by the instance manager inside every PostgreSQL pod, which let us without knowledge if it's behaving properly or not \n### Describe the solution you'd like\nAdd the possibility to start the pprof server inside every instance to report the proper information to the pyroscope/grafana to gather the data and generate the proper graphs.\r\nThis needs to be disabled or enabled independently to the pprof server running on the operator itself since it may not be desired to have both running at the same time.\r\nThe proposal is to add this as another parameter passed to the pods at starting point, for every process including: bootstrap, run, join, etc. the meaning it's to facilitate the gather of data on every process.\r\nDoing this profiling could help to improve the memory and cpu usage as well as the resources used by the goroutines.\n### Describe alternatives you've considered\nThere's no alternative solution to do a proper profiling that can be consistently used in the future\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrently we don't know the amount of resources consumed by the instance manager inside every PostgreSQL pod, which let us without knowledge if it's behaving properly or not \n### Describe the solution you'd like\nAdd the possibility to start the pprof server inside every instance to report the proper information to the pyroscope/grafana to gather the data and generate the proper graphs.\r\nThis needs to be disabled or enabled independently to the pprof server running on the operator itself since it may not be desired to have both running at the same time.\r\nThe proposal is to add this as another parameter passed to the pods at starting point, for every process including: bootstrap, run, join, etc. the meaning it's to facilitate the gather of data on every process.\r\nDoing this profiling could help to improve the memory and cpu usage as well as the resources used by the goroutines.\n### Describe alternatives you've considered\nThere's no alternative solution to do a proper profiling that can be consistently used in the future\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]:  No Target Backup Found with backup available",
        "id": 2172315242,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ninfo@kjeldschouten.nl\n### Version\n1.22.1\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nHelm\n### What happened?\nHaving made a base-backup to backblaze B2, which is visible from the B2 webinterface in the designated bucket.\r\nThe restore fails stating \"no target backup found\", yet the backup is there.\r\nAlso trying to set serverName to no avail.\r\nThe error is non-descriptive as-fuck, doesn't give any reasonable output.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    cnpg.io/hibernation: 'off'\r\n    meta.helm.sh/release-name: authentik\r\n    meta.helm.sh/release-namespace: authentik\r\n    rollme: PkRth\r\n  creationTimestamp: '2024-03-06T19:52:31Z'\r\n  generation: 1\r\n  labels:\r\n    app: authentik-24.2.5\r\n    app.kubernetes.io/instance: authentik\r\n    app.kubernetes.io/managed-by: Helm\r\n    app.kubernetes.io/name: authentik\r\n    app.kubernetes.io/version: 2024.2.1\r\n    cnpg.io/reload: 'on'\r\n    helm-revision: '1'\r\n    helm.sh/chart: authentik-24.2.5\r\n    helm.toolkit.fluxcd.io/name: authentik\r\n    helm.toolkit.fluxcd.io/namespace: authentik\r\n    release: authentik\r\n  managedFields:\r\n    - apiVersion: postgresql.cnpg.io/v1\r\n      fieldsType: FieldsV1\r\n      fieldsV1:\r\n        f:metadata:\r\n          f:annotations:\r\n            .: {}\r\n            f:cnpg.io/hibernation: {}\r\n            f:meta.helm.sh/release-name: {}\r\n            f:meta.helm.sh/release-namespace: {}\r\n            f:rollme: {}\r\n          f:labels:\r\n            .: {}\r\n            f:app: {}\r\n            f:app.kubernetes.io/instance: {}\r\n            f:app.kubernetes.io/managed-by: {}\r\n            f:app.kubernetes.io/name: {}\r\n            f:app.kubernetes.io/version: {}\r\n            f:cnpg.io/reload: {}\r\n            f:helm-revision: {}\r\n            f:helm.sh/chart: {}\r\n            f:helm.toolkit.fluxcd.io/name: {}\r\n            f:helm.toolkit.fluxcd.io/namespace: {}\r\n            f:release: {}\r\n        f:spec:\r\n          .: {}\r\n          f:bootstrap:\r\n            .: {}\r\n            f:recovery:\r\n              .: {}\r\n              f:database: {}\r\n              f:owner: {}\r\n              f:secret:\r\n                .: {}\r\n                f:name: {}\r\n              f:source: {}\r\n          f:enableSuperuserAccess: {}\r\n          f:externalClusters: {}\r\n          f:failoverDelay: {}\r\n          f:instances: {}\r\n          f:logLevel: {}\r\n          f:maxSyncReplicas: {}\r\n          f:minSyncReplicas: {}\r\n          f:monitoring:\r\n            .: {}\r\n            f:disableDefaultQueries: {}\r\n            f:enablePodMonitor: {}\r\n          f:nodeMaintenanceWindow:\r\n            .: {}\r\n            f:inProgress: {}\r\n            f:reusePVC: {}\r\n          f:postgresGID: {}\r\n          f:postgresUID: {}\r\n          f:primaryUpdateMethod: {}\r\n          f:primaryUpdateStrategy: {}\r\n          f:replicationSlots:\r\n            .: {}\r\n            f:highAvailability:\r\n              .: {}\r\n              f:enabled: {}\r\n              f:slotPrefix: {}\r\n            f:updateInterval: {}\r\n          f:resources:\r\n            .: {}\r\n            f:limits:\r\n              .: {}\r\n              f:cpu: {}\r\n              f:memory: {}\r\n            f:requests:\r\n              .: {}\r\n              f:cpu: {}\r\n              f:memory: {}\r\n          f:smartShutdownTimeout: {}\r\n          f:startDelay: {}\r\n          f:stopDelay: {}\r\n          f:storage:\r\n            .: {}\r\n            f:pvcTemplate:\r\n              .: {}\r\n              f:accessModes: {}\r\n              f:resources:\r\n                .: {}\r\n                f:requests:\r\n                  .: {}\r\n                  f:storage: {}\r\n              f:storageClassName: {}\r\n            f:resizeInUseVolumes: {}\r\n          f:switchoverDelay: {}\r\n          f:walStorage:\r\n            .: {}\r\n            f:pvcTemplate:\r\n              .: {}\r\n              f:accessModes: {}\r\n              f:resources:\r\n                .: {}\r\n                f:requests:\r\n                  .: {}\r\n                  f:storage: {}\r\n              f:storageClassName: {}\r\n            f:resizeInUseVolumes: {}\r\n      manager: helm-controller\r\n      operation: Update\r\n      time: '2024-03-06T19:52:31Z'\r\n    - apiVersion: postgresql.cnpg.io/v1\r\n      fieldsType: FieldsV1\r\n      fieldsV1:\r\n        f:status:\r\n          .: {}\r\n          f:certificates:\r\n            .: {}\r\n            f:clientCASecret: {}\r\n            f:expirations:\r\n              .: {}\r\n              f:authentik-cnpg-main-ca: {}\r\n              f:authentik-cnpg-main-replication: {}\r\n              f:authentik-cnpg-main-server: {}\r\n            f:replicationTLSSecret: {}\r\n            f:serverAltDNSNames: {}\r\n            f:serverCASecret: {}\r\n            f:serverTLSSecret: {}\r\n          f:cloudNativePGCommitHash: {}\r\n          f:cloudNativePGOperatorHash: {}\r\n          f:conditions: {}\r\n          f:configMapResourceVersion:\r\n            .: {}\r\n            f:metrics:\r\n              .: {}\r\n              f:cnpg-default-monitoring: {}\r\n          f:initializingPVC: {}\r\n          f:instanceNames: {}\r\n          f:instances: {}\r\n          f:jobCount: {}\r\n          f:latestGeneratedNode: {}\r\n          f:managedRolesStatus: {}\r\n          f:phase: {}\r\n          f:phaseReason: {}\r\n          f:poolerIntegrations:\r\n            .: {}\r\n            f:pgBouncerIntegration:\r\n              .: {}\r\n              f:secrets: {}\r\n          f:pvcCount: {}\r\n          f:readService: {}\r\n          f:secretsResourceVersion:\r\n            .: {}\r\n            f:applicationSecretVersion: {}\r\n            f:clientCaSecretVersion: {}\r\n            f:replicationSecretVersion: {}\r\n            f:serverCaSecretVersion: {}\r\n            f:serverSecretVersion: {}\r\n            f:superuserSecretVersion: {}\r\n          f:targetPrimary: {}\r\n          f:targetPrimaryTimestamp: {}\r\n          f:topology:\r\n            .: {}\r\n            f:successfullyExtracted: {}\r\n          f:writeService: {}\r\n      manager: manager\r\n      operation: Update\r\n      subresource: status\r\n      time: '2024-03-06T19:52:37Z'\r\n  name: authentik-cnpg-main\r\n  namespace: authentik\r\n  resourceVersion: '57200282'\r\n  uid: 983c6b00-9052-457d-83c4-7e78b6400dfe\r\n  selfLink: >-\r\n    /apis/postgresql.cnpg.io/v1/namespaces/authentik/clusters/authentik-cnpg-main\r\nstatus:\r\n  certificates:\r\n    clientCASecret: authentik-cnpg-main-ca\r\n    expirations:\r\n      authentik-cnpg-main-ca: 2024-06-04 19:47:33 +0000 UTC\r\n      authentik-cnpg-main-replication: 2024-06-04 19:47:33 +0000 UTC\r\n      authentik-cnpg-main-server: 2024-06-04 19:47:33 +0000 UTC\r\n    replicationTLSSecret: authentik-cnpg-main-replication\r\n    serverAltDNSNames:\r\n      - authentik-cnpg-main-rw\r\n      - authentik-cnpg-main-rw.authentik\r\n      - authentik-cnpg-main-rw.authentik.svc\r\n      - authentik-cnpg-main-r\r\n      - authentik-cnpg-main-r.authentik\r\n      - authentik-cnpg-main-r.authentik.svc\r\n      - authentik-cnpg-main-ro\r\n      - authentik-cnpg-main-ro.authentik\r\n      - authentik-cnpg-main-ro.authentik.svc\r\n    serverCASecret: authentik-cnpg-main-ca\r\n    serverTLSSecret: authentik-cnpg-main-server\r\n  cloudNativePGCommitHash: c7be872e\r\n  cloudNativePGOperatorHash: 262d86af058f59462fdccaec51231a1afd888153413f63224f60e458fcd335be\r\n  conditions:\r\n    - lastTransitionTime: '2024-03-06T19:52:34Z'\r\n      message: Cluster Is Not Ready\r\n      reason: ClusterIsNotReady\r\n      status: 'False'\r\n      type: Ready\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: '57200127'\r\n  initializingPVC:\r\n    - authentik-cnpg-main-1\r\n    - authentik-cnpg-main-1-wal\r\n  instanceNames:\r\n    - authentik-cnpg-main-1\r\n  instances: 1\r\n  jobCount: 1\r\n  latestGeneratedNode: 1\r\n  managedRolesStatus: {}\r\n  phase: Setting up primary\r\n  phaseReason: Creating primary instance authentik-cnpg-main-1\r\n  poolerIntegrations:\r\n    pgBouncerIntegration:\r\n      secrets:\r\n        - authentik-cnpg-main-pooler\r\n  pvcCount: 2\r\n  readService: authentik-cnpg-main-r\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: '57199801'\r\n    clientCaSecretVersion: '57200026'\r\n    replicationSecretVersion: '57200031'\r\n    serverCaSecretVersion: '57200026'\r\n    serverSecretVersion: '57200028'\r\n    superuserSecretVersion: '57200038'\r\n  targetPrimary: authentik-cnpg-main-1\r\n  targetPrimaryTimestamp: '2024-03-06T19:52:34.615107Z'\r\n  topology:\r\n    successfullyExtracted: true\r\n  writeService: authentik-cnpg-main-rw\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n  bootstrap:\r\n    recovery:\r\n      database: authentik\r\n      owner: authentik\r\n      secret:\r\n        name: authentik-cnpg-main-user\r\n      source: objectStoreRecoveryCluster\r\n  enableSuperuserAccess: true\r\n  externalClusters:\r\n    - barmanObjectStore:\r\n        destinationPath: s3://kjeldschouten-cnpg-authentik/\r\n        endpointURL: https://s3.eu-central-003.backblazeb2.com\r\n        s3Credentials:\r\n          accessKeyId:\r\n            key: ACCESS_KEY_ID\r\n            name: authentik-cnpg-main-provider-recovery-s3-creds\r\n          secretAccessKey:\r\n            key: ACCESS_SECRET_KEY\r\n            name: authentik-cnpg-main-provider-recovery-s3-creds\r\n      name: objectStoreRecoveryCluster\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  instances: 2\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n      - key: queries\r\n        name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: 'on'\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: '0'\r\n      log_rotation_size: '0'\r\n      log_truncate_on_rotation: 'false'\r\n      logging_collector: 'on'\r\n      max_parallel_workers: '32'\r\n      max_replication_slots: '32'\r\n      max_worker_processes: '32'\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: ''\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: switchover\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      cpu: '4'\r\n      memory: 8Gi\r\n    requests:\r\n      cpu: 10m\r\n      memory: 50Mi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 100Gi\r\n      storageClassName: openebs-hostpath\r\n    resizeInUseVolumes: true\r\n  switchoverDelay: 3600\r\n  walStorage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 100Gi\r\n      storageClassName: openebs-hostpath\r\n    resizeInUseVolumes: true\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-03-06T19:52:44Z\",\"msg\":\"Recovering from external cluster\",\"logging_pod\":\"authentik-cnpg-main-1-full-recovery\",\"sourceName\":\"objectStoreRecoveryCluster\"}\r\n{\"level\":\"error\",\"ts\":\"2024-03-06T19:52:44Z\",\"msg\":\"Error while restoring a backup\",\"logging_pod\":\"authentik-cnpg-main-1-full-recovery\",\"error\":\"no target backup found\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/cmd.go:89\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.NewCmd.func2\\n\\tinternal/cmd/manager/instance/restore/cmd.go:60\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.6/x64/src/runtime/proc.go:267\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ninfo@kjeldschouten.nl\n### Version\n1.22.1\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nHelm\n### What happened?\nHaving made a base-backup to backblaze B2, which is visible from the B2 webinterface in the designated bucket.\r\nThe restore fails stating \"no target backup found\", yet the backup is there.\r\nAlso trying to set serverName to no avail.\r\nThe error is non-descriptive as-fuck, doesn't give any reasonable output.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    cnpg.io/hibernation: 'off'\r\n    meta.helm.sh/release-name: authentik\r\n    meta.helm.sh/release-namespace: authentik\r\n    rollme: PkRth\r\n  creationTimestamp: '2024-03-06T19:52:31Z'\r\n  generation: 1\r\n  labels:\r\n    app: authentik-24.2.5\r\n    app.kubernetes.io/instance: authentik\r\n    app.kubernetes.io/managed-by: Helm\r\n    app.kubernetes.io/name: authentik\r\n    app.kubernetes.io/version: 2024.2.1\r\n    cnpg.io/reload: 'on'\r\n    helm-revision: '1'\r\n    helm.sh/chart: authentik-24.2.5\r\n    helm.toolkit.fluxcd.io/name: authentik\r\n    helm.toolkit.fluxcd.io/namespace: authentik\r\n    release: authentik\r\n  managedFields:\r\n    - apiVersion: postgresql.cnpg.io/v1\r\n      fieldsType: FieldsV1\r\n      fieldsV1:\r\n        f:metadata:\r\n          f:annotations:\r\n            .: {}\r\n            f:cnpg.io/hibernation: {}\r\n            f:meta.helm.sh/release-name: {}\r\n            f:meta.helm.sh/release-namespace: {}\r\n            f:rollme: {}\r\n          f:labels:\r\n            .: {}\r\n            f:app: {}\r\n            f:app.kubernetes.io/instance: {}\r\n            f:app.kubernetes.io/managed-by: {}\r\n            f:app.kubernetes.io/name: {}\r\n            f:app.kubernetes.io/version: {}\r\n            f:cnpg.io/reload: {}\r\n            f:helm-revision: {}\r\n            f:helm.sh/chart: {}\r\n            f:helm.toolkit.fluxcd.io/name: {}\r\n            f:helm.toolkit.fluxcd.io/namespace: {}\r\n            f:release: {}\r\n        f:spec:\r\n          .: {}\r\n          f:bootstrap:\r\n            .: {}\r\n            f:recovery:\r\n              .: {}\r\n              f:database: {}\r\n              f:owner: {}\r\n              f:secret:\r\n                .: {}\r\n                f:name: {}\r\n              f:source: {}\r\n          f:enableSuperuserAccess: {}\r\n          f:externalClusters: {}\r\n          f:failoverDelay: {}\r\n          f:instances: {}\r\n          f:logLevel: {}\r\n          f:maxSyncReplicas: {}\r\n          f:minSyncReplicas: {}\r\n          f:monitoring:\r\n            .: {}\r\n            f:disableDefaultQueries: {}\r\n            f:enablePodMonitor: {}\r\n          f:nodeMaintenanceWindow:\r\n            .: {}\r\n            f:inProgress: {}\r\n            f:reusePVC: {}\r\n          f:postgresGID: {}\r\n          f:postgresUID: {}\r\n          f:primaryUpdateMethod: {}\r\n          f:primaryUpdateStrategy: {}\r\n          f:replicationSlots:\r\n            .: {}\r\n            f:highAvailability:\r\n              .: {}\r\n              f:enabled: {}\r\n              f:slotPrefix: {}\r\n            f:updateInterval: {}\r\n          f:resources:\r\n            .: {}\r\n            f:limits:\r\n              .: {}\r\n              f:cpu: {}\r\n              f:memory: {}\r\n            f:requests:\r\n              .: {}\r\n              f:cpu: {}\r\n              f:memory: {}\r\n          f:smartShutdownTimeout: {}\r\n          f:startDelay: {}\r\n          f:stopDelay: {}\r\n          f:storage:\r\n            .: {}\r\n            f:pvcTemplate:\r\n              .: {}\r\n              f:accessModes: {}\r\n              f:resources:\r\n                .: {}\r\n                f:requests:\r\n                  .: {}\r\n                  f:storage: {}\r\n              f:storageClassName: {}\r\n            f:resizeInUseVolumes: {}\r\n          f:switchoverDelay: {}\r\n          f:walStorage:\r\n            .: {}\r\n            f:pvcTemplate:\r\n              .: {}\r\n              f:accessModes: {}\r\n              f:resources:\r\n                .: {}\r\n                f:requests:\r\n                  .: {}\r\n                  f:storage: {}\r\n              f:storageClassName: {}\r\n            f:resizeInUseVolumes: {}\r\n      manager: helm-controller\r\n      operation: Update\r\n      time: '2024-03-06T19:52:31Z'\r\n    - apiVersion: postgresql.cnpg.io/v1\r\n      fieldsType: FieldsV1\r\n      fieldsV1:\r\n        f:status:\r\n          .: {}\r\n          f:certificates:\r\n            .: {}\r\n            f:clientCASecret: {}\r\n            f:expirations:\r\n              .: {}\r\n              f:authentik-cnpg-main-ca: {}\r\n              f:authentik-cnpg-main-replication: {}\r\n              f:authentik-cnpg-main-server: {}\r\n            f:replicationTLSSecret: {}\r\n            f:serverAltDNSNames: {}\r\n            f:serverCASecret: {}\r\n            f:serverTLSSecret: {}\r\n          f:cloudNativePGCommitHash: {}\r\n          f:cloudNativePGOperatorHash: {}\r\n          f:conditions: {}\r\n          f:configMapResourceVersion:\r\n            .: {}\r\n            f:metrics:\r\n              .: {}\r\n              f:cnpg-default-monitoring: {}\r\n          f:initializingPVC: {}\r\n          f:instanceNames: {}\r\n          f:instances: {}\r\n          f:jobCount: {}\r\n          f:latestGeneratedNode: {}\r\n          f:managedRolesStatus: {}\r\n          f:phase: {}\r\n          f:phaseReason: {}\r\n          f:poolerIntegrations:\r\n            .: {}\r\n            f:pgBouncerIntegration:\r\n              .: {}\r\n              f:secrets: {}\r\n          f:pvcCount: {}\r\n          f:readService: {}\r\n          f:secretsResourceVersion:\r\n            .: {}\r\n            f:applicationSecretVersion: {}\r\n            f:clientCaSecretVersion: {}\r\n            f:replicationSecretVersion: {}\r\n            f:serverCaSecretVersion: {}\r\n            f:serverSecretVersion: {}\r\n            f:superuserSecretVersion: {}\r\n          f:targetPrimary: {}\r\n          f:targetPrimaryTimestamp: {}\r\n          f:topology:\r\n            .: {}\r\n            f:successfullyExtracted: {}\r\n          f:writeService: {}\r\n      manager: manager\r\n      operation: Update\r\n      subresource: status\r\n      time: '2024-03-06T19:52:37Z'\r\n  name: authentik-cnpg-main\r\n  namespace: authentik\r\n  resourceVersion: '57200282'\r\n  uid: 983c6b00-9052-457d-83c4-7e78b6400dfe\r\n  selfLink: >-\r\n    /apis/postgresql.cnpg.io/v1/namespaces/authentik/clusters/authentik-cnpg-main\r\nstatus:\r\n  certificates:\r\n    clientCASecret: authentik-cnpg-main-ca\r\n    expirations:\r\n      authentik-cnpg-main-ca: 2024-06-04 19:47:33 +0000 UTC\r\n      authentik-cnpg-main-replication: 2024-06-04 19:47:33 +0000 UTC\r\n      authentik-cnpg-main-server: 2024-06-04 19:47:33 +0000 UTC\r\n    replicationTLSSecret: authentik-cnpg-main-replication\r\n    serverAltDNSNames:\r\n      - authentik-cnpg-main-rw\r\n      - authentik-cnpg-main-rw.authentik\r\n      - authentik-cnpg-main-rw.authentik.svc\r\n      - authentik-cnpg-main-r\r\n      - authentik-cnpg-main-r.authentik\r\n      - authentik-cnpg-main-r.authentik.svc\r\n      - authentik-cnpg-main-ro\r\n      - authentik-cnpg-main-ro.authentik\r\n      - authentik-cnpg-main-ro.authentik.svc\r\n    serverCASecret: authentik-cnpg-main-ca\r\n    serverTLSSecret: authentik-cnpg-main-server\r\n  cloudNativePGCommitHash: c7be872e\r\n  cloudNativePGOperatorHash: 262d86af058f59462fdccaec51231a1afd888153413f63224f60e458fcd335be\r\n  conditions:\r\n    - lastTransitionTime: '2024-03-06T19:52:34Z'\r\n      message: Cluster Is Not Ready\r\n      reason: ClusterIsNotReady\r\n      status: 'False'\r\n      type: Ready\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: '57200127'\r\n  initializingPVC:\r\n    - authentik-cnpg-main-1\r\n    - authentik-cnpg-main-1-wal\r\n  instanceNames:\r\n    - authentik-cnpg-main-1\r\n  instances: 1\r\n  jobCount: 1\r\n  latestGeneratedNode: 1\r\n  managedRolesStatus: {}\r\n  phase: Setting up primary\r\n  phaseReason: Creating primary instance authentik-cnpg-main-1\r\n  poolerIntegrations:\r\n    pgBouncerIntegration:\r\n      secrets:\r\n        - authentik-cnpg-main-pooler\r\n  pvcCount: 2\r\n  readService: authentik-cnpg-main-r\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: '57199801'\r\n    clientCaSecretVersion: '57200026'\r\n    replicationSecretVersion: '57200031'\r\n    serverCaSecretVersion: '57200026'\r\n    serverSecretVersion: '57200028'\r\n    superuserSecretVersion: '57200038'\r\n  targetPrimary: authentik-cnpg-main-1\r\n  targetPrimaryTimestamp: '2024-03-06T19:52:34.615107Z'\r\n  topology:\r\n    successfullyExtracted: true\r\n  writeService: authentik-cnpg-main-rw\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n  bootstrap:\r\n    recovery:\r\n      database: authentik\r\n      owner: authentik\r\n      secret:\r\n        name: authentik-cnpg-main-user\r\n      source: objectStoreRecoveryCluster\r\n  enableSuperuserAccess: true\r\n  externalClusters:\r\n    - barmanObjectStore:\r\n        destinationPath: s3://kjeldschouten-cnpg-authentik/\r\n        endpointURL: https://s3.eu-central-003.backblazeb2.com\r\n        s3Credentials:\r\n          accessKeyId:\r\n            key: ACCESS_KEY_ID\r\n            name: authentik-cnpg-main-provider-recovery-s3-creds\r\n          secretAccessKey:\r\n            key: ACCESS_SECRET_KEY\r\n            name: authentik-cnpg-main-provider-recovery-s3-creds\r\n      name: objectStoreRecoveryCluster\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  instances: 2\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n      - key: queries\r\n        name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: 'on'\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: '0'\r\n      log_rotation_size: '0'\r\n      log_truncate_on_rotation: 'false'\r\n      logging_collector: 'on'\r\n      max_parallel_workers: '32'\r\n      max_replication_slots: '32'\r\n      max_worker_processes: '32'\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: ''\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: switchover\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      cpu: '4'\r\n      memory: 8Gi\r\n    requests:\r\n      cpu: 10m\r\n      memory: 50Mi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 100Gi\r\n      storageClassName: openebs-hostpath\r\n    resizeInUseVolumes: true\r\n  switchoverDelay: 3600\r\n  walStorage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 100Gi\r\n      storageClassName: openebs-hostpath\r\n    resizeInUseVolumes: true\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-03-06T19:52:44Z\",\"msg\":\"Recovering from external cluster\",\"logging_pod\":\"authentik-cnpg-main-1-full-recovery\",\"sourceName\":\"objectStoreRecoveryCluster\"}\r\n{\"level\":\"error\",\"ts\":\"2024-03-06T19:52:44Z\",\"msg\":\"Error while restoring a backup\",\"logging_pod\":\"authentik-cnpg-main-1-full-recovery\",\"error\":\"no target backup found\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.restoreSubCommand\\n\\tinternal/cmd/manager/instance/restore/cmd.go:89\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/restore.NewCmd.func2\\n\\tinternal/cmd/manager/instance/restore/cmd.go:60\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:983\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1115\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.8.0/command.go:1039\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.6/x64/src/runtime/proc.go:267\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductIn case people wonder, the backup is there:\r\n<img width=\"626\" alt=\"image\" src=\"https://github.com/cloudnative-pg/cloudnative-pg/assets/7613738/ad46f201-8b77-4862-a786-cd3c74967810\">\r\n<img width=\"583\" alt=\"image\" src=\"https://github.com/cloudnative-pg/cloudnative-pg/assets/7613738/8682050b-212d-4063-a1ef-c26fed1ff5a7\">\n---\nand yes the credentials have access (same credentials as used for backup creation, read-write)\n---\nFound the issue:\r\nThe name of the externalClusters has to match the name of the folder the backup is under (in my case the old cluster name)\r\nI don't think this is documented very well.\n---\nThough besides this, the verbosity on backup and restore errors is completely abysmal.\r\nThe amount of \"exit 1\" errors I got during research with no explaination is shocking\n---\n> Found the issue: The name of the externalClusters has to match the name of the folder the backup is under (in my case the old cluster name)\r\n> \r\n> I don't think this is documented very well.\r\nYou saved my day, thanks!\n---\n@PrivatePuffin I'm hitting the same kind of issue. Would you mind sharing the config that worked for you? Thanks!\n---\n> @PrivatePuffin I'm hitting the same kind of issue. Would you mind sharing the config that worked for you? Thanks!\r\nExplaination is above.\r\nWe, as TrueCharts, have helm-templated this all away by now. \r\nSo I dont have \"vanilla\" examples for this"
    },
    {
        "title": "[Bug]: Cluster stuck in \"Failing over\" from defunct pod",
        "id": 2171606246,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.22.1\r\n### What version of Kubernetes are you using?\r\n1.28\r\n### What is your Kubernetes environment?\r\nOther\r\n### How did you install the operator?\r\nOLM\r\n### What happened?\r\nI have a cluster that is in the following state:\r\n```\r\nStatus:              Failing over Failing over from some-db-2 to some-db-1\r\nInstances:           2\r\nReady instances:     1\r\n[...]\r\nInstances status\r\nName                Database Size  Current LSN   Replication role  Status  QoS        Manager Version  Node\r\n----                -------------  -----------   ----------------  ------  ---        ---------------  ----\r\nsome-db-1  472 GB         31E/73000000  Primary           OK      Burstable  1.22.1           some-node.example.com\r\n```\r\nSo it's available but no new replica is created.\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  creationTimestamp: \"2024-03-03T09:48:30Z\"\r\n  generation: 11\r\n  name: some-db16\r\n  namespace: db\r\n  resourceVersion: \"447809275\"\r\n  uid: d43d2015-2b32-4f87-8253-8042af3e68d0\r\nspec:\r\n  affinity:\r\n    nodeAffinity:\r\n      requiredDuringSchedulingIgnoredDuringExecution:\r\n        nodeSelectorTerms:\r\n        - matchExpressions:\r\n          - key: node-role.kubernetes.io/db\r\n            operator: Exists\r\n    podAntiAffinityType: required\r\n    tolerations:\r\n    - effect: NoSchedule\r\n      key: dedicated\r\n      operator: Equal\r\n      value: db\r\n  backup:\r\n    barmanObjectStore:\r\n      data:\r\n        compression: bzip2\r\n      destinationPath: s3://db-some-db-wal/cnpg/16/\r\n      endpointURL: https://s3.example.com\r\n      s3Credentials:\r\n        accessKeyId:\r\n          key: AWS_ACCESS_KEY_ID\r\n          name: some-db-wal\r\n        secretAccessKey:\r\n          key: AWS_SECRET_ACCESS_KEY\r\n          name: some-db-wal\r\n      wal:\r\n        compression: bzip2\r\n        maxParallel: 8\r\n    retentionPolicy: 92d\r\n    target: prefer-standby\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enableSuperuserAccess: false\r\n  failoverDelay: 0\r\n  imageName: image-registry.openshift-image-registry.svc:5000/db/postgres:16-20240304\r\n  instances: 2\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      pg_stat_statements.track: all\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    pg_hba:\r\n    - hostssl all all all scram-sha-256\r\n    shared_preload_libraries:\r\n    - citus\r\n    - pg_uuidv7\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: supervised\r\n  projectedVolumeTemplate:\r\n    sources:\r\n    - configMap:\r\n        items:\r\n        - key: archive_gzip.sh\r\n          mode: 365\r\n          path: scripts/archive_gzip.sh\r\n        - key: archive_zstd.sh\r\n          mode: 365\r\n          path: scripts/archive_zstd.sh\r\n        - key: head_gzip.sh\r\n          mode: 365\r\n          path: scripts/head_gzip.sh\r\n        - key: head_zstd.sh\r\n          mode: 365\r\n          path: scripts/head_zstd.sh\r\n        - key: unarchive_gzip.sh\r\n          mode: 365\r\n          path: scripts/unarchive_gzip.sh\r\n        - key: unarchive_zstd.sh\r\n          mode: 365\r\n          path: scripts/unarchive_zstd.sh\r\n        name: some-db-scripts\r\n    - configMap:\r\n        items:\r\n        - key: BUCKET_HOST\r\n          path: db-archives/BUCKET_HOST\r\n        - key: BUCKET_NAME\r\n          path: db-archives/BUCKET_NAME\r\n        - key: BUCKET_PORT\r\n          path: db-archives/BUCKET_PORT\r\n        - key: BUCKET_REGION\r\n          path: db-archives/BUCKET_REGION\r\n        - key: BUCKET_SUBREGION\r\n          path: db-archives/BUCKET_SUBREGION\r\n        name: some-db-db-archives\r\n    - secret:\r\n        items:\r\n        - key: AWS_ACCESS_KEY_ID\r\n          path: db-archives/AWS_ACCESS_KEY_ID\r\n        - key: AWS_SECRET_ACCESS_KEY\r\n          path: db-archives/AWS_SECRET_ACCESS_KEY\r\n        name: some-db-db-archives\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      cpu: \"12\"\r\n      memory: 64Gi\r\n    requests:\r\n      cpu: \"1\"\r\n      memory: 4Gi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 1500Gi\r\n    storageClass: local-nvme\r\n  switchoverDelay: 3600\r\nstatus:\r\n  certificates:\r\n    clientCASecret: some-db16-ca\r\n    expirations:\r\n      some-db16-ca: 2024-06-01 09:43:30 +0000 UTC\r\n      some-db16-replication: 2024-06-01 09:43:30 +0000 UTC\r\n      some-db16-server: 2024-06-01 09:43:30 +0000 UTC\r\n    replicationTLSSecret: some-db16-replication\r\n    serverAltDNSNames:\r\n    - some-db16-rw\r\n    - some-db16-rw.db\r\n    - some-db16-rw.db.svc\r\n    - some-db16-r\r\n    - some-db16-r.db\r\n    - some-db16-r.db.svc\r\n    - some-db16-ro\r\n    - some-db16-ro.db\r\n    - some-db16-ro.db.svc\r\n    serverCASecret: some-db16-ca\r\n    serverTLSSecret: some-db16-server\r\n  cloudNativePGCommitHash: c7be872e\r\n  cloudNativePGOperatorHash: 262d86af058f59462fdccaec51231a1afd888153413f63224f60e458fcd335be\r\n  conditions:\r\n  - lastTransitionTime: \"2024-03-06T10:18:35Z\"\r\n    message: Cluster Is Not Ready\r\n    reason: ClusterIsNotReady\r\n    status: \"False\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-03-06T13:04:38Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  - lastTransitionTime: \"2024-03-06T13:39:05Z\"\r\n    message: New Backup starting up\r\n    reason: BackupStarted\r\n    status: \"False\"\r\n    type: LastBackupSucceeded\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"444990073\"\r\n  currentPrimary: some-db16-1\r\n  currentPrimaryTimestamp: \"2024-03-06T10:18:37.628192Z\"\r\n  firstRecoverabilityPoint: \"2024-03-03T17:51:00Z\"\r\n  firstRecoverabilityPointByMethod:\r\n    barmanObjectStore: \"2024-03-03T17:51:00Z\"\r\n  healthyPVC:\r\n  - some-db16-1\r\n  instanceNames:\r\n  - some-db16-1\r\n  instances: 1\r\n  instancesReportedState:\r\n    some-db16-1:\r\n      isPrimary: true\r\n      timeLineID: 5\r\n  instancesStatus:\r\n    healthy:\r\n    - some-db16-1\r\n  lastSuccessfulBackup: \"2024-03-06T05:06:09Z\"\r\n  lastSuccessfulBackupByMethod:\r\n    barmanObjectStore: \"2024-03-06T05:06:09Z\"\r\n  latestGeneratedNode: 2\r\n  managedRolesStatus:\r\n    byStatus:\r\n      not-managed:\r\n      - [...]\r\n      reconciled:\r\n      - [...]\r\n      reserved:\r\n      - postgres\r\n      - streaming_replica\r\n    passwordStatus:\r\n  onlineUpdateEnabled: true\r\n  phase: Failing over\r\n  phaseReason: Failing over from some-db16-2 to some-db16-1\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 1\r\n  readService: some-db16-r\r\n  readyInstances: 1\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"444990045\"\r\n    clientCaSecretVersion: \"444990042\"\r\n    replicationSecretVersion: \"444990044\"\r\n    serverCaSecretVersion: \"444990042\"\r\n    serverSecretVersion: \"444990043\"\r\n  targetPrimary: some-db16-1\r\n  targetPrimaryTimestamp: \"2024-03-06T10:18:35.513668Z\"\r\n  timelineID: 5\r\n  topology:\r\n    instances:\r\n      some-db16-1: {}\r\n    nodesUsed: 1\r\n    successfullyExtracted: true\r\n  writeService: some-db16-rw\r\n```\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.22.1\r\n### What version of Kubernetes are you using?\r\n1.28\r\n### What is your Kubernetes environment?\r\nOther\r\n### How did you install the operator?\r\nOLM\r\n### What happened?\r\nI have a cluster that is in the following state:\r\n```\r\nStatus:              Failing over Failing over from some-db-2 to some-db-1\r\nInstances:           2\r\nReady instances:     1\r\n[...]\r\nInstances status\r\nName                Database Size  Current LSN   Replication role  Status  QoS        Manager Version  Node\r\n----                -------------  -----------   ----------------  ------  ---        ---------------  ----\r\nsome-db-1  472 GB         31E/73000000  Primary           OK      Burstable  1.22.1           some-node.example.com\r\n```\r\nSo it's available but no new replica is created.\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  creationTimestamp: \"2024-03-03T09:48:30Z\"\r\n  generation: 11\r\n  name: some-db16\r\n  namespace: db\r\n  resourceVersion: \"447809275\"\r\n  uid: d43d2015-2b32-4f87-8253-8042af3e68d0\r\nspec:\r\n  affinity:\r\n    nodeAffinity:\r\n      requiredDuringSchedulingIgnoredDuringExecution:\r\n        nodeSelectorTerms:\r\n        - matchExpressions:\r\n          - key: node-role.kubernetes.io/db\r\n            operator: Exists\r\n    podAntiAffinityType: required\r\n    tolerations:\r\n    - effect: NoSchedule\r\n      key: dedicated\r\n      operator: Equal\r\n      value: db\r\n  backup:\r\n    barmanObjectStore:\r\n      data:\r\n        compression: bzip2\r\n      destinationPath: s3://db-some-db-wal/cnpg/16/\r\n      endpointURL: https://s3.example.com\r\n      s3Credentials:\r\n        accessKeyId:\r\n          key: AWS_ACCESS_KEY_ID\r\n          name: some-db-wal\r\n        secretAccessKey:\r\n          key: AWS_SECRET_ACCESS_KEY\r\n          name: some-db-wal\r\n      wal:\r\n        compression: bzip2\r\n        maxParallel: 8\r\n    retentionPolicy: 92d\r\n    target: prefer-standby\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enableSuperuserAccess: false\r\n  failoverDelay: 0\r\n  imageName: image-registry.openshift-image-registry.svc:5000/db/postgres:16-20240304\r\n  instances: 2\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      pg_stat_statements.track: all\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    pg_hba:\r\n    - hostssl all all all scram-sha-256\r\n    shared_preload_libraries:\r\n    - citus\r\n    - pg_uuidv7\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: supervised\r\n  projectedVolumeTemplate:\r\n    sources:\r\n    - configMap:\r\n        items:\r\n        - key: archive_gzip.sh\r\n          mode: 365\r\n          path: scripts/archive_gzip.sh\r\n        - key: archive_zstd.sh\r\n          mode: 365\r\n          path: scripts/archive_zstd.sh\r\n        - key: head_gzip.sh\r\n          mode: 365\r\n          path: scripts/head_gzip.sh\r\n        - key: head_zstd.sh\r\n          mode: 365\r\n          path: scripts/head_zstd.sh\r\n        - key: unarchive_gzip.sh\r\n          mode: 365\r\n          path: scripts/unarchive_gzip.sh\r\n        - key: unarchive_zstd.sh\r\n          mode: 365\r\n          path: scripts/unarchive_zstd.sh\r\n        name: some-db-scripts\r\n    - configMap:\r\n        items:\r\n        - key: BUCKET_HOST\r\n          path: db-archives/BUCKET_HOST\r\n        - key: BUCKET_NAME\r\n          path: db-archives/BUCKET_NAME\r\n        - key: BUCKET_PORT\r\n          path: db-archives/BUCKET_PORT\r\n        - key: BUCKET_REGION\r\n          path: db-archives/BUCKET_REGION\r\n        - key: BUCKET_SUBREGION\r\n          path: db-archives/BUCKET_SUBREGION\r\n        name: some-db-db-archives\r\n    - secret:\r\n        items:\r\n        - key: AWS_ACCESS_KEY_ID\r\n          path: db-archives/AWS_ACCESS_KEY_ID\r\n        - key: AWS_SECRET_ACCESS_KEY\r\n          path: db-archives/AWS_SECRET_ACCESS_KEY\r\n        name: some-db-db-archives\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      cpu: \"12\"\r\n      memory: 64Gi\r\n    requests:\r\n      cpu: \"1\"\r\n      memory: 4Gi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 1500Gi\r\n    storageClass: local-nvme\r\n  switchoverDelay: 3600\r\nstatus:\r\n  certificates:\r\n    clientCASecret: some-db16-ca\r\n    expirations:\r\n      some-db16-ca: 2024-06-01 09:43:30 +0000 UTC\r\n      some-db16-replication: 2024-06-01 09:43:30 +0000 UTC\r\n      some-db16-server: 2024-06-01 09:43:30 +0000 UTC\r\n    replicationTLSSecret: some-db16-replication\r\n    serverAltDNSNames:\r\n    - some-db16-rw\r\n    - some-db16-rw.db\r\n    - some-db16-rw.db.svc\r\n    - some-db16-r\r\n    - some-db16-r.db\r\n    - some-db16-r.db.svc\r\n    - some-db16-ro\r\n    - some-db16-ro.db\r\n    - some-db16-ro.db.svc\r\n    serverCASecret: some-db16-ca\r\n    serverTLSSecret: some-db16-server\r\n  cloudNativePGCommitHash: c7be872e\r\n  cloudNativePGOperatorHash: 262d86af058f59462fdccaec51231a1afd888153413f63224f60e458fcd335be\r\n  conditions:\r\n  - lastTransitionTime: \"2024-03-06T10:18:35Z\"\r\n    message: Cluster Is Not Ready\r\n    reason: ClusterIsNotReady\r\n    status: \"False\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-03-06T13:04:38Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  - lastTransitionTime: \"2024-03-06T13:39:05Z\"\r\n    message: New Backup starting up\r\n    reason: BackupStarted\r\n    status: \"False\"\r\n    type: LastBackupSucceeded\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"444990073\"\r\n  currentPrimary: some-db16-1\r\n  currentPrimaryTimestamp: \"2024-03-06T10:18:37.628192Z\"\r\n  firstRecoverabilityPoint: \"2024-03-03T17:51:00Z\"\r\n  firstRecoverabilityPointByMethod:\r\n    barmanObjectStore: \"2024-03-03T17:51:00Z\"\r\n  healthyPVC:\r\n  - some-db16-1\r\n  instanceNames:\r\n  - some-db16-1\r\n  instances: 1\r\n  instancesReportedState:\r\n    some-db16-1:\r\n      isPrimary: true\r\n      timeLineID: 5\r\n  instancesStatus:\r\n    healthy:\r\n    - some-db16-1\r\n  lastSuccessfulBackup: \"2024-03-06T05:06:09Z\"\r\n  lastSuccessfulBackupByMethod:\r\n    barmanObjectStore: \"2024-03-06T05:06:09Z\"\r\n  latestGeneratedNode: 2\r\n  managedRolesStatus:\r\n    byStatus:\r\n      not-managed:\r\n      - [...]\r\n      reconciled:\r\n      - [...]\r\n      reserved:\r\n      - postgres\r\n      - streaming_replica\r\n    passwordStatus:\r\n  onlineUpdateEnabled: true\r\n  phase: Failing over\r\n  phaseReason: Failing over from some-db16-2 to some-db16-1\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 1\r\n  readService: some-db16-r\r\n  readyInstances: 1\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"444990045\"\r\n    clientCaSecretVersion: \"444990042\"\r\n    replicationSecretVersion: \"444990044\"\r\n    serverCaSecretVersion: \"444990042\"\r\n    serverSecretVersion: \"444990043\"\r\n  targetPrimary: some-db16-1\r\n  targetPrimaryTimestamp: \"2024-03-06T10:18:35.513668Z\"\r\n  timelineID: 5\r\n  topology:\r\n    instances:\r\n      some-db16-1: {}\r\n    nodesUsed: 1\r\n    successfullyExtracted: true\r\n  writeService: some-db16-rw\r\n```\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct@ibotty Hi\r\nI can see you have used `citus` extension, I was kinda trying to implement it in my current setup, so before I go ahead and try to figure out things, it would be nice if you can share how you went ahead and implement this.\r\nThanks\n---\ncool, so simply build a new image and use the same image.\r\nThanks\n---\nHave you found a solution for this?  I have a cluster in the same state."
    },
    {
        "title": "[Feature]: Save and restore a cluster's imageName to/from Backup cluster resource",
        "id": 2169330059,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI burned a few hours today rebuilding a development cluster from a Backup cluster resource. The cluster was originally created under cloudnative-pg 1.20.2 without imageName set, so the default-at-the-time image was postgresql:15.3. Since creating the cluster I had upgraded to cloudnative-pg 1.22.0 (so postgresql>16 is now default), but without upgrading the underlying postgresql data in the cluster.\r\nSince I was initially trying to restore without knowing I should specify the older imageName in the cluster definition, my restores were failing with:\r\n`DETAIL:  The data directory was initialized by PostgreSQL version 15, which is not compatible with this version 16.2 (Debian 16.2-1.pgdg110+2)`\r\nIt seems to me that since the Backup cluster resource already contains some metadata about the cluster from which it was taken, it would make sense for it to have a record of the postgresql version number / imageName behind the data being backed up. This would be helpful in recovering from the Backup in the future (if the version used to create the cluster was not immediately known, for example).\n### Describe the solution you'd like\n- Backup object could contain imageName of the Instance from which the backup had been taken\r\n- (optionally) when recovering a new Cluster from such a Backup object, if imageName is not set in the Cluster object, the version in the Backup object could be used (rather than the default latest version set in the operator).\n### Describe alternatives you've considered\nt\n### Additional context\nRecovering this particular cluster was a bit annoying due to changes made between 1.20.2 and 1.22. I tried rolling cpn operator back to 1.20.2 to rebuild the original state, but there seems to have been breaking changes saved into my backup (instance startup failure due to bad permissions on postgres.auto.conf, which could not be read after being restored).\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nI burned a few hours today rebuilding a development cluster from a Backup cluster resource. The cluster was originally created under cloudnative-pg 1.20.2 without imageName set, so the default-at-the-time image was postgresql:15.3. Since creating the cluster I had upgraded to cloudnative-pg 1.22.0 (so postgresql>16 is now default), but without upgrading the underlying postgresql data in the cluster.\r\nSince I was initially trying to restore without knowing I should specify the older imageName in the cluster definition, my restores were failing with:\r\n`DETAIL:  The data directory was initialized by PostgreSQL version 15, which is not compatible with this version 16.2 (Debian 16.2-1.pgdg110+2)`\r\nIt seems to me that since the Backup cluster resource already contains some metadata about the cluster from which it was taken, it would make sense for it to have a record of the postgresql version number / imageName behind the data being backed up. This would be helpful in recovering from the Backup in the future (if the version used to create the cluster was not immediately known, for example).\n### Describe the solution you'd like\n- Backup object could contain imageName of the Instance from which the backup had been taken\r\n- (optionally) when recovering a new Cluster from such a Backup object, if imageName is not set in the Cluster object, the version in the Backup object could be used (rather than the default latest version set in the operator).\n### Describe alternatives you've considered\nt\n### Additional context\nRecovering this particular cluster was a bit annoying due to changes made between 1.20.2 and 1.22. I tried rolling cpn operator back to 1.20.2 to rebuild the original state, but there seems to have been breaking changes saved into my backup (instance startup failure due to bad permissions on postgres.auto.conf, which could not be read after being restored).\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug/Perf]: Lots of S3 HeadBucket calls",
        "id": 2167844684,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ninfo@kjeldschouten.nl\n### Version\n1.22.1\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nAfter backing up a very active (on purpose) test database to S3 automatically, which worked like a charm, I noticed:\r\nA LOT (thousands) of \"headbucket\" calls to S3.\r\nAs those are not free on backblaze for example, I wonder why those are done.\r\nAs those are primarily intended to check for bucket presence and access.\r\nIt looks a bit like current code does a `headbucket` call on every write.\r\nWhich shouldn't be needed.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ninfo@kjeldschouten.nl\n### Version\n1.22.1\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nAfter backing up a very active (on purpose) test database to S3 automatically, which worked like a charm, I noticed:\r\nA LOT (thousands) of \"headbucket\" calls to S3.\r\nAs those are not free on backblaze for example, I wonder why those are done.\r\nAs those are primarily intended to check for bucket presence and access.\r\nIt looks a bit like current code does a `headbucket` call on every write.\r\nWhich shouldn't be needed.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "Backup problems with MinIO",
        "id": 2167732441,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ninfo@karimi.dev\n### Version\n1.22.1\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nHi\r\nI have successfully installed cloudnative-pg on my K8s cluster\r\nBut when I want to use the backup features with MinIO storage I will get into trouble that\r\nFirst of all Access Key and Secret Key is not passed correctly (envs are not set) and fixed by manually setting env field in Cluster manifest (or given creds in secret in not working some how)\r\nAlso the -e option in barman cloud cli for wal archiving is not getting set I have set AES256 but barman is trying to use kms (@mikewallace1979 I think this one relates to barman)\n### Cluster resource\n```shell\n## TODO\n```\n### Relevant log output\n```shell\n## TODO\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ninfo@karimi.dev\n### Version\n1.22.1\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nHi\r\nI have successfully installed cloudnative-pg on my K8s cluster\r\nBut when I want to use the backup features with MinIO storage I will get into trouble that\r\nFirst of all Access Key and Secret Key is not passed correctly (envs are not set) and fixed by manually setting env field in Cluster manifest (or given creds in secret in not working some how)\r\nAlso the -e option in barman cloud cli for wal archiving is not getting set I have set AES256 but barman is trying to use kms (@mikewallace1979 I think this one relates to barman)\n### Cluster resource\n```shell\n## TODO\n```\n### Relevant log output\n```shell\n## TODO\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductUpdate: Encryption problem solved by setting encryption to empty string :)\r\nbut problem for env vars are persist"
    },
    {
        "title": "[Bug]: How to Prevent pg_shadow Table Password Hash from Changing When cnpg Cluster Restarts",
        "id": 2166539225,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nshkim009@gmail.com\n### Version\nolder minor (unsupported)\n### What version of Kubernetes are you using?\n1.26\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nYAML manifest\n### What happened?\ncnpg clusver version: 1.20.1\r\nazure aks verion: 1.26.6\r\nposgresql image: ghcr.io/cloudnative-pg/postgresql:15\r\nservice architecture: cnpg cluster(in aks ) < pg bouncer(vm) < client app(pod or vm)\r\npg_hba.conf: \r\n  - host all all all scram-sha-256\r\nSymptom: When a CNPG CLUTER deployed and in use on K8S is restarted due to a resource fix deployment or an unspecified issue, the PG_SHADOW table is not recognized by the PG BOUNCER.\r\nThe actual password does not change, but the hash value changes, causing the hash value of the password to not be authenticated to PG Bouncer.\r\nThis results in client applications failing to log in to the CNPG cluster.\r\nIs it possible to add a feature to set the password hash value in the pg_shadow table to not change even if the cnpg cluster is restarted?\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nshkim009@gmail.com\n### Version\nolder minor (unsupported)\n### What version of Kubernetes are you using?\n1.26\n### What is your Kubernetes environment?\nCloud: Azure AKS\n### How did you install the operator?\nYAML manifest\n### What happened?\ncnpg clusver version: 1.20.1\r\nazure aks verion: 1.26.6\r\nposgresql image: ghcr.io/cloudnative-pg/postgresql:15\r\nservice architecture: cnpg cluster(in aks ) < pg bouncer(vm) < client app(pod or vm)\r\npg_hba.conf: \r\n  - host all all all scram-sha-256\r\nSymptom: When a CNPG CLUTER deployed and in use on K8S is restarted due to a resource fix deployment or an unspecified issue, the PG_SHADOW table is not recognized by the PG BOUNCER.\r\nThe actual password does not change, but the hash value changes, causing the hash value of the password to not be authenticated to PG Bouncer.\r\nThis results in client applications failing to log in to the CNPG cluster.\r\nIs it possible to add a feature to set the password hash value in the pg_shadow table to not change even if the cnpg cluster is restarted?\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "add additional copy of release manifest at `releases/cnpg-latest.yaml`",
        "id": 2164944881,
        "state": "open",
        "first": "During the release, generate an additional file `releases/cnpg-latest.yaml` that is identical to `releases/cnpg-${release_version}.yaml`.\r\nSo in addition to `https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/v${VERSION}/releases/cnpg-${VERSION}.yaml` where the version appears twice in the url, we can reference an identical file at `https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/v${VERSION}/releases/cnpg-latest.yaml` where the version only appears once.\r\nThis should be a possible solution to #3400\r\nI did not run any e2e tests as this only changes the release process.\r\nThis change most likely doesn't need to be backported to previous releases",
        "messages": "During the release, generate an additional file `releases/cnpg-latest.yaml` that is identical to `releases/cnpg-${release_version}.yaml`.\r\nSo in addition to `https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/v${VERSION}/releases/cnpg-${VERSION}.yaml` where the version appears twice in the url, we can reference an identical file at `https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/v${VERSION}/releases/cnpg-latest.yaml` where the version only appears once.\r\nThis should be a possible solution to #3400\r\nI did not run any e2e tests as this only changes the release process.\r\nThis change most likely doesn't need to be backported to previous releases"
    },
    {
        "title": "[Feature]: Add a way to prefer one replica to another in case of failover",
        "id": 2163002899,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nIt could be great if we can choose which replica will be prefered to become the new primary in case of a failover.\r\n### Describe the solution you'd like\r\nAdd a label or a new spec (example : `primariesOrder`) in ClusterSpec to indicate which replica is preferred.\r\nIt could be based on the name of the pod / instance. For example : \r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-prod\r\nspec:\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.2\r\n  instances: 3\r\n  primariesOrder:\r\n  - cluster-prod-1\r\n  - cluster-prod-3\r\n  - cluster-prod-2\r\n  storage:\r\n    size: 1G\r\n```\r\n1st failover : the new primary will be cluster-prod-3\r\ncluster-prod-1 is recreated and became a replica\r\n2st failover : the new primary will be cluster-prod-1 as it is the first one in the list\r\n### Describe alternatives you've considered\r\nNone\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nNo\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nIt could be great if we can choose which replica will be prefered to become the new primary in case of a failover.\r\n### Describe the solution you'd like\r\nAdd a label or a new spec (example : `primariesOrder`) in ClusterSpec to indicate which replica is preferred.\r\nIt could be based on the name of the pod / instance. For example : \r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-prod\r\nspec:\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.2\r\n  instances: 3\r\n  primariesOrder:\r\n  - cluster-prod-1\r\n  - cluster-prod-3\r\n  - cluster-prod-2\r\n  storage:\r\n    size: 1G\r\n```\r\n1st failover : the new primary will be cluster-prod-3\r\ncluster-prod-1 is recreated and became a replica\r\n2st failover : the new primary will be cluster-prod-1 as it is the first one in the list\r\n### Describe alternatives you've considered\r\nNone\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nNo\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductThis approach relies on the pod names, which basically means treating the pods like pets.\r\nWe should focus more on labels or annotations on the Kubernetes nodes, something like `cnpg.io/nodePriority`. What do others think?\n---\n> This approach relies on the pod names, which basically means treating the pods like pets.\r\nThat's true !\r\nIt could be a solution to use label or annotations on the Kubernetes nodes, but what if I have multiple PostgreSQL Pods on the same Node ?"
    },
    {
        "title": "[Chore]: Drop support for Barman legacy backup (<= 3.4)",
        "id": 2157228127,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.22.1\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWe still supporting Barman <= 3.4 which is not ideal since those are really old versions and we shouldn't support this anymore.\r\nSupporting old versions require more tests and Barman <= 3.4 is not even in our latest images because we updated all the images, even PostgreSQL 11 until November 2023, meaning that even the latest version of that image with the latest security patches contains a newer version of Barman\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.22.1\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWe still supporting Barman <= 3.4 which is not ideal since those are really old versions and we shouldn't support this anymore.\r\nSupporting old versions require more tests and Barman <= 3.4 is not even in our latest images because we updated all the images, even PostgreSQL 11 until November 2023, meaning that even the latest version of that image with the latest security patches contains a newer version of Barman\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductLet's get rid of all that logic.\n---\nLess is more! \u2702\ufe0f"
    },
    {
        "title": "[Bug]: Logs error while parsing custom queries in ConfigMap despite no queries provided",
        "id": 2153904298,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nanton.friberg@outlook.com\n### Version\n1.22.1\n### What version of Kubernetes are you using?\n1.24 (unsupported)\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nHelm\n### What happened?\nCurrently evaluating cloudnative-pg and finding it excellent for my use-case with really good documentation. Thanks for that!\r\nNoticed a minor bug where I get INFO level logs with errors about not being able to parse custom queries in my database instance pods. This is not expected since I did not even provide any custom queries in this case. \n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-example\r\n  namespace: cnpg-test\r\nspec:\r\n  instances: 2\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:14.10\r\n  storage:\r\n    storageClass: ontap-bronze\r\n    size: 1Gi\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3://cnpg-backup-test/\r\n      endpointURL: <custom-minio-endpoint>\r\n      s3Credentials:\r\n        region:\r\n          name: cluster-example-backup-creds\r\n          key: region\r\n        accessKeyId:\r\n          name: cluster-example-backup-creds\r\n          key: id\r\n        secretAccessKey:\r\n          name: cluster-example-backup-creds\r\n          key: secret\r\n      wal:\r\n        compression: bzip2\r\n        encryption: \"\"\r\n      data:\r\n        compression: bzip2\r\n        encryption: \"\"\r\n        jobs: 2\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-02-26T09:45:55Z\",\"msg\":\"Error while parsing custom queries in ConfigMap\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-example\",\"namespace\":\"cnpg-test\"},\"namespace\":\"cnpg-test\",\"name\":\"cluster-example\",\"reconcileID\":\"9d33837c-bb2f-4882-80fb-79efd7bc60e2\",\"uuid\":\"d63a337e-d48b-11ee-b719-2ae8b842b0ab\",\"logging_pod\":\"cluster-example-1\",\"reference\":{\"name\":\"cnpg-default-monitoring\",\"key\":\"queries\"},\"error\":\"parsing user queries: yaml: line 3: could not find expected ':'\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-26T09:45:55Z\",\"msg\":\"Error while parsing custom queries in ConfigMap\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-example\",\"namespace\":\"cnpg-test\"},\"namespace\":\"cnpg-test\",\"name\":\"cluster-example\",\"reconcileID\":\"e7a008a8-f8c3-4328-a8e3-e41de80678bd\",\"uuid\":\"d64e371f-d48b-11ee-b719-2ae8b842b0ab\",\"logging_pod\":\"cluster-example-1\",\"reference\":{\"name\":\"cnpg-default-monitoring\",\"key\":\"queries\"},\"error\":\"parsing user queries: yaml: line 3: could not find expected ':'\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-26T09:45:56Z\",\"msg\":\"Error while parsing custom queries in ConfigMap\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-example\",\"namespace\":\"cnpg-test\"},\"namespace\":\"cnpg-test\",\"name\":\"cluster-example\",\"reconcileID\":\"ea711b59-3680-49c3-a01e-8f149a4a0f92\",\"uuid\":\"d6d0c80f-d48b-11ee-b719-2ae8b842b0ab\",\"logging_pod\":\"cluster-example-1\",\"reference\":{\"name\":\"cnpg-default-monitoring\",\"key\":\"queries\"},\"error\":\"parsing user queries: yaml: line 3: could not find expected ':'\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-26T09:45:56Z\",\"msg\":\"Error while parsing custom queries in ConfigMap\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-example\",\"namespace\":\"cnpg-test\"},\"namespace\":\"cnpg-test\",\"name\":\"cluster-example\",\"reconcileID\":\"62a359c4-eaa0-420e-a87c-9f1a0311c0b2\",\"uuid\":\"d6e6e0e4-d48b-11ee-b719-2ae8b842b0ab\",\"logging_pod\":\"cluster-example-1\",\"reference\":{\"name\":\"cnpg-default-monitoring\",\"key\":\"queries\"},\"error\":\"parsing user queries: yaml: line 3: could not find expected ':'\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-26T09:45:56Z\",\"msg\":\"Error while parsing custom queries in ConfigMap\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-example\",\"namespace\":\"cnpg-test\"},\"namespace\":\"cnpg-test\",\"name\":\"cluster-example\",\"reconcileID\":\"d80a859a-772d-462d-a6b7-c3efc6501705\",\"uuid\":\"d6fa9975-d48b-11ee-b719-2ae8b842b0ab\",\"logging_pod\":\"cluster-example-1\",\"reference\":{\"name\":\"cnpg-default-monitoring\",\"key\":\"queries\"},\"error\":\"parsing user queries: yaml: line 3: could not find expected ':'\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-26T09:46:04Z\",\"msg\":\"Error while parsing custom queries in ConfigMap\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-example\",\"namespace\":\"cnpg-test\"},\"namespace\":\"cnpg-test\",\"name\":\"cluster-example\",\"reconcileID\":\"650fc93c-2d9c-4c3e-a845-27c4fb690b1b\",\"uuid\":\"dbc0b378-d48b-11ee-b719-2ae8b842b0ab\",\"logging_pod\":\"cluster-example-1\",\"reference\":{\"name\":\"cnpg-default-monitoring\",\"key\":\"queries\"},\"error\":\"parsing user queries: yaml: line 3: could not find expected ':'\"}\r\n```\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nanton.friberg@outlook.com\n### Version\n1.22.1\n### What version of Kubernetes are you using?\n1.24 (unsupported)\n### What is your Kubernetes environment?\nSelf-managed: RKE\n### How did you install the operator?\nHelm\n### What happened?\nCurrently evaluating cloudnative-pg and finding it excellent for my use-case with really good documentation. Thanks for that!\r\nNoticed a minor bug where I get INFO level logs with errors about not being able to parse custom queries in my database instance pods. This is not expected since I did not even provide any custom queries in this case. \n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-example\r\n  namespace: cnpg-test\r\nspec:\r\n  instances: 2\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:14.10\r\n  storage:\r\n    storageClass: ontap-bronze\r\n    size: 1Gi\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3://cnpg-backup-test/\r\n      endpointURL: <custom-minio-endpoint>\r\n      s3Credentials:\r\n        region:\r\n          name: cluster-example-backup-creds\r\n          key: region\r\n        accessKeyId:\r\n          name: cluster-example-backup-creds\r\n          key: id\r\n        secretAccessKey:\r\n          name: cluster-example-backup-creds\r\n          key: secret\r\n      wal:\r\n        compression: bzip2\r\n        encryption: \"\"\r\n      data:\r\n        compression: bzip2\r\n        encryption: \"\"\r\n        jobs: 2\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2024-02-26T09:45:55Z\",\"msg\":\"Error while parsing custom queries in ConfigMap\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-example\",\"namespace\":\"cnpg-test\"},\"namespace\":\"cnpg-test\",\"name\":\"cluster-example\",\"reconcileID\":\"9d33837c-bb2f-4882-80fb-79efd7bc60e2\",\"uuid\":\"d63a337e-d48b-11ee-b719-2ae8b842b0ab\",\"logging_pod\":\"cluster-example-1\",\"reference\":{\"name\":\"cnpg-default-monitoring\",\"key\":\"queries\"},\"error\":\"parsing user queries: yaml: line 3: could not find expected ':'\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-26T09:45:55Z\",\"msg\":\"Error while parsing custom queries in ConfigMap\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-example\",\"namespace\":\"cnpg-test\"},\"namespace\":\"cnpg-test\",\"name\":\"cluster-example\",\"reconcileID\":\"e7a008a8-f8c3-4328-a8e3-e41de80678bd\",\"uuid\":\"d64e371f-d48b-11ee-b719-2ae8b842b0ab\",\"logging_pod\":\"cluster-example-1\",\"reference\":{\"name\":\"cnpg-default-monitoring\",\"key\":\"queries\"},\"error\":\"parsing user queries: yaml: line 3: could not find expected ':'\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-26T09:45:56Z\",\"msg\":\"Error while parsing custom queries in ConfigMap\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-example\",\"namespace\":\"cnpg-test\"},\"namespace\":\"cnpg-test\",\"name\":\"cluster-example\",\"reconcileID\":\"ea711b59-3680-49c3-a01e-8f149a4a0f92\",\"uuid\":\"d6d0c80f-d48b-11ee-b719-2ae8b842b0ab\",\"logging_pod\":\"cluster-example-1\",\"reference\":{\"name\":\"cnpg-default-monitoring\",\"key\":\"queries\"},\"error\":\"parsing user queries: yaml: line 3: could not find expected ':'\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-26T09:45:56Z\",\"msg\":\"Error while parsing custom queries in ConfigMap\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-example\",\"namespace\":\"cnpg-test\"},\"namespace\":\"cnpg-test\",\"name\":\"cluster-example\",\"reconcileID\":\"62a359c4-eaa0-420e-a87c-9f1a0311c0b2\",\"uuid\":\"d6e6e0e4-d48b-11ee-b719-2ae8b842b0ab\",\"logging_pod\":\"cluster-example-1\",\"reference\":{\"name\":\"cnpg-default-monitoring\",\"key\":\"queries\"},\"error\":\"parsing user queries: yaml: line 3: could not find expected ':'\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-26T09:45:56Z\",\"msg\":\"Error while parsing custom queries in ConfigMap\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-example\",\"namespace\":\"cnpg-test\"},\"namespace\":\"cnpg-test\",\"name\":\"cluster-example\",\"reconcileID\":\"d80a859a-772d-462d-a6b7-c3efc6501705\",\"uuid\":\"d6fa9975-d48b-11ee-b719-2ae8b842b0ab\",\"logging_pod\":\"cluster-example-1\",\"reference\":{\"name\":\"cnpg-default-monitoring\",\"key\":\"queries\"},\"error\":\"parsing user queries: yaml: line 3: could not find expected ':'\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-26T09:46:04Z\",\"msg\":\"Error while parsing custom queries in ConfigMap\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cluster-example\",\"namespace\":\"cnpg-test\"},\"namespace\":\"cnpg-test\",\"name\":\"cluster-example\",\"reconcileID\":\"650fc93c-2d9c-4c3e-a845-27c4fb690b1b\",\"uuid\":\"dbc0b378-d48b-11ee-b719-2ae8b842b0ab\",\"logging_pod\":\"cluster-example-1\",\"reference\":{\"name\":\"cnpg-default-monitoring\",\"key\":\"queries\"},\"error\":\"parsing user queries: yaml: line 3: could not find expected ':'\"}\r\n```\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: Database becomes inaccessible after 2nd failed scheduled backup",
        "id": 2153840836,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nemre.yildirim@bibecoffee.com\r\n### Version\r\n1.22.1\r\n### What version of Kubernetes are you using?\r\n1.27\r\n### What is your Kubernetes environment?\r\nCloud: Azure AKS\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nI was trying cloudnative-pg (helm chart version 0.20.1 with default values.yaml) and I've deployed a single postgresql instance on AKS. And I have managed to render the DB inaccessible because of a failed scheduled backup, therefore I wanted to report it here.\r\nOn Friday, I was trying the backups, and so I've set up the volumeSnapshot backup with misconfiguration (incorrect storage class driver), and I've tried to take a manual backup, and obviously it failed because the misconfiguration and I wrapped the day to continue on Monday, but I've left scheduled backups enabled, knowing that they would fail.\r\nOn Monday when I checked the database, I see that the DB pod is not ready state, therefore inaccessible.\r\nI start troubleshootingand there is nothing wrong in the DB logs. And in the operator logs, I see that after the **2nd failed scheduled backup attempt**, the readiness probe of the database have stopped responding, therefore rendering the DB inaccessible.\r\nInterestingly, the pod was never restarted even though the liveness probe seems to be enabled by default. Suggesting that the failed scheduled backup only affected the /readyz endpoint and not the /healthz endpoint.\r\nNote: I didn't encounter the issue when I tried to take a manual backup, and the issue also didn't happen after the 1st failed scheduled backup attempt, but it happened after the 2nd failed \"scheduled\" volumeSnapshot backup attempt as it thought the backup is already running, as you can also see from the logs.\r\nFinally, the database pod was kept in \"unready\" state until I've manually intervened and deleted the pod with force flag.\r\nI would have expected that the failing backup attempts should not affect the readiness of the database. So in my opinion the readiness probe shouldn't have affected to begin with, or if needs to be affected, liveness probe should also be affected, so the DB pod will be restarted automatically over the liveness probe, but maybe I am missing something here.\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: demo-postgresql-db\r\n  namespace: demo\r\nspec:\r\n  imageName: ghcr.io/cloudnative-pg/postgis:13-3.3-16\r\n  imagePullPolicy: IfNotPresent \r\n  instances: 1\r\n  affinity:\r\n    tolerations:\r\n    - key: \"dedicated\"\r\n      operator: \"Equal\"\r\n      value: \"demo\"\r\n      effect: \"NoSchedule\"\r\n  storage:\r\n    storageClass: managed-csi\r\n    size: 250Gi\r\n  walStorage:\r\n    storageClass: managed-csi\r\n    size: 50Gi\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: \"https://redacted.blob.core.windows.net/redacted/backup\"\r\n      azureCredentials:\r\n        connectionString:\r\n          name: redacted-creds\r\n          key: redacted_STORAGE_CONNECTION_STRING\r\n    volumeSnapshot:\r\n      className: managed-csi #(misconfiguration is this!!)\r\n      walClassName: managed-csi #(misconfiguration is this!!)\r\n      online: true\r\n      onlineConfiguration:\r\n        waitForArchive: true\r\n        immediateCheckpoint: false\r\n  enableSuperuserAccess: true\r\n  superuserSecret:\r\n    name: redacted-secret\r\n  managed:\r\n    roles:\r\n    - name: redacted\r\n      ensure: present\r\n      comment: DB owner\r\n      login: true\r\n      passwordSecret:\r\n        name: redacted-secret\r\n      superuser: true\r\n      createdb: true\r\n      createrole: true\r\n      replication: true\r\n    - name: redacted\r\n      ensure: present\r\n      comment: Read only role\r\n      login: false\r\n      superuser: false\r\n    - name: redacted\r\n      ensure: present\r\n      comment: User for redacted\r\n      login: true\r\n      passwordSecret:\r\n        name: redacted-secret\r\n      superuser: false\r\n      inRoles:\r\n        - redacted\r\n  bootstrap:\r\n    initdb:\r\n      database: redacted\r\n      owner: redacted\r\n      secret:\r\n        name: redacted-secret\r\n      encoding: UTF8 # this is the default, just leaving it here for transparency\r\n      localeCollate: 'en_US.utf8'\r\n      localeCType: 'en_US.utf8'\r\n      postInitApplicationSQL:\r\n        - CREATE EXTENSION IF NOT EXISTS postgis VERSION '3.3.1';\r\n        - CREATE EXTENSION IF NOT EXISTS dblink VERSION '1.2';\r\n        - CREATE EXTENSION IF NOT EXISTS hstore VERSION '1.7';\r\n        - CREATE EXTENSION IF NOT EXISTS ltree VERSION '1.2';\r\n        - CREATE EXTENSION IF NOT EXISTS pg_stat_statements VERSION '1.8';\r\n        - CREATE EXTENSION IF NOT EXISTS pgstattuple VERSION '1.5';\r\n        - CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\" VERSION '1.1';\r\n  postgresql:\r\n    parameters:\r\n      autovacuum_max_workers: \"10\" # max number of autovacuum subprocesses\r\n      autovacuum_naptime: \"10\" # time between autovacuum runs\r\n      checkpoint_completion_target: \"0.9\" # checkpoint target duration, 0.0 - 1.0\r\n      default_statistics_target: \"500\" # range 1-10000\r\n      effective_cache_size: \"48000MB\"\r\n      effective_io_concurrency: \"256\" # 1-1000; 0 disables prefetching\r\n      maintenance_work_mem: \"2047MB\"\r\n      max_locks_per_transaction: \"512\"\r\n      max_parallel_workers: \"12\" # maximum number of max_worker_processes\r\n      max_parallel_workers_per_gather: \"6\"\r\n      max_replication_slots: \"10\"\r\n      max_worker_processes: \"23\"\r\n      min_wal_size: \"512MB\"\r\n      pg_stat_statements.track: \"all\"\r\n      random_page_cost: \"1.1\"\r\n      shared_buffers: \"16000MB\"\r\n      work_mem: \"27459kB\"\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Backup\r\nmetadata:\r\n  name: demo-postgres-backup\r\n  namespace: demo\r\nspec:\r\n  cluster:\r\n    name: demo-postgresql-db\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: ScheduledBackup\r\nmetadata:\r\n  name: demo-postgres-scheduled-snapshot-backup\r\n  namespace: demo\r\nspec:\r\n  method: volumeSnapshot \r\n  schedule: \"0 0 0 * * *\"\r\n  backupOwnerReference: self\r\n  cluster:\r\n    name: demo-postgresql-db\r\n```\r\n### Relevant log output\r\n[full db logs .txt](https://github.com/cloudnative-pg/cloudnative-pg/files/14403446/db.logs.scheduled.backups.txt)\r\n```shell\r\n{\"level\":\"info\",\"ts\":\"2024-02-25T00:00:30Z\",\"msg\":\"Backup is already running on\",\"controller\":\"backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Backup\",\"Backup\":{\"name\":\"demo-postgres-scheduled-backup-20240225000000\",\"namespace\":\"demo\"},\"namespace\":\"demo\",\"name\":\"demo-postgres-scheduled-backup-20240225000000\",\"reconcileID\":\"7610326b-daa7-4823-bd94-4b32558bd982\",\"uuid\":\"e383712c-d370-11ee-8804-ea2969c0a2ef\",\"cluster\":\"demo-postgresql-db\",\"pod\":\"demo-postgresql-db-1\",\"started at\":\"2024-02-25 00:00:00 +0000 UTC\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-25T00:00:30Z\",\"msg\":\"found a previously elected pod, reusing it\",\"controller\":\"backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Backup\",\"Backup\":{\"name\":\"demo-postgres-scheduled-backup-20240225000000\",\"namespace\":\"demo\"},\"namespace\":\"demo\",\"name\":\"demo-postgres-scheduled-backup-20240225000000\",\"reconcileID\":\"7610326b-daa7-4823-bd94-4b32558bd982\",\"uuid\":\"e383712c-d370-11ee-8804-ea2969c0a2ef\",\"targetPodName\":\"demo-postgresql-db-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-25T00:00:30Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"demo-postgresql-db\",\"namespace\":\"demo\"},\"namespace\":\"demo\",\"name\":\"demo-postgresql-db\",\"reconcileID\":\"19018392-a883-4700-9d8a-fdadbe386b1e\",\"uuid\":\"d19ed59f-d370-11ee-8804-ea2969c0a2ef\",\"name\":\"demo-postgresql-db-1\",\"error\":\"Get \\\"http://10.244.1.5:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-25T00:00:30Z\",\"msg\":\"Failed to extract instance status from ready instances. Attempting to requeue...\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"demo-postgresql-db\",\"namespace\":\"demo\"},\"namespace\":\"demo\",\"name\":\"demo-postgresql-db\",\"reconcileID\":\"19018392-a883-4700-9d8a-fdadbe386b1e\",\"uuid\":\"d19ed59f-d370-11ee-8804-ea2969c0a2ef\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-25T00:00:35Z\",\"msg\":\"Backup is already running on\",\"controller\":\"backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Backup\",\"Backup\":{\"name\":\"demo-postgres-scheduled-backup-20240225000000\",\"namespace\":\"demo\"},\"namespace\":\"demo\",\"name\":\"demo-postgres-scheduled-backup-20240225000000\",\"reconcileID\":\"8dc13bdb-fd88-4777-aeb9-b8584ff7ab45\",\"uuid\":\"e67ed42a-d370-11ee-8804-ea2969c0a2ef\",\"cluster\":\"demo-postgresql-db\",\"pod\":\"demo-postgresql-db-1\",\"started at\":\"2024-02-25 00:00:00 +0000 UTC\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-25T00:00:35Z\",\"msg\":\"found a previously elected pod, reusing it\",\"controller\":\"backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Backup\",\"Backup\":{\"name\":\"demo-postgres-scheduled-backup-20240225000000\",\"namespace\":\"demo\"},\"namespace\":\"demo\",\"name\":\"demo-postgres-scheduled-backup-20240225000000\",\"reconcileID\":\"8dc13bdb-fd88-4777-aeb9-b8584ff7ab45\",\"uuid\":\"e67ed42a-d370-11ee-8804-ea2969c0a2ef\",\"targetPodName\":\"demo-postgresql-db-1\"}\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nemre.yildirim@bibecoffee.com\r\n### Version\r\n1.22.1\r\n### What version of Kubernetes are you using?\r\n1.27\r\n### What is your Kubernetes environment?\r\nCloud: Azure AKS\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nI was trying cloudnative-pg (helm chart version 0.20.1 with default values.yaml) and I've deployed a single postgresql instance on AKS. And I have managed to render the DB inaccessible because of a failed scheduled backup, therefore I wanted to report it here.\r\nOn Friday, I was trying the backups, and so I've set up the volumeSnapshot backup with misconfiguration (incorrect storage class driver), and I've tried to take a manual backup, and obviously it failed because the misconfiguration and I wrapped the day to continue on Monday, but I've left scheduled backups enabled, knowing that they would fail.\r\nOn Monday when I checked the database, I see that the DB pod is not ready state, therefore inaccessible.\r\nI start troubleshootingand there is nothing wrong in the DB logs. And in the operator logs, I see that after the **2nd failed scheduled backup attempt**, the readiness probe of the database have stopped responding, therefore rendering the DB inaccessible.\r\nInterestingly, the pod was never restarted even though the liveness probe seems to be enabled by default. Suggesting that the failed scheduled backup only affected the /readyz endpoint and not the /healthz endpoint.\r\nNote: I didn't encounter the issue when I tried to take a manual backup, and the issue also didn't happen after the 1st failed scheduled backup attempt, but it happened after the 2nd failed \"scheduled\" volumeSnapshot backup attempt as it thought the backup is already running, as you can also see from the logs.\r\nFinally, the database pod was kept in \"unready\" state until I've manually intervened and deleted the pod with force flag.\r\nI would have expected that the failing backup attempts should not affect the readiness of the database. So in my opinion the readiness probe shouldn't have affected to begin with, or if needs to be affected, liveness probe should also be affected, so the DB pod will be restarted automatically over the liveness probe, but maybe I am missing something here.\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: demo-postgresql-db\r\n  namespace: demo\r\nspec:\r\n  imageName: ghcr.io/cloudnative-pg/postgis:13-3.3-16\r\n  imagePullPolicy: IfNotPresent \r\n  instances: 1\r\n  affinity:\r\n    tolerations:\r\n    - key: \"dedicated\"\r\n      operator: \"Equal\"\r\n      value: \"demo\"\r\n      effect: \"NoSchedule\"\r\n  storage:\r\n    storageClass: managed-csi\r\n    size: 250Gi\r\n  walStorage:\r\n    storageClass: managed-csi\r\n    size: 50Gi\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: \"https://redacted.blob.core.windows.net/redacted/backup\"\r\n      azureCredentials:\r\n        connectionString:\r\n          name: redacted-creds\r\n          key: redacted_STORAGE_CONNECTION_STRING\r\n    volumeSnapshot:\r\n      className: managed-csi #(misconfiguration is this!!)\r\n      walClassName: managed-csi #(misconfiguration is this!!)\r\n      online: true\r\n      onlineConfiguration:\r\n        waitForArchive: true\r\n        immediateCheckpoint: false\r\n  enableSuperuserAccess: true\r\n  superuserSecret:\r\n    name: redacted-secret\r\n  managed:\r\n    roles:\r\n    - name: redacted\r\n      ensure: present\r\n      comment: DB owner\r\n      login: true\r\n      passwordSecret:\r\n        name: redacted-secret\r\n      superuser: true\r\n      createdb: true\r\n      createrole: true\r\n      replication: true\r\n    - name: redacted\r\n      ensure: present\r\n      comment: Read only role\r\n      login: false\r\n      superuser: false\r\n    - name: redacted\r\n      ensure: present\r\n      comment: User for redacted\r\n      login: true\r\n      passwordSecret:\r\n        name: redacted-secret\r\n      superuser: false\r\n      inRoles:\r\n        - redacted\r\n  bootstrap:\r\n    initdb:\r\n      database: redacted\r\n      owner: redacted\r\n      secret:\r\n        name: redacted-secret\r\n      encoding: UTF8 # this is the default, just leaving it here for transparency\r\n      localeCollate: 'en_US.utf8'\r\n      localeCType: 'en_US.utf8'\r\n      postInitApplicationSQL:\r\n        - CREATE EXTENSION IF NOT EXISTS postgis VERSION '3.3.1';\r\n        - CREATE EXTENSION IF NOT EXISTS dblink VERSION '1.2';\r\n        - CREATE EXTENSION IF NOT EXISTS hstore VERSION '1.7';\r\n        - CREATE EXTENSION IF NOT EXISTS ltree VERSION '1.2';\r\n        - CREATE EXTENSION IF NOT EXISTS pg_stat_statements VERSION '1.8';\r\n        - CREATE EXTENSION IF NOT EXISTS pgstattuple VERSION '1.5';\r\n        - CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\" VERSION '1.1';\r\n  postgresql:\r\n    parameters:\r\n      autovacuum_max_workers: \"10\" # max number of autovacuum subprocesses\r\n      autovacuum_naptime: \"10\" # time between autovacuum runs\r\n      checkpoint_completion_target: \"0.9\" # checkpoint target duration, 0.0 - 1.0\r\n      default_statistics_target: \"500\" # range 1-10000\r\n      effective_cache_size: \"48000MB\"\r\n      effective_io_concurrency: \"256\" # 1-1000; 0 disables prefetching\r\n      maintenance_work_mem: \"2047MB\"\r\n      max_locks_per_transaction: \"512\"\r\n      max_parallel_workers: \"12\" # maximum number of max_worker_processes\r\n      max_parallel_workers_per_gather: \"6\"\r\n      max_replication_slots: \"10\"\r\n      max_worker_processes: \"23\"\r\n      min_wal_size: \"512MB\"\r\n      pg_stat_statements.track: \"all\"\r\n      random_page_cost: \"1.1\"\r\n      shared_buffers: \"16000MB\"\r\n      work_mem: \"27459kB\"\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Backup\r\nmetadata:\r\n  name: demo-postgres-backup\r\n  namespace: demo\r\nspec:\r\n  cluster:\r\n    name: demo-postgresql-db\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: ScheduledBackup\r\nmetadata:\r\n  name: demo-postgres-scheduled-snapshot-backup\r\n  namespace: demo\r\nspec:\r\n  method: volumeSnapshot \r\n  schedule: \"0 0 0 * * *\"\r\n  backupOwnerReference: self\r\n  cluster:\r\n    name: demo-postgresql-db\r\n```\r\n### Relevant log output\r\n[full db logs .txt](https://github.com/cloudnative-pg/cloudnative-pg/files/14403446/db.logs.scheduled.backups.txt)\r\n```shell\r\n{\"level\":\"info\",\"ts\":\"2024-02-25T00:00:30Z\",\"msg\":\"Backup is already running on\",\"controller\":\"backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Backup\",\"Backup\":{\"name\":\"demo-postgres-scheduled-backup-20240225000000\",\"namespace\":\"demo\"},\"namespace\":\"demo\",\"name\":\"demo-postgres-scheduled-backup-20240225000000\",\"reconcileID\":\"7610326b-daa7-4823-bd94-4b32558bd982\",\"uuid\":\"e383712c-d370-11ee-8804-ea2969c0a2ef\",\"cluster\":\"demo-postgresql-db\",\"pod\":\"demo-postgresql-db-1\",\"started at\":\"2024-02-25 00:00:00 +0000 UTC\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-25T00:00:30Z\",\"msg\":\"found a previously elected pod, reusing it\",\"controller\":\"backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Backup\",\"Backup\":{\"name\":\"demo-postgres-scheduled-backup-20240225000000\",\"namespace\":\"demo\"},\"namespace\":\"demo\",\"name\":\"demo-postgres-scheduled-backup-20240225000000\",\"reconcileID\":\"7610326b-daa7-4823-bd94-4b32558bd982\",\"uuid\":\"e383712c-d370-11ee-8804-ea2969c0a2ef\",\"targetPodName\":\"demo-postgresql-db-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-25T00:00:30Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"demo-postgresql-db\",\"namespace\":\"demo\"},\"namespace\":\"demo\",\"name\":\"demo-postgresql-db\",\"reconcileID\":\"19018392-a883-4700-9d8a-fdadbe386b1e\",\"uuid\":\"d19ed59f-d370-11ee-8804-ea2969c0a2ef\",\"name\":\"demo-postgresql-db-1\",\"error\":\"Get \\\"http://10.244.1.5:8000/pg/status\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-25T00:00:30Z\",\"msg\":\"Failed to extract instance status from ready instances. Attempting to requeue...\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"demo-postgresql-db\",\"namespace\":\"demo\"},\"namespace\":\"demo\",\"name\":\"demo-postgresql-db\",\"reconcileID\":\"19018392-a883-4700-9d8a-fdadbe386b1e\",\"uuid\":\"d19ed59f-d370-11ee-8804-ea2969c0a2ef\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-25T00:00:35Z\",\"msg\":\"Backup is already running on\",\"controller\":\"backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Backup\",\"Backup\":{\"name\":\"demo-postgres-scheduled-backup-20240225000000\",\"namespace\":\"demo\"},\"namespace\":\"demo\",\"name\":\"demo-postgres-scheduled-backup-20240225000000\",\"reconcileID\":\"8dc13bdb-fd88-4777-aeb9-b8584ff7ab45\",\"uuid\":\"e67ed42a-d370-11ee-8804-ea2969c0a2ef\",\"cluster\":\"demo-postgresql-db\",\"pod\":\"demo-postgresql-db-1\",\"started at\":\"2024-02-25 00:00:00 +0000 UTC\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-25T00:00:35Z\",\"msg\":\"found a previously elected pod, reusing it\",\"controller\":\"backup\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Backup\",\"Backup\":{\"name\":\"demo-postgres-scheduled-backup-20240225000000\",\"namespace\":\"demo\"},\"namespace\":\"demo\",\"name\":\"demo-postgres-scheduled-backup-20240225000000\",\"reconcileID\":\"8dc13bdb-fd88-4777-aeb9-b8584ff7ab45\",\"uuid\":\"e67ed42a-d370-11ee-8804-ea2969c0a2ef\",\"targetPodName\":\"demo-postgresql-db-1\"}\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct@localbubble I can confirm this, as I am setting up new cluster I gets inactive on all 3 instances because of failing backups (misconfigured storage). This should not be the case, the cluster should continue to operate as normal as if there is no backup configured. Now I get `Readiness probe failed: Get \"http://10.233.65.248:8000/readyz\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)` on all instances which effectively renders the cluster useless and system down..  \r\nPlease anyone responsible for this advice on why is this behavious and how to prevent it from happening :pray:\n---\nThis issue is the same as https://github.com/cloudnative-pg/cloudnative-pg/issues/5505\n---\n@99hops: Are you also on Azure AKS by any chance? I'm see a lot of this and trying to narrow down the causes with @sxd."
    },
    {
        "title": "[Feature]: Add extra selectors to ScheduledBackups clusters",
        "id": 2138528697,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrent ScheduledBackups CRD takes spec.cluster.name as the only cluster selector. In (perhaps not so) rare cases, where multiple pg clusters with the same name may exist - i.e. a Kubernetes cluster hosting several environments of the same app like dev and prod. - seems ScheduledBackups of the first deployed cluster works fine, but subsequent ones get confused and unable to perform backups. \n### Describe the solution you'd like\nPerhaps add namespace or label selectors to spec.cluster?\n### Describe alternatives you've considered\nNot a single one so far. Just discovered the problem. \n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCurrent ScheduledBackups CRD takes spec.cluster.name as the only cluster selector. In (perhaps not so) rare cases, where multiple pg clusters with the same name may exist - i.e. a Kubernetes cluster hosting several environments of the same app like dev and prod. - seems ScheduledBackups of the first deployed cluster works fine, but subsequent ones get confused and unable to perform backups. \n### Describe the solution you'd like\nPerhaps add namespace or label selectors to spec.cluster?\n### Describe alternatives you've considered\nNot a single one so far. Just discovered the problem. \n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductOh sorry, no backport needed... I was thinking if I can add that fix myself (contribute)..."
    },
    {
        "title": "[Bug]: retentionPolicy not applied on backblaze",
        "id": 2134032582,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.22.1\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nHi,\r\nThe retentionPolicy doesn't seem to apply to my scheduled backups, looking at my backblaze bucket I have base backups that are multiple months old even though the retention is set to 7d.\r\nI'm using Talos 1.6 and k8s 1.29.1, but I've only recently upgraded to these, I have backups dating back to 1.28 that weren't deleted either so I don't think it's related.\r\nEverything is applied through ArgoCD.\n### Cluster resource\n```shell\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-homeassistant\r\n  namespace: homeassistant\r\nspec:\r\n  instances: 1\r\n  primaryUpdateStrategy: unsupervised\r\n  postgresql:\r\n    parameters:\r\n      timezone: \"Europe/Dublin\"\r\n  bootstrap:\r\n    initdb:\r\n      database: homeassistant\r\n      owner: homeassistant_user\r\n  resources:\r\n    requests:\r\n      memory: \"512Mi\"\r\n    limits:\r\n      memory: \"1Gi\"\r\n  storage:\r\n    size: 10Gi\r\n  backup:\r\n    retentionPolicy: \"7d\"\r\n    barmanObjectStore:\r\n      destinationPath: \"s3://.../homeassistant/\"\r\n      endpointURL: \"https://s3.eu-central-003.backblazeb2.com\"\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: pg-barman-backblaze\r\n          key: ACCESS_KEY_ID\r\n        secretAccessKey:\r\n          name: pg-barman-backblaze\r\n          key: ACCESS_SECRET_KEY\r\n      wal:\r\n        compression: bzip2\r\n        maxParallel: 8\r\n        encryption: AES256\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: ScheduledBackup\r\nmetadata:\r\n  name: cluster-homeassistant-backup-daily\r\nspec:\r\n  schedule: \"0 30 1 * * *\"\r\n  backupOwnerReference: self\r\n  cluster:\r\n    name: cluster-homeassistant\n```\n### Relevant log output\n```shell\nI'm not sure what to look for here, the controller logs do show the backup running at the correct time but no mentions of deletions anywhere.\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.22.1\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nHi,\r\nThe retentionPolicy doesn't seem to apply to my scheduled backups, looking at my backblaze bucket I have base backups that are multiple months old even though the retention is set to 7d.\r\nI'm using Talos 1.6 and k8s 1.29.1, but I've only recently upgraded to these, I have backups dating back to 1.28 that weren't deleted either so I don't think it's related.\r\nEverything is applied through ArgoCD.\n### Cluster resource\n```shell\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-homeassistant\r\n  namespace: homeassistant\r\nspec:\r\n  instances: 1\r\n  primaryUpdateStrategy: unsupervised\r\n  postgresql:\r\n    parameters:\r\n      timezone: \"Europe/Dublin\"\r\n  bootstrap:\r\n    initdb:\r\n      database: homeassistant\r\n      owner: homeassistant_user\r\n  resources:\r\n    requests:\r\n      memory: \"512Mi\"\r\n    limits:\r\n      memory: \"1Gi\"\r\n  storage:\r\n    size: 10Gi\r\n  backup:\r\n    retentionPolicy: \"7d\"\r\n    barmanObjectStore:\r\n      destinationPath: \"s3://.../homeassistant/\"\r\n      endpointURL: \"https://s3.eu-central-003.backblazeb2.com\"\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: pg-barman-backblaze\r\n          key: ACCESS_KEY_ID\r\n        secretAccessKey:\r\n          name: pg-barman-backblaze\r\n          key: ACCESS_SECRET_KEY\r\n      wal:\r\n        compression: bzip2\r\n        maxParallel: 8\r\n        encryption: AES256\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: ScheduledBackup\r\nmetadata:\r\n  name: cluster-homeassistant-backup-daily\r\nspec:\r\n  schedule: \"0 30 1 * * *\"\r\n  backupOwnerReference: self\r\n  cluster:\r\n    name: cluster-homeassistant\n```\n### Relevant log output\n```shell\nI'm not sure what to look for here, the controller logs do show the backup running at the correct time but no mentions of deletions anywhere.\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductWe have the same issue here, with Minio.\n---\n> We have the same issue here, with Minio.\r\nWe thought we had the same problem but in fact not at all, the retention period is well applied.\r\nFor better comprehension, this is our use case, and what append for us : \r\nSimple cluster : \r\n```\r\nbackup:\r\n  barmanObjectStore:\r\n    destinationPath: s3://backup/postgres\r\n    endpointURL: https://my-minio.svc.cluster.local\r\n    s3Credentials:\r\n      accessKeyId:\r\n        key: CONSOLE_ACCESS_KEY\r\n        name: minio-user\r\n      secretAccessKey:\r\n        key: CONSOLE_SECRET_KEY\r\n        name: minio-user\r\n    wal:\r\n      compression: gzip\r\n  retentionPolicy: 2d\r\n  target: prefer-standby\r\n```\r\nNote the retention is 2 days and we run a full backup every day.\r\nThe backup was removed when this happened :\r\n-  The backup hits 3 days of age (more than 2 days)\r\n- We scheduled a manual backup, and it was completed successfully.\r\nSo after careful reading of the documentation, our understanding of this issue is :\r\nThe backup must be older than the retention, and it will be removed after the next valid backup, for us on the third day.\n---\nUnfortunately that's not the case for me, I've got months of backups left over with a 7 day retention\n---\nSame issue here with Minio and CNPG 1.22.1\nI've set my retention to 14d and multiple backups for the last 2 months with them set to backup weekly."
    },
    {
        "title": "[Bug]: Evicted pods are not restarted after node recovers",
        "id": 2129085857,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.22.1\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhen a node enters a `DiskPressure` the pods in that node are evicted, including the cluster pod. When the node recovers from the status, the other pods in the node are restarted except the one from the cluster that remains in a `Completed` status.\r\nHow to reproduce:\r\n* Fill the disk of the node until a `DiskPressure` state is triggered and and wait until the pods are evicted.\r\n* Clean up space and wait until the `DiskPressure` state is cleared.\r\n* The pod belonging to the cluster doesn't come back.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: pgtest\r\nspec:\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  instances: 1\r\n  storage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 10Gi\n```\n### Relevant log output\n```shell\n\u2502   Warning  Evicted    45m   kubelet            The node was low on resource: ephemeral-storage. Threshold quantity: 5190418099, available: 1817428Ki. Container postgres was using 40Ki \u2502\r\n\u2502 , request is 0, has larger consumption of ephemeral-storage.\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.22.1\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhen a node enters a `DiskPressure` the pods in that node are evicted, including the cluster pod. When the node recovers from the status, the other pods in the node are restarted except the one from the cluster that remains in a `Completed` status.\r\nHow to reproduce:\r\n* Fill the disk of the node until a `DiskPressure` state is triggered and and wait until the pods are evicted.\r\n* Clean up space and wait until the `DiskPressure` state is cleared.\r\n* The pod belonging to the cluster doesn't come back.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: pgtest\r\nspec:\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  instances: 1\r\n  storage:\r\n    pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 10Gi\n```\n### Relevant log output\n```shell\n\u2502   Warning  Evicted    45m   kubelet            The node was low on resource: ephemeral-storage. Threshold quantity: 5190418099, available: 1817428Ki. Container postgres was using 40Ki \u2502\r\n\u2502 , request is 0, has larger consumption of ephemeral-storage.\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductFor what it's worth, this also happens with memory pressure. i.e. it's not limited to `DiskPressure`\n---\nIn our case, we have define a failover delay of 15 minutes to favorise the RPO. As the default toleration is 5 minutes, pods are evicted after this delay. If the node come back between 5 and 15 minutes, nothing happend, we have to wait the end of the failover delay or to promote a replica manually.\n---\nAlso experiencing this bug on GKE. One important thing to note: this only occurs when the primary is evicted. If a replica is evicted it will be restarted on a different node as expected."
    },
    {
        "title": "[Bug]: PGData already exists, can't overwrite for scale down and scale up .",
        "id": 2120094428,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nselvarajchennappan@gmail.com\n### Version\n1.22.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nHow to launch a cluster on existing pvc . We have scenario to scale down and scale up .\r\ninitially had 3 replicas and scale down to 1\r\nthen scaled up to i.e instances : 3 and applied yaml . initdb failed .\r\nat the time of scale up it says that\r\n{\"level\":\"info\",\"ts\":\"2024-02-03T06:08:14Z\",\"msg\":\"PGData already exists, can't overwrite\",\"logging_pod\":\"srims-prod-1-initdb\"}\r\nError: PGData directories already exist\n### Cluster resource\n```shell\napiVersion: v1\r\nkind: Namespace\r\nmetadata:\r\n  name: postgres\r\n---\r\napiVersion: v1\r\ndata:\r\n  username: cG9zdGdyZXM=\r\n  password: cGFzc3dvcmQ=\r\nkind: Secret\r\nmetadata:\r\n  name: cluster-example-superuser\r\n  namespace: postgres\r\ntype: kubernetes.io/basic-auth\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: abc-prod\r\n  namespace: postgres\r\nspec:\r\n  description: \"Cluster for srims\"\r\n  # Choose your PostGres Database Version\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  # Number of Replicas\r\n  instances: 3\r\n  minSyncReplicas: 1\r\n  maxSyncReplicas: 1\r\n  startDelay: 100\r\n  stopDelay: 100\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n    updateInterval: 300\r\n  primaryUpdateStrategy: unsupervised\r\n  postgresql:\r\n    parameters:\r\n      shared_buffers: 256MB\r\n      pg_stat_statements.max: '10000'\r\n      pg_stat_statements.track: all\r\n      auto_explain.log_min_duration: '10s'\r\n      wal_keep_size: '512MB'\r\n      pgaudit.log: \"all, -misc\"\r\n      logging_collector: \"on\"\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      pgaudit.log_catalog: \"off\"\r\n      pgaudit.log_parameter: \"on\"\r\n      pgaudit.log_relation: \"on\"\r\n    pg_hba:\r\n      - host app app all password\r\n    enableAlterSystem: true\r\n  enableSuperuserAccess: true\r\n  superuserSecret:\r\n    name: cluster-example-superuser\r\n  logLevel: debug\r\n  storage:\r\n          #size: 100Gi\r\n     pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 75Gi\r\n      storageClassName: local-storage\r\n    #  volumeName: postgresnopg-local-storage-pv\r\n      volumeMode: Filesystem\r\n  resources: # m5large: m5xlarge 2vCPU, 8GI RAM\r\n    requests:\r\n      memory: \"512Mi\"\r\n      cpu: \"3\"\r\n    limits:\r\n      memory: \"1Gi\"\r\n      cpu: \"4\"\r\n  affinity:\r\n      enablePodAntiAffinity: true\r\n      nodeAffinity:\r\n          requiredDuringSchedulingIgnoredDuringExecution:\r\n            nodeSelectorTerms:\r\n            - matchExpressions:\r\n              - key: tools\r\n                operator: In\r\n                values:\r\n                - common\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: true\r\n  monitoring:\r\n    enablePodMonitor: false\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nselvarajchennappan@gmail.com\n### Version\n1.22.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nHow to launch a cluster on existing pvc . We have scenario to scale down and scale up .\r\ninitially had 3 replicas and scale down to 1\r\nthen scaled up to i.e instances : 3 and applied yaml . initdb failed .\r\nat the time of scale up it says that\r\n{\"level\":\"info\",\"ts\":\"2024-02-03T06:08:14Z\",\"msg\":\"PGData already exists, can't overwrite\",\"logging_pod\":\"srims-prod-1-initdb\"}\r\nError: PGData directories already exist\n### Cluster resource\n```shell\napiVersion: v1\r\nkind: Namespace\r\nmetadata:\r\n  name: postgres\r\n---\r\napiVersion: v1\r\ndata:\r\n  username: cG9zdGdyZXM=\r\n  password: cGFzc3dvcmQ=\r\nkind: Secret\r\nmetadata:\r\n  name: cluster-example-superuser\r\n  namespace: postgres\r\ntype: kubernetes.io/basic-auth\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: abc-prod\r\n  namespace: postgres\r\nspec:\r\n  description: \"Cluster for srims\"\r\n  # Choose your PostGres Database Version\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  # Number of Replicas\r\n  instances: 3\r\n  minSyncReplicas: 1\r\n  maxSyncReplicas: 1\r\n  startDelay: 100\r\n  stopDelay: 100\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n    updateInterval: 300\r\n  primaryUpdateStrategy: unsupervised\r\n  postgresql:\r\n    parameters:\r\n      shared_buffers: 256MB\r\n      pg_stat_statements.max: '10000'\r\n      pg_stat_statements.track: all\r\n      auto_explain.log_min_duration: '10s'\r\n      wal_keep_size: '512MB'\r\n      pgaudit.log: \"all, -misc\"\r\n      logging_collector: \"on\"\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      pgaudit.log_catalog: \"off\"\r\n      pgaudit.log_parameter: \"on\"\r\n      pgaudit.log_relation: \"on\"\r\n    pg_hba:\r\n      - host app app all password\r\n    enableAlterSystem: true\r\n  enableSuperuserAccess: true\r\n  superuserSecret:\r\n    name: cluster-example-superuser\r\n  logLevel: debug\r\n  storage:\r\n          #size: 100Gi\r\n     pvcTemplate:\r\n      accessModes:\r\n        - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 75Gi\r\n      storageClassName: local-storage\r\n    #  volumeName: postgresnopg-local-storage-pv\r\n      volumeMode: Filesystem\r\n  resources: # m5large: m5xlarge 2vCPU, 8GI RAM\r\n    requests:\r\n      memory: \"512Mi\"\r\n      cpu: \"3\"\r\n    limits:\r\n      memory: \"1Gi\"\r\n      cpu: \"4\"\r\n  affinity:\r\n      enablePodAntiAffinity: true\r\n      nodeAffinity:\r\n          requiredDuringSchedulingIgnoredDuringExecution:\r\n            nodeSelectorTerms:\r\n            - matchExpressions:\r\n              - key: tools\r\n                operator: In\r\n                values:\r\n                - common\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: true\r\n  monitoring:\r\n    enablePodMonitor: false\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductWe have the same issue.  This bug is very critical not just for scale up or down.  When we need to redeploy the postgres cluster in case of any failure due to node issue, we can't reschedule the pod to run in another node with existing PVC because it keeps complaining `PGDATA already exists` error.\n---\nI'm exactly facing this as I'm evaluating Cloudnative. Looks like a show stopper for now \ud83d\ude15.\r\nIt can be easily replicated:\r\n- Create a cluster, set up the db, etc.\r\n- Prevent the volume deletion (I did this directly in Hetzner web interface).\r\n- Delete the cluster. Thankfully, the volume is still there.\r\nNow I want to resurrect my cluster with the data of the volume.\r\n- I removed the claim reference from the PV.\r\n- I created a new PV to bind my pv.\r\n- I used the pvcTemplate in the cluster definition to point to my pvc.\r\nLike @Hashdhi, the initialization fails with \"PGData directories already exist\"...\n---\n> We have the same issue. This bug is very critical not just for scale up or down. When we need to redeploy the postgres cluster in case of any failure due to node issue, we can't reschedule the pod to run in another node with existing PVC because it keeps complaining `PGDATA already exists` error.\r\n@gbartolini This really is an issue, why is the same `cluster` object, when restored, not able to consume its PVC's anymore?\r\nIt looks like CNPG sets some sort of flag on a cluster object to indicate weither it needs to init or not.\r\nThis is not wanted behavior, at least not when it's not overridable.\r\nAs it makes infrastructure as code a mess.\r\n---\r\nAnother example:\r\nWe need to reinstall/move some PVCs and with that specific platform, it's easier to just reinstall the helm chart and move the old PVC data to the new PVCs.\r\nThat works fine with *literally* every piece of software, except CNPG.\r\n---\r\nWhat would easily solve all of these issues is:\r\n`initdb.useExisting: true`\r\nThat would skip the initdb steps if existing pgdata folder is found and instead tries to use the database in said folder.\r\nThis should work 100% without any negative consequences.\r\nYou can even set it to `false` by default, to ensure it doesn't cause any issues for existing users.\n---\nIs there any progress on this issue ?\n---\n@gbartolini or @Hashdhi , is this feature started development or onhold ?  @dperetti did you manage to workaround this issue while waiting for a permanent solution ?\n---\nHi all, i have the same issue, there is no solution to avoid initdb when we provision a new cluster with existing pv, and existing pgdata ?\n---\nWe defer this feature implementation to when the CNPG-I pluggable interface is defined for restore capabilities (similar use case: restore plugins must prepare the volumes from backup data and when they are done, CNPG starts).\n---\n> We defer this feature implementation to when the CNPG-I pluggable interface is defined for restore capabilities (similar use case: restore plugins must prepare the volumes from backup data and when they are done, CNPG starts).\nI think its indeed reasonable to first draw things out before expanding it even more :)"
    },
    {
        "title": "[Feature]: add automatic generation of password secrets for managed roles",
        "id": 2118382108,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nThe current implementation of managed roles allows giving the name\r\nof a secret where a role's password will be kept, but that secret is\r\nassumed to exist, and its creation falls on the user.\r\nFor users migrating from other solutions, or users wanting to get on\r\nCloudNativePG and have password-protected roles from the get-go,\r\nit would be convenient if the secrets were generated by CloudNativePG\r\nas needed.\r\nSee thread https://cloudnativepg.slack.com/archives/C03AX0J5P29/p1706780445388369\r\n### Describe the solution you'd like\r\nAllow the managed roles stanza to specify either an existing secret\r\nto hold a role's password, or an auto-generated secret.\r\n### Describe alternatives you've considered\r\nat the moment, automatic creation of password secrets would\r\nfall outside of the CNPG envelope\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nThe current implementation of managed roles allows giving the name\r\nof a secret where a role's password will be kept, but that secret is\r\nassumed to exist, and its creation falls on the user.\r\nFor users migrating from other solutions, or users wanting to get on\r\nCloudNativePG and have password-protected roles from the get-go,\r\nit would be convenient if the secrets were generated by CloudNativePG\r\nas needed.\r\nSee thread https://cloudnativepg.slack.com/archives/C03AX0J5P29/p1706780445388369\r\n### Describe the solution you'd like\r\nAllow the managed roles stanza to specify either an existing secret\r\nto hold a role's password, or an auto-generated secret.\r\n### Describe alternatives you've considered\r\nat the moment, automatic creation of password secrets would\r\nfall outside of the CNPG envelope\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductI agree that it would be a nice thing to have. Additionally, and maybe this should be filed in a separate issue, if one sets a value for `passwordSecret.name` and that secret does not exist, the cluster does not complain at all; I think this is not correct. I tried to set a random password secret name just to see if the secret would be created if it does not exist. IMHO either the cluster creation should fail (or at least report a warning), or the secret should be created. Although I think it would be better to explicitly require the secret to be created.\n---\nHello guys ! \nis this a good candidate for the roadmap ? \nWe are currently facing the same issue."
    },
    {
        "title": "[Feature]: Check `postgresql.auto.conf` is empty",
        "id": 2114253680,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWith the disabled ALTER SYSTEM feature, we should report from the status or as a metric that no GUC is defined through `postgresql.auto.conf`.\n### Describe the solution you'd like\nStart with a default Prometheus exporter metric that returns the number of GUCs defined in `postgresql.auto.conf`.\n### Describe alternatives you've considered\nEvaluate whether we should also check the number of entries defined in `override.conf.`\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWith the disabled ALTER SYSTEM feature, we should report from the status or as a metric that no GUC is defined through `postgresql.auto.conf`.\n### Describe the solution you'd like\nStart with a default Prometheus exporter metric that returns the number of GUCs defined in `postgresql.auto.conf`.\n### Describe alternatives you've considered\nEvaluate whether we should also check the number of entries defined in `override.conf.`\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Split out cnpg-api to be used from CNPG-I plugins",
        "id": 2112092531,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWith the plugin interface, every plugin must depend on the cloudnative-pg API, but depending to the main cloudnative-pg repository would bring all the dependencies of the project.\r\nWe also want to include some helper code in cnpg-i repository, but that's impossible now because it would depend on the main cloudnative-pg API, and with the present state it would create a circular dependency.\n### Describe the solution you'd like\nWe could create a `cloudnative-pg/cnpg-api` repository and create automation to mirror in that repository the `cloudnative-pg/cloudnative-pg/api` directory.\n### Describe alternatives you've considered\nWe have also considered moving the API to a separate package, removing it from the `cloudnative-pg/cloudnative-pg` repository. However, this solution would significantly increase the complexity of introducing changes in CloudnativePG.\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWith the plugin interface, every plugin must depend on the cloudnative-pg API, but depending to the main cloudnative-pg repository would bring all the dependencies of the project.\r\nWe also want to include some helper code in cnpg-i repository, but that's impossible now because it would depend on the main cloudnative-pg API, and with the present state it would create a circular dependency.\n### Describe the solution you'd like\nWe could create a `cloudnative-pg/cnpg-api` repository and create automation to mirror in that repository the `cloudnative-pg/cloudnative-pg/api` directory.\n### Describe alternatives you've considered\nWe have also considered moving the API to a separate package, removing it from the `cloudnative-pg/cloudnative-pg` repository. However, this solution would significantly increase the complexity of introducing changes in CloudnativePG.\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: failed calling webhook \u201ccnpg-webhook-service\u201d tls: failed to verify certificate x590 certificate signed by unknown authority",
        "id": 2109615496,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\nolder in 1.20.x\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhen applying the helm chart getting error : one or more objects failed to apply, reason: Internal error occurred: failed calling webhook \"[mcluster.kb.io](http://mcluster.kb.io/)\": failed to call webhook: Post [https://cnpg-webhook-service.edp.svc:443/mutate-postgresql-cnpg-io-v1-cluster?timeout=10s](https://cnpg-webhook-service.edp.svc/mutate-postgresql-cnpg-io-v1-cluster?timeout=10s): tls: failed to verify certificate: x509: certificate signed by unknown authority\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\nolder in 1.20.x\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWhen applying the helm chart getting error : one or more objects failed to apply, reason: Internal error occurred: failed calling webhook \"[mcluster.kb.io](http://mcluster.kb.io/)\": failed to call webhook: Post [https://cnpg-webhook-service.edp.svc:443/mutate-postgresql-cnpg-io-v1-cluster?timeout=10s](https://cnpg-webhook-service.edp.svc/mutate-postgresql-cnpg-io-v1-cluster?timeout=10s): tls: failed to verify certificate: x509: certificate signed by unknown authority\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductMaybe related: https://github.com/cloudnative-pg/cloudnative-pg/issues/2143\n---\nProbably ArgoCD, I have the same error\n---\nwhere you able to fix this? I have run into the same issue, and I wanted to use cert managers CA injector\n---\n> where you able to fix this? I have run into the same issue, and I wanted to use cert managers CA injector\r\nYou could try to manually create the cert, it works for me \ud83e\udd37\r\nArgoCD just add annotations on the file if it already exist\n---\nI did a manual create to fix it, but I wanted to solve this long term with the CA injector here is a link on how it works: https://cert-manager.io/docs/concepts/ca-injector/\n---\nIt seems like the documented (via comments in the Kustomize files) way to configure this to use cert-manager is not working, the `Certificate` creating the secert `webhook-server-cert` while the manager uses the secret `cnpg-webhook-cert`.\r\nEven more strange, seems like the manager is creating that other secret itself - replicating functionality from `cert-manager`: https://github.com/cloudnative-pg/cloudnative-pg/blob/main/internal/cmd/manager/controller/controller.go#L358 and even updating the Webhook configurations itself - leading it and cert-manager to fight: https://github.com/cloudnative-pg/cloudnative-pg/blob/main/pkg/certs/k8s.go#L234\n---\nKilling the cnpg controller pod and having it recreated fixed this issue for me.\n---\n> Killing the cnpg controller pod and having it recreated fixed this issue for me.\r\nWe had the same issue and it worked for us too.\n---\nKilling the cnpg controller pod didn't help me. Using v1.25.0, it was deployed via customization manifest with ArgoCD.\nAny updates?\n---\n> Killing the cnpg controller pod didn't help me. Using v1.25.0, it was deployed via customization manifest with ArgoCD. Any updates?\nBe sure to not have multiple deployment version of the CNPG controller which can lead to a desynchronization state. They try to have the lead due to a mismatch version."
    },
    {
        "title": "[Bug]: unable to setup PKI infrastructure, error: cnpg-mutating-webhook-configuration not found",
        "id": 2107977865,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nmuskangupta2027@gmail.com\n### Version\nolder in 1.20.x\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nwhile mutatingwebhook create is set to be false but the cnpg operator is throwing error unable to setup PKI infrastructure, error: cnpg-mutating-webhook-configuration not found , looks like its finding mutating webhook even it is set to false\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nmuskangupta2027@gmail.com\n### Version\nolder in 1.20.x\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nwhile mutatingwebhook create is set to be false but the cnpg operator is throwing error unable to setup PKI infrastructure, error: cnpg-mutating-webhook-configuration not found , looks like its finding mutating webhook even it is set to false\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductThis is actually an issue with the helm chart not properly configuring the certificate directory when mutating webhooks are disabled\n---\nSo how can we fix the issue?\n---\nI ran into this error without mutating webhook create is set to true. Is there a fix for this issue?\n---\nIn my case I was trying to instantiate the PostgreSQL Cluster at the same time as the Operator in my deployment, I'm suspecting it was communicating with the Controller and killing it before it had time to do anything.\r\nInstead of deploying at the same time I had to go sequentially or simply add the Cluster deployment after the Controller is initialised properly\n---\nI started with that approach and then later disabled the postgresql cluster to allow the operator to instantiate but still no luck.\n---\nAs a workaround you can specify any non-existent directory in the undocumented `WEBHOOK_CERT_DIR` config option.\nThis works because of this check: https://github.com/cloudnative-pg/cloudnative-pg/blob/3e82045b6a5dd00445c2abd62c703b938a7b9710/internal/cmd/manager/controller/controller.go#L368-L371\nIt's undocumented and only used for OLM, so it might break after any update. Imo there should be an option to explicitly disable all admission functionality, as some clusters have strict policies preventing its use. Please correct me if there is such an option already. Along with this I've encountered a few other problems with CNPG in such clusters, will open an issue to discuss them later."
    },
    {
        "title": "Rethink the website's home page",
        "id": 2107198274,
        "state": "open",
        "first": "It's not a very welcoming page. There's the link to the documentation, and the link to the blog.\nThe information from CONTRIBUTING.md is not shown anywhere on the site.\nI think we could have:\n- information on the community: the slack, the community meetings\n- links to talks, e.g. Gabriele's talk at Kubecon\n- rethink the little snippets we have currently: Autopilot // Data Persistence // Designed for Kubernetes",
        "messages": "It's not a very welcoming page. There's the link to the documentation, and the link to the blog.\nThe information from CONTRIBUTING.md is not shown anywhere on the site.\nI think we could have:\n- information on the community: the slack, the community meetings\n- links to talks, e.g. Gabriele's talk at Kubecon\n- rethink the little snippets we have currently: Autopilot // Data Persistence // Designed for Kubernetes@jsilvela good afternoon. I'm new to the community and want to contribute. Can I help with this task?\n---\n@diegonayalazo certainly you can help! Thank you.\r\nAt the moment we don't have agreement on what to do here.\r\nDo you have suggestions?\n---\nThanks @jsilvela 'for your response. Maybe we can start by getting those agreements in place. I would suggest the community to offer examples of great open source web pages so we can get some inspiration. \r\nFrom my viewpoint having a super easy quickstart is essential to foster adoption. For example a quickstart just to install the operator, and then a few steps to test the fail-over capabilities. \r\nAnother quickstart that imports an existing pgsql db into the cluster so any dev team can start experiencing the K8s PG experience that our level-5 operator offers.\r\nWhat do you think?\r\nThanks! Gracias :)\n---\nThese sound good @diegonayalazo .\r\nSomething I've been thinking is that the blog section isn't so useful. It's basically there for announcements.\r\nWe have some posts that are of a tutorial nature.\r\nPerhaps have a new section for tutorials / quickstarts?\r\nAlso, the CNPG repo does have a quickstart. Do you think that one is adequate?\r\nThanks, Obrigado :)\n---\n@jsilvela Assuming our current audience has Kubernetes knowledge : \r\nFor the welcome page, I propose to simplify even more the quickstart in the repo. \r\nFor example: taking a bit of part 2 \"Installation\" where we install the CRD as \"Step1\". And then deploy the Part 3, were we apply the cluster-example.yaml as \"Step 2\" .\r\nFinally, offer commands to describe the Cluster or watch the creation process. Also to kill the master and watch the replicas take over. \r\nSo, in 15 minutes the potential adoptor / community member can quickly test our technology and evalute if it fits their needs (or even exceeds :) \ud83d\udc4d )\r\nAs inspiration, I look at the welcome pages of Kubernetes and Postgresql. Both of them offer upfront a way to learn about the technology. So, along with the quickstart, I would present a link to the documentation.\r\nWhat do you think?\n---\nGot inspired by the 2 click install page from Stackgres. That would be amazing to have so our community can very quickly test CNPG in just a one liner and a few instructions. What's your take on it?\n---\nGood suggestion Diego. I like that Stackgres page.\r\nDo you use Slack much or at all? I can see you\u2019re registered but don\u2019t know if you use it or like it.\r\nI ask because we have a dedicated channel, `#website` which would hopefully be a good meeting point for interested people.\r\nAt the moment, active content like that would need some thinking. For the moment it\u2019s a statically generated Hugo site. I do agree though about having a QuickStart.\r\n> On 9 Apr 2024, at 00:42, Diego Naya ***@***.***> wrote:\r\n> \r\n> \r\n> Got inspired by the 2 click install page from Stackgres. That would be amazing to have so our community can very quickly test CNPG in just a one liner and a few instructions. What's your take on it?\r\n> \r\n> \u2014\r\n> Reply to this email directly, view it on GitHub <https://github.com/cloudnative-pg/cloudnative-pg/issues/3749#issuecomment-2043758170>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AADHPOE6WB4VBS7WBSK7NG3Y4MMMFAVCNFSM6AAAAABCQY723OVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDANBTG42TQMJXGA>.\r\n> You are receiving this because you were mentioned.\r\n>"
    },
    {
        "title": "[Bug]: CNPG11/1.22.0 replica bootstrap - DB fails to start: \"FATAL:  could not create lock file \\\"/var/run/postgresql/.s.PGSQL.5432.lock\\\": No such file or directory\"",
        "id": 2106403603,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\ndarius@digitalitteam.com\r\n### Version\r\n1.22.0\r\n### What version of Kubernetes are you using?\r\n1.28\r\n### What is your Kubernetes environment?\r\nCloud: Google GKE\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nAlso tried with 1.21.1.\r\nSee slack: https://cloudnativepg.slack.com/archives/C03AX0J5P29/p1706316108348809\r\nHello. We're trying to replace PG:11 with CNPG:11 in our GKE k8s cluster. We're currently attempting to perform a live migration by spinning up a CNPG cluster and bootstrapping it as a replica to the running PG.\r\nProblem is, the database pod `debezium-postgres-replica-1` fails with an error\r\n`FATAL:  could not create lock file \\\"/var/run/postgresql/.s.PGSQL.5432.lock\\\": No such file or directory\",\"pipe\":\"stderr\"`\r\nI tried to `while :; do kubectl -n db exec -it debezium-postgres-replica-1 -- ls -l /var/run/postgresql ; sleep .1; done` and I can confirm the path does not exist: `ls: cannot access '/var/run/postgresql': No such file or directory`.\r\nI'm bootstrapping with this yaml (in replica mode)\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: debezium-postgres-replica\r\n  namespace: db\r\n  labels:\r\n    purpose: testing\r\nspec:\r\n  imageName: europe-docker.pkg.dev/<client-registry>/postgresql-cnpg:11\r\n  imagePullPolicy: Always\r\n  bootstrap:\r\n    pg_basebackup:\r\n      source: debezium-postgres\r\n  replica:\r\n    enabled: true\r\n    source: debezium-postgres\r\n  externalClusters:\r\n    - name: debezium-postgres\r\n      connectionParameters:\r\n        host: debezium-postgresql-primary.db.svc\r\n        user: postgres\r\n      password:\r\n        name: db-replica-secret\r\n        key: password\r\n  instances: 1\r\n  storage:\r\n    storageClass: standard-rwo\r\n    size: \"50Gi\"\r\n  resources:\r\n    requests:\r\n      cpu: 100m\r\n      memory: 500M\r\n    limits:\r\n      cpu: \"1\"\r\n      memory: 700M\r\n```\r\nAnd it fails with\r\n![image](https://github.com/cloudnative-pg/cloudnative-pg/assets/153668027/b8616804-705a-47b9-b198-5a8aba1ee50b)\r\n![image](https://github.com/cloudnative-pg/cloudnative-pg/assets/153668027/07b7b2fd-fb7c-4559-b4e3-34cc16ba06ad)\r\n[cnpg_operator.log](https://github.com/cloudnative-pg/cloudnative-pg/files/14089431/cnpg_operator.log)\r\nIf I try to bootstrap a new DB using the `initdb option, the DB starts up just fine.\r\n![image](https://github.com/cloudnative-pg/cloudnative-pg/assets/153668027/df20770d-9ed5-44ac-a32f-a86569afa512)\r\nWhy is that so? What am I doing wrong?\r\n### Cluster resource\r\n_No response_\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\ndarius@digitalitteam.com\r\n### Version\r\n1.22.0\r\n### What version of Kubernetes are you using?\r\n1.28\r\n### What is your Kubernetes environment?\r\nCloud: Google GKE\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nAlso tried with 1.21.1.\r\nSee slack: https://cloudnativepg.slack.com/archives/C03AX0J5P29/p1706316108348809\r\nHello. We're trying to replace PG:11 with CNPG:11 in our GKE k8s cluster. We're currently attempting to perform a live migration by spinning up a CNPG cluster and bootstrapping it as a replica to the running PG.\r\nProblem is, the database pod `debezium-postgres-replica-1` fails with an error\r\n`FATAL:  could not create lock file \\\"/var/run/postgresql/.s.PGSQL.5432.lock\\\": No such file or directory\",\"pipe\":\"stderr\"`\r\nI tried to `while :; do kubectl -n db exec -it debezium-postgres-replica-1 -- ls -l /var/run/postgresql ; sleep .1; done` and I can confirm the path does not exist: `ls: cannot access '/var/run/postgresql': No such file or directory`.\r\nI'm bootstrapping with this yaml (in replica mode)\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: debezium-postgres-replica\r\n  namespace: db\r\n  labels:\r\n    purpose: testing\r\nspec:\r\n  imageName: europe-docker.pkg.dev/<client-registry>/postgresql-cnpg:11\r\n  imagePullPolicy: Always\r\n  bootstrap:\r\n    pg_basebackup:\r\n      source: debezium-postgres\r\n  replica:\r\n    enabled: true\r\n    source: debezium-postgres\r\n  externalClusters:\r\n    - name: debezium-postgres\r\n      connectionParameters:\r\n        host: debezium-postgresql-primary.db.svc\r\n        user: postgres\r\n      password:\r\n        name: db-replica-secret\r\n        key: password\r\n  instances: 1\r\n  storage:\r\n    storageClass: standard-rwo\r\n    size: \"50Gi\"\r\n  resources:\r\n    requests:\r\n      cpu: 100m\r\n      memory: 500M\r\n    limits:\r\n      cpu: \"1\"\r\n      memory: 700M\r\n```\r\nAnd it fails with\r\n![image](https://github.com/cloudnative-pg/cloudnative-pg/assets/153668027/b8616804-705a-47b9-b198-5a8aba1ee50b)\r\n![image](https://github.com/cloudnative-pg/cloudnative-pg/assets/153668027/07b7b2fd-fb7c-4559-b4e3-34cc16ba06ad)\r\n[cnpg_operator.log](https://github.com/cloudnative-pg/cloudnative-pg/files/14089431/cnpg_operator.log)\r\nIf I try to bootstrap a new DB using the `initdb option, the DB starts up just fine.\r\n![image](https://github.com/cloudnative-pg/cloudnative-pg/assets/153668027/df20770d-9ed5-44ac-a32f-a86569afa512)\r\nWhy is that so? What am I doing wrong?\r\n### Cluster resource\r\n_No response_\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductI think we need to revisit the current documentation of the `pg_basebackup` recovery bootstrap, especially reducing the expectations in terms of migrations from other environments. There are so many things that are not under our control in the source database, that it makes it difficult for us to predict every possible use case. I fear that the list of requirements we put when developing this feature (https://cloudnative-pg.io/documentation/current/bootstrap/#requirements) is not exhaustive. Simply pointing to Postgres documentation is not enough, unfortunately.\r\nIn any case, the issue is that our operator expects the following:\r\n```\r\npostgres=# SHOW unix_socket_directories ;\r\n unix_socket_directories\r\n-------------------------\r\n /controller/run\r\n(1 row)\r\n```\r\nYour source system on the other hand expects `/var/run/postgresql`. This means that unless you also change the source configuration this won't work. And this might not be the only parameter you need to change (for example, look at all the fixed postgres configuration options or the ones that are managed by the operator).\r\nI am sure you have, but I still ask you: have you thought at using logical replication instead?\r\nCheers,\r\nGabriele\n---\n@gbartolini thank you, that explains a lot. \r\nIs this source's postgresql.conf replication also the reason why the destination (replica) DB complains about not being able to bind to the ipv6 interface?\r\n> unless you also change the source configuration this won't work\r\nIs it possible to do it the other way around: change the `unix_socket_directories` property on the CNPG side of the matter?\r\n> I am sure you have, but I still ask you: have you thought at using logical replication instead?\r\nFrankly, we didn't consider this approach yet. From what I've read, this seems like a lot more elaborate replication mechanism. So the idea would be to bootstrap an empty CNPG DB with initdb and set it up as a subscriber? Or do I have to create schemas manually? Would a barman bootstrap make it easier by precreating all the globals, schemas and prefetching most of the data, and then just using logical replication to 'top up' what's missing + keep the datasets in sync?\n---\nWhy on earth did I link this ticket in that commit!?!?"
    },
    {
        "title": "[Bug]: override.conf not recreated on the primary",
        "id": 2105865141,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.22.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nIn case `override.conf` is erased on a primary, the operator doesn't rewrite it's content. I deleted the pod, and the file was written, as an empty file. On the other hand, `custom.conf` is recreated.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.22.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nIn case `override.conf` is erased on a primary, the operator doesn't rewrite it's content. I deleted the pod, and the file was written, as an empty file. On the other hand, `custom.conf` is recreated.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Bug]: initdb import error PGData already exists, can't overwrite",
        "id": 2099120657,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nalex@rusa.at\n### Version\n1.22.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nI created a new cluster with a bootstrap section. The initdb fails with the error \"PGData already exists, can't overwrite\".\r\nWhen I start the cluster without the bootstrap configuration everything works fine.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: postgres\r\n  namespace: demo-staging\r\n  labels:\r\n    workload: database\r\n    environment: staging\r\nspec:\r\n  instances: 3\r\n  inheritedMetadata:\r\n    labels:\r\n      workload: database\r\n      app: decidim-demo\r\n      environment: staging\r\n  primaryUpdateStrategy: unsupervised\r\n  affinity:\r\n    enablePodAntiAffinity: true #default value\r\n    topologyKey: kubernetes.io/hostname #defaul value\r\n    podAntiAffinityType: required\r\n  storage:\r\n    size: 1Gi\r\n    storageClass: openebs-lvmpv\r\n  bootstrap:\r\n    initdb:\r\n      import:\r\n        type: microservice\r\n        databases:\r\n          - mydb\r\n        source:\r\n          externalCluster: olddb\r\n  externalClusters:\r\n    - name: olddb\r\n      connectionParameters:\r\n        host: postgres-server-url\r\n        user: postgres\r\n        dbname: postgres\r\n        port: \"29829\"\r\n      password:\r\n        name: demo-postgres\r\n        key: password\n```\n### Relevant log output\n```shell\nDefaulted container \"import\" out of: import, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-01-24T10:53:32Z\",\"msg\":\"PGData already exists, can't overwrite\",\"logging_pod\":\"postgres-1-import\"}\r\nError: PGData directories already exist\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nalex@rusa.at\n### Version\n1.22.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nI created a new cluster with a bootstrap section. The initdb fails with the error \"PGData already exists, can't overwrite\".\r\nWhen I start the cluster without the bootstrap configuration everything works fine.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: postgres\r\n  namespace: demo-staging\r\n  labels:\r\n    workload: database\r\n    environment: staging\r\nspec:\r\n  instances: 3\r\n  inheritedMetadata:\r\n    labels:\r\n      workload: database\r\n      app: decidim-demo\r\n      environment: staging\r\n  primaryUpdateStrategy: unsupervised\r\n  affinity:\r\n    enablePodAntiAffinity: true #default value\r\n    topologyKey: kubernetes.io/hostname #defaul value\r\n    podAntiAffinityType: required\r\n  storage:\r\n    size: 1Gi\r\n    storageClass: openebs-lvmpv\r\n  bootstrap:\r\n    initdb:\r\n      import:\r\n        type: microservice\r\n        databases:\r\n          - mydb\r\n        source:\r\n          externalCluster: olddb\r\n  externalClusters:\r\n    - name: olddb\r\n      connectionParameters:\r\n        host: postgres-server-url\r\n        user: postgres\r\n        dbname: postgres\r\n        port: \"29829\"\r\n      password:\r\n        name: demo-postgres\r\n        key: password\n```\n### Relevant log output\n```shell\nDefaulted container \"import\" out of: import, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2024-01-24T10:53:32Z\",\"msg\":\"PGData already exists, can't overwrite\",\"logging_pod\":\"postgres-1-import\"}\r\nError: PGData directories already exist\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductSame issue here, CNPG 1.24 and K8s 1.31.\nThe issue is \"solved\" by the `microservice` to `monolith`."
    },
    {
        "title": "[Feature]: Allow configuration of ReadOnlyRootFilesystem.",
        "id": 2095864888,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWe are experiencing a crash in our production Postgres, the logs are not particularly revealing as to what is causing the crash. In order to investigate more we need the core dumps. As mentioned in the docs we have set the `cnpg.io/coredumpFilter` annotation:\r\n```\r\napiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  name: cnpg-controller-manager-config\r\n  namespace: cnpg-system\r\ndata:\r\n  INHERITED_ANNOTATIONS: categories, cnpg.io/coredumpFilter\r\n  INHERITED_LABELS: environment, workload, app\r\n```\r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    cnpg.io/coredumpFilter: \"0xFF\"\r\n...\r\n```\r\nThe annotation is correctly applied to the pod but we are still not getting core dumps. The reason for this we believe is that we are running on a node (specifically GKE autopilot) where the `/proc/sys/kernel/core_pattern` is set to:\r\n```\r\n/core.%e.%p.%t\r\n```\r\nThe leading slash in the pattern prevents the core dumps from being written into the current working directory of the process and instead to a place where the file-system is read only, as set by cnpg.\r\nAs we are running on GKE autopilot privileged containers are forbidden and we are unable to set the `core_pattern` variable using a DaemonSet or some other method.\n### Describe the solution you'd like\nI would like the cluster spec to include on option to explicitly set the `ReadOnlyRootFilesystem` value in the container security context to false. This can be documented suggesting very strongly that you should only do this if you understand the security implications, and have a reason for doing it.\r\n### Describe alternatives you've considered\nGiven the locked down nature of the environment we are running in, I can't think of any other way to get a core dump to be written.\r\nHowever if you were on a more permissive k8s cluster it would usually be possible to set the `/proc/sys/kernel/core_pattern` to a value without a leading slash.\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWe are experiencing a crash in our production Postgres, the logs are not particularly revealing as to what is causing the crash. In order to investigate more we need the core dumps. As mentioned in the docs we have set the `cnpg.io/coredumpFilter` annotation:\r\n```\r\napiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  name: cnpg-controller-manager-config\r\n  namespace: cnpg-system\r\ndata:\r\n  INHERITED_ANNOTATIONS: categories, cnpg.io/coredumpFilter\r\n  INHERITED_LABELS: environment, workload, app\r\n```\r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    cnpg.io/coredumpFilter: \"0xFF\"\r\n...\r\n```\r\nThe annotation is correctly applied to the pod but we are still not getting core dumps. The reason for this we believe is that we are running on a node (specifically GKE autopilot) where the `/proc/sys/kernel/core_pattern` is set to:\r\n```\r\n/core.%e.%p.%t\r\n```\r\nThe leading slash in the pattern prevents the core dumps from being written into the current working directory of the process and instead to a place where the file-system is read only, as set by cnpg.\r\nAs we are running on GKE autopilot privileged containers are forbidden and we are unable to set the `core_pattern` variable using a DaemonSet or some other method.\n### Describe the solution you'd like\nI would like the cluster spec to include on option to explicitly set the `ReadOnlyRootFilesystem` value in the container security context to false. This can be documented suggesting very strongly that you should only do this if you understand the security implications, and have a reason for doing it.\r\n### Describe alternatives you've considered\nGiven the locked down nature of the environment we are running in, I can't think of any other way to get a core dump to be written.\r\nHowever if you were on a more permissive k8s cluster it would usually be possible to set the `/proc/sys/kernel/core_pattern` to a value without a leading slash.\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[docs] fix azure connection string",
        "id": 2083605540,
        "state": "open",
        "first": "Minor doc issue when creating a secret. The secret key should be `AZURE_CONNECTION_STRING` instead of `AZURE_STORAGE_CONNECTION_STRING`",
        "messages": "Minor doc issue when creating a secret. The secret key should be `AZURE_CONNECTION_STRING` instead of `AZURE_STORAGE_CONNECTION_STRING`Thanks for your fix @srikiz !\r\nNot sure it's quite right. There is actual operator code using `AZURE_STORAGE_CONNECTION_STRING`.\r\nSee extract below from `./pkg/management/barman/credentials/env.go`.\r\nOn the other hand, `AZURE_CONNECTION_STRING` only appears in test code, not operator code.\r\nWe need to figure this out, something is definitely wrong, we should consolidate on one...\r\n``` go\r\n\tif configuration.BarmanCredentials.Azure.ConnectionString != nil {\r\n\t\tconnString, err := extractValueFromSecret(\r\n\t\t\tctx,\r\n\t\t\tc,\r\n\t\t\tconfiguration.BarmanCredentials.Azure.ConnectionString,\r\n\t\t\tnamespace,\r\n\t\t)\r\n\t\tif err != nil {\r\n\t\t\treturn nil, err\r\n\t\t}\r\n\t\tenv = append(env, fmt.Sprintf(\"AZURE_STORAGE_CONNECTION_STRING=%s\", connString))\r\n\t}\r\n```"
    },
    {
        "title": "[Bug]: Backing up to backblaze fails with \"exit status 4\"",
        "id": 2076367894,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nu6d0qlvcl@mozmail.com\r\n### Version\r\n1.22.0\r\n### What version of Kubernetes are you using?\r\n1.27\r\n### What is your Kubernetes environment?\r\nSelf-managed: kind (evaluation)\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nAll backups are failing to backblaze using its s3 endpoint\r\n![240111_19h24m34s_screenshot](https://github.com/cloudnative-pg/cloudnative-pg/assets/3724809/4dc11fac-906f-42f5-903f-3baeda968040)\r\n![240111_19h24m53s_screenshot](https://github.com/cloudnative-pg/cloudnative-pg/assets/3724809/3789d5e5-5e87-4070-9bfa-95333d74742b)\r\n![240109_22h34m29s_screenshot](https://github.com/cloudnative-pg/cloudnative-pg/assets/3724809/622bfa09-93b4-4592-b62b-3abfeef8e7c1)\r\n. I can see that files are being uploaded but the Backup resource fails. The only error I can see says `exit status 4` (full logs below)\r\n### Cluster resource\r\n```shell\r\n# yaml-language-server: $schema=https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/postgresql.cnpg.io/cluster_v1.json\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cloudnative-pg\r\n  namespace: default\r\n  annotations:\r\n    kyverno.io/ignore: \"true\"\r\nspec:\r\n  instances: 1\r\n  imageName: ghcr.io/bo0tzz/cnpgvecto.rs:16.1-v0.1.11\r\n  primaryUpdateStrategy: unsupervised\r\n  storage:\r\n    size: 10Gi\r\n    storageClass: local-hostpath\r\n  superuserSecret:\r\n    name: cloudnative-pg-superuser\r\n  enableSuperuserAccess: true\r\n  postgresql:\r\n    shared_preload_libraries:\r\n      - \"vectors.so\"\r\n    parameters:\r\n      max_connections: \"600\"\r\n      max_slot_wal_keep_size: 10GB\r\n      shared_buffers: 512MB\r\n  resources:\r\n    requests:\r\n      memory: \"512Mi\"\r\n    limits:\r\n      hugepages-2Mi: \"512Mi\"\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  backup:\r\n    retentionPolicy: 30d\r\n    barmanObjectStore:\r\n      wal:\r\n        compression: bzip2\r\n        maxParallel: 8\r\n      destinationPath: s3://postgres/\r\n      endpointURL: https://${S3_ENDPOINT}/${S3_POSTGRESQL_BUCKET}\r\n      serverName: cloudative-pg-v1\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: cloudnative-pg-superuser\r\n          key: aws-access-key-id\r\n        secretAccessKey:\r\n          name: cloudnative-pg-superuser\r\n          key: aws-secret-access-key\r\n```\r\n### Relevant log output\r\n```shell\r\n{\"level\":\"info\",\"ts\":\"2024-01-11T00:01:00Z\",\"msg\":\"Backup completed\",\"backupName\":\"cloudnative-pg-20240111000000\",\"backupNamespace\":\"cloudnative-pg-20240111000000\",\"logging_pod\":\"cloudnative-pg-1\"}\r\n{\"level\":\"error\",\"ts\":\"2024-01-11T00:01:01Z\",\"logger\":\"barman\",\"msg\":\"Can't extract backup id\",\"logging_pod\":\"cloudnative-pg-1\",\"command\":\"barman-cloud-backup-show\",\"options\":[\"--format\",\"json\",\"--endpoint-url\",\"https://s3.us-west-002.backblazeb2.com/<redacted>-postgresql\",\"--cloud-provider\",\"aws-s3\",\"s3://postgres/\",\"cloudative-pg-v1\",\"backup-20240111000000\"],\"stdout\":\"\",\"stderr\":\"2024-01-11 00:01:01,952 [102476] ERROR: Barman cloud backup show exception: Unknown backup 'backup-20240111000000' for server 'cloudative-pg-v1'\\n\",\"error\":\"exit status 4\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/barman.executeQueryCommand\\n\\tpkg/management/barman/backuplist.go:87\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/barman.GetBackupByName\\n\\tpkg/management/barman/backuplist.go:140\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*BackupCommand).getExecutedBackupInfo\\n\\tpkg/management/postgres/backup.go:369\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*BackupCommand).takeBackup\\n\\tpkg/management/postgres/backup.go:343\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*BackupCommand).run\\n\\tpkg/management/postgres/backup.go:268\"}\r\n{\"level\":\"error\",\"ts\":\"2024-01-11T00:01:01Z\",\"msg\":\"Backup failed\",\"backupName\":\"cloudnative-pg-20240111000000\",\"backupNamespace\":\"cloudnative-pg-20240111000000\",\"logging_pod\":\"cloudnative-pg-1\",\"error\":\"exit status 4\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*BackupCommand).run\\n\\tpkg/management/postgres/backup.go:272\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-11T00:01:01Z\",\"msg\":\"Applying backup retention policy\",\"backupName\":\"cloudnative-pg-20240111000000\",\"backupNamespace\":\"cloudnative-pg-20240111000000\",\"logging_pod\":\"cloudnative-pg-1\",\"retentionPolicy\":\"30d\"}\r\n```\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nu6d0qlvcl@mozmail.com\r\n### Version\r\n1.22.0\r\n### What version of Kubernetes are you using?\r\n1.27\r\n### What is your Kubernetes environment?\r\nSelf-managed: kind (evaluation)\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nAll backups are failing to backblaze using its s3 endpoint\r\n![240111_19h24m34s_screenshot](https://github.com/cloudnative-pg/cloudnative-pg/assets/3724809/4dc11fac-906f-42f5-903f-3baeda968040)\r\n![240111_19h24m53s_screenshot](https://github.com/cloudnative-pg/cloudnative-pg/assets/3724809/3789d5e5-5e87-4070-9bfa-95333d74742b)\r\n![240109_22h34m29s_screenshot](https://github.com/cloudnative-pg/cloudnative-pg/assets/3724809/622bfa09-93b4-4592-b62b-3abfeef8e7c1)\r\n. I can see that files are being uploaded but the Backup resource fails. The only error I can see says `exit status 4` (full logs below)\r\n### Cluster resource\r\n```shell\r\n# yaml-language-server: $schema=https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/postgresql.cnpg.io/cluster_v1.json\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cloudnative-pg\r\n  namespace: default\r\n  annotations:\r\n    kyverno.io/ignore: \"true\"\r\nspec:\r\n  instances: 1\r\n  imageName: ghcr.io/bo0tzz/cnpgvecto.rs:16.1-v0.1.11\r\n  primaryUpdateStrategy: unsupervised\r\n  storage:\r\n    size: 10Gi\r\n    storageClass: local-hostpath\r\n  superuserSecret:\r\n    name: cloudnative-pg-superuser\r\n  enableSuperuserAccess: true\r\n  postgresql:\r\n    shared_preload_libraries:\r\n      - \"vectors.so\"\r\n    parameters:\r\n      max_connections: \"600\"\r\n      max_slot_wal_keep_size: 10GB\r\n      shared_buffers: 512MB\r\n  resources:\r\n    requests:\r\n      memory: \"512Mi\"\r\n    limits:\r\n      hugepages-2Mi: \"512Mi\"\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  backup:\r\n    retentionPolicy: 30d\r\n    barmanObjectStore:\r\n      wal:\r\n        compression: bzip2\r\n        maxParallel: 8\r\n      destinationPath: s3://postgres/\r\n      endpointURL: https://${S3_ENDPOINT}/${S3_POSTGRESQL_BUCKET}\r\n      serverName: cloudative-pg-v1\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: cloudnative-pg-superuser\r\n          key: aws-access-key-id\r\n        secretAccessKey:\r\n          name: cloudnative-pg-superuser\r\n          key: aws-secret-access-key\r\n```\r\n### Relevant log output\r\n```shell\r\n{\"level\":\"info\",\"ts\":\"2024-01-11T00:01:00Z\",\"msg\":\"Backup completed\",\"backupName\":\"cloudnative-pg-20240111000000\",\"backupNamespace\":\"cloudnative-pg-20240111000000\",\"logging_pod\":\"cloudnative-pg-1\"}\r\n{\"level\":\"error\",\"ts\":\"2024-01-11T00:01:01Z\",\"logger\":\"barman\",\"msg\":\"Can't extract backup id\",\"logging_pod\":\"cloudnative-pg-1\",\"command\":\"barman-cloud-backup-show\",\"options\":[\"--format\",\"json\",\"--endpoint-url\",\"https://s3.us-west-002.backblazeb2.com/<redacted>-postgresql\",\"--cloud-provider\",\"aws-s3\",\"s3://postgres/\",\"cloudative-pg-v1\",\"backup-20240111000000\"],\"stdout\":\"\",\"stderr\":\"2024-01-11 00:01:01,952 [102476] ERROR: Barman cloud backup show exception: Unknown backup 'backup-20240111000000' for server 'cloudative-pg-v1'\\n\",\"error\":\"exit status 4\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/barman.executeQueryCommand\\n\\tpkg/management/barman/backuplist.go:87\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/barman.GetBackupByName\\n\\tpkg/management/barman/backuplist.go:140\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*BackupCommand).getExecutedBackupInfo\\n\\tpkg/management/postgres/backup.go:369\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*BackupCommand).takeBackup\\n\\tpkg/management/postgres/backup.go:343\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*BackupCommand).run\\n\\tpkg/management/postgres/backup.go:268\"}\r\n{\"level\":\"error\",\"ts\":\"2024-01-11T00:01:01Z\",\"msg\":\"Backup failed\",\"backupName\":\"cloudnative-pg-20240111000000\",\"backupNamespace\":\"cloudnative-pg-20240111000000\",\"logging_pod\":\"cloudnative-pg-1\",\"error\":\"exit status 4\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*BackupCommand).run\\n\\tpkg/management/postgres/backup.go:272\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-11T00:01:01Z\",\"msg\":\"Applying backup retention policy\",\"backupName\":\"cloudnative-pg-20240111000000\",\"backupNamespace\":\"cloudnative-pg-20240111000000\",\"logging_pod\":\"cloudnative-pg-1\",\"retentionPolicy\":\"30d\"}\r\n```\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductI was having the same problem lately and discovered it was an issue with my `destinationPath` and `endpointURL` configuration. \r\nThis is working for me:\r\n```yaml\r\nspec:\r\n  backup:\r\n    barmanObjectStore:\r\n      data:\r\n        compression: bzip2\r\n      wal:\r\n        compression: bzip2\r\n        maxParallel: 8\r\n      destinationPath: s3://<bucket_name>/<folder>/\r\n      endpointURL: https://s3.<endpoint>.backblazeb2.com\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: cnpg-secret\r\n          key: aws-access-key-id\r\n        secretAccessKey:\r\n          name: cpng-secret\r\n          key: aws-secret-access-key\r\n```"
    },
    {
        "title": "[Bug]:   Replica cluster Status is healthy while connection to the source cluster is broken.  Metric is also missing to report that",
        "id": 2074716229,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n### Version\r\n1.22.0\r\n### What version of Kubernetes are you using?\r\n1.26\r\n### What is your Kubernetes environment?\r\nOpenshift - Kubernetes 1.26.9\r\n### How did you install the operator?\r\nHelm chart : https://cloudnative-pg.github.io/charts\r\n### What happened?\r\nAs part of a disaster recovery plan, I cut the connection between my replica cluster (designated primary) and the source cluster (primary) for hours in order to identify a metric I could monitor.\r\nIn the log of one of the 3 pods/instance of my replica cluster, I clearly see the connection error\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T11:09:41Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"replica-cnpg-cluster-1\",\"record\":{\"log_time\":\"2024-01-10 11:09:41.382 UTC\",\"process_id\":\"28360\",\"session_id\":\"659e7af2.6ec8\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-01-10 11:09:38 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"08006\",\"message\":\"could not connect to the primary server: connection to server at \\\"10.xyx.yxz.zyx\\\", port 5432 failed: No route to host\\n\\tIs the server running on that host and accepting TCP/IP connections?\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\n```\r\nHowever, the status of my cluster is always `Cluster in healthy state` when I do `k get cluster` or `kubectl-cnpg status`.\r\nSimilarly, I can't find any metrics either from the cloudnative-operator or from the cluster reporting the fact that the connection is broken between the replica cluster and the primary\r\nHow can I monitor replication between the primary and replica clusters?\r\nHow can I make sure that my replica is up to date with the primary without having to do it manually every time?\r\n### Cluster resource\r\n```shell\r\nkubectl-cnpg status replica-cnpg-cluster\r\nReplica Cluster Summary\r\nName:                replica-cnpg-cluster\r\nNamespace:           skxxx\r\nSystem ID:           y32xxxx229zyx\r\nPostgreSQL Image:    ghcr.io/cloudnative-pg/postgresql:16.0\r\nDesignated primary:  replica-cnpg-cluster-1\r\nSource cluster:      main-pg-cluster\r\nPrimary start time:  2024-01-10 09:34:53 +0000 UTC (uptime 6h21m31s)\r\nStatus:              Cluster in healthy state \r\nInstances:           3\r\nReady instances:     3\r\nCertificates Status\r\nCertificate Name                  Expiration Date                Days Left Until Expiration\r\n----------------                  ---------------                --------------------------\r\nreplica-cnpg-cluster-ca           2024-04-09 09:29:17 +0000 UTC  89.73\r\nreplica-cnpg-cluster-replication  2024-04-09 09:29:17 +0000 UTC  89.73\r\nreplica-cnpg-cluster-server       2024-04-09 09:29:17 +0000 UTC  89.73\r\nContinuous Backup status\r\nNot configured\r\nPhysical backups\r\nNo running physical backups found\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nManaged roles status\r\nNo roles managed\r\nTablespaces status\r\nNo managed tablespaces\r\nInstances status\r\nName                    Database Size  Current LSN  Replication role              Status  QoS         Manager Version  Node\r\n----                    -------------  -----------  ----------------              ------  ---         ---------------  ----\r\nreplica-cnpg-cluster-1  SKO MB         1/84324B8    Designated primary            OK      BestEffort  1.22.0           integ-infra-ocp-6gcfd-worker-0-qhxrn\r\nreplica-cnpg-cluster-2  SKO MB         1/84324B8    Standby (in Replica Cluster)  OK      BestEffort  1.22.0           integ-infra-ocp-6gcfd-worker-0-8w24r\r\nreplica-cnpg-cluster-3  SKO MB         1/84324B8    Standby (in Replica Cluster)  OK      BestEffort  1.22.0           integ-infra-ocp-6gcfd-worker-0-6xwt6\r\noc get cluster\r\nNAME                   AGE     INSTANCES   READY   STATUS                     PRIMARY\r\nreplica-cnpg-cluster   6h27m   3           3       Cluster in healthy state   replica-cnpg-cluster-1\r\n```\r\n```\r\n### Relevant log output\r\n```shell\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T11:09:41Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"replica-cnpg-cluster-1\",\"record\":{\"log_time\":\"2024-01-10 11:09:41.382 UTC\",\"process_id\":\"28360\",\"session_id\":\"659e7af2.6ec8\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-01-10 11:09:38 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"08006\",\"message\":\"could not connect to the primary server: connection to server at \\\"10.xyx.yxz.zyx\\\", port 5432 failed: No route to host\\n\\tIs the server running on that host and accepting TCP/IP connections?\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\n```\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n### Version\r\n1.22.0\r\n### What version of Kubernetes are you using?\r\n1.26\r\n### What is your Kubernetes environment?\r\nOpenshift - Kubernetes 1.26.9\r\n### How did you install the operator?\r\nHelm chart : https://cloudnative-pg.github.io/charts\r\n### What happened?\r\nAs part of a disaster recovery plan, I cut the connection between my replica cluster (designated primary) and the source cluster (primary) for hours in order to identify a metric I could monitor.\r\nIn the log of one of the 3 pods/instance of my replica cluster, I clearly see the connection error\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T11:09:41Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"replica-cnpg-cluster-1\",\"record\":{\"log_time\":\"2024-01-10 11:09:41.382 UTC\",\"process_id\":\"28360\",\"session_id\":\"659e7af2.6ec8\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-01-10 11:09:38 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"08006\",\"message\":\"could not connect to the primary server: connection to server at \\\"10.xyx.yxz.zyx\\\", port 5432 failed: No route to host\\n\\tIs the server running on that host and accepting TCP/IP connections?\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\n```\r\nHowever, the status of my cluster is always `Cluster in healthy state` when I do `k get cluster` or `kubectl-cnpg status`.\r\nSimilarly, I can't find any metrics either from the cloudnative-operator or from the cluster reporting the fact that the connection is broken between the replica cluster and the primary\r\nHow can I monitor replication between the primary and replica clusters?\r\nHow can I make sure that my replica is up to date with the primary without having to do it manually every time?\r\n### Cluster resource\r\n```shell\r\nkubectl-cnpg status replica-cnpg-cluster\r\nReplica Cluster Summary\r\nName:                replica-cnpg-cluster\r\nNamespace:           skxxx\r\nSystem ID:           y32xxxx229zyx\r\nPostgreSQL Image:    ghcr.io/cloudnative-pg/postgresql:16.0\r\nDesignated primary:  replica-cnpg-cluster-1\r\nSource cluster:      main-pg-cluster\r\nPrimary start time:  2024-01-10 09:34:53 +0000 UTC (uptime 6h21m31s)\r\nStatus:              Cluster in healthy state \r\nInstances:           3\r\nReady instances:     3\r\nCertificates Status\r\nCertificate Name                  Expiration Date                Days Left Until Expiration\r\n----------------                  ---------------                --------------------------\r\nreplica-cnpg-cluster-ca           2024-04-09 09:29:17 +0000 UTC  89.73\r\nreplica-cnpg-cluster-replication  2024-04-09 09:29:17 +0000 UTC  89.73\r\nreplica-cnpg-cluster-server       2024-04-09 09:29:17 +0000 UTC  89.73\r\nContinuous Backup status\r\nNot configured\r\nPhysical backups\r\nNo running physical backups found\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nManaged roles status\r\nNo roles managed\r\nTablespaces status\r\nNo managed tablespaces\r\nInstances status\r\nName                    Database Size  Current LSN  Replication role              Status  QoS         Manager Version  Node\r\n----                    -------------  -----------  ----------------              ------  ---         ---------------  ----\r\nreplica-cnpg-cluster-1  SKO MB         1/84324B8    Designated primary            OK      BestEffort  1.22.0           integ-infra-ocp-6gcfd-worker-0-qhxrn\r\nreplica-cnpg-cluster-2  SKO MB         1/84324B8    Standby (in Replica Cluster)  OK      BestEffort  1.22.0           integ-infra-ocp-6gcfd-worker-0-8w24r\r\nreplica-cnpg-cluster-3  SKO MB         1/84324B8    Standby (in Replica Cluster)  OK      BestEffort  1.22.0           integ-infra-ocp-6gcfd-worker-0-6xwt6\r\noc get cluster\r\nNAME                   AGE     INSTANCES   READY   STATUS                     PRIMARY\r\nreplica-cnpg-cluster   6h27m   3           3       Cluster in healthy state   replica-cnpg-cluster-1\r\n```\r\n```\r\n### Relevant log output\r\n```shell\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T11:09:41Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"replica-cnpg-cluster-1\",\"record\":{\"log_time\":\"2024-01-10 11:09:41.382 UTC\",\"process_id\":\"28360\",\"session_id\":\"659e7af2.6ec8\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-01-10 11:09:38 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"08006\",\"message\":\"could not connect to the primary server: connection to server at \\\"10.xyx.yxz.zyx\\\", port 5432 failed: No route to host\\n\\tIs the server running on that host and accepting TCP/IP connections?\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\n```\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductThis is missing. Consider that at the moment our \"replica cluster\" story is not yet complete. In this specific case, we should monitor if the WAL receiver is up. Our recommendation is to always use the WAL archive as a fallback mechanism.\r\n\"Cluster is healthy\" is referred to the local replica cluster. It means that all replicas are up and so the designated primary.\n---\nI checked and we already monitor this: https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-WAL-RECEIVER-VIEW\r\nLook at the `pg_replication` metric in the Prometheus exporter.\r\nSee: https://github.com/cloudnative-pg/cloudnative-pg/blob/main/config/manager/default-monitoring.yaml#L109\n---\nHi @gbartolini \r\nThanks for your feedback\r\nI looked at the content of the \"pg_stat_wal_receiver\" view and I have the impression that the data in pg_stat_wal_receiver only concerns the primary pod and the replica pods within the same cluster.\r\nI've checked the primary and replica clusters, both of which only contain data for their replica pods. The hosts correspond only to their replica pods.\r\nSo I'm afraid the `pg_replication` metric is only valid within the same cluster.\r\n`cnpg_pg_replication_streaming_replicas` value (2) matchs only the clusters replica pods\r\nAnd only replica pods name and addr are present in metrics labels .... no information to  Replica cluster and its pods\r\nI'll also have a look at wal archiving"
    },
    {
        "title": "[Bug]: Primary PostgreSQL Instance Not Gracefully Shutting Down and Missing WAL File",
        "id": 2074372852,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\noren@aperio.ai\n### Version\n1.22.0\n### What version of Kubernetes are you using?\n1.27\n### What is your Kubernetes environment?\nCloud: Amazon EKS\n### How did you install the operator?\nHelm\n### What happened?\nWe deployed the operator using a Helm chart and the PostgreSQL cluster resource using a separate Helm chart. Three instances of PostgreSQL were deployed on EC2 Spot instances. After a few hours, it appears that the primary instance (postgres-1) did not gracefully shut down, leading to the absence of the relevant Write-Ahead Logging (WAL) file.\r\nSubsequently, attempts to use pg_rewind have failed, with the process encountering an empty file. Additionally, the file \"pg_xact/0000\" is also empty, contributing to the inability to resolve the issue and leaving the old primary instance in a stuck state.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    meta.helm.sh/release-name: postgres-cn\r\n    meta.helm.sh/release-namespace: stg\r\n  creationTimestamp: \"2024-01-09T12:06:49Z\"\r\n  generation: 4\r\n  labels:\r\n    app.kubernetes.io/managed-by: Helm\r\n  name: postgres\r\n  namespace: stg\r\n  resourceVersion: \"346234286\"\r\n  uid: 69ddea55-1c72-4e9f-abf6-aa2e43a32db4\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      import:\r\n        databases:\r\n        - webapi\r\n        source:\r\n          externalCluster: xxxxxx\r\n        type: microservice\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enableSuperuserAccess: true\r\n  externalClusters:\r\n  - connectionParameters:\r\n      dbname: webapi\r\n      host: xxxxxxxxxxxx\r\n      user: postgres\r\n    name: xxxxxxx\r\n    password:\r\n      key: postgres-password\r\n      name: xxxxxxxxxxxxxx\r\n  failoverDelay: 0\r\n  imageName: xxxxxxx/postgresql:16.1-xxxxxxxxx\r\n  inheritedMetadata:\r\n    labels:\r\n      app.kubernetes.io/name: postgres\r\n      app.kubernetes.io/version: \"16.1\"\r\n  instances: 3\r\n  logLevel: info\r\n  maxSyncReplicas: 2\r\n  minSyncReplicas: 1\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: false\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources: {}\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 30Gi\r\n    storageClass: gp3\r\n  superuserSecret:\r\n    name: postgres-superuser\r\n  switchoverDelay: 3600\r\nstatus:\r\n  certificates:\r\n    clientCASecret: postgres-ca\r\n    expirations:\r\n      postgres-ca: 2024-04-08 12:01:49 +0000 UTC\r\n      postgres-replication: 2024-04-08 12:01:49 +0000 UTC\r\n      postgres-server: 2024-04-08 12:01:49 +0000 UTC\r\n    replicationTLSSecret: postgres-replication\r\n    serverAltDNSNames:\r\n    - postgres-rw\r\n    - postgres-rw.stg\r\n    - postgres-rw.stg.svc\r\n    - postgres-r\r\n    - postgres-r.stg\r\n    - postgres-r.stg.svc\r\n    - postgres-ro\r\n    - postgres-ro.stg\r\n    - postgres-ro.stg.svc\r\n    serverCASecret: postgres-ca\r\n    serverTLSSecret: postgres-server\r\n  cloudNativePGCommitHash: 86b9dc80\r\n  cloudNativePGOperatorHash: 411a67468c5e1357d387221ce5b11d906bd33d62324846a8c776d206b6c94162\r\n  conditions:\r\n  - lastTransitionTime: \"2024-01-10T08:20:16Z\"\r\n    message: Cluster Is Not Ready\r\n    reason: ClusterIsNotReady\r\n    status: \"False\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-01-10T12:18:14Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"345484626\"\r\n  currentPrimary: postgres-2\r\n  currentPrimaryTimestamp: \"2024-01-10T08:21:59.663283Z\"\r\n  healthyPVC:\r\n  - postgres-1\r\n  - postgres-2\r\n  - postgres-3\r\n  instanceNames:\r\n  - postgres-1\r\n  - postgres-2\r\n  - postgres-3\r\n  instances: 3\r\n  instancesReportedState:\r\n    postgres-1:\r\n      isPrimary: false\r\n    postgres-2:\r\n      isPrimary: true\r\n      timeLineID: 2\r\n    postgres-3:\r\n      isPrimary: false\r\n      timeLineID: 2\r\n  instancesStatus:\r\n    healthy:\r\n    - postgres-2\r\n    - postgres-3\r\n    replicating:\r\n    - postgres-1\r\n  latestGeneratedNode: 3\r\n  managedRolesStatus: {}\r\n  phase: Waiting for the instances to become active\r\n  phaseReason: Some instances are not yet active. Please wait.\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 3\r\n  readService: postgres-r\r\n  readyInstances: 2\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"345484596\"\r\n    clientCaSecretVersion: \"345484593\"\r\n    externalClusterSecretVersion:\r\n      stg-postgres-postgresql: \"231319709\"\r\n    replicationSecretVersion: \"345484595\"\r\n    serverCaSecretVersion: \"345484593\"\r\n    serverSecretVersion: \"345484594\"\r\n    superuserSecretVersion: \"341642273\"\r\n  targetPrimary: postgres-2\r\n  targetPrimaryTimestamp: \"2024-01-10T08:20:16.701057Z\"\r\n  timelineID: 2\r\n  topology:\r\n    instances:\r\n      postgres-1: {}\r\n      postgres-2: {}\r\n      postgres-3: {}\r\n    nodesUsed: 2\r\n    successfullyExtracted: true\r\n  writeService: postgres-rw\n```\n### Relevant log output\n```shell\n{\"level\":\"error\",\"ts\":\"2024-01-10T10:58:19Z\",\"msg\":\"Reconciler error\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"9b64cee9-7e71-4e05-9bb0-e1cbaee76c9d\",\"error\":\"while activating instance: error starting PostgreSQL instance: exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Cluster status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"b6dbc5ac-aab1-4528-bc0c-f67aa8dac9b1\",\"uuid\":\"2f587c83-afa7-11ee-9aef-929973a308f2\",\"logging_pod\":\"postgres-1\",\"currentPrimary\":\"postgres-2\",\"targetPrimary\":\"postgres-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"This is an old primary instance, waiting for the switchover to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"b6dbc5ac-aab1-4528-bc0c-f67aa8dac9b1\",\"uuid\":\"2f587c83-afa7-11ee-9aef-929973a308f2\",\"logging_pod\":\"postgres-1\",\"currentPrimary\":\"postgres-2\",\"targetPrimary\":\"postgres-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Switchover completed\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"b6dbc5ac-aab1-4528-bc0c-f67aa8dac9b1\",\"uuid\":\"2f587c83-afa7-11ee-9aef-929973a308f2\",\"logging_pod\":\"postgres-1\",\"targetPrimary\":\"postgres-2\",\"currentPrimary\":\"postgres-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Waiting for the new primary to be available\",\"logging_pod\":\"postgres-1\",\"primaryConnInfo\":\"host=postgres-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=postgres-1 sslmode=verify-ca dbname=postgres connect_timeout=5\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Extracting pg_controldata information\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"b6dbc5ac-aab1-4528-bc0c-f67aa8dac9b1\",\"uuid\":\"2f587c83-afa7-11ee-9aef-929973a308f2\",\"logging_pod\":\"postgres-1\",\"reason\":\"before pg_rewind\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7322068936411099156\\nDatabase cluster state:               shut down\\npg_control last modified:             Wed 10 Jan 2024 08:19:35 AM UTC\\nLatest checkpoint location:           2/AA000028\\nLatest checkpoint's REDO location:    2/AA000028\\nLatest checkpoint's REDO WAL file:    0000000100000002000000AA\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:36923\\nLatest checkpoint's NextOID:          25092\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  0\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Wed 10 Jan 2024 08:19:35 AM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            4e9ac5de19e20369e90184039bdcf5c867fa08bd811982f2edb29d1f11d8fd7b\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Starting up pg_rewind\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"b6dbc5ac-aab1-4528-bc0c-f67aa8dac9b1\",\"uuid\":\"2f587c83-afa7-11ee-9aef-929973a308f2\",\"logging_pod\":\"postgres-1\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"options\":[\"-P\",\"--source-server\",\"host=postgres-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=postgres-1 sslmode=verify-ca dbname=postgres\",\"--target-pgdata\",\"/var/lib/postgresql/data/pgdata\",\"--restore-target-wal\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: connected to server\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: servers diverged at WAL location 2/A9000860 on timeline 1\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: error: could not read file \\\"/var/lib/postgresql/data/pgdata/pg_wal/0000000100000002000000A8\\\": read 0 of 8192\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: error: could not find previous WAL record at 2/A8241C48\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"error\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Failed to execute pg_rewind\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"b6dbc5ac-aab1-4528-bc0c-f67aa8dac9b1\",\"uuid\":\"2f587c83-afa7-11ee-9aef-929973a308f2\",\"logging_pod\":\"postgres-1\",\"options\":[\"-P\",\"--source-server\",\"host=postgres-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=postgres-1 sslmode=verify-ca dbname=postgres\",\"--target-pgdata\",\"/var/lib/postgresql/data/pgdata\",\"--restore-target-wal\"],\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*Instance).Rewind\\n\\tpkg/management/postgres/instance.go:972\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).verifyPgDataCoherenceForPrimary\\n\\tinternal/management/controller/instance_startup.go:235\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).initialize\\n\\tinternal/management/controller/instance_controller.go:342\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).Reconcile\\n\\tinternal/management/controller/instance_controller.go:141\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"pg_rewind failed, starting the server to complete the crash recovery\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"b6dbc5ac-aab1-4528-bc0c-f67aa8dac9b1\",\"uuid\":\"2f587c83-afa7-11ee-9aef-929973a308f2\",\"logging_pod\":\"postgres-1\",\"err\":\"error executing pg_rewind: exit status 1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Waiting for server to complete crash recovery\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Starting up instance\",\"logging_pod\":\"postgres-1\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"options\":[\"start\",\"-w\",\"-D\",\"/var/lib/postgresql/data/pgdata\",\"-o\",\"-c port=5432 -c unix_socket_directories=/controller/run\",\"-t 40000000\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_ctl\",\"msg\":\"waiting for server to start....2024-01-10 10:58:28.483 UTC [348] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stdout\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_ctl\",\"msg\":\"2024-01-10 10:58:28.483 UTC [348] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stdout\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"2024-01-10 10:58:28.484 UTC [348] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"2024-01-10 10:58:28.484 UTC [348] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.484 UTC\",\"process_id\":\"348\",\"session_id\":\"659e7854.15c\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.484 UTC\",\"process_id\":\"348\",\"session_id\":\"659e7854.15c\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 16.1 (Debian 16.1-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.484 UTC\",\"process_id\":\"348\",\"session_id\":\"659e7854.15c\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.484 UTC\",\"process_id\":\"348\",\"session_id\":\"659e7854.15c\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.488 UTC\",\"process_id\":\"348\",\"session_id\":\"659e7854.15c\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.495 UTC\",\"process_id\":\"352\",\"session_id\":\"659e7854.160\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was shut down at 2024-01-10 08:19:35 UTC\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.495 UTC\",\"process_id\":\"352\",\"session_id\":\"659e7854.160\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"XX000\",\"message\":\"could not access status of transaction 36923\",\"detail\":\"Could not read from file \\\"pg_xact/0000\\\" at offset 8192: read too few bytes.\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.496 UTC\",\"process_id\":\"348\",\"session_id\":\"659e7854.15c\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 352) exited with exit code 1\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.497 UTC\",\"process_id\":\"348\",\"session_id\":\"659e7854.15c\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"aborting startup due to startup process failure\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.497 UTC\",\"process_id\":\"348\",\"session_id\":\"659e7854.15c\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_ctl\",\"msg\":\" stopped waiting\",\"pipe\":\"stdout\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_ctl\",\"msg\":\"pg_ctl: could not start server\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_ctl\",\"msg\":\"Examine the log output.\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"error\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Reconciler error\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"b6dbc5ac-aab1-4528-bc0c-f67aa8dac9b1\",\"error\":\"while activating instance: error starting PostgreSQL instance: exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\noren@aperio.ai\n### Version\n1.22.0\n### What version of Kubernetes are you using?\n1.27\n### What is your Kubernetes environment?\nCloud: Amazon EKS\n### How did you install the operator?\nHelm\n### What happened?\nWe deployed the operator using a Helm chart and the PostgreSQL cluster resource using a separate Helm chart. Three instances of PostgreSQL were deployed on EC2 Spot instances. After a few hours, it appears that the primary instance (postgres-1) did not gracefully shut down, leading to the absence of the relevant Write-Ahead Logging (WAL) file.\r\nSubsequently, attempts to use pg_rewind have failed, with the process encountering an empty file. Additionally, the file \"pg_xact/0000\" is also empty, contributing to the inability to resolve the issue and leaving the old primary instance in a stuck state.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    meta.helm.sh/release-name: postgres-cn\r\n    meta.helm.sh/release-namespace: stg\r\n  creationTimestamp: \"2024-01-09T12:06:49Z\"\r\n  generation: 4\r\n  labels:\r\n    app.kubernetes.io/managed-by: Helm\r\n  name: postgres\r\n  namespace: stg\r\n  resourceVersion: \"346234286\"\r\n  uid: 69ddea55-1c72-4e9f-abf6-aa2e43a32db4\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      import:\r\n        databases:\r\n        - webapi\r\n        source:\r\n          externalCluster: xxxxxx\r\n        type: microservice\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enableSuperuserAccess: true\r\n  externalClusters:\r\n  - connectionParameters:\r\n      dbname: webapi\r\n      host: xxxxxxxxxxxx\r\n      user: postgres\r\n    name: xxxxxxx\r\n    password:\r\n      key: postgres-password\r\n      name: xxxxxxxxxxxxxx\r\n  failoverDelay: 0\r\n  imageName: xxxxxxx/postgresql:16.1-xxxxxxxxx\r\n  inheritedMetadata:\r\n    labels:\r\n      app.kubernetes.io/name: postgres\r\n      app.kubernetes.io/version: \"16.1\"\r\n  instances: 3\r\n  logLevel: info\r\n  maxSyncReplicas: 2\r\n  minSyncReplicas: 1\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: false\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources: {}\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 30Gi\r\n    storageClass: gp3\r\n  superuserSecret:\r\n    name: postgres-superuser\r\n  switchoverDelay: 3600\r\nstatus:\r\n  certificates:\r\n    clientCASecret: postgres-ca\r\n    expirations:\r\n      postgres-ca: 2024-04-08 12:01:49 +0000 UTC\r\n      postgres-replication: 2024-04-08 12:01:49 +0000 UTC\r\n      postgres-server: 2024-04-08 12:01:49 +0000 UTC\r\n    replicationTLSSecret: postgres-replication\r\n    serverAltDNSNames:\r\n    - postgres-rw\r\n    - postgres-rw.stg\r\n    - postgres-rw.stg.svc\r\n    - postgres-r\r\n    - postgres-r.stg\r\n    - postgres-r.stg.svc\r\n    - postgres-ro\r\n    - postgres-ro.stg\r\n    - postgres-ro.stg.svc\r\n    serverCASecret: postgres-ca\r\n    serverTLSSecret: postgres-server\r\n  cloudNativePGCommitHash: 86b9dc80\r\n  cloudNativePGOperatorHash: 411a67468c5e1357d387221ce5b11d906bd33d62324846a8c776d206b6c94162\r\n  conditions:\r\n  - lastTransitionTime: \"2024-01-10T08:20:16Z\"\r\n    message: Cluster Is Not Ready\r\n    reason: ClusterIsNotReady\r\n    status: \"False\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2024-01-10T12:18:14Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"345484626\"\r\n  currentPrimary: postgres-2\r\n  currentPrimaryTimestamp: \"2024-01-10T08:21:59.663283Z\"\r\n  healthyPVC:\r\n  - postgres-1\r\n  - postgres-2\r\n  - postgres-3\r\n  instanceNames:\r\n  - postgres-1\r\n  - postgres-2\r\n  - postgres-3\r\n  instances: 3\r\n  instancesReportedState:\r\n    postgres-1:\r\n      isPrimary: false\r\n    postgres-2:\r\n      isPrimary: true\r\n      timeLineID: 2\r\n    postgres-3:\r\n      isPrimary: false\r\n      timeLineID: 2\r\n  instancesStatus:\r\n    healthy:\r\n    - postgres-2\r\n    - postgres-3\r\n    replicating:\r\n    - postgres-1\r\n  latestGeneratedNode: 3\r\n  managedRolesStatus: {}\r\n  phase: Waiting for the instances to become active\r\n  phaseReason: Some instances are not yet active. Please wait.\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 3\r\n  readService: postgres-r\r\n  readyInstances: 2\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"345484596\"\r\n    clientCaSecretVersion: \"345484593\"\r\n    externalClusterSecretVersion:\r\n      stg-postgres-postgresql: \"231319709\"\r\n    replicationSecretVersion: \"345484595\"\r\n    serverCaSecretVersion: \"345484593\"\r\n    serverSecretVersion: \"345484594\"\r\n    superuserSecretVersion: \"341642273\"\r\n  targetPrimary: postgres-2\r\n  targetPrimaryTimestamp: \"2024-01-10T08:20:16.701057Z\"\r\n  timelineID: 2\r\n  topology:\r\n    instances:\r\n      postgres-1: {}\r\n      postgres-2: {}\r\n      postgres-3: {}\r\n    nodesUsed: 2\r\n    successfullyExtracted: true\r\n  writeService: postgres-rw\n```\n### Relevant log output\n```shell\n{\"level\":\"error\",\"ts\":\"2024-01-10T10:58:19Z\",\"msg\":\"Reconciler error\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"9b64cee9-7e71-4e05-9bb0-e1cbaee76c9d\",\"error\":\"while activating instance: error starting PostgreSQL instance: exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Cluster status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"b6dbc5ac-aab1-4528-bc0c-f67aa8dac9b1\",\"uuid\":\"2f587c83-afa7-11ee-9aef-929973a308f2\",\"logging_pod\":\"postgres-1\",\"currentPrimary\":\"postgres-2\",\"targetPrimary\":\"postgres-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"This is an old primary instance, waiting for the switchover to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"b6dbc5ac-aab1-4528-bc0c-f67aa8dac9b1\",\"uuid\":\"2f587c83-afa7-11ee-9aef-929973a308f2\",\"logging_pod\":\"postgres-1\",\"currentPrimary\":\"postgres-2\",\"targetPrimary\":\"postgres-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Switchover completed\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"b6dbc5ac-aab1-4528-bc0c-f67aa8dac9b1\",\"uuid\":\"2f587c83-afa7-11ee-9aef-929973a308f2\",\"logging_pod\":\"postgres-1\",\"targetPrimary\":\"postgres-2\",\"currentPrimary\":\"postgres-2\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Waiting for the new primary to be available\",\"logging_pod\":\"postgres-1\",\"primaryConnInfo\":\"host=postgres-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=postgres-1 sslmode=verify-ca dbname=postgres connect_timeout=5\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Extracting pg_controldata information\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"b6dbc5ac-aab1-4528-bc0c-f67aa8dac9b1\",\"uuid\":\"2f587c83-afa7-11ee-9aef-929973a308f2\",\"logging_pod\":\"postgres-1\",\"reason\":\"before pg_rewind\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7322068936411099156\\nDatabase cluster state:               shut down\\npg_control last modified:             Wed 10 Jan 2024 08:19:35 AM UTC\\nLatest checkpoint location:           2/AA000028\\nLatest checkpoint's REDO location:    2/AA000028\\nLatest checkpoint's REDO WAL file:    0000000100000002000000AA\\nLatest checkpoint's TimeLineID:       1\\nLatest checkpoint's PrevTimeLineID:   1\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:36923\\nLatest checkpoint's NextOID:          25092\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  0\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Wed 10 Jan 2024 08:19:35 AM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            4e9ac5de19e20369e90184039bdcf5c867fa08bd811982f2edb29d1f11d8fd7b\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Starting up pg_rewind\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"b6dbc5ac-aab1-4528-bc0c-f67aa8dac9b1\",\"uuid\":\"2f587c83-afa7-11ee-9aef-929973a308f2\",\"logging_pod\":\"postgres-1\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"options\":[\"-P\",\"--source-server\",\"host=postgres-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=postgres-1 sslmode=verify-ca dbname=postgres\",\"--target-pgdata\",\"/var/lib/postgresql/data/pgdata\",\"--restore-target-wal\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: connected to server\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: servers diverged at WAL location 2/A9000860 on timeline 1\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: error: could not read file \\\"/var/lib/postgresql/data/pgdata/pg_wal/0000000100000002000000A8\\\": read 0 of 8192\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_rewind\",\"msg\":\"pg_rewind: error: could not find previous WAL record at 2/A8241C48\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"error\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Failed to execute pg_rewind\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"b6dbc5ac-aab1-4528-bc0c-f67aa8dac9b1\",\"uuid\":\"2f587c83-afa7-11ee-9aef-929973a308f2\",\"logging_pod\":\"postgres-1\",\"options\":[\"-P\",\"--source-server\",\"host=postgres-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=postgres-1 sslmode=verify-ca dbname=postgres\",\"--target-pgdata\",\"/var/lib/postgresql/data/pgdata\",\"--restore-target-wal\"],\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*Instance).Rewind\\n\\tpkg/management/postgres/instance.go:972\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).verifyPgDataCoherenceForPrimary\\n\\tinternal/management/controller/instance_startup.go:235\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).initialize\\n\\tinternal/management/controller/instance_controller.go:342\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).Reconcile\\n\\tinternal/management/controller/instance_controller.go:141\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"pg_rewind failed, starting the server to complete the crash recovery\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"b6dbc5ac-aab1-4528-bc0c-f67aa8dac9b1\",\"uuid\":\"2f587c83-afa7-11ee-9aef-929973a308f2\",\"logging_pod\":\"postgres-1\",\"err\":\"error executing pg_rewind: exit status 1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Waiting for server to complete crash recovery\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Starting up instance\",\"logging_pod\":\"postgres-1\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"options\":[\"start\",\"-w\",\"-D\",\"/var/lib/postgresql/data/pgdata\",\"-o\",\"-c port=5432 -c unix_socket_directories=/controller/run\",\"-t 40000000\"]}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_ctl\",\"msg\":\"waiting for server to start....2024-01-10 10:58:28.483 UTC [348] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stdout\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_ctl\",\"msg\":\"2024-01-10 10:58:28.483 UTC [348] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stdout\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"2024-01-10 10:58:28.484 UTC [348] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"2024-01-10 10:58:28.484 UTC [348] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.484 UTC\",\"process_id\":\"348\",\"session_id\":\"659e7854.15c\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.484 UTC\",\"process_id\":\"348\",\"session_id\":\"659e7854.15c\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 16.1 (Debian 16.1-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.484 UTC\",\"process_id\":\"348\",\"session_id\":\"659e7854.15c\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.484 UTC\",\"process_id\":\"348\",\"session_id\":\"659e7854.15c\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.488 UTC\",\"process_id\":\"348\",\"session_id\":\"659e7854.15c\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.495 UTC\",\"process_id\":\"352\",\"session_id\":\"659e7854.160\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was shut down at 2024-01-10 08:19:35 UTC\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.495 UTC\",\"process_id\":\"352\",\"session_id\":\"659e7854.160\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"XX000\",\"message\":\"could not access status of transaction 36923\",\"detail\":\"Could not read from file \\\"pg_xact/0000\\\" at offset 8192: read too few bytes.\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.496 UTC\",\"process_id\":\"348\",\"session_id\":\"659e7854.15c\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 352) exited with exit code 1\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.497 UTC\",\"process_id\":\"348\",\"session_id\":\"659e7854.15c\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"aborting startup due to startup process failure\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"postgres-1\",\"record\":{\"log_time\":\"2024-01-10 10:58:28.497 UTC\",\"process_id\":\"348\",\"session_id\":\"659e7854.15c\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-01-10 10:58:28 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_ctl\",\"msg\":\" stopped waiting\",\"pipe\":\"stdout\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_ctl\",\"msg\":\"pg_ctl: could not start server\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"logger\":\"pg_ctl\",\"msg\":\"Examine the log output.\",\"pipe\":\"stderr\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"info\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"postgres-1\"}\r\n{\"level\":\"error\",\"ts\":\"2024-01-10T10:58:28Z\",\"msg\":\"Reconciler error\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres\",\"namespace\":\"stg\"},\"namespace\":\"stg\",\"name\":\"postgres\",\"reconcileID\":\"b6dbc5ac-aab1-4528-bc0c-f67aa8dac9b1\",\"error\":\"while activating instance: error starting PostgreSQL instance: exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductFrom what I can see, it looks like an issue with the underlying file system (data corruption of that file). You need to recreate the PVC of that instance.\n---\nIt happens 2 days in a row, we are using EC2 spot instances, and we managed to solve it by deleting the problematic PVC (gp3)  but that manual PVC creation every day is probably not the desired behavior.\n---\nThis might be related to #3698. Give us some time to investigate.\n---\nI face the same issue in multiple clusters in the same k8s cluster. I think it occurs when a network error happens.\r\nThe only way to resolve it is to delete the PVC of the failing pod and remove the pod. In this case, the operator is going to recreate the failing replica.\r\nLogs:\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"harbor-postgres-3\",\"version\":\"1.22.1\",\"build\":{\"Version\":\"1.22.1\",\"Commit\":\"c7be872e\",\"Date\":\"2024-02-02\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"logger\":\"setup\",\"msg\":\"starting tablespace manager\",\"logging_pod\":\"harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"logger\":\"setup\",\"msg\":\"starting external server manager\",\"logging_pod\":\"harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"logger\":\"roles_reconciler\",\"msg\":\"skipping the RoleSynchronizer in replicas\",\"logging_pod\":\"harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"harbor-postgres-3\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"harbor-postgres-3\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"harbor-postgres-3\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:28Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"msg\":\"Installed configuration file\",\"logging_pod\":\"harbor-postgres-3\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"filename\":\"pg_ident.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"harbor-postgres-3\",\"filename\":\"override.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"msg\":\"Found previous run flag\",\"logging_pod\":\"harbor-postgres-3\",\"filename\":\"/var/lib/postgresql/data/pgdata/cnpg_initialized-harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"harbor-postgres-3\",\"reason\":\"postmaster start up\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202307071\\nDatabase system identifier:           7321087858539200534\\nDatabase cluster state:               shut down in recovery\\npg_control last modified:             Thu 29 Feb 2024 06:43:08 AM UTC\\nLatest checkpoint location:           E/10000028\\nLatest checkpoint's REDO location:    E/10000028\\nLatest checkpoint's REDO WAL file:    0000000C0000000E00000010\\nLatest checkpoint's TimeLineID:       12\\nLatest checkpoint's PrevTimeLineID:   12\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:93004\\nLatest checkpoint's NextOID:          25537\\nLatest checkpoint's NextMultiXactId:  2\\nLatest checkpoint's NextMultiOffset:  3\\nLatest checkpoint's oldestXID:        722\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  0\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Wed 28 Feb 2024 09:53:23 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     E/100000A0\\nMin recovery ending loc's timeline:   12\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            431926e09764e6c576b934232250a417e85da4f29af6306f5db1e88e1660bc4f\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"harbor-postgres\",\"namespace\":\"harbor\"},\"namespace\":\"harbor\",\"name\":\"harbor-postgres\",\"reconcileID\":\"0846c8b9-adae-4fa0-9085-82724a409769\",\"uuid\":\"e5f15169-d6d1-11ee-83aa-4ade4de24c82\",\"logging_pod\":\"harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"logger\":\"postgres\",\"msg\":\"2024-02-29 07:12:29.099 UTC [24] LOG:  pgaudit extension initialized\",\"pipe\":\"stderr\",\"logging_pod\":\"harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"logger\":\"postgres\",\"msg\":\"2024-02-29 07:12:29.152 UTC [24] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"logger\":\"postgres\",\"msg\":\"2024-02-29 07:12:29.152 UTC [24] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:29.152 UTC\",\"process_id\":\"24\",\"session_id\":\"65e02e5d.18\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:29.152 UTC\",\"process_id\":\"24\",\"session_id\":\"65e02e5d.18\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-02-29 07:12:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 16.1 (Debian 16.1-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:29.152 UTC\",\"process_id\":\"24\",\"session_id\":\"65e02e5d.18\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-02-29 07:12:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:29.152 UTC\",\"process_id\":\"24\",\"session_id\":\"65e02e5d.18\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-02-29 07:12:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"logger\":\"postgres\",\"msg\":\"2024-02-29 07:12:29.152 UTC [24] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"logger\":\"postgres\",\"msg\":\"2024-02-29 07:12:29.152 UTC [24] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:29.167 UTC\",\"process_id\":\"24\",\"session_id\":\"65e02e5d.18\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-02-29 07:12:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:29.198 UTC\",\"process_id\":\"28\",\"session_id\":\"65e02e5d.1c\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was shut down in recovery at 2024-02-29 06:43:08 UTC\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"harbor-postgres-3\",\"walName\":\"0000000D.history\",\"startTime\":\"2024-02-29T07:12:29Z\",\"endTime\":\"2024-02-29T07:12:29Z\",\"elapsedWalTime\":0.633726065}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"harbor-postgres-3\",\"walName\":\"0000000D.history\",\"maxParallel\":1,\"successfulWalRestore\":1,\"failedWalRestore\":0,\"endOfWALStream\":false,\"startTime\":\"2024-02-29T07:12:29Z\",\"downloadStartTime\":\"2024-02-29T07:12:29Z\",\"downloadTotalTime\":0.634063937,\"totalTime\":0.734779978}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:29Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:29.964 UTC\",\"process_id\":\"28\",\"session_id\":\"65e02e5d.1c\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-02-29 07:12:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000D.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:30Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:30.066 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"52\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e5e.34\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:30 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:30Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"harbor-postgres-3\",\"filename\":\"override.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:30Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:30.222 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"55\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e5e.37\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:30 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:30Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:30.245 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"56\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e5e.38\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:30 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:30Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:30.246 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"57\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e5e.39\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:30 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:30Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"harbor-postgres\",\"namespace\":\"harbor\"},\"namespace\":\"harbor\",\"name\":\"harbor-postgres\",\"reconcileID\":\"fbc281dc-1514-4b0a-9c39-0689e3a17f0a\",\"uuid\":\"e6a01d01-d6d1-11ee-83aa-4ade4de24c82\",\"logging_pod\":\"harbor-postgres-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:30Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:30.250 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"58\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e5e.3a\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:30 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:30Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:30.305 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"59\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e5e.3b\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:30 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:30Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:30.571 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"60\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e5e.3c\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:30 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:30Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"harbor-postgres-3\",\"walName\":\"0000000E.history\",\"options\":[\"--endpoint-url\",\"https://s3.eu-central-1.amazonaws.com\",\"--cloud-provider\",\"aws-s3\",\"s3://TRUNCATED/\",\"harbor-postgres\"],\"startTime\":\"2024-02-29T07:12:30Z\",\"endTime\":\"2024-02-29T07:12:30Z\",\"elapsedWalTime\":0.555448566}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:30Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:30.751 UTC\",\"process_id\":\"28\",\"session_id\":\"65e02e5d.1c\",\"session_line_num\":\"3\",\"session_start_time\":\"2024-02-29 07:12:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"entering standby mode\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:31Z\",\"msg\":\"Updated replication settings\",\"logging_pod\":\"harbor-postgres-3\",\"filename\":\"override.conf\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:31.397 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"74\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e5f.4a\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:31 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:31.399 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"75\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e5f.4b\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:31 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:31Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"harbor-postgres\",\"namespace\":\"harbor\"},\"namespace\":\"harbor\",\"name\":\"harbor-postgres\",\"reconcileID\":\"70ee1fc8-5c19-43f4-a7d1-3bc8df2a218d\",\"uuid\":\"e75275e9-d6d1-11ee-83aa-4ade4de24c82\",\"logging_pod\":\"harbor-postgres-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:31Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"harbor-postgres-3\",\"walName\":\"0000000D.history\",\"startTime\":\"2024-02-29T07:12:30Z\",\"endTime\":\"2024-02-29T07:12:31Z\",\"elapsedWalTime\":0.524848024}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:31Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"harbor-postgres-3\",\"walName\":\"0000000D.history\",\"maxParallel\":1,\"successfulWalRestore\":1,\"failedWalRestore\":0,\"endOfWALStream\":false,\"startTime\":\"2024-02-29T07:12:30Z\",\"downloadStartTime\":\"2024-02-29T07:12:30Z\",\"downloadTotalTime\":0.525037538,\"totalTime\":0.633526301}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:31.418 UTC\",\"process_id\":\"28\",\"session_id\":\"65e02e5d.1c\",\"session_line_num\":\"4\",\"session_start_time\":\"2024-02-29 07:12:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000D.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:31Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:31.911 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"88\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e5f.58\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:31 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:32Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:32.059 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"89\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e60.59\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:32 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:32Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:32.072 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"90\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e60.5a\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:32 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:32Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:32.129 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"91\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e60.5b\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:32 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:32Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"harbor-postgres-3\",\"walName\":\"0000000D0000000E00000010\",\"startTime\":\"2024-02-29T07:12:31Z\",\"endTime\":\"2024-02-29T07:12:32Z\",\"elapsedWalTime\":0.614393645}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:32Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"harbor-postgres-3\",\"walName\":\"0000000D0000000E00000010\",\"maxParallel\":1,\"successfulWalRestore\":1,\"failedWalRestore\":0,\"endOfWALStream\":false,\"startTime\":\"2024-02-29T07:12:31Z\",\"downloadStartTime\":\"2024-02-29T07:12:31Z\",\"downloadTotalTime\":0.61485673,\"totalTime\":0.708725379}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:32Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:32.200 UTC\",\"process_id\":\"28\",\"session_id\":\"65e02e5d.1c\",\"session_line_num\":\"5\",\"session_start_time\":\"2024-02-29 07:12:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000D0000000E00000010\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:32Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:32.392 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"93\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e60.5d\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:32 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:32Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:32.416 UTC\",\"process_id\":\"28\",\"session_id\":\"65e02e5d.1c\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-02-29 07:12:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"invalid resource manager ID in checkpoint record\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:32Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:32.416 UTC\",\"process_id\":\"28\",\"session_id\":\"65e02e5d.1c\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-02-29 07:12:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"PANIC\",\"sql_state_code\":\"XX000\",\"message\":\"could not locate a valid checkpoint record\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:32Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:32.434 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"94\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e60.5e\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:32 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:32Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:32.494 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"96\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e60.60\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:32 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:32Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"harbor-postgres\",\"namespace\":\"harbor\"},\"namespace\":\"harbor\",\"name\":\"harbor-postgres\",\"reconcileID\":\"d5a0ece4-2194-454f-80c1-1c370fda5f19\",\"uuid\":\"e801d87a-d6d1-11ee-83aa-4ade4de24c82\",\"logging_pod\":\"harbor-postgres-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:32Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:32.496 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"97\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e60.61\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:32 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:32Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:32.634 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"98\",\"connection_from\":\"[local]\",\"session_id\":\"65e02e60.62\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-02-29 07:12:32 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:33.283 UTC\",\"process_id\":\"24\",\"session_id\":\"65e02e5d.18\",\"session_line_num\":\"6\",\"session_start_time\":\"2024-02-29 07:12:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 28) was terminated by signal 6: Aborted\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:33.283 UTC\",\"process_id\":\"24\",\"session_id\":\"65e02e5d.18\",\"session_line_num\":\"7\",\"session_start_time\":\"2024-02-29 07:12:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"aborting startup due to startup process failure\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"harbor-postgres-3\",\"record\":{\"log_time\":\"2024-02-29 07:12:33.285 UTC\",\"process_id\":\"24\",\"session_id\":\"65e02e5d.18\",\"session_line_num\":\"8\",\"session_start_time\":\"2024-02-29 07:12:29 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"harbor-postgres-3\",\"reason\":\"postmaster has exited\"}\r\n{\"level\":\"error\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"PostgreSQL process exited with errors\",\"logging_pod\":\"harbor-postgres-3\",\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:98\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/manager/runnable_group.go:223\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"Stopping and waiting for leader election runnables\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"error\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"error received after stop sequence was engaged\",\"error\":\"exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).engageStopProcedure.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/manager/internal.go:490\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"harbor-postgres-3\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"harbor-postgres-3\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"harbor-postgres-3\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"Stopping and waiting for caches\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"harbor-postgres-3\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"Stopping and waiting for webhooks\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"Stopping and waiting for HTTP servers\"}\r\n{\"level\":\"info\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\r\n```\n---\nWe are facing the same problem on Google GKE clusters. After each node upgrade random instances have problems with recovering. The only workaround we found is removing one of failing instance PVC.\n---\nJust encountered into the same issue in GKE\n---\nI keep running into this every other day\n---\nAlso discovered this now.. is CNPG team aware of this? Whom to ping about this issue?\r\nonly working solution was to delete the failing Pod and PVC so a new node is created to join the cluster\n---\nI am facing similar issue. Do we have any fix available for below issue ? I tried to delete the pod but did not help.\r\n`{\"level\":\"error\",\"ts\":\"2024-02-29T07:12:33Z\",\"msg\":\"PostgreSQL process exited with errors\",\"logging_pod\":\"harbor-postgres-3\",\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:98\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/manager/runnable_group.go:223\"}`\n---\n@gbartolini is it possible to modify the health check behavior of the pods managed by CNPG?\n---\n> This might be related to #3698. Give us some time to investigate.\r\n@gbartolini Any updates on this would be greatly appreciated \ud83d\ude4f\ud83c\udfff\n---\nThe only way I've been able to recover from this is to delete both the PVC and Pod\n---\nSame Issue here. Luckily, Our production cluster survived because one of the 3 replicas was working. Deleting both PVC and Pod solved the issue but it is not a permanent solution. \r\nwe use GKE\n---\nI'm facing the same issue using the CNPG community addon for Microk8s, i.e. CNPG v1.22.0.\r\nAfter a reboot of the cluster, pods sometimes end up in `0/1 Running` with the `Reconciler error`.\r\nI will try with v1.23.2.\n---\nSeems to be gone with latest v1.23. The same for you?\n---\nIndeed, it seems good for my cluster too\n---\nI updated the operator to the latest version yesterday and restored the database; today it's the same again. It makes me nervous and angry\r\nI have expanded the PVC and everything is fixed. There are no related logs!\n---\nI think I am facing the similar issue. I am running CNPG on AWS Spot Instances provisioned through Karpenter Cluster Autoscaler. Currently I have only one of the three instances working. The other two instance don't seem to come up. \r\nI am using CNPG Operator  **v1.23.1**\r\nSeeing error messages like this on the instance that won't come up:\r\n```JSON\r\n{\"level\":\"info\",\"ts\":\"2024-08-06T10:12:20Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg-cluster-3\",\"record\":{\"log_time\":\"2024-08-06 10:12:20.737 UTC\",\"process_id\":\"18434\",\"session_id\":\"66b1f704.4802\",\"session_line_num\":\"2\",\"session_start_time\":\"2024-08-06 10:12:20 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"XX000\",\"message\":\"could not receive data from WAL stream: ERROR:  requested WAL segment 0000000A000001BB00000044 has already been removed\",\"backend_type\":\"walreceiver\"}}\r\n{\"level\":\"info\",\"ts\":\"2024-08-06T10:12:20Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pg-cluster-3\",\"record\":{\"log_time\":\"2024-08-06 10:12:20.913 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"18441\",\"connection_from\":\"[local]\",\"session_id\":\"66b1f704.4809\",\"session_line_num\":\"1\",\"session_start_time\":\"2024-08-06 10:12:20 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\"}}\r\n```\r\nand\r\n```JSON\r\n{\"level\":\"info\",\"ts\":\"2024-08-06T10:12:21Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pg-cluster\",\"namespace\":\"xxx-development\"},\"namespace\":\"xxx-development\",\"name\":\"pg-cluster\",\"reconcileID\":\"f6812cba-7a90-41fe-a115-01cc8edac2d5\",\"logging_pod\":\"pg-cluster-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n```\r\n**Update:** I managed to get the Cluster back into an healthy state. While the CRD Events were indicating a communication issue (saying somtehing like nodes not reporting healthy status). Turns out the issue was actually corrupted storage. After I deleted the PVC's before deleting the pods, new cluster instances werde created which after a short while showed up as healthy in CNPG's Grafana Dashboard.\n---\nDo we have a solution for this? \r\nWhat about `reclaimPolicy` of the underlying storage class? When I look at the PV's I can see that the reclaim policy is set to `Delete`, would it make sense to use a strorage class with `reclaimPolicy` set to  `retain`? For the Pod restarting on another node it would have the same effect as if the server has restarted and it would continue where it left of.\n---\nWe are facing the same issue also with version 1.24.0. We have faced this now multiple times, which is super annoying since our database is of 1TB size. Destroying the replica is therefore not very attractive to us. Unfortunately I do not see any other solution currently.\n---\nHad this happen to us today with CNPG 1.24 running PG17.\nHetzner had scheduled network downtime for about 30 minutes and this somehow caused our replica to end up in this broken state, failing to `pg_rewind`.\nHow do people get out of this situation once it's happened?\nWe don't seem to be able to get rid of our broken replica. We tried scaling down number of replicas from 2 to 1 but the operator fails to make progress since it seems to want to get the status of the replica but can't connect to it at all.\nOur assumption was that scaling down to 1 would just kill the replica and leave the primary as is but that doesn't seem to be the case.\n---\nI found that deleting both the problematic PVC and Pod seemed to \u201cfix\u201d it until the next occurrence, not a solution just temporary band-aid (albeit an annoying one)\n---\nJust thought I'd clarify that it turned out that our specific issue here was the bug in Postgres 17.1 which could cause WAL files needed by `pg_rewind` to get deleted. It's mentioned as fixed on the 17.2 release: https://www.postgresql.org/about/news/postgresql-172-166-1510-1415-1318-and-1222-released-2965"
    },
    {
        "title": "[Feature]: Add a job in GitHub Action to check requirements for subsequent jobs",
        "id": 2070937610,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nSome of our github action jobs will require specific secrets to be set in the repository and when these secrets are not set the jobs will fail. For instance, the Openshift e2e tests need to have certain aws secrets set. \n### Describe the solution you'd like\nIt would be more clear if we had a job dedicated to explicitly checking these conditions that the next job could \"need\".\n### Describe alternatives you've considered\nN/A\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nSome of our github action jobs will require specific secrets to be set in the repository and when these secrets are not set the jobs will fail. For instance, the Openshift e2e tests need to have certain aws secrets set. \n### Describe the solution you'd like\nIt would be more clear if we had a job dedicated to explicitly checking these conditions that the next job could \"need\".\n### Describe alternatives you've considered\nN/A\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Make /tmp folder writable",
        "id": 2070276081,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nSome of my stored procedures are running `plpython3u` code, which creates temporary files. Because `/tmp` is not writable, those are currently created in the `pgdata` directory (the current working directory).\r\n### Describe the solution you'd like\nMake `/tmp` writable. A read-only temporary folder is really... useless.\n### Describe alternatives you've considered\nI found #681, which was turned into a discussion. However, I think this should not need to be worked around.\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nSome of my stored procedures are running `plpython3u` code, which creates temporary files. Because `/tmp` is not writable, those are currently created in the `pgdata` directory (the current working directory).\r\n### Describe the solution you'd like\nMake `/tmp` writable. A read-only temporary folder is really... useless.\n### Describe alternatives you've considered\nI found #681, which was turned into a discussion. However, I think this should not need to be worked around.\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductThe latest testing container, at [ghcr.io/cloudnative-pg/postgresql-testing:16.2-1-bookworm](https://github.com/cloudnative-pg/postgres-containers/pkgs/container/postgresql-testing/214253827?tag=16.2-1-bookworm) for example, appears to have a writable `/tmp` directory.\n---\nI don't think this is about the container image, actually. I am using a custom image and have a writable `/tmp` folder, too. I think it's the way how the pod is created?\n---\nI see. That may well be the case. The remainder of the filesystem outside `/tmp` isn't writable by the postgres user in the container while `/tmp` is, but the pod configuration may be created in such a way that `/tmp` also becomes read-only.\nfwiw I was investigating with regard to the discussion in https://github.com/cloudnative-pg/cloudnative-pg/discussions/4510. I guess I should find the permalink to the template the operator ultimately uses to create pods or the parent/associated resources to check."
    },
    {
        "title": "[Bug]: Problems with local volumes and anti-affinity requiredDuringSchedulingIgnoredDuringExecution",
        "id": 2068834544,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nalex@rusa.at\r\n### Version\r\n1.22.0\r\n### What version of Kubernetes are you using?\r\n1.27\r\n### What is your Kubernetes environment?\r\nOther\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nI changed the anti-affinity to requiredDuringSchedulingIgnoredDuringExecution, because the cluster members where all scheduled on the same node with the default preferredDuringScheduling...\r\nFor Storage I'm using local storage with [openebs/lvm-localpv](https://github.com/openebs/lvm-localpv).\r\nNow one new \"join\" pod got stuck in pending status, because the volume for the pvc was created on a node that already had a different cluster member running. (It would be nice if the anti-affinity was also possible on the pvc, but that's a different topic I will look into afterwards.)\r\nEDIT: The problem, that the volume gets created on the wrong node, should be solved by setting `volumeBindingMode: WaitForFirstConsumer` on the StorageClass.\r\nWhen I saw that status, I deleted the \"join\" pod and the pvc, as I also did it before with pods that I wanted to failover to a different node (`k delete -n test pvc/test-pg-6 pod/test-pg-6-join-dfmvt`).\r\n**After deleting the join-pod, the operator immediatly created a new pod `test-pg-6-join-XXXXX`, but it didn't recreate the pvc! It seems the PVC only gets recreated, when deleting a running cluster member, but not in the state where the member wasn't yet joined to the cluster, because the join-node was deleted.**\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: test-pg\r\n  namespace: test\r\n  labels:\r\n    workload: database\r\n    app: test\r\nspec:\r\n  instances: 3\r\n  inheritedMetadata:\r\n    labels:\r\n      workload: database\r\n      app: test\r\n  primaryUpdateStrategy: unsupervised\r\n  affinity:\r\n    enablePodAntiAffinity: true #default value\r\n    topologyKey: kubernetes.io/hostname #defaul value\r\n    podAntiAffinityType: required\r\n  storage:\r\n    size: 5Gi\r\n    storageClass: openebs-lvmpv\r\n```\r\n### Relevant log output\r\n```shell\r\ndefault-scheduler  0/8 nodes are available: persistentvolumeclaim \"test-pg-6\" not found...\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nalex@rusa.at\r\n### Version\r\n1.22.0\r\n### What version of Kubernetes are you using?\r\n1.27\r\n### What is your Kubernetes environment?\r\nOther\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nI changed the anti-affinity to requiredDuringSchedulingIgnoredDuringExecution, because the cluster members where all scheduled on the same node with the default preferredDuringScheduling...\r\nFor Storage I'm using local storage with [openebs/lvm-localpv](https://github.com/openebs/lvm-localpv).\r\nNow one new \"join\" pod got stuck in pending status, because the volume for the pvc was created on a node that already had a different cluster member running. (It would be nice if the anti-affinity was also possible on the pvc, but that's a different topic I will look into afterwards.)\r\nEDIT: The problem, that the volume gets created on the wrong node, should be solved by setting `volumeBindingMode: WaitForFirstConsumer` on the StorageClass.\r\nWhen I saw that status, I deleted the \"join\" pod and the pvc, as I also did it before with pods that I wanted to failover to a different node (`k delete -n test pvc/test-pg-6 pod/test-pg-6-join-dfmvt`).\r\n**After deleting the join-pod, the operator immediatly created a new pod `test-pg-6-join-XXXXX`, but it didn't recreate the pvc! It seems the PVC only gets recreated, when deleting a running cluster member, but not in the state where the member wasn't yet joined to the cluster, because the join-node was deleted.**\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: test-pg\r\n  namespace: test\r\n  labels:\r\n    workload: database\r\n    app: test\r\nspec:\r\n  instances: 3\r\n  inheritedMetadata:\r\n    labels:\r\n      workload: database\r\n      app: test\r\n  primaryUpdateStrategy: unsupervised\r\n  affinity:\r\n    enablePodAntiAffinity: true #default value\r\n    topologyKey: kubernetes.io/hostname #defaul value\r\n    podAntiAffinityType: required\r\n  storage:\r\n    size: 5Gi\r\n    storageClass: openebs-lvmpv\r\n```\r\n### Relevant log output\r\n```shell\r\ndefault-scheduler  0/8 nodes are available: persistentvolumeclaim \"test-pg-6\" not found...\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductI\u2019m also facing the same issue. Have you found any solutions or workarounds?"
    },
    {
        "title": "[Feature]:  Add the possibility to create custom PVC",
        "id": 2067427017,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nMount a shared storage into the pod where I can store created backup files that will be used by the developers \n### Describe the solution you'd like\nBe able to add additional volume by specifying the storageclass, PVC size and where I need it mounted.\n### Describe alternatives you've considered\nNone.\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nMount a shared storage into the pod where I can store created backup files that will be used by the developers \n### Describe the solution you'd like\nBe able to add additional volume by specifying the storageclass, PVC size and where I need it mounted.\n### Describe alternatives you've considered\nNone.\n### Additional context\n_No response_\n### Backport?\nN/A\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conductmy +1\n---\nI would contribute to the feature but I don't have the required skills for it.\n---\n+1 would like to attach nfs per volume or hostpath for backups"
    },
    {
        "title": "[Feature]: Allow customization of --read-timeout for barman-cloud-backup process",
        "id": 2065437451,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nThe current CloudNativePG Operator lacks the ability to customize the --read-timeout configuration option for the barman-cloud-backup process. This becomes problematic when backups encounter timeouts, as the process continues to display the status as \"Running\" instead of transitioning to a failed state. An example of this issue is evident in the following error message:\r\n```bash\r\nvmaas-vri-3 postgres {\"level\":\"info\",\"ts\":\"2023-12-11T13:47:07Z\",\"logger\":\"barman-cloud-backup\",\"msg\":\"2023-12-11 13:47:07,066 [218919] ERROR: Upload error: Connection was closed before we received a valid response from endpoint URL: \\\"http://s3.sdm.xxx.nl:9000/postgres/prod/vri/vmaas-vri/base/20231209T000002/data_0084.tar?uploadId=c9082674-b2c0-4993-838c-b9608e5c7376&partNumber=3046\\\". (\r\nworker 0)\",\"pipe\":\"stderr\",\"logging_pod\":\"vmaas-vri-3\"}\r\n```\r\nThis error illustrates a scenario where the barman-cloud-backup process encounters a timeout during the upload phase, yet the operator fails to accurately reflect the failed state, persistently displaying the status as \"Running.\" The addition of the proposed feature will address this issue by allowing users to customize the --read-timeout parameter, providing flexibility in handling backup scenarios.\r\n### Describe the solution you'd like\r\nIntroduce a configuration option or an environment variable in the CloudNativePG Operator that enables users to specify the --read-timeout value for the barman-cloud-backup process. This enhancement will empower users to customize timeout settings based on their specific use cases, allowing for a more adaptable and resilient backup process. Furthermore, upon encountering a timeout, the operator should accurately transition the corresponding backup status from \"Running\" to a \"Failed\" state, preventing the backup process from remaining non-functional.\r\n### Describe alternatives you've considered\r\nAn alternative could involve manually adjusting the --read-timeout within barman-cloud-backup, but this lacks user-friendly flexibility and might not align with the operator's overall strategy. The proposed solution, integrating a configurable option within the operator, provides a simpler and more adaptable approach.\r\n### Additional context\r\nThis enhancement request aims to simplify the CloudNativePG Operator's functionality by allowing users to set timeout preferences for barman-cloud-backup. This not only provides customization but also ensures accurate status updates, preventing backups from getting stuck in a non-functional state.\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nThe current CloudNativePG Operator lacks the ability to customize the --read-timeout configuration option for the barman-cloud-backup process. This becomes problematic when backups encounter timeouts, as the process continues to display the status as \"Running\" instead of transitioning to a failed state. An example of this issue is evident in the following error message:\r\n```bash\r\nvmaas-vri-3 postgres {\"level\":\"info\",\"ts\":\"2023-12-11T13:47:07Z\",\"logger\":\"barman-cloud-backup\",\"msg\":\"2023-12-11 13:47:07,066 [218919] ERROR: Upload error: Connection was closed before we received a valid response from endpoint URL: \\\"http://s3.sdm.xxx.nl:9000/postgres/prod/vri/vmaas-vri/base/20231209T000002/data_0084.tar?uploadId=c9082674-b2c0-4993-838c-b9608e5c7376&partNumber=3046\\\". (\r\nworker 0)\",\"pipe\":\"stderr\",\"logging_pod\":\"vmaas-vri-3\"}\r\n```\r\nThis error illustrates a scenario where the barman-cloud-backup process encounters a timeout during the upload phase, yet the operator fails to accurately reflect the failed state, persistently displaying the status as \"Running.\" The addition of the proposed feature will address this issue by allowing users to customize the --read-timeout parameter, providing flexibility in handling backup scenarios.\r\n### Describe the solution you'd like\r\nIntroduce a configuration option or an environment variable in the CloudNativePG Operator that enables users to specify the --read-timeout value for the barman-cloud-backup process. This enhancement will empower users to customize timeout settings based on their specific use cases, allowing for a more adaptable and resilient backup process. Furthermore, upon encountering a timeout, the operator should accurately transition the corresponding backup status from \"Running\" to a \"Failed\" state, preventing the backup process from remaining non-functional.\r\n### Describe alternatives you've considered\r\nAn alternative could involve manually adjusting the --read-timeout within barman-cloud-backup, but this lacks user-friendly flexibility and might not align with the operator's overall strategy. The proposed solution, integrating a configurable option within the operator, provides a simpler and more adaptable approach.\r\n### Additional context\r\nThis enhancement request aims to simplify the CloudNativePG Operator's functionality by allowing users to set timeout preferences for barman-cloud-backup. This not only provides customization but also ensures accurate status updates, preventing backups from getting stuck in a non-functional state.\r\n### Backport?\r\nYes\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductDoes a workaround exist for this? \r\nOur 400Gb cluster can't be backed up because this error happens in 95% of tries. Storage is based on Minio deployed over a network file system (~100MiB upload rate). Unfortunately, backup errors block operator adoption at all. No such problems with wal-e and similar backup systems on the same storage."
    },
    {
        "title": "Operations: plugin command to create cluster from PV",
        "id": 2065349414,
        "state": "open",
        "first": "We have seen in many cases a cluster with a PVC whose corresponding pod is in CrashLoop due to timeline conflict or some other reason.\nDeleting the PVC without knowing the contents of the database in it is risky.\nIt would be convenient to:\n1. clone the PVC\n2. create a (single instance) cluster from the cloned PVC\n3. examine the database once the instance is active\nA plugin like this would be helpful\n``` sh\nkubectl cnpg cluster-from-pv <cloned-pv>\n```",
        "messages": "We have seen in many cases a cluster with a PVC whose corresponding pod is in CrashLoop due to timeline conflict or some other reason.\nDeleting the PVC without knowing the contents of the database in it is risky.\nIt would be convenient to:\n1. clone the PVC\n2. create a (single instance) cluster from the cloned PVC\n3. examine the database once the instance is active\nA plugin like this would be helpful\n``` sh\nkubectl cnpg cluster-from-pv <cloned-pv>\n```Should this be a plug-in or a new option in the `spec.bootstrap`?\r\nThe new cluster could either use the detached PV as a source for a fresh import or try to attach the pod to PV and resume normally with it (more risky).\n---\nspec.bootstrap would be very nice.\n---\nWe have clusters we are trying to migrate from CrunchyData to CNPG with 1.5TB of data.\r\nWe really need a way to reuse an existing set of PVs rather than using pgdump or pg_basebackup.\r\nEven pg_basebackup is too slow as it takes hours to copy the lead and rebuild the replicas.\n---\n> We have clusters we are trying to migrate from CrunchyData to CNPG with 1.5TB of data. We really need a way to reuse an existing set of PVs rather than using pgdump or pg_basebackup. Even pg_basebackup is too slow as it takes hours to copy the lead and rebuild the replicas.\nThis is not a use case we are looking into at the moment (migrating from Crunchy PVs to CNPG PVs). You can migrate using logical replication, without downtime. Version 1.25 introduces logical subscriptions. Alternatively, you can already use this: https://www.gabrielebartolini.it/articles/2024/03/cloudnativepg-recipe-5-how-to-migrate-your-postgresql-database-in-kubernetes-with-~0-downtime-from-anywhere/\n---\n@dperetti This entire area is subject to a huge refactoring of the code, that will give birth to CNPG-I, a pluggable interface.\nWe are also planning to revisit the entire bootstrap/cloning strategy, but we defer any analysis to when we have released 1.25.0 and the first plugins."
    },
    {
        "title": "[Feature]: Add metric `cnpg_collector_last_available_backup_age`",
        "id": 2064164006,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCloudNativePG instances currently return a metric `cnpg_collector_last_available_backup_timestamp` that returns the timestamp of the last backup.\r\nHowever, some monitoring solutions like Datadog do not handle well a metric returning a timestamp: For example, it is not possible in Datadog query language to easily formulate something like \"compute the value of this metric minus the current timestamp\".\r\nThis make using this metric for alerts such as \"no backup has been taken in the last x hours\" more difficult when using these solutions compared to solutions like Prometheus.\n### Describe the solution you'd like\nAdd a new metric, `cnpg_collector_last_available_backup_age` that would return the number of seconds elapsed since the last backup. \r\nThis metric's value would be easier to handle with the query language of some monitoring solutions.\r\nThe same could be done for `cnpg_collector_last_failed_backup_timestamp`\n### Describe alternatives you've considered\nIt is possible to obtain something similar on Datadog's side by having a metric return the current timestamp, and substract the value of `cnpg_collector_last_available_backup_timestamp`.\r\nIt is also possible to do log based alerting (that triggers when a log has not been seen since x hours), but when using Snapshot Volume backups, I have not found such log in the operator or CNPG instance logs\r\nLastly it is also possible to write more convoluted queries in Datadog metrics query language to get a similar results (\"The value of the metric has not changed in the last x hours\") but by experience it is error-prone and unreliable.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nCloudNativePG instances currently return a metric `cnpg_collector_last_available_backup_timestamp` that returns the timestamp of the last backup.\r\nHowever, some monitoring solutions like Datadog do not handle well a metric returning a timestamp: For example, it is not possible in Datadog query language to easily formulate something like \"compute the value of this metric minus the current timestamp\".\r\nThis make using this metric for alerts such as \"no backup has been taken in the last x hours\" more difficult when using these solutions compared to solutions like Prometheus.\n### Describe the solution you'd like\nAdd a new metric, `cnpg_collector_last_available_backup_age` that would return the number of seconds elapsed since the last backup. \r\nThis metric's value would be easier to handle with the query language of some monitoring solutions.\r\nThe same could be done for `cnpg_collector_last_failed_backup_timestamp`\n### Describe alternatives you've considered\nIt is possible to obtain something similar on Datadog's side by having a metric return the current timestamp, and substract the value of `cnpg_collector_last_available_backup_timestamp`.\r\nIt is also possible to do log based alerting (that triggers when a log has not been seen since x hours), but when using Snapshot Volume backups, I have not found such log in the operator or CNPG instance logs\r\nLastly it is also possible to write more convoluted queries in Datadog metrics query language to get a similar results (\"The value of the metric has not changed in the last x hours\") but by experience it is error-prone and unreliable.\n### Additional context\n_No response_\n### Backport?\nYes\n### Are you willing to actively contribute to this feature?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Docs]: More detailed documentation on retention policy not being enforced when using volumeSnapshots and its consequences",
        "id": 2062767669,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nI recently switched to VolumeSnapshot backups and noticed that some of the WALs archived in the bucket were older than the retention policy I set.\r\nI believe the cause is the same as [this issue](https://github.com/cloudnative-pg/cloudnative-pg/issues/2429#issuecomment-1634350335), as Barman apparently enforce the retention policy only doing a backup: Since I am not doing object-store backups, barman will never enforce this policy and old WAL archives will not be deleted.\r\n[The CloudNativePG API documentation regarding retention policy](https://cloudnative-pg.io/documentation/1.22/cloudnative-pg.v1/#postgresql-cnpg-io-v1-BackupConfiguration) points toward that direction:\r\n> [retention policy is] currently only applicable when using the BarmanObjectStore method.\r\nSo while this behavior is documented in the API, I feel like it should be more visible and better highlight the consequences (needing to implement your own garbage collection mechanism for old snapshots and wals, not having a recovery window retention policy handled by barman). \n### Describe what additions need to be done to the documentation\nCNPG documentation could benefit from a paragraph \"Retention Policies\" in the \"Backup on volume snapshots\" section, just like the \"Backup on object stores\", that would indicate that retention policies are currently ignored for volume snapshots and WAL archiving, making it necessary to implement your own garbage collection mechanisms (for example set retention policies on the bucket storing the WAL).\n### Describe what pages need to change in the documentation, if any\nAdding a paragraph to: https://cloudnative-pg.io/documentation/1.22/backup_volumesnapshot/\r\nAnd eventually a disclaimer in: https://cloudnative-pg.io/documentation/1.22/wal_archiving/ stating that retention policies will not be enforced when relying on Volume Snapshot backups\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\nI can make a proposal of the documentation changes if you want\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\n### What problem in the existing documentation this issue aims to solve?\nI recently switched to VolumeSnapshot backups and noticed that some of the WALs archived in the bucket were older than the retention policy I set.\r\nI believe the cause is the same as [this issue](https://github.com/cloudnative-pg/cloudnative-pg/issues/2429#issuecomment-1634350335), as Barman apparently enforce the retention policy only doing a backup: Since I am not doing object-store backups, barman will never enforce this policy and old WAL archives will not be deleted.\r\n[The CloudNativePG API documentation regarding retention policy](https://cloudnative-pg.io/documentation/1.22/cloudnative-pg.v1/#postgresql-cnpg-io-v1-BackupConfiguration) points toward that direction:\r\n> [retention policy is] currently only applicable when using the BarmanObjectStore method.\r\nSo while this behavior is documented in the API, I feel like it should be more visible and better highlight the consequences (needing to implement your own garbage collection mechanism for old snapshots and wals, not having a recovery window retention policy handled by barman). \n### Describe what additions need to be done to the documentation\nCNPG documentation could benefit from a paragraph \"Retention Policies\" in the \"Backup on volume snapshots\" section, just like the \"Backup on object stores\", that would indicate that retention policies are currently ignored for volume snapshots and WAL archiving, making it necessary to implement your own garbage collection mechanisms (for example set retention policies on the bucket storing the WAL).\n### Describe what pages need to change in the documentation, if any\nAdding a paragraph to: https://cloudnative-pg.io/documentation/1.22/backup_volumesnapshot/\r\nAnd eventually a disclaimer in: https://cloudnative-pg.io/documentation/1.22/wal_archiving/ stating that retention policies will not be enforced when relying on Volume Snapshot backups\n### Describe what pages need to be removed from the documentation, if any\n_No response_\n### Additional context\nI can make a proposal of the documentation changes if you want\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "feat(plugin): add options for database user an password in `pgbench` command",
        "id": 2062592463,
        "state": "open",
        "first": "We want pgbench to run against a different DB/user. DB is already configurable. This MR allows to configure the user and password pgbench will use.",
        "messages": "We want pgbench to run against a different DB/user. DB is already configurable. This MR allows to configure the user and password pgbench will use.@mnencia we should put this in our work queue\n---\n@mnencia is there a way for us to use PGPASSFILE instead of PGPASSWORD when specifying a password?"
    },
    {
        "title": "Follow current pod security standards for pgbench job",
        "id": 2062589076,
        "state": "open",
        "first": "We run our cluster with restricted [pod security standards](https://kubernetes.io/docs/concepts/security/pod-security-standards/) making the pgbench job fail due to missing security context configuration and similar. I feel they're a good idea for any setup, thus this MR implements them by default.",
        "messages": "We run our cluster with restricted [pod security standards](https://kubernetes.io/docs/concepts/security/pod-security-standards/) making the pgbench job fail due to missing security context configuration and similar. I feel they're a good idea for any setup, thus this MR implements them by default."
    },
    {
        "title": "[Bug]: Pod completed, not restarting",
        "id": 2057783674,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nfrickx.yentl@gmail.com\n### Version\n1.22.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nI noticed lately that after a while the cnpg pod stopped running (I used to only have 1 pod running). The pod status in Kubernetes was 'completed' and status code is 0, so looks like a 'good' shutdown, but I didn't ask for a shutdown. I then added an extra replica to not have downtime, now the same happened but without downtime, just 1 pod is not running at the moment. The cluster resource indicates that not all instances are healthy, but the operator is not fixing it it seems.\r\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  creationTimestamp: \"2023-12-14T19:30:39Z\"\r\n  generation: 4\r\n  labels:\r\n    kustomize.toolkit.fluxcd.io/name: infra\r\n    kustomize.toolkit.fluxcd.io/namespace: flux-system\r\n  name: cnpg\r\n  namespace: postgres\r\n  resourceVersion: \"72883685\"\r\n  uid: d0ae48a4-53c3-4bb3-9740-d93ab7716c07\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  description: Postgresql cluster\r\n  enableSuperuserAccess: true\r\n  externalClusters:\r\n  - connectionParameters:\r\n      dbname: postgres\r\n      host: postgres.database.svc.cluster.local\r\n      user: postgres\r\n    name: standalone-pg\r\n    password:\r\n      key: password\r\n      name: standalone-postgres\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/ahinko/postgres-extended:16.1.4\r\n  instances: 2\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_connections: \"100\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      shared_buffers: 128MB\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    shared_preload_libraries:\r\n    - vectors.so\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      memory: 500Mi\r\n    requests:\r\n      cpu: 100m\r\n      memory: 500Mi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 10Gi\r\n    storageClass: freenas-api-nfs\r\n  superuserSecret:\r\n    name: cloudnative-pg-secret\r\n  switchoverDelay: 3600\r\nstatus:\r\n  certificates:\r\n    clientCASecret: cnpg-ca\r\n    expirations:\r\n      cnpg-ca: 2024-03-13 19:25:39 +0000 UTC\r\n      cnpg-replication: 2024-03-13 19:25:39 +0000 UTC\r\n      cnpg-server: 2024-03-13 19:25:39 +0000 UTC\r\n    replicationTLSSecret: cnpg-replication\r\n    serverAltDNSNames:\r\n    - cnpg-rw\r\n    - cnpg-rw.postgres\r\n    - cnpg-rw.postgres.svc\r\n    - cnpg-r\r\n    - cnpg-r.postgres\r\n    - cnpg-r.postgres.svc\r\n    - cnpg-ro\r\n    - cnpg-ro.postgres\r\n    - cnpg-ro.postgres.svc\r\n    serverCASecret: cnpg-ca\r\n    serverTLSSecret: cnpg-server\r\n  cloudNativePGCommitHash: 86b9dc80\r\n  cloudNativePGOperatorHash: 411a67468c5e1357d387221ce5b11d906bd33d62324846a8c776d206b6c94162\r\n  conditions:\r\n  - lastTransitionTime: \"2023-12-26T13:38:15Z\"\r\n    message: Cluster Is Not Ready\r\n    reason: ClusterIsNotReady\r\n    status: \"False\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2023-12-15T19:28:58Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"68421441\"\r\n  currentPrimary: cnpg-2\r\n  currentPrimaryTimestamp: \"2023-12-26T13:38:49.499078Z\"\r\n  healthyPVC:\r\n  - cnpg-1\r\n  - cnpg-2\r\n  instanceNames:\r\n  - cnpg-1\r\n  - cnpg-2\r\n  instances: 2\r\n  instancesReportedState:\r\n    cnpg-2:\r\n      isPrimary: true\r\n      timeLineID: 2\r\n  instancesStatus:\r\n    failed:\r\n    - cnpg-1\r\n    healthy:\r\n    - cnpg-2\r\n  latestGeneratedNode: 2\r\n  managedRolesStatus: {}\r\n  phase: Waiting for the instances to become active\r\n  phaseReason: Some instances are not yet active. Please wait.\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 2\r\n  readService: cnpg-r\r\n  readyInstances: 1\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"71057067\"\r\n    clientCaSecretVersion: \"68421414\"\r\n    externalClusterSecretVersion:\r\n      standalone-postgres: \"68413748\"\r\n    replicationSecretVersion: \"68421417\"\r\n    serverCaSecretVersion: \"68421414\"\r\n    serverSecretVersion: \"68421415\"\r\n    superuserSecretVersion: \"68413740\"\r\n  targetPrimary: cnpg-2\r\n  targetPrimaryTimestamp: \"2023-12-26T13:38:15.220438Z\"\r\n  timelineID: 2\r\n  topology:\r\n    instances:\r\n      cnpg-1: {}\r\n      cnpg-2: {}\r\n    nodesUsed: 2\r\n    successfullyExtracted: true\r\n  writeService: cnpg-rw\n```\n### Relevant log output\n```shell\nlogs from the pod itself:\r\n2023-12-26T13:32:08.168286371Z info record\r\n2023-12-26T13:32:08.358375199Z info record\r\n2023-12-26T13:34:55.627167824Z info Backup not configured, skip WAL archiving\r\n2023-12-26T13:35:11.539852574Z info Received termination signal\r\n2023-12-26T13:35:11.539927095Z info Requesting smart shutdown of the PostgreSQL instance\r\n2023-12-26T13:35:11.545646195Z info pg_ctl: server is running (PID: 25)\r\n/usr/lib/postgresql/16/bin/postgres \"-D\" \"/var/lib/postgresql/data/pgdata\"\r\n2023-12-26T13:35:11.545742387Z info Shutting down instance\r\n2023-12-26T13:35:11.54808071Z info record\r\n2023-12-26T13:35:38.986647291Z info record\r\n2023-12-26T13:36:08.988512708Z info record\r\n2023-12-26T13:36:38.98724377Z info record\r\n2023-12-26T13:37:08.646127012Z info record\r\n2023-12-26T13:37:08.82545931Z info record\r\n2023-12-26T13:37:09.225363172Z info record\r\n2023-12-26T13:37:38.992231841Z info record\r\n2023-12-26T13:38:08.987132094Z info record\r\n2023-12-26T13:38:13.034153224Z info waiting for server to shut down....................................................................................................................................................................................... failed\r\n2023-12-26T13:38:13.034198059Z info pg_ctl: server does not shut down\r\n2023-12-26T13:38:13.034202868Z info HINT: The \"-m fast\" option immediately disconnects sessions rather than\r\n2023-12-26T13:38:13.034205773Z info waiting for session-initiated disconnection.\r\n2023-12-26T13:38:13.034288941Z info Error while handling the smart shutdown request\r\n2023-12-26T13:38:13.03429904Z info Requesting fast shutdown of the PostgreSQL instance\r\n2023-12-26T13:38:13.036935347Z info pg_ctl: server is running (PID: 25)\r\n/usr/lib/postgresql/16/bin/postgres \"-D\" \"/var/lib/postgresql/data/pgdata\"\r\n2023-12-26T13:38:13.03697339Z info Shutting down instance\r\n2023-12-26T13:38:13.038815174Z info record\r\n2023-12-26T13:38:13.053422869Z info record\r\n2023-12-26T13:38:13.058017942Z info record\r\n2023-12-26T13:38:13.058079198Z info record\r\n2023-12-26T13:38:13.058105397Z info record\r\n2023-12-26T13:38:13.05811724Z info record\r\n2023-12-26T13:38:13.058125927Z info record\r\n2023-12-26T13:38:13.058237167Z info record\r\n2023-12-26T13:38:13.066026984Z info record\r\n2023-12-26T13:38:13.164462207Z info record\r\n2023-12-26T13:38:13.187776434Z info Backup not configured, skip WAL archiving\r\n2023-12-26T13:38:13.408698121Z info record\r\n2023-12-26T13:38:13.528088538Z info record\r\n2023-12-26T13:38:13.533385188Z info Extracting pg_controldata information\r\n2023-12-26T13:38:13.536842569Z info pg_control version number:            1300\r\nCatalog version number:               202307071\r\nDatabase system identifier:           7312535110852399128\r\nDatabase cluster state:               shut down\r\npg_control last modified:             Tue 26 Dec 2023 01:38:13 PM UTC\r\nLatest checkpoint location:           C/1E000028\r\nLatest checkpoint's REDO location:    C/1E000028\r\nLatest checkpoint's REDO WAL file:    000000010000000C0000001E\r\nLatest checkpoint's TimeLineID:       1\r\nLatest checkpoint's PrevTimeLineID:   1\r\nLatest checkpoint's full_page_writes: on\r\nLatest checkpoint's NextXID:          0:573136\r\nLatest checkpoint's NextOID:          53548\r\nLatest checkpoint's NextMultiXactId:  1\r\nLatest checkpoint's NextMultiOffset:  0\r\nLatest checkpoint's oldestXID:        722\r\nLatest checkpoint's oldestXID's DB:   1\r\nLatest checkpoint's oldestActiveXID:  0\r\nLatest checkpoint's oldestMultiXid:   1\r\nLatest checkpoint's oldestMulti's DB: 1\r\nLatest checkpoint's oldestCommitTsXid:0\r\nLatest checkpoint's newestCommitTsXid:0\r\nTime of latest checkpoint:            Tue 26 Dec 2023 01:38:13 PM UTC\r\nFake LSN counter for unlogged rels:   0/3E8\r\nMinimum recovery ending location:     0/0\r\nMin recovery ending loc's timeline:   0\r\nBackup start location:                0/0\r\nBackup end location:                  0/0\r\nEnd-of-backup record required:        no\r\nwal_level setting:                    logical\r\nwal_log_hints setting:                on\r\nmax_connections setting:              100\r\nmax_worker_processes setting:         32\r\nmax_wal_senders setting:              10\r\nmax_prepared_xacts setting:           0\r\nmax_locks_per_xact setting:           64\r\ntrack_commit_timestamp setting:       off\r\nMaximum data alignment:               8\r\nDatabase block size:                  8192\r\nBlocks per segment of large relation: 131072\r\nWAL block size:                       8192\r\nBytes per WAL segment:                16777216\r\nMaximum length of identifiers:        64\r\nMaximum columns in an index:          32\r\nMaximum size of a TOAST chunk:        1996\r\nSize of a large-object chunk:         2048\r\nDate/time type storage:               64-bit integers\r\nFloat8 argument passing:              by value\r\nData page checksum version:           0\r\nMock authentication nonce:            c2ed9930642aefd266f0e7e20e8039e0f76c8cccc0fbc83050c657b4b707962a\r\n2023-12-26T13:38:13.543046015Z info waiting for server to shut down.... done\r\n2023-12-26T13:38:13.543100508Z info server stopped\r\n2023-12-26T13:38:13.54313358Z info PostgreSQL instance shut down\r\n2023-12-26T13:38:13.54751016Z info Stopping and waiting for non leader election runnables\r\n2023-12-26T13:38:13.547572288Z info Stopping and waiting for leader election runnables\r\n2023-12-26T13:38:13.547580383Z info Shutdown signal received, waiting for all workers to finish\r\n2023-12-26T13:38:13.547583058Z info Shutdown signal received, waiting for all workers to finish\r\n2023-12-26T13:38:13.547585603Z info Shutdown signal received, waiting for all workers to finish\r\n2023-12-26T13:38:13.54758919Z info Exited log pipe\r\n2023-12-26T13:38:13.547592145Z info Terminated slot Replicator loop\r\n2023-12-26T13:38:13.54759489Z info Terminated RoleSynchronizer loop\r\n2023-12-26T13:38:13.547611763Z info Webserver exited\r\n2023-12-26T13:38:13.547622834Z info Webserver exited\r\n2023-12-26T13:38:13.547634315Z info Exited log pipe\r\n2023-12-26T13:38:13.547637221Z info Exited log pipe\r\n2023-12-26T13:38:13.547867026Z info Webserver exited\r\n2023-12-26T13:38:13.548187501Z info All workers finished\r\n2023-12-26T13:38:13.548230423Z info All workers finished\r\n2023-12-26T13:38:13.548669604Z info All workers finished\r\n2023-12-26T13:38:13.548715601Z info Stopping and waiting for caches\r\n2023-12-26T13:38:13.55076604Z info Stopping and waiting for webhooks\r\n2023-12-26T13:38:13.552333725Z info Stopping and waiting for HTTP servers\r\n2023-12-26T13:38:13.552381416Z info Wait completed, proceeding to shutdown the manager\r\nlogs from the operator:\r\n2023-12-26T13:30:07.838293432Z info default\r\n2023-12-26T13:30:07.847468306Z info validate update\r\n2023-12-26T13:31:11.254806256Z info default\r\n2023-12-26T13:31:11.261846872Z info validate update\r\n2023-12-26T13:32:12.418550271Z info default\r\n2023-12-26T13:32:12.425935332Z info validate update\r\n2023-12-26T13:33:10.907680708Z info default\r\n2023-12-26T13:33:10.919520212Z info validate update\r\n2023-12-26T13:34:45.240736085Z info default\r\n2023-12-26T13:34:45.252597314Z info validate update\r\n2023-12-26T13:35:43.224001927Z info default\r\n2023-12-26T13:35:43.231520447Z info validate update\r\n2023-12-26T13:36:42.425705756Z info default\r\n2023-12-26T13:36:42.433100347Z info validate update\r\n2023-12-26T13:37:40.770825377Z info default\r\n2023-12-26T13:37:40.777855185Z info validate update\r\n2023-12-26T13:38:15.171641658Z info Current primary isn't healthy, initiating a failover\r\n2023-12-26T13:38:15.171714025Z info pod status (1 of 1)\r\n2023-12-26T13:38:15.204667344Z info Failing over\r\n2023-12-26T13:38:15.204742596Z info pod status (1 of 1)\r\n2023-12-26T13:38:15.233253201Z info Waiting for the new primary to notice the promotion request\r\n2023-12-26T13:38:15.274005561Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:16.264252081Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:17.299244539Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:18.33139479Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:19.361146899Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:20.396244015Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:21.426219384Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:22.460770233Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:23.494688316Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:24.553090988Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:25.584818785Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:26.614168748Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:27.642852843Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:28.718719059Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:29.752370149Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:30.793116385Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:31.824270249Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:32.85809986Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:33.900796557Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:34.92994414Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:35.964291877Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:36.9939087Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:38.046868439Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:39.077943935Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:40.109038863Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:40.729802786Z info default\r\n2023-12-26T13:38:40.735467103Z info validate update\r\n2023-12-26T13:38:41.141223723Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:42.172292681Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:43.206941623Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:44.258830828Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:45.290743261Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:46.323961403Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:47.360967831Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:48.400779833Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:49.43493106Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:49.687248775Z info Setting primary label\r\n2023-12-26T13:39:39.312920389Z info default\r\n2023-12-26T13:39:39.323649384Z info validate update\r\n2023-12-26T13:40:40.220572944Z info default\r\n2023-12-26T13:40:40.227451587Z info validate update\r\n2023-12-26T13:42:12.650774215Z info default\r\n2023-12-26T13:42:12.666418368Z info validate update\r\n2023-12-26T13:43:12.352619375Z info default\r\n2023-12-26T13:43:12.358606887Z info validate update\r\n2023-12-26T13:44:13.73195222Z info default\r\n2023-12-26T13:44:13.741409825Z info validate update\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nfrickx.yentl@gmail.com\n### Version\n1.22.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nI noticed lately that after a while the cnpg pod stopped running (I used to only have 1 pod running). The pod status in Kubernetes was 'completed' and status code is 0, so looks like a 'good' shutdown, but I didn't ask for a shutdown. I then added an extra replica to not have downtime, now the same happened but without downtime, just 1 pod is not running at the moment. The cluster resource indicates that not all instances are healthy, but the operator is not fixing it it seems.\r\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  creationTimestamp: \"2023-12-14T19:30:39Z\"\r\n  generation: 4\r\n  labels:\r\n    kustomize.toolkit.fluxcd.io/name: infra\r\n    kustomize.toolkit.fluxcd.io/namespace: flux-system\r\n  name: cnpg\r\n  namespace: postgres\r\n  resourceVersion: \"72883685\"\r\n  uid: d0ae48a4-53c3-4bb3-9740-d93ab7716c07\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  description: Postgresql cluster\r\n  enableSuperuserAccess: true\r\n  externalClusters:\r\n  - connectionParameters:\r\n      dbname: postgres\r\n      host: postgres.database.svc.cluster.local\r\n      user: postgres\r\n    name: standalone-pg\r\n    password:\r\n      key: password\r\n      name: standalone-postgres\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/ahinko/postgres-extended:16.1.4\r\n  instances: 2\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_connections: \"100\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      shared_buffers: 128MB\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      ssl_max_protocol_version: TLSv1.3\r\n      ssl_min_protocol_version: TLSv1.3\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    shared_preload_libraries:\r\n    - vectors.so\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      memory: 500Mi\r\n    requests:\r\n      cpu: 100m\r\n      memory: 500Mi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 10Gi\r\n    storageClass: freenas-api-nfs\r\n  superuserSecret:\r\n    name: cloudnative-pg-secret\r\n  switchoverDelay: 3600\r\nstatus:\r\n  certificates:\r\n    clientCASecret: cnpg-ca\r\n    expirations:\r\n      cnpg-ca: 2024-03-13 19:25:39 +0000 UTC\r\n      cnpg-replication: 2024-03-13 19:25:39 +0000 UTC\r\n      cnpg-server: 2024-03-13 19:25:39 +0000 UTC\r\n    replicationTLSSecret: cnpg-replication\r\n    serverAltDNSNames:\r\n    - cnpg-rw\r\n    - cnpg-rw.postgres\r\n    - cnpg-rw.postgres.svc\r\n    - cnpg-r\r\n    - cnpg-r.postgres\r\n    - cnpg-r.postgres.svc\r\n    - cnpg-ro\r\n    - cnpg-ro.postgres\r\n    - cnpg-ro.postgres.svc\r\n    serverCASecret: cnpg-ca\r\n    serverTLSSecret: cnpg-server\r\n  cloudNativePGCommitHash: 86b9dc80\r\n  cloudNativePGOperatorHash: 411a67468c5e1357d387221ce5b11d906bd33d62324846a8c776d206b6c94162\r\n  conditions:\r\n  - lastTransitionTime: \"2023-12-26T13:38:15Z\"\r\n    message: Cluster Is Not Ready\r\n    reason: ClusterIsNotReady\r\n    status: \"False\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2023-12-15T19:28:58Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"68421441\"\r\n  currentPrimary: cnpg-2\r\n  currentPrimaryTimestamp: \"2023-12-26T13:38:49.499078Z\"\r\n  healthyPVC:\r\n  - cnpg-1\r\n  - cnpg-2\r\n  instanceNames:\r\n  - cnpg-1\r\n  - cnpg-2\r\n  instances: 2\r\n  instancesReportedState:\r\n    cnpg-2:\r\n      isPrimary: true\r\n      timeLineID: 2\r\n  instancesStatus:\r\n    failed:\r\n    - cnpg-1\r\n    healthy:\r\n    - cnpg-2\r\n  latestGeneratedNode: 2\r\n  managedRolesStatus: {}\r\n  phase: Waiting for the instances to become active\r\n  phaseReason: Some instances are not yet active. Please wait.\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 2\r\n  readService: cnpg-r\r\n  readyInstances: 1\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"71057067\"\r\n    clientCaSecretVersion: \"68421414\"\r\n    externalClusterSecretVersion:\r\n      standalone-postgres: \"68413748\"\r\n    replicationSecretVersion: \"68421417\"\r\n    serverCaSecretVersion: \"68421414\"\r\n    serverSecretVersion: \"68421415\"\r\n    superuserSecretVersion: \"68413740\"\r\n  targetPrimary: cnpg-2\r\n  targetPrimaryTimestamp: \"2023-12-26T13:38:15.220438Z\"\r\n  timelineID: 2\r\n  topology:\r\n    instances:\r\n      cnpg-1: {}\r\n      cnpg-2: {}\r\n    nodesUsed: 2\r\n    successfullyExtracted: true\r\n  writeService: cnpg-rw\n```\n### Relevant log output\n```shell\nlogs from the pod itself:\r\n2023-12-26T13:32:08.168286371Z info record\r\n2023-12-26T13:32:08.358375199Z info record\r\n2023-12-26T13:34:55.627167824Z info Backup not configured, skip WAL archiving\r\n2023-12-26T13:35:11.539852574Z info Received termination signal\r\n2023-12-26T13:35:11.539927095Z info Requesting smart shutdown of the PostgreSQL instance\r\n2023-12-26T13:35:11.545646195Z info pg_ctl: server is running (PID: 25)\r\n/usr/lib/postgresql/16/bin/postgres \"-D\" \"/var/lib/postgresql/data/pgdata\"\r\n2023-12-26T13:35:11.545742387Z info Shutting down instance\r\n2023-12-26T13:35:11.54808071Z info record\r\n2023-12-26T13:35:38.986647291Z info record\r\n2023-12-26T13:36:08.988512708Z info record\r\n2023-12-26T13:36:38.98724377Z info record\r\n2023-12-26T13:37:08.646127012Z info record\r\n2023-12-26T13:37:08.82545931Z info record\r\n2023-12-26T13:37:09.225363172Z info record\r\n2023-12-26T13:37:38.992231841Z info record\r\n2023-12-26T13:38:08.987132094Z info record\r\n2023-12-26T13:38:13.034153224Z info waiting for server to shut down....................................................................................................................................................................................... failed\r\n2023-12-26T13:38:13.034198059Z info pg_ctl: server does not shut down\r\n2023-12-26T13:38:13.034202868Z info HINT: The \"-m fast\" option immediately disconnects sessions rather than\r\n2023-12-26T13:38:13.034205773Z info waiting for session-initiated disconnection.\r\n2023-12-26T13:38:13.034288941Z info Error while handling the smart shutdown request\r\n2023-12-26T13:38:13.03429904Z info Requesting fast shutdown of the PostgreSQL instance\r\n2023-12-26T13:38:13.036935347Z info pg_ctl: server is running (PID: 25)\r\n/usr/lib/postgresql/16/bin/postgres \"-D\" \"/var/lib/postgresql/data/pgdata\"\r\n2023-12-26T13:38:13.03697339Z info Shutting down instance\r\n2023-12-26T13:38:13.038815174Z info record\r\n2023-12-26T13:38:13.053422869Z info record\r\n2023-12-26T13:38:13.058017942Z info record\r\n2023-12-26T13:38:13.058079198Z info record\r\n2023-12-26T13:38:13.058105397Z info record\r\n2023-12-26T13:38:13.05811724Z info record\r\n2023-12-26T13:38:13.058125927Z info record\r\n2023-12-26T13:38:13.058237167Z info record\r\n2023-12-26T13:38:13.066026984Z info record\r\n2023-12-26T13:38:13.164462207Z info record\r\n2023-12-26T13:38:13.187776434Z info Backup not configured, skip WAL archiving\r\n2023-12-26T13:38:13.408698121Z info record\r\n2023-12-26T13:38:13.528088538Z info record\r\n2023-12-26T13:38:13.533385188Z info Extracting pg_controldata information\r\n2023-12-26T13:38:13.536842569Z info pg_control version number:            1300\r\nCatalog version number:               202307071\r\nDatabase system identifier:           7312535110852399128\r\nDatabase cluster state:               shut down\r\npg_control last modified:             Tue 26 Dec 2023 01:38:13 PM UTC\r\nLatest checkpoint location:           C/1E000028\r\nLatest checkpoint's REDO location:    C/1E000028\r\nLatest checkpoint's REDO WAL file:    000000010000000C0000001E\r\nLatest checkpoint's TimeLineID:       1\r\nLatest checkpoint's PrevTimeLineID:   1\r\nLatest checkpoint's full_page_writes: on\r\nLatest checkpoint's NextXID:          0:573136\r\nLatest checkpoint's NextOID:          53548\r\nLatest checkpoint's NextMultiXactId:  1\r\nLatest checkpoint's NextMultiOffset:  0\r\nLatest checkpoint's oldestXID:        722\r\nLatest checkpoint's oldestXID's DB:   1\r\nLatest checkpoint's oldestActiveXID:  0\r\nLatest checkpoint's oldestMultiXid:   1\r\nLatest checkpoint's oldestMulti's DB: 1\r\nLatest checkpoint's oldestCommitTsXid:0\r\nLatest checkpoint's newestCommitTsXid:0\r\nTime of latest checkpoint:            Tue 26 Dec 2023 01:38:13 PM UTC\r\nFake LSN counter for unlogged rels:   0/3E8\r\nMinimum recovery ending location:     0/0\r\nMin recovery ending loc's timeline:   0\r\nBackup start location:                0/0\r\nBackup end location:                  0/0\r\nEnd-of-backup record required:        no\r\nwal_level setting:                    logical\r\nwal_log_hints setting:                on\r\nmax_connections setting:              100\r\nmax_worker_processes setting:         32\r\nmax_wal_senders setting:              10\r\nmax_prepared_xacts setting:           0\r\nmax_locks_per_xact setting:           64\r\ntrack_commit_timestamp setting:       off\r\nMaximum data alignment:               8\r\nDatabase block size:                  8192\r\nBlocks per segment of large relation: 131072\r\nWAL block size:                       8192\r\nBytes per WAL segment:                16777216\r\nMaximum length of identifiers:        64\r\nMaximum columns in an index:          32\r\nMaximum size of a TOAST chunk:        1996\r\nSize of a large-object chunk:         2048\r\nDate/time type storage:               64-bit integers\r\nFloat8 argument passing:              by value\r\nData page checksum version:           0\r\nMock authentication nonce:            c2ed9930642aefd266f0e7e20e8039e0f76c8cccc0fbc83050c657b4b707962a\r\n2023-12-26T13:38:13.543046015Z info waiting for server to shut down.... done\r\n2023-12-26T13:38:13.543100508Z info server stopped\r\n2023-12-26T13:38:13.54313358Z info PostgreSQL instance shut down\r\n2023-12-26T13:38:13.54751016Z info Stopping and waiting for non leader election runnables\r\n2023-12-26T13:38:13.547572288Z info Stopping and waiting for leader election runnables\r\n2023-12-26T13:38:13.547580383Z info Shutdown signal received, waiting for all workers to finish\r\n2023-12-26T13:38:13.547583058Z info Shutdown signal received, waiting for all workers to finish\r\n2023-12-26T13:38:13.547585603Z info Shutdown signal received, waiting for all workers to finish\r\n2023-12-26T13:38:13.54758919Z info Exited log pipe\r\n2023-12-26T13:38:13.547592145Z info Terminated slot Replicator loop\r\n2023-12-26T13:38:13.54759489Z info Terminated RoleSynchronizer loop\r\n2023-12-26T13:38:13.547611763Z info Webserver exited\r\n2023-12-26T13:38:13.547622834Z info Webserver exited\r\n2023-12-26T13:38:13.547634315Z info Exited log pipe\r\n2023-12-26T13:38:13.547637221Z info Exited log pipe\r\n2023-12-26T13:38:13.547867026Z info Webserver exited\r\n2023-12-26T13:38:13.548187501Z info All workers finished\r\n2023-12-26T13:38:13.548230423Z info All workers finished\r\n2023-12-26T13:38:13.548669604Z info All workers finished\r\n2023-12-26T13:38:13.548715601Z info Stopping and waiting for caches\r\n2023-12-26T13:38:13.55076604Z info Stopping and waiting for webhooks\r\n2023-12-26T13:38:13.552333725Z info Stopping and waiting for HTTP servers\r\n2023-12-26T13:38:13.552381416Z info Wait completed, proceeding to shutdown the manager\r\nlogs from the operator:\r\n2023-12-26T13:30:07.838293432Z info default\r\n2023-12-26T13:30:07.847468306Z info validate update\r\n2023-12-26T13:31:11.254806256Z info default\r\n2023-12-26T13:31:11.261846872Z info validate update\r\n2023-12-26T13:32:12.418550271Z info default\r\n2023-12-26T13:32:12.425935332Z info validate update\r\n2023-12-26T13:33:10.907680708Z info default\r\n2023-12-26T13:33:10.919520212Z info validate update\r\n2023-12-26T13:34:45.240736085Z info default\r\n2023-12-26T13:34:45.252597314Z info validate update\r\n2023-12-26T13:35:43.224001927Z info default\r\n2023-12-26T13:35:43.231520447Z info validate update\r\n2023-12-26T13:36:42.425705756Z info default\r\n2023-12-26T13:36:42.433100347Z info validate update\r\n2023-12-26T13:37:40.770825377Z info default\r\n2023-12-26T13:37:40.777855185Z info validate update\r\n2023-12-26T13:38:15.171641658Z info Current primary isn't healthy, initiating a failover\r\n2023-12-26T13:38:15.171714025Z info pod status (1 of 1)\r\n2023-12-26T13:38:15.204667344Z info Failing over\r\n2023-12-26T13:38:15.204742596Z info pod status (1 of 1)\r\n2023-12-26T13:38:15.233253201Z info Waiting for the new primary to notice the promotion request\r\n2023-12-26T13:38:15.274005561Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:16.264252081Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:17.299244539Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:18.33139479Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:19.361146899Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:20.396244015Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:21.426219384Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:22.460770233Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:23.494688316Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:24.553090988Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:25.584818785Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:26.614168748Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:27.642852843Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:28.718719059Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:29.752370149Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:30.793116385Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:31.824270249Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:32.85809986Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:33.900796557Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:34.92994414Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:35.964291877Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:36.9939087Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:38.046868439Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:39.077943935Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:40.109038863Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:40.729802786Z info default\r\n2023-12-26T13:38:40.735467103Z info validate update\r\n2023-12-26T13:38:41.141223723Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:42.172292681Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:43.206941623Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:44.258830828Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:45.290743261Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:46.323961403Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:47.360967831Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:48.400779833Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:49.43493106Z info There is a switchover or a failover in progress, waiting for the operation to complete\r\n2023-12-26T13:38:49.687248775Z info Setting primary label\r\n2023-12-26T13:39:39.312920389Z info default\r\n2023-12-26T13:39:39.323649384Z info validate update\r\n2023-12-26T13:40:40.220572944Z info default\r\n2023-12-26T13:40:40.227451587Z info validate update\r\n2023-12-26T13:42:12.650774215Z info default\r\n2023-12-26T13:42:12.666418368Z info validate update\r\n2023-12-26T13:43:12.352619375Z info default\r\n2023-12-26T13:43:12.358606887Z info validate update\r\n2023-12-26T13:44:13.73195222Z info default\r\n2023-12-26T13:44:13.741409825Z info validate update\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI can confirm this, when I shut down a node yesterday, the database wasn't started on another node. The single 'primary\" instance stopped cleanly and was left in status: Completed.\r\nThe operator repeated the following message while the node was down:\r\n```json\r\n{\r\n  \"level\": \"info\",\r\n  \"ts\": \"2024-01-02T22:51:47Z\",\r\n  \"msg\": \"Ignoring not active Pod during label update\",\r\n  \"controller\": \"cluster\",\r\n  \"controllerGroup\": \"postgresql.cnpg.io\",\r\n  \"controllerKind\": \"Cluster\",\r\n  \"Cluster\": {\r\n    \"name\": \"linkding-database\",\r\n    \"namespace\": \"linkding\"\r\n  },\r\n  \"namespace\": \"linkding\",\r\n  \"name\": \"linkding-database\",\r\n  \"reconcileID\": \"d2bc4fc4-ca1a-4076-94e3-22ffc68b6cf3\",\r\n  \"uuid\": \"826ef5ec-a9c1-11ee-9d53-f603fdc2adf9\",\r\n  \"pod\": \"linkding-database-1\",\r\n  \"status\": {\r\n    \"phase\": \"Pending\",\r\n    \"conditions\": [\r\n      {\r\n        \"type\": \"PodReadyToStartContainers\",\r\n        \"status\": \"False\",\r\n        \"lastProbeTime\": null,\r\n        \"lastTransitionTime\": \"2024-01-02T22:50:49Z\"\r\n      },\r\n      {\r\n        \"type\": \"Initialized\",\r\n        \"status\": \"False\",\r\n        \"lastProbeTime\": null,\r\n        \"lastTransitionTime\": \"2024-01-02T22:50:49Z\",\r\n        \"reason\": \"ContainersNotInitialized\",\r\n        \"message\": \"containers with incomplete status: [bootstrap-controller]\"\r\n      },\r\n      {\r\n        \"type\": \"Ready\",\r\n        \"status\": \"False\",\r\n        \"lastProbeTime\": null,\r\n        \"lastTransitionTime\": \"2024-01-02T22:50:49Z\",\r\n        \"reason\": \"ContainersNotReady\",\r\n        \"message\": \"containers with unready status: [postgres]\"\r\n      },\r\n      {\r\n        \"type\": \"ContainersReady\",\r\n        \"status\": \"False\",\r\n        \"lastProbeTime\": null,\r\n        \"lastTransitionTime\": \"2024-01-02T22:50:49Z\",\r\n        \"reason\": \"ContainersNotReady\",\r\n        \"message\": \"containers with unready status: [postgres]\"\r\n      },\r\n      {\r\n        \"type\": \"PodScheduled\",\r\n        \"status\": \"True\",\r\n        \"lastProbeTime\": null,\r\n        \"lastTransitionTime\": \"2024-01-02T22:50:49Z\"\r\n      }\r\n    ],\r\n    \"hostIP\": \"192.168.77.153\",\r\n    \"startTime\": \"2024-01-02T22:50:49Z\",\r\n    \"initContainerStatuses\": [\r\n      {\r\n        \"name\": \"bootstrap-controller\",\r\n        \"state\": {\r\n          \"waiting\": {\r\n            \"reason\": \"PodInitializing\"\r\n          }\r\n        },\r\n        \"lastState\": {},\r\n        \"ready\": false,\r\n        \"restartCount\": 0,\r\n        \"image\": \"ghcr.io/cloudnative-pg/cloudnative-pg:1.20.2\",\r\n        \"imageID\": \"\",\r\n        \"started\": false\r\n      }\r\n    ],\r\n    \"containerStatuses\": [\r\n      {\r\n        \"name\": \"postgres\",\r\n        \"state\": {\r\n          \"waiting\": {\r\n            \"reason\": \"PodInitializing\"\r\n          }\r\n        },\r\n        \"lastState\": {},\r\n        \"ready\": false,\r\n        \"restartCount\": 0,\r\n        \"image\": \"ghcr.io/cloudnative-pg/postgresql:15.3\",\r\n        \"imageID\": \"\",\r\n        \"started\": false\r\n      }\r\n    ],\r\n    \"qosClass\": \"BestEffort\"\r\n  }\r\n}\r\n```\r\nWhen the node came on line, came this message and the instance was restarted again:\r\n```json\r\n{\r\n  \"level\": \"info\",\r\n  \"ts\": \"2024-01-02T22:51:52Z\",\r\n  \"msg\": \"Setting primary label\",\r\n  \"controller\": \"cluster\",\r\n  \"controllerGroup\": \"postgresql.cnpg.io\",\r\n  \"controllerKind\": \"Cluster\",\r\n  \"Cluster\": {\r\n    \"name\": \"linkding-database\",\r\n    \"namespace\": \"linkding\"\r\n  },\r\n  \"namespace\": \"linkding\",\r\n  \"name\": \"linkding-database\",\r\n  \"reconcileID\": \"b6361396-371d-4cb5-a769-9e37bcbfd7ab\",\r\n  \"uuid\": \"8534bd84-a9c1-11ee-9d53-f603fdc2adf9\",\r\n  \"pod\": \"linkding-database-1\"\r\n}\r\n```\n---\nThe message \"Ignoring not active Pod during label update\" was every second, so I cut out all the repeats.\r\nHere is the log:\r\n[cloudnative-pg-74c95d6bb9-7m5cw.log](https://github.com/cloudnative-pg/cloudnative-pg/files/13816224/cloudnative-pg-74c95d6bb9-7m5cw.log)\n---\nI get the same issues whenever some cluster nodes go down temporarily - the PVC remains attached to the physical instances and the status of the pod becomes completed (when the node get restored). I assume this is an edge case that might not be yet managed? No log in the operator though.\n---\nI experience the same problem. We try thttps://cloudnative-pg.io/documentation/1.20/troubleshooting/#cluster-information.  \"Replicas out of sync when no backup is configured\" in order to fix it. It kind of work, but we have to do it manually. And it happens 2 times for a week.\n---\nThanks a lot for your work, the CNPG operator is absolutely great.\r\nI'd like to chime in here. We're experiencing the same problem, though we're on an older version of the operator (`1.19.0`).\r\nA single replica CNPG cluster has been terminated by the kubelet because, during a backup, the node was low on ephemeral storage. So that's a failure in the middle of a `ScheduledBackup`.\r\nFrom that moment onward, operator's logs have began showing this neverending loop\r\n```\r\n2024-09-15 02:10:34 | {\"level\":\"info\",\"ts\":\"2024-09-15T00:10:34Z\",\"msg\":\"Ignoring not active Pod during label update\",\"controller\"....\r\n[.....]\r\n2024-09-15 02:10:34 | {\"level\":\"info\",\"ts\":\"2024-09-15T00:10:34Z\",\"msg\":\"No primary instance found for this cluster\"....\r\n```\r\nWe had to manually restart the primary to fix the loop. From what I can see, the problem is apparently there with a newer version of the operator, so upgrade is not a way to fix it."
    },
    {
        "title": "[Bug]: pgbouncer service fixed to type ClusterIP in latest release",
        "id": 2057647012,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nthorsten.schwander@gmail.com\n### Version\n1.22.0\n### What version of Kubernetes are you using?\n1.26\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nYAML manifest\n### What happened?\nBefore release 1.22.0 it was possible to configure pgbouncer as type LoadBalancer. This was useful for exposing psql externally without having to rely on an ingress controller or when not having permissions to change the ingress controller configuration. This means the approach outlined here https://cloudnative-pg.io/documentation/1.22/expose_pg_services/\r\nisn't always viable.\r\nThe new reconciliation introduced with (I believe this patch)\r\nhttps://github.com/cloudnative-pg/cloudnative-pg/commit/b1c81134bbb08f9b6cfd04013c97897d553dec81\r\nmakes it impossible to operate the pgbouncer service as type loadbalancer. This is breaking external access in our deployment.\r\nCould the service type be allowed to be of type LoadBalancer?\r\nThanks\r\nThorsten\r\n### Cluster resource\n```shell\nThis is the service resource under cnpg-1.21.1 . Under 1.22.0 the type is forced to ClusterIp\r\n$ kubectl get svc ....-pgbouncer -o yaml\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  annotations:\r\n    metallb.universe.tf/address-pool: ....-services\r\n  creationTimestamp: \"2023-04-18T02:06:39Z\"\r\n  labels:\r\n    cnpg.io/cluster: ...-psql\r\n    cnpg.io/poolerName: ...-pgbouncer\r\n  name: ...-pgbouncer\r\n  namespace: ...\r\n  ownerReferences:\r\n  - apiVersion: postgresql.cnpg.io/v1\r\n    blockOwnerDeletion: true\r\n    controller: true\r\n    kind: Pooler\r\n    name: ...-pgbouncer\r\n    uid: 0a2383df-9ac2-401a-8eae-8f227907c73f\r\n  resourceVersion: \"40706767\"\r\n  uid: 0f7edc87-e201-4759-a3c5-497987a93166\r\nspec:\r\n  allocateLoadBalancerNodePorts: true\r\n  clusterIP: 10.103.126.188\r\n  clusterIPs:\r\n  - 10.103.126.188\r\n  externalTrafficPolicy: Cluster\r\n  internalTrafficPolicy: Cluster\r\n  ipFamilies:\r\n  - IPv4\r\n  ipFamilyPolicy: SingleStack\r\n  loadBalancerIP: 172.24.......\r\n  ports:\r\n  - name: pgbouncer\r\n    nodePort: 30562\r\n    port: 5432\r\n    protocol: TCP\r\n    targetPort: 5432\r\n  selector:\r\n    cnpg.io/poolerName: ...-pgbouncer\r\n  sessionAffinity: None\r\n  type: LoadBalancer\r\nstatus:\r\n  loadBalancer:\r\n    ingress:\r\n    - ip: 172.24......\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nthorsten.schwander@gmail.com\n### Version\n1.22.0\n### What version of Kubernetes are you using?\n1.26\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nYAML manifest\n### What happened?\nBefore release 1.22.0 it was possible to configure pgbouncer as type LoadBalancer. This was useful for exposing psql externally without having to rely on an ingress controller or when not having permissions to change the ingress controller configuration. This means the approach outlined here https://cloudnative-pg.io/documentation/1.22/expose_pg_services/\r\nisn't always viable.\r\nThe new reconciliation introduced with (I believe this patch)\r\nhttps://github.com/cloudnative-pg/cloudnative-pg/commit/b1c81134bbb08f9b6cfd04013c97897d553dec81\r\nmakes it impossible to operate the pgbouncer service as type loadbalancer. This is breaking external access in our deployment.\r\nCould the service type be allowed to be of type LoadBalancer?\r\nThanks\r\nThorsten\r\n### Cluster resource\n```shell\nThis is the service resource under cnpg-1.21.1 . Under 1.22.0 the type is forced to ClusterIp\r\n$ kubectl get svc ....-pgbouncer -o yaml\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  annotations:\r\n    metallb.universe.tf/address-pool: ....-services\r\n  creationTimestamp: \"2023-04-18T02:06:39Z\"\r\n  labels:\r\n    cnpg.io/cluster: ...-psql\r\n    cnpg.io/poolerName: ...-pgbouncer\r\n  name: ...-pgbouncer\r\n  namespace: ...\r\n  ownerReferences:\r\n  - apiVersion: postgresql.cnpg.io/v1\r\n    blockOwnerDeletion: true\r\n    controller: true\r\n    kind: Pooler\r\n    name: ...-pgbouncer\r\n    uid: 0a2383df-9ac2-401a-8eae-8f227907c73f\r\n  resourceVersion: \"40706767\"\r\n  uid: 0f7edc87-e201-4759-a3c5-497987a93166\r\nspec:\r\n  allocateLoadBalancerNodePorts: true\r\n  clusterIP: 10.103.126.188\r\n  clusterIPs:\r\n  - 10.103.126.188\r\n  externalTrafficPolicy: Cluster\r\n  internalTrafficPolicy: Cluster\r\n  ipFamilies:\r\n  - IPv4\r\n  ipFamilyPolicy: SingleStack\r\n  loadBalancerIP: 172.24.......\r\n  ports:\r\n  - name: pgbouncer\r\n    nodePort: 30562\r\n    port: 5432\r\n    protocol: TCP\r\n    targetPort: 5432\r\n  selector:\r\n    cnpg.io/poolerName: ...-pgbouncer\r\n  sessionAffinity: None\r\n  type: LoadBalancer\r\nstatus:\r\n  loadBalancer:\r\n    ingress:\r\n    - ip: 172.24......\n```\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductThis is an issue to as well. It would be nice if the reconciler could let service type change to either `NodePort` or `LoadBalancer`\n---\nthis has been in status `triage` for over a month. can we get some idea whether this is considered as a feature?\r\notherwise I have to either stay with older cnpg version or create and manage separate pgbouncer instances outside of the cnpg operator's control\r\nThanks\r\nThorsten\n---\nthis seems to have been fixed in https://github.com/cloudnative-pg/cloudnative-pg/issues/3141"
    },
    {
        "title": "Edits to troubleshooting, use cases and wal archiving topics",
        "id": 2056717625,
        "state": "no reaction",
        "first": "",
        "messages": ""
    },
    {
        "title": "[Docs]: Clarify the status of CloudNative-pg regarding CNCF",
        "id": 2056612632,
        "state": "open",
        "first": "### Is there an existing issue already for your request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\r\n### What problem in the existing documentation this issue aims to solve?\r\nI am trying to understand the relationship of CloudNative PG to the CNCF. The [launch Blog Post](https://cloudnative-pg.io/blog/running-postgresql-the-kubernetes-way/) states\r\n> We\u2019ve made the first step: we set the foundations for a prosperous and healthy community and committed ourselves to go through the CNCF graduation process, having officially submitted our request to join the Sandbox - the first PostgreSQL project to ever do so.\r\nHowever I can't find CloudNative-PG on the CNCF landscape or any mentions of CNCF in recent plog posts. The Launch Blog post is from May 2022.  Questions:  \r\n- Does the project still plan to join CNCF? \r\n- Why has it not already when it said it would do so 19 months ago? \r\nGiven the recent change of software license by HashiCorp on Terraform the issue of license change is a big risk. Users want to clearly understand the intentions / status of the project re governance. Adding a governance section to the docs that explains the status and rationale behind it is help folks decide if they want to use or contribute to the project.\r\n### Describe what additions need to be done to the documentation\r\nAdd a Governance page to the docs that explains who owns the project, if it is in a foundations ... etc. \r\n### Describe what pages need to change in the documentation, if any\r\n_No response_\r\n### Describe what pages need to be removed from the documentation, if any\r\n_No response_\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for your request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new documentation enhancement to be evaluated.\r\n### What problem in the existing documentation this issue aims to solve?\r\nI am trying to understand the relationship of CloudNative PG to the CNCF. The [launch Blog Post](https://cloudnative-pg.io/blog/running-postgresql-the-kubernetes-way/) states\r\n> We\u2019ve made the first step: we set the foundations for a prosperous and healthy community and committed ourselves to go through the CNCF graduation process, having officially submitted our request to join the Sandbox - the first PostgreSQL project to ever do so.\r\nHowever I can't find CloudNative-PG on the CNCF landscape or any mentions of CNCF in recent plog posts. The Launch Blog post is from May 2022.  Questions:  \r\n- Does the project still plan to join CNCF? \r\n- Why has it not already when it said it would do so 19 months ago? \r\nGiven the recent change of software license by HashiCorp on Terraform the issue of license change is a big risk. Users want to clearly understand the intentions / status of the project re governance. Adding a governance section to the docs that explains the status and rationale behind it is help folks decide if they want to use or contribute to the project.\r\n### Describe what additions need to be done to the documentation\r\nAdd a Governance page to the docs that explains who owns the project, if it is in a foundations ... etc. \r\n### Describe what pages need to change in the documentation, if any\r\n_No response_\r\n### Describe what pages need to be removed from the documentation, if any\r\n_No response_\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nYes\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductI am curious about the CNCF status as well. \r\nIt is worth noting that the Hashicorp license changes only affect companies using Hashicorp products to _compete_ with Hashicorp. It does not seem like that huge of a deal to me, as compared to some of the sensational headlines I saw regarding it.\n---\nIt's not just about Hashicorp's license change only affecting companies using their projects, its about defining the long term model towards open source. While hashicorp went with a BSL that only affected companies for competing with them, they could have went to any other license that doesn't allow that.  The CNCF process helps add a layer of protection against that happening in general. The fact that this hasn't been answered does raise some concerns on adopting this operator.\n---\nWe have looked through sandbox on cncf today and still no results. We would like someone to give a concrete answer here please.\n---\nhi @gbartolini and team, we are interested in knowing what's the status of the CNCF application as well. After skimming existing and past CNCF sandbox applications I can't find any entry for cloudnative-pg, but I believe the process to apply was different in 2022:\r\nhttps://github.com/cncf/sandbox/issues?q="
    },
    {
        "title": "Resource management and rolling update topics copy edits",
        "id": 2049493534,
        "state": "no reaction",
        "first": "",
        "messages": ""
    },
    {
        "title": "Edites to replica_cluster and replication topics",
        "id": 2049418957,
        "state": "open",
        "first": "Please check my queries.",
        "messages": "Please check my queries."
    },
    {
        "title": "[Feature]: Connection SNI Routing with Traefik",
        "id": 2047566289,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nCurrent ingress requires a separate port for each Postgres cluster because TCP cannot be dynamically routed to the host like with HTTP. TLS TCP traffic can be routed with Server Name Indication (SNI) and TLS/SSL database connections are default with CNPG. \r\nTraefik v3 includes proxy SNI routing of Postgres traffic. This would allow host-based routing of postgres clusters on a single port residing in the same K8s cluster. \r\n### Describe the solution you'd like\r\nInclude a standalone Traefik v3 deployment in the CNPG operator that handles Postgres ingress into the k8s cluster.  The operator would handle deployment and basic configuration of the Traefik SNI proxy.\r\n### Describe alternatives you've considered\r\nRecommended approach in the documentation only, no operator support.\r\n### Additional context\r\nI have successfully tested this in a K8s cluster for ingress connections. I have not tested postgres replication between K8S clusters.\r\n### Backport?\r\nNo\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nCurrent ingress requires a separate port for each Postgres cluster because TCP cannot be dynamically routed to the host like with HTTP. TLS TCP traffic can be routed with Server Name Indication (SNI) and TLS/SSL database connections are default with CNPG. \r\nTraefik v3 includes proxy SNI routing of Postgres traffic. This would allow host-based routing of postgres clusters on a single port residing in the same K8s cluster. \r\n### Describe the solution you'd like\r\nInclude a standalone Traefik v3 deployment in the CNPG operator that handles Postgres ingress into the k8s cluster.  The operator would handle deployment and basic configuration of the Traefik SNI proxy.\r\n### Describe alternatives you've considered\r\nRecommended approach in the documentation only, no operator support.\r\n### Additional context\r\nI have successfully tested this in a K8s cluster for ingress connections. I have not tested postgres replication between K8S clusters.\r\n### Backport?\r\nNo\r\n### Are you willing to actively contribute to this feature?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct@bkief I notice you have already tested Traefik successfully, can you please share the details about the solution?\n---\n> @bkief I notice you have already tested Traefik successfully, can you please share the details about the solution?\r\nAgree. Can you please share what you have?\n---\n@xieydd @gbartolini - Here is a gist for a basic k8s setup. https://gist.github.com/bkief/32aeafe7c33db92fafda0308ec9c31dd\r\nIn this example it would be connected to using `mydb.foobar.net:30007` It uses a NodePort on 30007, but it could easily be a LoadBalancer on 5432 if you wanted to keep the standard port. The only thing that needs editing when adding a new database is an addition to the Traefik config in the configmap, specifically adding a new router and service for each database. Unfortunately Traefik does not yet support routing rules for SNI, so this has to be manually maintained. Not included here but its assumed that the CNPG service is running at mydb-rw.pgdb.svc.cluster.local:5432 on the local K8s DNS. Also please note that only Traefik 3 support Postgres style TLS, and v3 is still in beta.\r\nHopefully that is all clear and good luck!\n---\nSince this could be just documentation and since traefikv3 still in dev, @bkief can you consider create a PR with this documentation? something like an example?\r\nRegards\n---\nhello, I'm starting to use cloudnative-pg and I have the same problem using traefik. I solved it using the following [Traefik ingress route](https://github.com/ghimele/homelab/blob/e01d2ef6c1930acd64f59cc24e769544fdce7551/kubernetes/apps/prod/cloudnative-pg/cluster/ingress-route.yaml).\r\nThen to connect to the db we need just to put the correct hostname (the one we configured inside the HostSNI ) and port **443**\r\n![image](https://github.com/user-attachments/assets/88de5a12-03b8-4971-9c8f-87860a083348)\r\nand we need to configure the SSL mode to **require**\r\n![image](https://github.com/user-attachments/assets/2567d280-bb6f-4163-beb4-558ed17cfbd3)\r\nRegards"
    },
    {
        "title": "[Bug]: Old primary instance does not get restarted with new image",
        "id": 2044022299,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ncnpg@bo0tzz.me\n### Version\n1.21.1\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nI updated my Cluster from the default image to a custom one (ghcr.io/bo0tzz/cnpgvecto.rs:14.10-v0.1.11). This is the relevant commit: https://github.com/bo0tzz/kube/commit/85d322a80c287c7d05f118a31a71da0689edbbf3. The app-db-5 pod, which was previously the primary before the switchover caused by this change, did not get updated with the new image. The other pods did (and they came up successfully). Manually deleting the app-db-5 pod caused it to be rescheduled with the correct image.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: app-db\r\n  namespace: database\r\nspec:\r\n  instances: 3\r\n  imageName: ghcr.io/bo0tzz/cnpgvecto.rs:14.10-v0.1.11\r\n  enableSuperuserAccess: true\r\n  postgresql:\r\n    shared_preload_libraries:\r\n      - \"vectors.so\"\r\n  storage:\r\n    storageClass: local-path\r\n    size: 10Gi\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  backup:\r\n    retentionPolicy: \"30d\"\r\n    barmanObjectStore:\r\n      destinationPath: s3://app-db-backup/\r\n      endpointURL: http://minio:9000\r\n      wal:\r\n        compression: bzip2\r\n      s3Credentials:\r\n          accessKeyId:\r\n            name: minio-creds\r\n            key: MINIO_ROOT_USER\r\n          secretAccessKey:\r\n            name: minio-creds\r\n            key: MINIO_ROOT_PASSWORD\n```\n### Relevant log output\n```shell\nboet@Boets-Air database % k cnpg -n database status app-db\r\nCluster Summary\r\nName:                app-db\r\nNamespace:           database\r\nSystem ID:           7111368576186621974\r\nPostgreSQL Image:    ghcr.io/bo0tzz/cnpgvecto.rs:14.10-v0.1.11\r\nPrimary instance:    app-db-3\r\nPrimary start time:  2023-12-15 16:23:35 +0000 UTC (uptime 1m25s)\r\nStatus:              Waiting for the instances to become active Some instances are not yet active. Please wait.\r\nInstances:           3\r\nReady instances:     2\r\nCurrent Write LSN:   222/14062108 (Timeline: 30 - WAL File: 0000001E0000022200000014)\r\nCertificates Status\r\nCertificate Name    Expiration Date                Days Left Until Expiration\r\n----------------    ---------------                --------------------------\r\napp-db-ca           2024-01-29 23:55:00 +0000 UTC  45.31\r\napp-db-replication  2024-01-29 23:55:00 +0000 UTC  45.31\r\napp-db-server       2024-01-29 23:55:00 +0000 UTC  45.31\r\nContinuous Backup status\r\nFirst Point of Recoverability:  2023-11-15T00:00:07Z\r\nWorking WAL archiving:          OK\r\nWALs waiting to be archived:    0\r\nLast Archived WAL:              0000001D0000022200000014.partial   @   2023-12-15T16:23:37.055416Z\r\nLast Failed WAL:                0000001E.history                   @   2023-12-15T16:23:34.484409Z\r\nStreaming Replication status\r\nNot available yet\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nInstances status\r\nName      Database Size  Current LSN   Replication role      Status             QoS         Manager Version  Node\r\n----      -------------  -----------   ----------------      ------             ---         ---------------  ----\r\napp-db-3  343 MB         222/14062108  Primary               OK                 BestEffort  1.21.1           k3s-0\r\napp-db-6  343 MB         222/140000A0  Standby (file based)  OK                 BestEffort  1.21.1           k3s-2\r\napp-db-5  -              -             -                     pod not available  BestEffort  -                k3s-1\r\nboet@Boets-Air database % k -n database get pod app-db-5\r\nNAME       READY   STATUS             RESTARTS       AGE\r\napp-db-5   0/1     CrashLoopBackOff   10 (43s ago)   14d\r\nboet@Boets-Air database % k -n database describe pod app-db-5\r\nName:             app-db-5\r\nNamespace:        database\r\nPriority:         0\r\nService Account:  app-db\r\nNode:             k3s-1/192.168.4.3\r\nStart Time:       Fri, 01 Dec 2023 13:57:32 +0100\r\nLabels:           cnpg.io/cluster=app-db\r\n                  cnpg.io/instanceName=app-db-5\r\n                  cnpg.io/instanceRole=replica\r\n                  cnpg.io/podRole=instance\r\n                  role=replica\r\nAnnotations:      cnpg.io/nodeSerial: 5\r\n                  cnpg.io/operatorVersion: 1.21.1\r\n                  cnpg.io/podEnvHash: 57fd75bd4d\r\n                  cnpg.io/podSpec:\r\n                    {\"volumes\":[{\"name\":\"pgdata\",\"persistentVolumeClaim\":{\"claimName\":\"app-db-5\"}},{\"name\":\"scratch-data\",\"emptyDir\":{}},{\"name\":\"shm\",\"emptyD...\r\nStatus:           Running\r\nIP:               10.42.2.97\r\nIPs:\r\n  IP:           10.42.2.97\r\nControlled By:  Cluster/app-db\r\nInit Containers:\r\n  bootstrap-controller:\r\n    Container ID:  containerd://833a6048211a042e2f3f17baa19ae682978bb0e324c7ef60bc6c87722277d467\r\n    Image:         ghcr.io/cloudnative-pg/cloudnative-pg:1.21.1\r\n    Image ID:      ghcr.io/cloudnative-pg/cloudnative-pg@sha256:9f707d91de1c92975ab1a098b52e804d8f5b90c7934a0b1c40d00e773522a123\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      /manager\r\n      bootstrap\r\n      /controller/manager\r\n      --log-level=info\r\n    State:          Terminated\r\n      Reason:       Completed\r\n      Exit Code:    0\r\n      Started:      Fri, 01 Dec 2023 13:57:38 +0100\r\n      Finished:     Fri, 01 Dec 2023 13:57:38 +0100\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Environment:    <none>\r\n    Mounts:\r\n      /controller from scratch-data (rw)\r\n      /dev/shm from shm (rw)\r\n      /etc/app-secret from app-secret (rw)\r\n      /etc/superuser-secret from superuser-secret (rw)\r\n      /run from scratch-data (rw)\r\n      /var/lib/postgresql/data from pgdata (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-v9qqg (ro)\r\nContainers:\r\n  postgres:\r\n    Container ID:  containerd://0f83e02b253a95e70c6d193874f4d1e233d8460f018df1866bf55a570cf26ab7\r\n    Image:         ghcr.io/cloudnative-pg/postgresql:14.9\r\n    Image ID:      ghcr.io/cloudnative-pg/postgresql@sha256:aecaadbd1aa52bebb39e403d11f3242749b23e05a6d2afc4b242f4babd87f8e9\r\n    Ports:         5432/TCP, 9187/TCP, 8000/TCP\r\n    Host Ports:    0/TCP, 0/TCP, 0/TCP\r\n    Command:\r\n      /controller/manager\r\n      instance\r\n      run\r\n      --log-level=info\r\n    State:          Terminated\r\n      Reason:       Completed\r\n      Exit Code:    0\r\n      Started:      Fri, 15 Dec 2023 17:25:22 +0100\r\n      Finished:     Fri, 15 Dec 2023 17:25:22 +0100\r\n    Last State:     Terminated\r\n      Reason:       Completed\r\n      Exit Code:    0\r\n      Started:      Fri, 15 Dec 2023 17:24:39 +0100\r\n      Finished:     Fri, 15 Dec 2023 17:24:39 +0100\r\n    Ready:          False\r\n    Restart Count:  11\r\n    Liveness:       http-get http://:8000/healthz delay=0s timeout=5s period=10s #success=1 #failure=3\r\n    Readiness:      http-get http://:8000/readyz delay=0s timeout=5s period=10s #success=1 #failure=3\r\n    Startup:        http-get http://:8000/healthz delay=0s timeout=5s period=10s #success=1 #failure=3\r\n    Environment:\r\n      PGDATA:        /var/lib/postgresql/data/pgdata\r\n      POD_NAME:      app-db-5\r\n      NAMESPACE:     database\r\n      CLUSTER_NAME:  app-db\r\n      PGPORT:        5432\r\n      PGHOST:        /controller/run\r\n    Mounts:\r\n      /controller from scratch-data (rw)\r\n      /dev/shm from shm (rw)\r\n      /etc/app-secret from app-secret (rw)\r\n      /etc/superuser-secret from superuser-secret (rw)\r\n      /run from scratch-data (rw)\r\n      /var/lib/postgresql/data from pgdata (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-v9qqg (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True \r\n  Ready             False \r\n  ContainersReady   False \r\n  PodScheduled      True \r\nVolumes:\r\n  pgdata:\r\n    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\r\n    ClaimName:  app-db-5\r\n    ReadOnly:   false\r\n  scratch-data:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:     \r\n    SizeLimit:  <unset>\r\n  shm:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:     Memory\r\n    SizeLimit:  <unset>\r\n  superuser-secret:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  app-db-superuser\r\n    Optional:    false\r\n  app-secret:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  app-db-app\r\n    Optional:    false\r\n  kube-api-access-v9qqg:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  3607\r\n    ConfigMapName:           kube-root-ca.crt\r\n    ConfigMapOptional:       <nil>\r\n    DownwardAPI:             true\r\nQoS Class:                   BestEffort\r\nNode-Selectors:              <none>\r\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\r\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\r\nEvents:\r\n  Type     Reason     Age                     From     Message\r\n  ----     ------     ----                    ----     -------\r\n  Warning  Failed     5d14h (x41 over 5d14h)  kubelet  Error: failed to reserve container name \"postgres_app-db-5_database_29b3e46b-9c20-4850-a8f0-746acf1deedc_1\": name \"postgres_app-db-5_database_29b3e46b-9c20-4850-a8f0-746acf1deedc_1\" is reserved for \"0d1b5dafa86b118a418b2b0246088f1bb62c43be22621d60c49ae254266111f3\"\r\n  Warning  Unhealthy  105s (x5 over 5d14h)    kubelet  Startup probe failed: HTTP probe failed with statuscode: 500\r\n  Normal   Started    48s (x9 over 14d)       kubelet  Started container postgres\r\n  Warning  BackOff    19s (x39 over 5d14h)    kubelet  Back-off restarting failed container\r\n  Normal   Pulled     5s (x74 over 5d14h)     kubelet  Container image \"ghcr.io/cloudnative-pg/postgresql:14.9\" already present on machine\r\n  Normal   Created    5s (x10 over 14d)       kubelet  Created container postgres\r\nboet@Boets-Air database % k logs -n database app-db-5\r\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"app-db-5\",\"version\":\"1.21.1\",\"build\":{\"Version\":\"1.21.1\",\"Commit\":\"27f62cac\",\"Date\":\"2023-11-03\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"app-db-5\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"logger\":\"roles_reconciler\",\"msg\":\"skipping the RoleSynchronizer in replicas\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"app-db-5\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"app-db-5\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Found previous run flag\",\"logging_pod\":\"app-db-5\",\"filename\":\"/var/lib/postgresql/data/pgdata/cnpg_initialized-app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"app-db-5\",\"reason\":\"postmaster start up\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202107181\\nDatabase system identifier:           7111368576186621974\\nDatabase cluster state:               shut down\\npg_control last modified:             Fri 15 Dec 2023 04:23:29 PM UTC\\nLatest checkpoint location:           222/14000028\\nLatest checkpoint's REDO location:    222/14000028\\nLatest checkpoint's REDO WAL file:    0000001D0000022200000014\\nLatest checkpoint's TimeLineID:       29\\nLatest checkpoint's PrevTimeLineID:   29\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:15893451\\nLatest checkpoint's NextOID:          7038858\\nLatest checkpoint's NextMultiXactId:  27105\\nLatest checkpoint's NextMultiOffset:  55102\\nLatest checkpoint's oldestXID:        726\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  0\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Fri 15 Dec 2023 04:23:28 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            b6c0ca260ffbacdb48e7c5b502ec07b0aa88249aa8a443a45eeca10d270eaf4f\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"app-db\",\"namespace\":\"database\"},\"namespace\":\"database\",\"name\":\"app-db\",\"reconcileID\":\"ae4aa220-3501-4148-b3a9-169155dda01f\",\"uuid\":\"25016d89-9b67-11ee-a618-c20b1bd2c705\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"logger\":\"postgres\",\"msg\":\"2023-12-15 16:29:40.130 UTC [21] FATAL:  could not access file \\\"vectors.so\\\": No such file or directory\",\"pipe\":\"stderr\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"logger\":\"postgres\",\"msg\":\"2023-12-15 16:29:40.130 UTC [21] LOG:  database system is shut down\",\"pipe\":\"stderr\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"app-db-5\",\"reason\":\"postmaster has exited\"}\r\n{\"level\":\"error\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"PostgreSQL process exited with errors\",\"logging_pod\":\"app-db-5\",\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:98\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/manager/runnable_group.go:223\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Stopping and waiting for leader election runnables\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"error\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"error received after stop sequence was engaged\",\"error\":\"exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).engageStopProcedure.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/manager/internal.go:490\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"app-db-5\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"app-db-5\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"app-db-5\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Stopping and waiting for caches\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Stopping and waiting for webhooks\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Stopping and waiting for HTTP servers\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ncnpg@bo0tzz.me\n### Version\n1.21.1\n### What version of Kubernetes are you using?\nother (unsupported)\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\nI updated my Cluster from the default image to a custom one (ghcr.io/bo0tzz/cnpgvecto.rs:14.10-v0.1.11). This is the relevant commit: https://github.com/bo0tzz/kube/commit/85d322a80c287c7d05f118a31a71da0689edbbf3. The app-db-5 pod, which was previously the primary before the switchover caused by this change, did not get updated with the new image. The other pods did (and they came up successfully). Manually deleting the app-db-5 pod caused it to be rescheduled with the correct image.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: app-db\r\n  namespace: database\r\nspec:\r\n  instances: 3\r\n  imageName: ghcr.io/bo0tzz/cnpgvecto.rs:14.10-v0.1.11\r\n  enableSuperuserAccess: true\r\n  postgresql:\r\n    shared_preload_libraries:\r\n      - \"vectors.so\"\r\n  storage:\r\n    storageClass: local-path\r\n    size: 10Gi\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  backup:\r\n    retentionPolicy: \"30d\"\r\n    barmanObjectStore:\r\n      destinationPath: s3://app-db-backup/\r\n      endpointURL: http://minio:9000\r\n      wal:\r\n        compression: bzip2\r\n      s3Credentials:\r\n          accessKeyId:\r\n            name: minio-creds\r\n            key: MINIO_ROOT_USER\r\n          secretAccessKey:\r\n            name: minio-creds\r\n            key: MINIO_ROOT_PASSWORD\n```\n### Relevant log output\n```shell\nboet@Boets-Air database % k cnpg -n database status app-db\r\nCluster Summary\r\nName:                app-db\r\nNamespace:           database\r\nSystem ID:           7111368576186621974\r\nPostgreSQL Image:    ghcr.io/bo0tzz/cnpgvecto.rs:14.10-v0.1.11\r\nPrimary instance:    app-db-3\r\nPrimary start time:  2023-12-15 16:23:35 +0000 UTC (uptime 1m25s)\r\nStatus:              Waiting for the instances to become active Some instances are not yet active. Please wait.\r\nInstances:           3\r\nReady instances:     2\r\nCurrent Write LSN:   222/14062108 (Timeline: 30 - WAL File: 0000001E0000022200000014)\r\nCertificates Status\r\nCertificate Name    Expiration Date                Days Left Until Expiration\r\n----------------    ---------------                --------------------------\r\napp-db-ca           2024-01-29 23:55:00 +0000 UTC  45.31\r\napp-db-replication  2024-01-29 23:55:00 +0000 UTC  45.31\r\napp-db-server       2024-01-29 23:55:00 +0000 UTC  45.31\r\nContinuous Backup status\r\nFirst Point of Recoverability:  2023-11-15T00:00:07Z\r\nWorking WAL archiving:          OK\r\nWALs waiting to be archived:    0\r\nLast Archived WAL:              0000001D0000022200000014.partial   @   2023-12-15T16:23:37.055416Z\r\nLast Failed WAL:                0000001E.history                   @   2023-12-15T16:23:34.484409Z\r\nStreaming Replication status\r\nNot available yet\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nInstances status\r\nName      Database Size  Current LSN   Replication role      Status             QoS         Manager Version  Node\r\n----      -------------  -----------   ----------------      ------             ---         ---------------  ----\r\napp-db-3  343 MB         222/14062108  Primary               OK                 BestEffort  1.21.1           k3s-0\r\napp-db-6  343 MB         222/140000A0  Standby (file based)  OK                 BestEffort  1.21.1           k3s-2\r\napp-db-5  -              -             -                     pod not available  BestEffort  -                k3s-1\r\nboet@Boets-Air database % k -n database get pod app-db-5\r\nNAME       READY   STATUS             RESTARTS       AGE\r\napp-db-5   0/1     CrashLoopBackOff   10 (43s ago)   14d\r\nboet@Boets-Air database % k -n database describe pod app-db-5\r\nName:             app-db-5\r\nNamespace:        database\r\nPriority:         0\r\nService Account:  app-db\r\nNode:             k3s-1/192.168.4.3\r\nStart Time:       Fri, 01 Dec 2023 13:57:32 +0100\r\nLabels:           cnpg.io/cluster=app-db\r\n                  cnpg.io/instanceName=app-db-5\r\n                  cnpg.io/instanceRole=replica\r\n                  cnpg.io/podRole=instance\r\n                  role=replica\r\nAnnotations:      cnpg.io/nodeSerial: 5\r\n                  cnpg.io/operatorVersion: 1.21.1\r\n                  cnpg.io/podEnvHash: 57fd75bd4d\r\n                  cnpg.io/podSpec:\r\n                    {\"volumes\":[{\"name\":\"pgdata\",\"persistentVolumeClaim\":{\"claimName\":\"app-db-5\"}},{\"name\":\"scratch-data\",\"emptyDir\":{}},{\"name\":\"shm\",\"emptyD...\r\nStatus:           Running\r\nIP:               10.42.2.97\r\nIPs:\r\n  IP:           10.42.2.97\r\nControlled By:  Cluster/app-db\r\nInit Containers:\r\n  bootstrap-controller:\r\n    Container ID:  containerd://833a6048211a042e2f3f17baa19ae682978bb0e324c7ef60bc6c87722277d467\r\n    Image:         ghcr.io/cloudnative-pg/cloudnative-pg:1.21.1\r\n    Image ID:      ghcr.io/cloudnative-pg/cloudnative-pg@sha256:9f707d91de1c92975ab1a098b52e804d8f5b90c7934a0b1c40d00e773522a123\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      /manager\r\n      bootstrap\r\n      /controller/manager\r\n      --log-level=info\r\n    State:          Terminated\r\n      Reason:       Completed\r\n      Exit Code:    0\r\n      Started:      Fri, 01 Dec 2023 13:57:38 +0100\r\n      Finished:     Fri, 01 Dec 2023 13:57:38 +0100\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Environment:    <none>\r\n    Mounts:\r\n      /controller from scratch-data (rw)\r\n      /dev/shm from shm (rw)\r\n      /etc/app-secret from app-secret (rw)\r\n      /etc/superuser-secret from superuser-secret (rw)\r\n      /run from scratch-data (rw)\r\n      /var/lib/postgresql/data from pgdata (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-v9qqg (ro)\r\nContainers:\r\n  postgres:\r\n    Container ID:  containerd://0f83e02b253a95e70c6d193874f4d1e233d8460f018df1866bf55a570cf26ab7\r\n    Image:         ghcr.io/cloudnative-pg/postgresql:14.9\r\n    Image ID:      ghcr.io/cloudnative-pg/postgresql@sha256:aecaadbd1aa52bebb39e403d11f3242749b23e05a6d2afc4b242f4babd87f8e9\r\n    Ports:         5432/TCP, 9187/TCP, 8000/TCP\r\n    Host Ports:    0/TCP, 0/TCP, 0/TCP\r\n    Command:\r\n      /controller/manager\r\n      instance\r\n      run\r\n      --log-level=info\r\n    State:          Terminated\r\n      Reason:       Completed\r\n      Exit Code:    0\r\n      Started:      Fri, 15 Dec 2023 17:25:22 +0100\r\n      Finished:     Fri, 15 Dec 2023 17:25:22 +0100\r\n    Last State:     Terminated\r\n      Reason:       Completed\r\n      Exit Code:    0\r\n      Started:      Fri, 15 Dec 2023 17:24:39 +0100\r\n      Finished:     Fri, 15 Dec 2023 17:24:39 +0100\r\n    Ready:          False\r\n    Restart Count:  11\r\n    Liveness:       http-get http://:8000/healthz delay=0s timeout=5s period=10s #success=1 #failure=3\r\n    Readiness:      http-get http://:8000/readyz delay=0s timeout=5s period=10s #success=1 #failure=3\r\n    Startup:        http-get http://:8000/healthz delay=0s timeout=5s period=10s #success=1 #failure=3\r\n    Environment:\r\n      PGDATA:        /var/lib/postgresql/data/pgdata\r\n      POD_NAME:      app-db-5\r\n      NAMESPACE:     database\r\n      CLUSTER_NAME:  app-db\r\n      PGPORT:        5432\r\n      PGHOST:        /controller/run\r\n    Mounts:\r\n      /controller from scratch-data (rw)\r\n      /dev/shm from shm (rw)\r\n      /etc/app-secret from app-secret (rw)\r\n      /etc/superuser-secret from superuser-secret (rw)\r\n      /run from scratch-data (rw)\r\n      /var/lib/postgresql/data from pgdata (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-v9qqg (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True \r\n  Ready             False \r\n  ContainersReady   False \r\n  PodScheduled      True \r\nVolumes:\r\n  pgdata:\r\n    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\r\n    ClaimName:  app-db-5\r\n    ReadOnly:   false\r\n  scratch-data:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:     \r\n    SizeLimit:  <unset>\r\n  shm:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:     Memory\r\n    SizeLimit:  <unset>\r\n  superuser-secret:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  app-db-superuser\r\n    Optional:    false\r\n  app-secret:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  app-db-app\r\n    Optional:    false\r\n  kube-api-access-v9qqg:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  3607\r\n    ConfigMapName:           kube-root-ca.crt\r\n    ConfigMapOptional:       <nil>\r\n    DownwardAPI:             true\r\nQoS Class:                   BestEffort\r\nNode-Selectors:              <none>\r\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\r\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\r\nEvents:\r\n  Type     Reason     Age                     From     Message\r\n  ----     ------     ----                    ----     -------\r\n  Warning  Failed     5d14h (x41 over 5d14h)  kubelet  Error: failed to reserve container name \"postgres_app-db-5_database_29b3e46b-9c20-4850-a8f0-746acf1deedc_1\": name \"postgres_app-db-5_database_29b3e46b-9c20-4850-a8f0-746acf1deedc_1\" is reserved for \"0d1b5dafa86b118a418b2b0246088f1bb62c43be22621d60c49ae254266111f3\"\r\n  Warning  Unhealthy  105s (x5 over 5d14h)    kubelet  Startup probe failed: HTTP probe failed with statuscode: 500\r\n  Normal   Started    48s (x9 over 14d)       kubelet  Started container postgres\r\n  Warning  BackOff    19s (x39 over 5d14h)    kubelet  Back-off restarting failed container\r\n  Normal   Pulled     5s (x74 over 5d14h)     kubelet  Container image \"ghcr.io/cloudnative-pg/postgresql:14.9\" already present on machine\r\n  Normal   Created    5s (x10 over 14d)       kubelet  Created container postgres\r\nboet@Boets-Air database % k logs -n database app-db-5\r\nDefaulted container \"postgres\" out of: postgres, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"app-db-5\",\"version\":\"1.21.1\",\"build\":{\"Version\":\"1.21.1\",\"Commit\":\"27f62cac\",\"Date\":\"2023-11-03\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"app-db-5\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"logger\":\"roles_reconciler\",\"msg\":\"skipping the RoleSynchronizer in replicas\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"app-db-5\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"app-db-5\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:39Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Found previous run flag\",\"logging_pod\":\"app-db-5\",\"filename\":\"/var/lib/postgresql/data/pgdata/cnpg_initialized-app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"app-db-5\",\"reason\":\"postmaster start up\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202107181\\nDatabase system identifier:           7111368576186621974\\nDatabase cluster state:               shut down\\npg_control last modified:             Fri 15 Dec 2023 04:23:29 PM UTC\\nLatest checkpoint location:           222/14000028\\nLatest checkpoint's REDO location:    222/14000028\\nLatest checkpoint's REDO WAL file:    0000001D0000022200000014\\nLatest checkpoint's TimeLineID:       29\\nLatest checkpoint's PrevTimeLineID:   29\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:15893451\\nLatest checkpoint's NextOID:          7038858\\nLatest checkpoint's NextMultiXactId:  27105\\nLatest checkpoint's NextMultiOffset:  55102\\nLatest checkpoint's oldestXID:        726\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  0\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Fri 15 Dec 2023 04:23:28 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/0\\nMin recovery ending loc's timeline:   0\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            b6c0ca260ffbacdb48e7c5b502ec07b0aa88249aa8a443a45eeca10d270eaf4f\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"app-db\",\"namespace\":\"database\"},\"namespace\":\"database\",\"name\":\"app-db\",\"reconcileID\":\"ae4aa220-3501-4148-b3a9-169155dda01f\",\"uuid\":\"25016d89-9b67-11ee-a618-c20b1bd2c705\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"logger\":\"postgres\",\"msg\":\"2023-12-15 16:29:40.130 UTC [21] FATAL:  could not access file \\\"vectors.so\\\": No such file or directory\",\"pipe\":\"stderr\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"logger\":\"postgres\",\"msg\":\"2023-12-15 16:29:40.130 UTC [21] LOG:  database system is shut down\",\"pipe\":\"stderr\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"app-db-5\",\"reason\":\"postmaster has exited\"}\r\n{\"level\":\"error\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"PostgreSQL process exited with errors\",\"logging_pod\":\"app-db-5\",\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:98\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/manager/runnable_group.go:223\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Stopping and waiting for leader election runnables\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"error\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"error received after stop sequence was engaged\",\"error\":\"exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).engageStopProcedure.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/manager/internal.go:490\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"app-db-5\",\"address\":\":9187\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"app-db-5\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"app-db-5\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"app-db-5\",\"address\":\":8000\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Stopping and waiting for caches\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Stopping and waiting for webhooks\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Stopping and waiting for HTTP servers\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-15T16:29:40Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "Edits to scheduling and security topics",
        "id": 2042415267,
        "state": "no reaction",
        "first": "",
        "messages": ""
    },
    {
        "title": "[Bug]: Unable to recover the cluster after it gets into error state",
        "id": 2038964630,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njiaweig3@illinois.edu\n### Version\n1.21.1\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWe deployed the cluster with the example YAML from the repository. The operator successfully deploys the Postgres cluster according the desired state.\r\nThen we issued a misoperation, by changing the Affinity to one which cannot be satisfied currently in our Kubernetes cluster. We observe that the operator updates the Pods with the erroneous Affinity rule, and the restarted pods cannot be scheduled.\r\nThen we tries to fix the Affinity in the CR by reverting it back to the previous valid one. However, after applying the fixed CR with the correct Affinity rule, the operator is stuck and never updates the Pods.\r\nWe looked into the source code of the operator, and we think the root cause of this bug is caused by the logic to check if a pod is ready before rolling out: https://github.com/cloudnative-pg/cloudnative-pg/blob/10611d99b1846fbd7100dbd4769f3fca6cdd4337/controllers/cluster_upgrade.go#L246\r\nIn this case, the Pod is in a permanent error state, and it is not ready. So the operator skips rolling this erroneous Pod with the fixed Affinity rule.\r\nThis prevents us from recovering the cluster back to healthy state after some misoperation, or even just some Kubernetes topology changes. For example, if the Kubernetes topology changed and caused the existing Affinity rule to not be able to satisfied, we cannot correct the Affinity rule in the CR to save the cluster.\r\nWe also experimented this with other properties in the CR, and this bug is vulnerable not only to Affinity, but also all error states caused by other misoperations such as misconfigured containter spec, volume spec, and etc.\n### Cluster resource\n```shell\nInitial CR:\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: test-cluster\r\nspec:\r\n  affinity:\r\n    enablePodAntiAffinity: true\r\n    topologyKey: failure-domain.beta.kubernetes.io/zone\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      owner: app\r\n      secret:\r\n        name: cluster-example-app-user\r\n  description: Example of cluster\r\n  enableSuperuserAccess: true\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  instances: 3\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: false\r\nThen we apply:\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: test-cluster\r\nspec:\r\n  affinity:\r\n    enablePodAntiAffinity: true\r\n    nodeSelector:\r\n      SOMESELECTOR: BAR\r\n    topologyKey: failure-domain.beta.kubernetes.io/zone\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      owner: app\r\n      secret:\r\n        name: cluster-example-app-user\r\n  description: Example of cluster\r\n  enableSuperuserAccess: true\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  instances: 3\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: false\r\n```\r\nThen we fix the erroneous Affinity rule by reverting the CR:\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: test-cluster\r\nspec:\r\n  affinity:\r\n    enablePodAntiAffinity: true\r\n    topologyKey: failure-domain.beta.kubernetes.io/zone\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      owner: app\r\n      secret:\r\n        name: cluster-example-app-user\r\n  description: Example of cluster\r\n  enableSuperuserAccess: true\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  instances: 3\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: false\r\n```\n```\n### Relevant log output\n```shell\n--- After applying the erroneous Affinity change\r\n{\"level\":\"info\",\"ts\":\"2023-12-01T04:33:10Z\",\"logger\":\"cluster-resource\",\"msg\":\"validate update\",\"version\":\"v1\",\"name\":\"test-cluster\",\"namespace\":\"cnpg-system\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-01T04:33:10Z\",\"msg\":\"Pod rollout required\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"test-cluster\",\"namespace\":\"cnpg-system\"},\"namespace\":\"cnpg-system\",\"name\":\"test-cluster\",\"reconcileID\":\"1177953c-4cfd-4e74-aa14-181f08bd9db5\",\"uuid\":\"bb4da85c-9002-11ee-bd59-aed01ababf32\",\"pod\":\"test-cluster-3\",\"reason\":\"original and target PodSpec differ in node-selector\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-01T04:33:10Z\",\"msg\":\"Recreating instance pod\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"test-cluster\",\"namespace\":\"cnpg-system\"},\"namespace\":\"cnpg-system\",\"name\":\"test-cluster\",\"reconcileID\":\"1177953c-4cfd-4e74-aa14-181f08bd9db5\",\"uuid\":\"bb4da85c-9002-11ee-bd59-aed01ababf32\",\"pod\":\"test-cluster-3\",\"to\":\"ghcr.io/cloudnative-pg/postgresql:16.1\",\"reason\":\"Restarting instance test-cluster-3, because: original and target PodSpec differ in node-selector\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-01T04:33:11Z\",\"msg\":\"Creating new Pod to reattach a PVC\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"test-cluster\",\"namespace\":\"cnpg-system\"},\"namespace\":\"cnpg-system\",\"name\":\"test-cluster\",\"reconcileID\":\"c02ef355-37f1-4323-8729-c35517a37287\",\"uuid\":\"bbf3b697-9002-11ee-bd59-aed01ababf32\",\"pod\":\"test-cluster-3\",\"pvc\":\"test-cluster-3\"}\r\n--- After reverting to valid CR\r\n{\"level\":\"info\",\"ts\":\"2023-12-01T04:34:14Z\",\"logger\":\"cluster-resource\",\"msg\":\"validate update\",\"version\":\"v1\",\"name\":\"test-cluster\",\"namespace\":\"cnpg-system\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njiaweig3@illinois.edu\n### Version\n1.21.1\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nWe deployed the cluster with the example YAML from the repository. The operator successfully deploys the Postgres cluster according the desired state.\r\nThen we issued a misoperation, by changing the Affinity to one which cannot be satisfied currently in our Kubernetes cluster. We observe that the operator updates the Pods with the erroneous Affinity rule, and the restarted pods cannot be scheduled.\r\nThen we tries to fix the Affinity in the CR by reverting it back to the previous valid one. However, after applying the fixed CR with the correct Affinity rule, the operator is stuck and never updates the Pods.\r\nWe looked into the source code of the operator, and we think the root cause of this bug is caused by the logic to check if a pod is ready before rolling out: https://github.com/cloudnative-pg/cloudnative-pg/blob/10611d99b1846fbd7100dbd4769f3fca6cdd4337/controllers/cluster_upgrade.go#L246\r\nIn this case, the Pod is in a permanent error state, and it is not ready. So the operator skips rolling this erroneous Pod with the fixed Affinity rule.\r\nThis prevents us from recovering the cluster back to healthy state after some misoperation, or even just some Kubernetes topology changes. For example, if the Kubernetes topology changed and caused the existing Affinity rule to not be able to satisfied, we cannot correct the Affinity rule in the CR to save the cluster.\r\nWe also experimented this with other properties in the CR, and this bug is vulnerable not only to Affinity, but also all error states caused by other misoperations such as misconfigured containter spec, volume spec, and etc.\n### Cluster resource\n```shell\nInitial CR:\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: test-cluster\r\nspec:\r\n  affinity:\r\n    enablePodAntiAffinity: true\r\n    topologyKey: failure-domain.beta.kubernetes.io/zone\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      owner: app\r\n      secret:\r\n        name: cluster-example-app-user\r\n  description: Example of cluster\r\n  enableSuperuserAccess: true\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  instances: 3\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: false\r\nThen we apply:\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: test-cluster\r\nspec:\r\n  affinity:\r\n    enablePodAntiAffinity: true\r\n    nodeSelector:\r\n      SOMESELECTOR: BAR\r\n    topologyKey: failure-domain.beta.kubernetes.io/zone\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      owner: app\r\n      secret:\r\n        name: cluster-example-app-user\r\n  description: Example of cluster\r\n  enableSuperuserAccess: true\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  instances: 3\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: false\r\n```\r\nThen we fix the erroneous Affinity rule by reverting the CR:\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: test-cluster\r\nspec:\r\n  affinity:\r\n    enablePodAntiAffinity: true\r\n    topologyKey: failure-domain.beta.kubernetes.io/zone\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      owner: app\r\n      secret:\r\n        name: cluster-example-app-user\r\n  description: Example of cluster\r\n  enableSuperuserAccess: true\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  instances: 3\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: false\r\n```\n```\n### Relevant log output\n```shell\n--- After applying the erroneous Affinity change\r\n{\"level\":\"info\",\"ts\":\"2023-12-01T04:33:10Z\",\"logger\":\"cluster-resource\",\"msg\":\"validate update\",\"version\":\"v1\",\"name\":\"test-cluster\",\"namespace\":\"cnpg-system\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-01T04:33:10Z\",\"msg\":\"Pod rollout required\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"test-cluster\",\"namespace\":\"cnpg-system\"},\"namespace\":\"cnpg-system\",\"name\":\"test-cluster\",\"reconcileID\":\"1177953c-4cfd-4e74-aa14-181f08bd9db5\",\"uuid\":\"bb4da85c-9002-11ee-bd59-aed01ababf32\",\"pod\":\"test-cluster-3\",\"reason\":\"original and target PodSpec differ in node-selector\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-01T04:33:10Z\",\"msg\":\"Recreating instance pod\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"test-cluster\",\"namespace\":\"cnpg-system\"},\"namespace\":\"cnpg-system\",\"name\":\"test-cluster\",\"reconcileID\":\"1177953c-4cfd-4e74-aa14-181f08bd9db5\",\"uuid\":\"bb4da85c-9002-11ee-bd59-aed01ababf32\",\"pod\":\"test-cluster-3\",\"to\":\"ghcr.io/cloudnative-pg/postgresql:16.1\",\"reason\":\"Restarting instance test-cluster-3, because: original and target PodSpec differ in node-selector\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-01T04:33:11Z\",\"msg\":\"Creating new Pod to reattach a PVC\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"test-cluster\",\"namespace\":\"cnpg-system\"},\"namespace\":\"cnpg-system\",\"name\":\"test-cluster\",\"reconcileID\":\"c02ef355-37f1-4323-8729-c35517a37287\",\"uuid\":\"bbf3b697-9002-11ee-bd59-aed01ababf32\",\"pod\":\"test-cluster-3\",\"pvc\":\"test-cluster-3\"}\r\n--- After reverting to valid CR\r\n{\"level\":\"info\",\"ts\":\"2023-12-01T04:34:14Z\",\"logger\":\"cluster-resource\",\"msg\":\"validate update\",\"version\":\"v1\",\"name\":\"test-cluster\",\"namespace\":\"cnpg-system\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": " feat: Allow shrinking of storage",
        "id": 2037420040,
        "state": "open",
        "first": "Unfasten the restriction of shrinking of storage while the `resizeInUseVolumes` is set to false \r\nIntroduced a new type of condition `PVCResizeSucceeded` to record the result of offline resizing pvcs\r\ncloses: #2405",
        "messages": "Unfasten the restriction of shrinking of storage while the `resizeInUseVolumes` is set to false \r\nIntroduced a new type of condition `PVCResizeSucceeded` to record the result of offline resizing pvcs\r\ncloses: #2405the e2e result from the folk\r\nhttps://github.com/EnterpriseDB/cloudnative-pg/actions/runs/7197032167"
    },
    {
        "title": "add node prioritization capabilities through customer labels or annotations for the primary node",
        "id": 2036086210,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIt is possible to promote the primary on a specific cluster node with the command \"kubectl cnpg promote\".\r\nbut it is that possible to enforce this by configuration (ClusterSpec or other).\n### Describe the solution you'd like\nI would like to have the capability to choose a prefered k8s node for the primary postgres node \n### Describe alternatives you've considered\nimplement a custom cronjob which use the kubectl cnpg cli in order to force promotion when it is required\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nIt is possible to promote the primary on a specific cluster node with the command \"kubectl cnpg promote\".\r\nbut it is that possible to enforce this by configuration (ClusterSpec or other).\n### Describe the solution you'd like\nI would like to have the capability to choose a prefered k8s node for the primary postgres node \n### Describe alternatives you've considered\nimplement a custom cronjob which use the kubectl cnpg cli in order to force promotion when it is required\n### Additional context\n_No response_\n### Backport?\nNo\n### Are you willing to actively contribute to this feature?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]:  Add UUID in backup path",
        "id": 2035572008,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWhen recreating a cluster with existing backup files on S3, the new cluster cannot create backup.\r\nFor example:\r\n1.   We have a postgres cluster with backups on S3.\r\n2.   The cluster was deleted, but the backups on s3 were not cleared.\r\n3.   Create a cluster of the same variables (kubectl apply -f cluster.yalm)\r\n4.   Backup from new cluster failed because some files exist on s3.\r\n### Describe the solution you'd like\nThe Zolando operator uses the cluster uuid and sets it to the backup path, and the new cluster uses a different backup path. Is it possible to add variable to automatically set the uuid to the backup path?\n### Describe alternatives you've considered\nWe use helm chart and create custom config map with UUID. UUID generated by helm install command, and read by helm upgrade.\r\nThis uuid we set in backup path.\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWhen recreating a cluster with existing backup files on S3, the new cluster cannot create backup.\r\nFor example:\r\n1.   We have a postgres cluster with backups on S3.\r\n2.   The cluster was deleted, but the backups on s3 were not cleared.\r\n3.   Create a cluster of the same variables (kubectl apply -f cluster.yalm)\r\n4.   Backup from new cluster failed because some files exist on s3.\r\n### Describe the solution you'd like\nThe Zolando operator uses the cluster uuid and sets it to the backup path, and the new cluster uses a different backup path. Is it possible to add variable to automatically set the uuid to the backup path?\n### Describe alternatives you've considered\nWe use helm chart and create custom config map with UUID. UUID generated by helm install command, and read by helm upgrade.\r\nThis uuid we set in backup path.\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductHave you looked into the `.spec.backup.barmanObjectoStore.serverName` option? see https://cloudnative-pg.io/documentation/current/cloudnative-pg.v1/#postgresql-cnpg-io-v1-BarmanObjectStoreConfiguration\n---\nYes, but .spec.backup.barmanObjectoStore.serverName needs to be configured manually. We would like the operator to be able to use the cluster UUID in place of the cluster name."
    },
    {
        "title": "[Bug]: initdb: error: could not create directory Permission denied",
        "id": 2029895155,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.21.1\n### What version of Kubernetes are you using?\n1.26\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nHelm\n### What happened?\nI sometimes got the error that the initdb pod fails when I tried deploying a new pg cluster.\r\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-1\r\n  namespace: default\r\nspec:\r\n  postgresql:\r\n    parameters:\r\n      password_encryption: md5\r\n    pg_hba:\r\n      - host all all all md5       \r\n  instances: 2\r\n  primaryUpdateStrategy: unsupervised\r\n  storage:\r\n    size: 20Gi\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  enableSuperuserAccess: true\r\n  superuserSecret:\r\n    name: cluster-superuser\n```\n### Relevant log output\n```shell\n\"level\":\"info\",\"ts\":\"2023-12-04T09:53:25Z\",\"logger\":\"initdb\",\"msg\":\"The files belonging to this database system will be owned by user \\\"postgres\\\".\\nThis user must also own the server process.\\n\\nThe database cluster will be initialized with this locale configuration:\\n  provider:    libc\\n  LC_COLLATE:  C\\n  LC_CTYPE:    C\\n  LC_MESSAGES: en_US.utf8\\n  LC_MONETARY: en_US.utf8\\n  LC_NUMERIC:  en_US.utf8\\n  LC_TIME:     en_US.utf8\\nThe default text search configuration will be set to \\\"english\\\".\\n\\nData page checksums are disabled.\\n\\ncreating directory /var/lib/postgresql/data/pgdata ... \",\"pipe\":\"stdout\",\"logging_pod\":\"services-postgresql-cluster-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-04T09:53:25Z\",\"logger\":\"initdb\",\"msg\":\"initdb: error: could not create directory \\\"/var/lib/postgresql/data/pgdata\\\": Permission denied\\n\",\"pipe\":\"stderr\",\"logging_pod\":\"cluster-1-initdb\"}\r\n{\"level\":\"error\",\"ts\":\"2023-12-04T09:53:25Z\",\"msg\":\"Error while bootstrapping data directory\",\"logging_pod\":\"services-postgresql-cluster-1-initdb\",\"error\":\"error while creating the PostgreSQL instance: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.initSubCommand\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:151\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.NewCmd.func2\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:104\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.3/x64/src/runtime/proc.go:267\"}\r\nError: error while creating the PostgreSQL instance: exit status 1\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.21.1\n### What version of Kubernetes are you using?\n1.26\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nHelm\n### What happened?\nI sometimes got the error that the initdb pod fails when I tried deploying a new pg cluster.\r\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-1\r\n  namespace: default\r\nspec:\r\n  postgresql:\r\n    parameters:\r\n      password_encryption: md5\r\n    pg_hba:\r\n      - host all all all md5       \r\n  instances: 2\r\n  primaryUpdateStrategy: unsupervised\r\n  storage:\r\n    size: 20Gi\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  enableSuperuserAccess: true\r\n  superuserSecret:\r\n    name: cluster-superuser\n```\n### Relevant log output\n```shell\n\"level\":\"info\",\"ts\":\"2023-12-04T09:53:25Z\",\"logger\":\"initdb\",\"msg\":\"The files belonging to this database system will be owned by user \\\"postgres\\\".\\nThis user must also own the server process.\\n\\nThe database cluster will be initialized with this locale configuration:\\n  provider:    libc\\n  LC_COLLATE:  C\\n  LC_CTYPE:    C\\n  LC_MESSAGES: en_US.utf8\\n  LC_MONETARY: en_US.utf8\\n  LC_NUMERIC:  en_US.utf8\\n  LC_TIME:     en_US.utf8\\nThe default text search configuration will be set to \\\"english\\\".\\n\\nData page checksums are disabled.\\n\\ncreating directory /var/lib/postgresql/data/pgdata ... \",\"pipe\":\"stdout\",\"logging_pod\":\"services-postgresql-cluster-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-04T09:53:25Z\",\"logger\":\"initdb\",\"msg\":\"initdb: error: could not create directory \\\"/var/lib/postgresql/data/pgdata\\\": Permission denied\\n\",\"pipe\":\"stderr\",\"logging_pod\":\"cluster-1-initdb\"}\r\n{\"level\":\"error\",\"ts\":\"2023-12-04T09:53:25Z\",\"msg\":\"Error while bootstrapping data directory\",\"logging_pod\":\"services-postgresql-cluster-1-initdb\",\"error\":\"error while creating the PostgreSQL instance: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.initSubCommand\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:151\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.NewCmd.func2\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:104\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.3/x64/src/runtime/proc.go:267\"}\r\nError: error while creating the PostgreSQL instance: exit status 1\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI had a similar problem\r\nI used minikube and Kubernetes v1.26.3\r\nLogs output:\r\n`Defaulted container \"initdb\" out of: initdb, bootstrap-controller (init)\r\n{\"level\":\"info\",\"ts\":\"2023-12-13T12:15:35Z\",\"msg\":\"Creating new data directory\",\"logging_pod\":\"cluster-example-1-initdb\",\"pgdata\":\"/var/lib/postgresql/data/pgdata\",\"initDbOptions\":[\"--username\",\"postgres\",\"-D\",\"/var/lib/postgresql/data/pgdata\",\"--encoding=UTF8\",\"--lc-collate=C\",\"--lc-ctype=C\"]}\r\n{\"level\":\"info\",\"ts\":\"2023-12-13T12:15:35Z\",\"logger\":\"initdb\",\"msg\":\"The files belonging to this database system will be owned by user \\\"postgres\\\".\\nThis user must also own the server process.\\n\\nThe database cluster will be initialized with this locale configuration:\\n  provider:    libc\\n  LC_COLLATE:  C\\n  LC_CTYPE:    C\\n  LC_MESSAGES: en_US.utf8\\n  LC_MONETARY: en_US.utf8\\n  LC_NUMERIC:  en_US.utf8\\n  LC_TIME:     en_US.utf8\\nThe default text search configuration will be set to \\\"english\\\".\\n\\nData page checksums are disabled.\\n\\ncreating directory /var/lib/postgresql/data/pgdata ... \",\"pipe\":\"stdout\",\"logging_pod\":\"cluster-example-1-initdb\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-13T12:15:35Z\",\"logger\":\"initdb\",\"msg\":\"initdb: error: could not create directory \\\"/var/lib/postgresql/data/pgdata\\\": Permission denied\\n\",\"pipe\":\"stderr\",\"logging_pod\":\"cluster-example-1-initdb\"}\r\n{\"level\":\"error\",\"ts\":\"2023-12-13T12:15:35Z\",\"msg\":\"Error while bootstrapping data directory\",\"logging_pod\":\"cluster-example-1-initdb\",\"error\":\"error while creating the PostgreSQL instance: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.initSubCommand\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:151\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.NewCmd.func2\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:104\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.3/x64/src/runtime/proc.go:267\"}\r\nError: error while creating the PostgreSQL instance: exit status 1`\r\nAfter updating the kubernetes version to v1.28.4 everything started working correctly:\r\n![\u0417\u043d\u0456\u043c\u043e\u043a \u0435\u043a\u0440\u0430\u043d\u0430 \u0437 2023-12-13 15-48-52](https://github.com/cloudnative-pg/cloudnative-pg/assets/104345657/07aab109-c9ef-480e-b88c-d8be5bc12073)\n---\nsame issue here with same version of k8s 1.26\n---\nThe same issue on the latest OpenShift release 4.14.10 and the official EDB Operator 1.22\n---\nI have the same problem.\r\nMy k8s cluster was on version 1.27.3. I updated to version 1.28.4 and the error still persists.\r\nEDIT: Using cloudstack 4.18.1.0-1.el7 on-promise\n---\nI'm having this issue too but it's not due to the CNPG controller in my case.\r\nThe CSI I'm using (HPE) does not apply the `fsGroup` to the pgdata volume, so it can't be used by the init container (and leads to the above error message).\r\nMy workaround was to specify the fsOwner either in the StorageClass or in the PVC (using annotations).\n---\n> I'm having this issue too but it's not due to the CNPG controller in my case. The CSI I'm using (HPE) does not apply the `fsGroup` to the pgdata volume, so it can't be used by the init container (and leads to the above error message). My workaround was to specify the fsOwner either in the StorageClass or in the PVC (using annotations).\r\nCould you show the code you used?\n---\nSimply as describe [here](https://scod.hpedev.io/csi_driver/using.html#base_storageclass_parameters):\r\n```yaml\r\napiVersion: storage.k8s.io/v1\r\nkind: StorageClass\r\nmetadata:\r\n  annotations:\r\n    storageclass.kubernetes.io/is-default-class: \"true\"\r\n  name: hpe-standard\r\nprovisioner: csi.hpe.com\r\nparameters:\r\n  fsOwner: \"19:19\"\r\n  csi.storage.k8s.io/fstype: xfs\r\n  csi.storage.k8s.io/controller-expand-secret-name: hpe-backend\r\n  csi.storage.k8s.io/controller-expand-secret-namespace: hpe-storage\r\n  csi.storage.k8s.io/controller-publish-secret-name: hpe-backend\r\n  csi.storage.k8s.io/controller-publish-secret-namespace: hpe-storage\r\n  csi.storage.k8s.io/node-publish-secret-name: hpe-backend\r\n  csi.storage.k8s.io/node-publish-secret-namespace: hpe-storage\r\n  csi.storage.k8s.io/node-stage-secret-name: hpe-backend\r\n  csi.storage.k8s.io/node-stage-secret-namespace: hpe-storage\r\n  csi.storage.k8s.io/provisioner-secret-name: hpe-backend\r\n  csi.storage.k8s.io/provisioner-secret-namespace: hpe-storage\r\n  description: \"Volume created by the HPE CSI Driver for Kubernetes\"\r\nreclaimPolicy: Delete\r\nallowVolumeExpansion: true\r\n```\r\nEventually I found out the HPE CSI driver wouldn't properly set the GID when mounting an XFS volume, but it works OK with ext4. So I simply switched to ext4 in my case, no need to override the FS owner whatsoever.\n---\nI'm using the synology-csi and trying fad3t's suggestion didn't work for me, unfortunately.  My storage class is:\r\n```\r\nallowVolumeExpansion: true\r\napiVersion: storage.k8s.io/v1\r\nkind: StorageClass\r\nmetadata:\r\n  creationTimestamp: \"2024-10-21T01:08:19Z\"\r\n  name: synology-csi-iscsi-cnpg\r\n  resourceVersion: \"4360358\"\r\n  uid: 19ee5311-0861-4e81-bd60-269729739d4c\r\nparameters:\r\n  fsOwner: \"19:19\"\r\nprovisioner: csi.san.synology.com\r\nreclaimPolicy: Delete\r\nvolumeBindingMode: Immediate\r\n```\r\nNot sure if there's something else I should try.  Snooping around seeing if there's a way I can convert the file perms manually, or make sure that 19 is truly postgresql on my install... Any tips appreciated.\n---\nSetting the **DEFAULT** UID and GIDs as stated in the doc [here](https://cloudnative-pg.io/documentation/current/cloudnative-pg.v1/#postgresql-cnpg-io-v1-ClusterSpec) to corresponding UID and GID values on my experimentally setup FS, worked for me.\n---\nThank you ahmedb25.  That didn't *quite* solve my issue, but got me a lot closer. Turns out my CSI (Synology) supports a 'root_owner' parameter I can specify a UID:GID combo, and that did the trick.\n---\n@eddieparker could you please elaborate on how you solved the issue? I'm also using the synology csi, but can't find the param you're talking about. Any hints/examples would be greatly appreciated. \ud83d\ude4f Thank you.\n---\n> [@eddieparker](https://github.com/eddieparker) could you please elaborate on how you solved the issue? I'm also using the synology csi, but can't find the param you're talking about. Any hints/examples would be greatly appreciated. \ud83d\ude4f Thank you.\nHopefully this helps!  This is an example of a storage-class I have defined that works:\n```\nallowVolumeExpansion: true\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"\n  creationTimestamp: \"2024-11-26T06:24:25Z\"\n  name: synology-csi-iscsi-default-delete\n  resourceVersion: \"14090516\"\n  uid: 40fd5925-0eb5-4b7d-88d9-d901dbb58bba\nparameters:\n  fsOwner: \"26:26\"\n  fsType: ext4\n  root_owner: \"26:26\"\nprovisioner: csi.san.synology.com\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\n```"
    },
    {
        "title": "edits to postgresql_conf and quickstart topics",
        "id": 2027151172,
        "state": "no reaction",
        "first": "",
        "messages": ""
    },
    {
        "title": "Edits to operator_conf and postgis topics",
        "id": 2026843364,
        "state": "no reaction",
        "first": "",
        "messages": ""
    },
    {
        "title": "[Bug]: `maxParallel` doesn't work during recovery",
        "id": 2025157816,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.21.1\n### What version of Kubernetes are you using?\n1.26\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nHelm\n### What happened?\nWith the provided config we'd expect barman to load WAL files in parallel during a full recovery.\r\n```yaml\r\nbootstrap:\r\n    recovery:\r\n        source: core-db-15\r\nexternalClusters:\r\n    - barmanObjectStore:\r\n            destinationPath: {{ .Values.dbBackupPath }}\r\n            s3Credentials:\r\n                accessKeyId:\r\n                    key: ***\r\n                    name: ***\r\n                secretAccessKey:\r\n                    key: ***\r\n                    name: ***\r\n            wal:\r\n                maxParallel: 15\r\n        name: core-db-15\r\n```\r\nDespite this configuration only 1 WAL is being restored at a time.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.21.1\n### What version of Kubernetes are you using?\n1.26\n### What is your Kubernetes environment?\nCloud: Other\n### How did you install the operator?\nHelm\n### What happened?\nWith the provided config we'd expect barman to load WAL files in parallel during a full recovery.\r\n```yaml\r\nbootstrap:\r\n    recovery:\r\n        source: core-db-15\r\nexternalClusters:\r\n    - barmanObjectStore:\r\n            destinationPath: {{ .Values.dbBackupPath }}\r\n            s3Credentials:\r\n                accessKeyId:\r\n                    key: ***\r\n                    name: ***\r\n                secretAccessKey:\r\n                    key: ***\r\n                    name: ***\r\n            wal:\r\n                maxParallel: 15\r\n        name: core-db-15\r\n```\r\nDespite this configuration only 1 WAL is being restored at a time.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI am seeing the same behavior and our recovery times are very slow\n---\nYeah, this is affecting us as well!\n---\nHi,\nthis is not resolved I guess?\nI see only one barman-cloud-wal-restore process, serially running per WAL.\nThis is quite confusing, because the documentation for recovery https://cloudnative-pg.io/documentation/1.24/recovery/ lists maxParallel as an option, and highlights that this is relevant. \nBut looking at https://github.com/cloudnative-pg/cloudnative-pg/blob/main/internal/cmd/manager/walrestore/cmd.go and it's history, it appears that parallel restore is not being done in case of a bootstrap (i.e. when using barman-cloud-restore).\nIs this intended behavior? Recovery now takes several hours for us."
    },
    {
        "title": "[Bug]: Failed postgres pod is never restarted",
        "id": 2018645876,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nskre@skre.me\r\n### Version\r\n1.21.1\r\n### What version of Kubernetes are you using?\r\n1.28\r\n### What is your Kubernetes environment?\r\nSelf-managed: kind (evaluation)\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nGraceful node drain resulted in a failed postgres pod (killed). It was never restarted by the controller.\r\nSee: https://github.com/kubernetes/kubernetes/issues/118310 and https://github.com/kubernetes/kubernetes/issues/122122\r\n### Cluster resource\r\n_No response_\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nskre@skre.me\r\n### Version\r\n1.21.1\r\n### What version of Kubernetes are you using?\r\n1.28\r\n### What is your Kubernetes environment?\r\nSelf-managed: kind (evaluation)\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nGraceful node drain resulted in a failed postgres pod (killed). It was never restarted by the controller.\r\nSee: https://github.com/kubernetes/kubernetes/issues/118310 and https://github.com/kubernetes/kubernetes/issues/122122\r\n### Cluster resource\r\n_No response_\r\n### Relevant log output\r\n_No response_\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductHi @buroa , can you make use of the `maintenanceWindow` feature, which lets the cnpg operator know the current k8s cluster is on the maintenance status, so it can delete the evicted pod, and recreate pod in another node"
    },
    {
        "title": "[Feature]: Provide wal-g as an option to perform backup/restore",
        "id": 2017403215,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nOur team currently performs backup/restore on existing clusters using wal-g, and we encrypt backups using PGP. This is not a supported encryption option in barman. Having the ability to use wal-g will enable us to maintain our exiting backup encryption process.\n### Describe the solution you'd like\nAbstract backup/restore related code to interfaces, barman being an implementation, wal-g being another. Expose an option in cluster manifest for the user to select which tool to use.\n### Describe alternatives you've considered\nChange how we encrypt backups, this means a tigheter security review.\n### Additional context\n_No response_\n### Backport?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nOur team currently performs backup/restore on existing clusters using wal-g, and we encrypt backups using PGP. This is not a supported encryption option in barman. Having the ability to use wal-g will enable us to maintain our exiting backup encryption process.\n### Describe the solution you'd like\nAbstract backup/restore related code to interfaces, barman being an implementation, wal-g being another. Expose an option in cluster manifest for the user to select which tool to use.\n### Describe alternatives you've considered\nChange how we encrypt backups, this means a tigheter security review.\n### Additional context\n_No response_\n### Backport?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductYou might find this discussion useful: https://github.com/cloudnative-pg/cloudnative-pg/discussions/3145\n---\nso overall direction is to make CNPG more modular with CNPG-I. \r\nwhere progress/roadmap of https://github.com/cloudnative-pg/cnpg-i can be tracked?"
    },
    {
        "title": "[Feature]: Include maintenance status in `kubectl cnpg status` output",
        "id": 2016891867,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWhen performing maintenance on a cluster I often forget to unset maintenance mode.  Having the maintenance status reflected in the `kubectl cnpg status` plugin output would make it easier to identify when maintenance mode is enabled.\n### Describe the solution you'd like\nI would like to see the values from `spec.nodeMaintenanceWindow` visible in the output of `kubectl cnpg status`.\n### Describe alternatives you've considered\nThe current method of checking the cluster object metadata is fine as an alternative.\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWhen performing maintenance on a cluster I often forget to unset maintenance mode.  Having the maintenance status reflected in the `kubectl cnpg status` plugin output would make it easier to identify when maintenance mode is enabled.\n### Describe the solution you'd like\nI would like to see the values from `spec.nodeMaintenanceWindow` visible in the output of `kubectl cnpg status`.\n### Describe alternatives you've considered\nThe current method of checking the cluster object metadata is fine as an alternative.\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "Copy edits to monitoring and networking content",
        "id": 2015097510,
        "state": "open",
        "first": "Please note query about possible missing text.",
        "messages": "Please note query about possible missing text."
    },
    {
        "title": "[Bug]: Misleading information from `cnpg status` command during bootstrap phase ",
        "id": 2007455166,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.21.0\r\n### What version of Kubernetes are you using?\r\n1.28\r\n### What is your Kubernetes environment?\r\nSelf-managed: kind (evaluation)\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\nWhen users run the command `cnpg status <cluster_name>` before the primary is up, they get the information in the section `Primary Instance` as below:\r\n```\r\nPrimary instance:   (switching to <cluster_name>-1)\r\n```\r\nIt seems to indicate that a switchover or failover is happening, but actually it's not.\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/last-applied-configuration: |\r\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"metadata\":{\"annotations\":{},\"name\":\"test\",\"namespace\":\"default\"},\"spec\":{\"instances\":1,\"logLevel\":\"info\",\"primaryUpdateMethod\":\"switchover\",\"primaryUpdateStrategy\":\"unsupervised\",\"resources\":{\"limits\":{\"cpu\":2,\"memory\":\"4Gi\"},\"requests\":{\"cpu\":\"250m\",\"memory\":\"1Gi\"}},\"storage\":{\"size\":\"2Gi\"}}}\r\n  creationTimestamp: \"2023-11-23T04:00:51Z\"\r\n  generation: 1\r\n  name: test\r\n  namespace: default\r\n  resourceVersion: \"349937\"\r\n  uid: 19d45dac-7a03-478b-b1ce-acd73b319de1\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enableSuperuserAccess: false\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  instances: 1\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: false\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: switchover\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      cpu: \"2\"\r\n      memory: 4Gi\r\n    requests:\r\n      cpu: 250m\r\n      memory: 1Gi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 2Gi\r\n  switchoverDelay: 3600\r\nstatus:\r\n  certificates:\r\n    clientCASecret: test-ca\r\n    expirations:\r\n      test-ca: 2024-02-21 03:55:51 +0000 UTC\r\n      test-replication: 2024-02-21 03:55:51 +0000 UTC\r\n      test-server: 2024-02-21 03:55:51 +0000 UTC\r\n    replicationTLSSecret: test-replication\r\n    serverAltDNSNames:\r\n    - test-rw\r\n    - test-rw.default\r\n    - test-rw.default.svc\r\n    - test-r\r\n    - test-r.default\r\n    - test-r.default.svc\r\n    - test-ro\r\n    - test-ro.default\r\n    - test-ro.default.svc\r\n    serverCASecret: test-ca\r\n    serverTLSSecret: test-server\r\n  cloudNativePGCommitHash: 300ae5e4\r\n  cloudNativePGOperatorHash: fe060bd8596bd65cd4199f6c9cb5a236beb43e574e663737b8e692bfd67beeb9\r\n  conditions:\r\n  - lastTransitionTime: \"2023-11-23T04:01:13Z\"\r\n    message: Cluster is Ready\r\n    reason: ClusterIsReady\r\n    status: \"True\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2023-11-23T04:01:03Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"349769\"\r\n  currentPrimary: test-1\r\n  currentPrimaryTimestamp: \"2023-11-23T04:01:03.200266Z\"\r\n  healthyPVC:\r\n  - test-1\r\n  instanceNames:\r\n  - test-1\r\n  instances: 1\r\n  instancesReportedState:\r\n    test-1:\r\n      isPrimary: true\r\n      timeLineID: 1\r\n  instancesStatus:\r\n    healthy:\r\n    - test-1\r\n  latestGeneratedNode: 1\r\n  managedRolesStatus: {}\r\n  phase: Cluster in healthy state\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 1\r\n  readService: test-r\r\n  readyInstances: 1\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"349746\"\r\n    clientCaSecretVersion: \"349743\"\r\n    replicationSecretVersion: \"349745\"\r\n    serverCaSecretVersion: \"349743\"\r\n    serverSecretVersion: \"349744\"\r\n  targetPrimary: test-1\r\n  targetPrimaryTimestamp: \"2023-11-23T04:00:52.128099Z\"\r\n  timelineID: 1\r\n  topology:\r\n    instances:\r\n      test-1: {}\r\n    nodesUsed: 1\r\n    successfullyExtracted: true\r\n  writeService: test-rw\r\n```\r\n### Relevant log output\r\n```shell\r\nCluster Summary\r\nPrimary server is initializing\r\nName:              test\r\nNamespace:         default\r\nPostgreSQL Image:  ghcr.io/cloudnative-pg/postgresql:16.1\r\nPrimary instance:   (switching to test-1)\r\nStatus:            Setting up primary Creating primary instance test-1\r\nInstances:         1\r\nReady instances:   0\r\nCertificates Status\r\nCertificate Name  Expiration Date                Days Left Until Expiration\r\n----------------  ---------------                --------------------------\r\ntest-ca           2024-02-21 03:55:51 +0000 UTC  90.00\r\ntest-replication  2024-02-21 03:55:51 +0000 UTC  90.00\r\ntest-server       2024-02-21 03:55:51 +0000 UTC  90.00\r\nContinuous Backup status\r\nNot configured\r\nPhysical backups\r\nPrimary instance not found\r\nStreaming Replication status\r\nNot configured\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nManaged roles status\r\nNo roles managed\r\nInstances status\r\nName  Database Size  Current LSN  Replication role  Status  QoS  Manager Version  Node\r\n----  -------------  -----------  ----------------  ------  ---  ---------------  ----\r\n```\r\n### Code of Conduct\r\n- [x] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.21.0\r\n### What version of Kubernetes are you using?\r\n1.28\r\n### What is your Kubernetes environment?\r\nSelf-managed: kind (evaluation)\r\n### How did you install the operator?\r\nYAML manifest\r\n### What happened?\r\nWhen users run the command `cnpg status <cluster_name>` before the primary is up, they get the information in the section `Primary Instance` as below:\r\n```\r\nPrimary instance:   (switching to <cluster_name>-1)\r\n```\r\nIt seems to indicate that a switchover or failover is happening, but actually it's not.\r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/last-applied-configuration: |\r\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"metadata\":{\"annotations\":{},\"name\":\"test\",\"namespace\":\"default\"},\"spec\":{\"instances\":1,\"logLevel\":\"info\",\"primaryUpdateMethod\":\"switchover\",\"primaryUpdateStrategy\":\"unsupervised\",\"resources\":{\"limits\":{\"cpu\":2,\"memory\":\"4Gi\"},\"requests\":{\"cpu\":\"250m\",\"memory\":\"1Gi\"}},\"storage\":{\"size\":\"2Gi\"}}}\r\n  creationTimestamp: \"2023-11-23T04:00:51Z\"\r\n  generation: 1\r\n  name: test\r\n  namespace: default\r\n  resourceVersion: \"349937\"\r\n  uid: 19d45dac-7a03-478b-b1ce-acd73b319de1\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enableSuperuserAccess: false\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.1\r\n  instances: 1\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: false\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: switchover\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      cpu: \"2\"\r\n      memory: 4Gi\r\n    requests:\r\n      cpu: 250m\r\n      memory: 1Gi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 2Gi\r\n  switchoverDelay: 3600\r\nstatus:\r\n  certificates:\r\n    clientCASecret: test-ca\r\n    expirations:\r\n      test-ca: 2024-02-21 03:55:51 +0000 UTC\r\n      test-replication: 2024-02-21 03:55:51 +0000 UTC\r\n      test-server: 2024-02-21 03:55:51 +0000 UTC\r\n    replicationTLSSecret: test-replication\r\n    serverAltDNSNames:\r\n    - test-rw\r\n    - test-rw.default\r\n    - test-rw.default.svc\r\n    - test-r\r\n    - test-r.default\r\n    - test-r.default.svc\r\n    - test-ro\r\n    - test-ro.default\r\n    - test-ro.default.svc\r\n    serverCASecret: test-ca\r\n    serverTLSSecret: test-server\r\n  cloudNativePGCommitHash: 300ae5e4\r\n  cloudNativePGOperatorHash: fe060bd8596bd65cd4199f6c9cb5a236beb43e574e663737b8e692bfd67beeb9\r\n  conditions:\r\n  - lastTransitionTime: \"2023-11-23T04:01:13Z\"\r\n    message: Cluster is Ready\r\n    reason: ClusterIsReady\r\n    status: \"True\"\r\n    type: Ready\r\n  - lastTransitionTime: \"2023-11-23T04:01:03Z\"\r\n    message: Continuous archiving is working\r\n    reason: ContinuousArchivingSuccess\r\n    status: \"True\"\r\n    type: ContinuousArchiving\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"349769\"\r\n  currentPrimary: test-1\r\n  currentPrimaryTimestamp: \"2023-11-23T04:01:03.200266Z\"\r\n  healthyPVC:\r\n  - test-1\r\n  instanceNames:\r\n  - test-1\r\n  instances: 1\r\n  instancesReportedState:\r\n    test-1:\r\n      isPrimary: true\r\n      timeLineID: 1\r\n  instancesStatus:\r\n    healthy:\r\n    - test-1\r\n  latestGeneratedNode: 1\r\n  managedRolesStatus: {}\r\n  phase: Cluster in healthy state\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 1\r\n  readService: test-r\r\n  readyInstances: 1\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"349746\"\r\n    clientCaSecretVersion: \"349743\"\r\n    replicationSecretVersion: \"349745\"\r\n    serverCaSecretVersion: \"349743\"\r\n    serverSecretVersion: \"349744\"\r\n  targetPrimary: test-1\r\n  targetPrimaryTimestamp: \"2023-11-23T04:00:52.128099Z\"\r\n  timelineID: 1\r\n  topology:\r\n    instances:\r\n      test-1: {}\r\n    nodesUsed: 1\r\n    successfullyExtracted: true\r\n  writeService: test-rw\r\n```\r\n### Relevant log output\r\n```shell\r\nCluster Summary\r\nPrimary server is initializing\r\nName:              test\r\nNamespace:         default\r\nPostgreSQL Image:  ghcr.io/cloudnative-pg/postgresql:16.1\r\nPrimary instance:   (switching to test-1)\r\nStatus:            Setting up primary Creating primary instance test-1\r\nInstances:         1\r\nReady instances:   0\r\nCertificates Status\r\nCertificate Name  Expiration Date                Days Left Until Expiration\r\n----------------  ---------------                --------------------------\r\ntest-ca           2024-02-21 03:55:51 +0000 UTC  90.00\r\ntest-replication  2024-02-21 03:55:51 +0000 UTC  90.00\r\ntest-server       2024-02-21 03:55:51 +0000 UTC  90.00\r\nContinuous Backup status\r\nNot configured\r\nPhysical backups\r\nPrimary instance not found\r\nStreaming Replication status\r\nNot configured\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nManaged roles status\r\nNo roles managed\r\nInstances status\r\nName  Database Size  Current LSN  Replication role  Status  QoS  Manager Version  Node\r\n----  -------------  -----------  ----------------  ------  ---  ---------------  ----\r\n```\r\n### Code of Conduct\r\n- [x] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Better support installation with kustomize",
        "id": 2006998203,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nWe use kustomize to provision everything in our clusters. While I can install cloudnative-pg with kustomize, some simple adjustments to the release automation could improve the experience. \u263a\ufe0f \r\nCurrently, we use the following kustomization:\r\n````yaml\r\napiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nresources:\r\n  - https://github.com/cloudnative-pg/cloudnative-pg//config/default?ref=v1.21.1\r\nimages:\r\n  - name: controller\r\n    newName: ghcr.io/cloudnative-pg/cloudnative-pg\r\n    newTag: 1.21.1\r\n````\r\nAnd as you can see, the version is repeated twice. This allows us to use [Renovate to suggest upgrades](https://docs.renovatebot.com/modules/manager/kustomize/), but upgrading the remote resource tag and image tag becomes two separate upgrades - if Renovate isn't configured for this specifically.\r\n### Describe the solution you'd like\r\nIt is apparent to me that the image in the `default` kustomize overlay is of no use to end-users, as the value is just `controller:latest`. I work with controllers myself, so I know kubebuilder scaffolds this by default, and it's very convenient it operator CI and when developing the operator. \ud83d\ude09 \r\nWhat I would suggest, is to create a new kustomize overlay on top of `default`, where you set the correct release image tag on the commit that is tagged for the release. I am not sure if you understand what I mean, but [here is an example](https://github.com/statnett/image-scanner-operator/blob/main/kustomization.yaml) from an operator I work on.\r\nIf the `kustomization.yaml` is put in the root of the repository, this will allow me (and others) to install cloudnative-pg with just a simple kustomization:\r\n````yaml\r\napiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nresources:\r\n  - https://github.com/cloudnative-pg/cloudnative-pg?ref=v1.21.1\r\n````\r\nIt seems like you already have some release automation that could do this, ref. https://github.com/cloudnative-pg/cloudnative-pg/commit/27f62cac9c882a10d78b52a14d49da5401730356 (tagged commit for the 1.21.1 release)\r\n### Describe alternatives you've considered\r\nIt is possible to use the [release artifact yaml](https://github.com/cloudnative-pg/cloudnative-pg/releases/download/v1.21.1/cnpg-1.21.1.yaml), but that is \"just\" and URL and nothing that Renovate understands by default.\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nWe use kustomize to provision everything in our clusters. While I can install cloudnative-pg with kustomize, some simple adjustments to the release automation could improve the experience. \u263a\ufe0f \r\nCurrently, we use the following kustomization:\r\n````yaml\r\napiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nresources:\r\n  - https://github.com/cloudnative-pg/cloudnative-pg//config/default?ref=v1.21.1\r\nimages:\r\n  - name: controller\r\n    newName: ghcr.io/cloudnative-pg/cloudnative-pg\r\n    newTag: 1.21.1\r\n````\r\nAnd as you can see, the version is repeated twice. This allows us to use [Renovate to suggest upgrades](https://docs.renovatebot.com/modules/manager/kustomize/), but upgrading the remote resource tag and image tag becomes two separate upgrades - if Renovate isn't configured for this specifically.\r\n### Describe the solution you'd like\r\nIt is apparent to me that the image in the `default` kustomize overlay is of no use to end-users, as the value is just `controller:latest`. I work with controllers myself, so I know kubebuilder scaffolds this by default, and it's very convenient it operator CI and when developing the operator. \ud83d\ude09 \r\nWhat I would suggest, is to create a new kustomize overlay on top of `default`, where you set the correct release image tag on the commit that is tagged for the release. I am not sure if you understand what I mean, but [here is an example](https://github.com/statnett/image-scanner-operator/blob/main/kustomization.yaml) from an operator I work on.\r\nIf the `kustomization.yaml` is put in the root of the repository, this will allow me (and others) to install cloudnative-pg with just a simple kustomization:\r\n````yaml\r\napiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nresources:\r\n  - https://github.com/cloudnative-pg/cloudnative-pg?ref=v1.21.1\r\n````\r\nIt seems like you already have some release automation that could do this, ref. https://github.com/cloudnative-pg/cloudnative-pg/commit/27f62cac9c882a10d78b52a14d49da5401730356 (tagged commit for the 1.21.1 release)\r\n### Describe alternatives you've considered\r\nIt is possible to use the [release artifact yaml](https://github.com/cloudnative-pg/cloudnative-pg/releases/download/v1.21.1/cnpg-1.21.1.yaml), but that is \"just\" and URL and nothing that Renovate understands by default.\r\n### Additional context\r\n_No response_\r\n### Backport?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conducthttps://github.com/cloudnative-pg/cloudnative-pg/blob/main/config/default/kustomization.yaml is not meant for consumption, it's used to generate the release manifests.\r\nWe could add an additional layer as you suggest, but would that make any difference for renovate w.r.t. pointing to https://github.com/cloudnative-pg/cloudnative-pg/releases/download/v1.21.1/cnpg-1.21.1.yaml (except that the double `1.21.1` is a pain to handle)? would it automatically detect the reference if you specified it as https://github.com/cloudnative-pg/cloudnative-pg?ref=v1.21.1 or would you have to setup a regexManager in both cases?\n---\n@phisco The latter (https://github.com/cloudnative-pg/cloudnative-pg?ref=v1.21.1) should be a standard kustomize remote resource (Git) URL supported by Renovate and Kustomize, and do not require additional configuration AFAIK.\n---\nI had a similar problem with renovate so I created #3979 as a work around for the double version substitution in the url.\n---\n@phisco would you support/merge an overhaul of the kustomize manifests to make them more generally useful?\r\nThe current kustomize code in the repository uses many deprecated features and is in an old style.\r\nLooks like it has been generated by kubebuilder quite some time ago? I wonder if as a starting point you could regenerate it\n---\nWhat's the idea here @erikgb ? why this can't be accomplish with the https://github.com/cloudnative-pg/artifacts project?\r\nOn the other hand @sgmitchell having a latest one, why? why cannot just consume the latest release using a version? this will be really dangerous if someone applies the latest manifest without knowing the possible break changes in that manifest, not even talking when there's a version 2, this will not make sense at all\r\n@daurnimator Yes it was generated long time ago and an update will be nice, but not much time for something that isn't that relevant when it's not meant to do something better than what's already there but, PRs are welcome =)\n---\n> What's the idea here @erikgb?\r\n@sxd Are you missing something from my well-described issue?\n---\n@erikgb the problem is that is trying to use something that it's not created for it, and adding another layer using kustomize it's kind of weird and more work to maintain, why not use the files inside the releases/ directory? what's the problem with using that ?Kustomize can perfectly handle that yaml file\r\nOn the other side, this statement \"but upgrading the remote resource tag and image tag becomes two separate upgrade\" you can just create  group in renovate, so I don't see the issue, that's my point.\n---\n> On the other hand @sgmitchell having a latest one, why? why cannot just consume the latest release using a version?\r\n@sxd The goal for me was to remove the need for repeating the version in the filename and rely on git for versioning since that's what git is good at.\r\nSome projects installed to my cluster that kind of shows what I was thinking are:\r\n* calico has [`calico.yaml` under `manifests/`](https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml).\r\n* `metallb` has files in [`config/manifests/`](https://github.com/metallb/metallb/tree/main/config/manifests).\r\n* longhorn has [`longhorn.yaml` under `deploy/`](https://github.com/longhorn/longhorn/blob/master/deploy/longhorn.yaml).\r\n* prometheus-operator has [`bundle.yaml`](https://github.com/prometheus-operator/prometheus-operator/blob/main/bundle.yaml) at the repo root.\r\nFor all of these, the manifests have a stable name and I pick which version I want to consume using the git tag associated with the release without also needing to change a file name. This also lets me use `git diff` to compare versions and easily see what changed between tags as opposed to diffing 2 different files across 2 different branches.\r\n> this will be really dangerous if someone applies the latest manifest without knowing the possible break changes in that manifest\r\nYes, this would be potentially dangerous if people were blindly applying the latest version from main. However I'm not advocating that they consume from main but from the tagged release. \r\nAs a user, I can be very confident that a tag will not change (you can actually enforce that they are immutable in github) where as I'm slightly less confident that a file's contents on main might not inadvertently change if some CI automation goes awry.\r\n> not even talking when there's a version 2, this will not make sense at all.\r\nI think it still makes perfect sense when you are using tags and not consuming from main, correct? Or is there some nuance here that I'm missing?\r\n> [...] you can just create group in renovate, so I don't see the issue, that's my point.\r\nThis is the little annoyance that caused me to try and contribute to this project. It seemed like a worthwhile change to have cnpg work with renovate out of the box instead of requiring an additional custom rule to handle the specific way that this project does versioning.\n---\nFWIW I wrote up my own kustomize manifests for cloudnative-pg that I'm maintaining over here: https://github.com/james-callahan/cloudnative-pg-kustomize/\r\nI plan on maintaining them into the future.\r\nI'd love it if they were upstreamed; but anyone should be able to point at my repository from their kustomize manifests"
    },
    {
        "title": "[Feature]: Dedicated phase for hibernated clusters",
        "id": 2003042967,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe operator, when performs the hibernations on clusters using [Declarative Hibernation](https://cloudnative-pg.io/documentation/1.21/declarative_hibernation/) keeps the `status.phase` as `Cluster in healthy state` once the hibernation get completed successfully.\n### Describe the solution you'd like\nIt would be useful to have a dedicated phase for it, like `Cluster hibernated`.\n### Describe alternatives you've considered\nIt can also be useful having a dedicated phase when the annotation is removed or set to `off` and the clusters get [rehydrated](https://cloudnative-pg.io/documentation/1.21/declarative_hibernation/#rehydration).\n### Additional context\n```\r\ngabrielefedi cloudnative-pg >>  (main) k get cluster\r\nNAME              AGE    INSTANCES   READY   STATUS                     PRIMARY\r\ncluster-example   4m9s   3           3       Cluster in healthy state   cluster-example-1\r\nk annotate cluster cluster-example \"cnpg.io/hibernation=on\"\r\ngabrielefedi cloudnative-pg >>  (main) k get cluster cluster-example -o yaml | yq .status.conditions\r\n...\r\n- lastTransitionTime: \"2023-11-20T21:15:38Z\"\r\n  message: Cluster has been hibernated\r\n  reason: Hibernated\r\n  status: \"True\"\r\n  type: cnpg.io/hibernation\r\ngabrielefedi cloudnative-pg >>  (main) k get cluster\r\nNAME              AGE   INSTANCES   READY   STATUS                     PRIMARY\r\ncluster-example   13m   3                   Cluster in healthy state   cluster-example-1\r\n```\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nThe operator, when performs the hibernations on clusters using [Declarative Hibernation](https://cloudnative-pg.io/documentation/1.21/declarative_hibernation/) keeps the `status.phase` as `Cluster in healthy state` once the hibernation get completed successfully.\n### Describe the solution you'd like\nIt would be useful to have a dedicated phase for it, like `Cluster hibernated`.\n### Describe alternatives you've considered\nIt can also be useful having a dedicated phase when the annotation is removed or set to `off` and the clusters get [rehydrated](https://cloudnative-pg.io/documentation/1.21/declarative_hibernation/#rehydration).\n### Additional context\n```\r\ngabrielefedi cloudnative-pg >>  (main) k get cluster\r\nNAME              AGE    INSTANCES   READY   STATUS                     PRIMARY\r\ncluster-example   4m9s   3           3       Cluster in healthy state   cluster-example-1\r\nk annotate cluster cluster-example \"cnpg.io/hibernation=on\"\r\ngabrielefedi cloudnative-pg >>  (main) k get cluster cluster-example -o yaml | yq .status.conditions\r\n...\r\n- lastTransitionTime: \"2023-11-20T21:15:38Z\"\r\n  message: Cluster has been hibernated\r\n  reason: Hibernated\r\n  status: \"True\"\r\n  type: cnpg.io/hibernation\r\ngabrielefedi cloudnative-pg >>  (main) k get cluster\r\nNAME              AGE   INSTANCES   READY   STATUS                     PRIMARY\r\ncluster-example   13m   3                   Cluster in healthy state   cluster-example-1\r\n```\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct+1"
    },
    {
        "title": "[Bug]: Declarative Hibernation not starting when cluster is not ready",
        "id": 2001033519,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nkjeld@truecharts.org\r\n### Version\r\n1.21.1\r\n### What version of Kubernetes are you using?\r\n1.28\r\n### What is your Kubernetes environment?\r\nSelf-managed: k3s\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nWhen hibernate is set through label on the cluster resource, it gets ignored if the cluster is still in a not-ready state:\r\n```\r\n    - lastTransitionTime: \"2023-11-15T23:46:59Z\"\r\n      message: Cluster Is Not Ready\r\n      reason: ClusterIsNotReady\r\n      status: \"False\"\r\n      type: Ready\r\n```\r\nIt keeps trying to create a primary pod, even though it fails.\r\nIt would be prefered if all pods, even those not functional, would be stopped and kept-stopped when declarative hibernation is set.\r\n### Cluster resource\r\n```shell\r\nroot@truenas[~]# k get cluster -n ix-home-assistant -o yaml\r\napiVersion: v1\r\nitems:\r\n- apiVersion: postgresql.cnpg.io/v1\r\n  kind: Cluster\r\n  metadata:\r\n    annotations:\r\n      cnpg.io/hibernation: \"on\"\r\n      meta.helm.sh/release-name: home-assistant\r\n      meta.helm.sh/release-namespace: ix-home-assistant\r\n    creationTimestamp: \"2023-10-16T22:04:38Z\"\r\n    generation: 2\r\n    labels:\r\n      app: home-assistant-20.0.23\r\n      app.kubernetes.io/instance: home-assistant\r\n      app.kubernetes.io/managed-by: Helm\r\n      app.kubernetes.io/name: home-assistant\r\n      app.kubernetes.io/version: 2023.11.2\r\n      cnpg.io/reload: \"on\"\r\n      helm-revision: \"29\"\r\n      helm.sh/chart: home-assistant-20.0.23\r\n      release: home-assistant\r\n    name: home-assistant-cnpg-main\r\n    namespace: ix-home-assistant\r\n    resourceVersion: \"665128\"\r\n    uid: 51762b22-0f74-4bb7-ae11-73ee87a0b5b3\r\n  spec:\r\n    affinity:\r\n      podAntiAffinityType: preferred\r\n    bootstrap:\r\n      initdb:\r\n        database: home-assistant\r\n        encoding: UTF8\r\n        localeCType: C\r\n        localeCollate: C\r\n        owner: home-assistant\r\n        secret:\r\n          name: home-assistant-cnpg-main-user\r\n    enableSuperuserAccess: true\r\n    failoverDelay: 0\r\n    imageName: ghcr.io/cloudnative-pg/postgresql:15.3\r\n    instances: 2\r\n    logLevel: info\r\n    maxSyncReplicas: 0\r\n    minSyncReplicas: 0\r\n    monitoring:\r\n      customQueriesConfigMap:\r\n      - key: queries\r\n        name: cnpg-default-monitoring\r\n      disableDefaultQueries: false\r\n      enablePodMonitor: true\r\n    nodeMaintenanceWindow:\r\n      inProgress: false\r\n      reusePVC: true\r\n    postgresGID: 26\r\n    postgresUID: 26\r\n    postgresql:\r\n      parameters:\r\n        archive_mode: \"on\"\r\n        archive_timeout: 5min\r\n        dynamic_shared_memory_type: posix\r\n        log_destination: csvlog\r\n        log_directory: /controller/log\r\n        log_filename: postgres\r\n        log_rotation_age: \"0\"\r\n        log_rotation_size: \"0\"\r\n        log_truncate_on_rotation: \"false\"\r\n        logging_collector: \"on\"\r\n        max_parallel_workers: \"32\"\r\n        max_replication_slots: \"32\"\r\n        max_worker_processes: \"32\"\r\n        shared_memory_type: mmap\r\n        shared_preload_libraries: \"\"\r\n        wal_keep_size: 512MB\r\n        wal_receiver_timeout: 5s\r\n        wal_sender_timeout: 5s\r\n      syncReplicaElectionConstraint:\r\n        enabled: false\r\n    primaryUpdateMethod: restart\r\n    primaryUpdateStrategy: unsupervised\r\n    replicationSlots:\r\n      highAvailability:\r\n        enabled: true\r\n        slotPrefix: _cnpg_\r\n      updateInterval: 30\r\n    resources:\r\n      limits:\r\n        cpu: \"4\"\r\n        memory: 8Gi\r\n      requests:\r\n        cpu: 10m\r\n        memory: 50Mi\r\n    smartShutdownTimeout: 180\r\n    startDelay: 30\r\n    stopDelay: 30\r\n    storage:\r\n      pvcTemplate:\r\n        accessModes:\r\n        - ReadWriteOnce\r\n        resources:\r\n          requests:\r\n            storage: 256Gi\r\n        storageClassName: ix-storage-class-home-assistant\r\n      resizeInUseVolumes: true\r\n    switchoverDelay: 40000000\r\n    walStorage:\r\n      pvcTemplate:\r\n        accessModes:\r\n        - ReadWriteOnce\r\n        resources:\r\n          requests:\r\n            storage: 256Gi\r\n        storageClassName: ix-storage-class-home-assistant\r\n      resizeInUseVolumes: true\r\n  status:\r\n    certificates:\r\n      clientCASecret: home-assistant-cnpg-main-ca\r\n      expirations:\r\n        home-assistant-cnpg-main-ca: 2024-01-14 21:59:38 +0000 UTC\r\n        home-assistant-cnpg-main-replication: 2024-01-14 21:59:38 +0000 UTC\r\n        home-assistant-cnpg-main-server: 2024-01-14 21:59:38 +0000 UTC\r\n      replicationTLSSecret: home-assistant-cnpg-main-replication\r\n      serverAltDNSNames:\r\n      - home-assistant-cnpg-main-rw\r\n      - home-assistant-cnpg-main-rw.ix-home-assistant\r\n      - home-assistant-cnpg-main-rw.ix-home-assistant.svc\r\n      - home-assistant-cnpg-main-r\r\n      - home-assistant-cnpg-main-r.ix-home-assistant\r\n      - home-assistant-cnpg-main-r.ix-home-assistant.svc\r\n      - home-assistant-cnpg-main-ro\r\n      - home-assistant-cnpg-main-ro.ix-home-assistant\r\n      - home-assistant-cnpg-main-ro.ix-home-assistant.svc\r\n      serverCASecret: home-assistant-cnpg-main-ca\r\n      serverTLSSecret: home-assistant-cnpg-main-server\r\n    cloudNativePGCommitHash: 27f62cac\r\n    cloudNativePGOperatorHash: 4912e5eb808aca3bf134923625f2346d404a3c860cf24aca4266493846f8fc3b\r\n    conditions:\r\n    - lastTransitionTime: \"2023-11-15T23:46:59Z\"\r\n      message: Cluster Is Not Ready\r\n      reason: ClusterIsNotReady\r\n      status: \"False\"\r\n      type: Ready\r\n    - lastTransitionTime: \"2023-11-07T14:41:05Z\"\r\n      message: Continuous archiving is working\r\n      reason: ContinuousArchivingSuccess\r\n      status: \"True\"\r\n      type: ContinuousArchiving\r\n    configMapResourceVersion:\r\n      metrics:\r\n        cnpg-default-monitoring: \"586854\"\r\n    currentPrimary: home-assistant-cnpg-main-2\r\n    currentPrimaryTimestamp: \"2023-11-15T23:49:17.670117Z\"\r\n    danglingPVC:\r\n    - home-assistant-cnpg-main-2\r\n    - home-assistant-cnpg-main-2-wal\r\n    healthyPVC:\r\n    - home-assistant-cnpg-main-1\r\n    - home-assistant-cnpg-main-1-wal\r\n    instanceNames:\r\n    - home-assistant-cnpg-main-1\r\n    - home-assistant-cnpg-main-2\r\n    instances: 2\r\n    instancesReportedState:\r\n      home-assistant-cnpg-main-1:\r\n        isPrimary: false\r\n    instancesStatus:\r\n      replicating:\r\n      - home-assistant-cnpg-main-1\r\n    latestGeneratedNode: 2\r\n    managedRolesStatus: {}\r\n    phase: Waiting for the instances to become active\r\n    phaseReason: Some instances are not yet active. Please wait.\r\n    poolerIntegrations:\r\n      pgBouncerIntegration:\r\n        secrets:\r\n        - home-assistant-cnpg-main-pooler\r\n    pvcCount: 4\r\n    readService: home-assistant-cnpg-main-r\r\n    secretsResourceVersion:\r\n      applicationSecretVersion: \"665099\"\r\n      clientCaSecretVersion: \"13830\"\r\n      replicationSecretVersion: \"13835\"\r\n      serverCaSecretVersion: \"13830\"\r\n      serverSecretVersion: \"13832\"\r\n      superuserSecretVersion: \"13836\"\r\n    targetPrimary: home-assistant-cnpg-main-2\r\n    targetPrimaryTimestamp: \"2023-11-15T23:49:16.984189Z\"\r\n    timelineID: 2\r\n    topology:\r\n      instances:\r\n        home-assistant-cnpg-main-1: {}\r\n      nodesUsed: 1\r\n      successfullyExtracted: true\r\n    writeService: home-assistant-cnpg-main-rw\r\nkind: List\r\nmetadata:\r\n  resourceVersion: \"\"\r\n```\r\n### Relevant log output\r\n```\r\n2023-11-19T21:32:43.405985829+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"version\":\"1.21.1\",\"build\":{\"Version\":\"1.21.1\",\"Commit\":\"27f62cac\",\"Date\":\"2023-11-03\"}}\r\n2023-11-19T21:32:43.408711398+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:32:43.408873981+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n2023-11-19T21:32:43.408893264+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n2023-11-19T21:32:43.409709897+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"address\":\"localhost:8010\"}\r\n2023-11-19T21:32:43.411464114+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:32:43.411478511+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:32:43.411912702+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"address\":\":9187\"}\r\n2023-11-19T21:32:43.411925713+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"address\":\":8000\"}\r\n2023-11-19T21:32:43.509015899+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n2023-11-19T21:32:43.509183585+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"syncReplicas\":-1,\"minSyncReplicas\":0,\"maxSyncReplicas\":0}\r\n2023-11-19T21:32:43.516098948+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"syncReplicas\":-1,\"minSyncReplicas\":0,\"maxSyncReplicas\":0}\r\n2023-11-19T21:32:43.516191657+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Cluster status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"home-assistant-cnpg-main\",\"namespace\":\"ix-home-assistant\"},\"namespace\":\"ix-home-assistant\",\"name\":\"home-assistant-cnpg-main\",\"reconcileID\":\"6aa12b26-3d85-4bce-ab55-393043955763\",\"uuid\":\"cab91103-871a-11ee-a73a-569df65b937e\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"currentPrimary\":\"home-assistant-cnpg-main-2\",\"targetPrimary\":\"home-assistant-cnpg-main-2\"}\r\n2023-11-19T21:32:43.516200129+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"This is an old primary instance, waiting for the switchover to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"home-assistant-cnpg-main\",\"namespace\":\"ix-home-assistant\"},\"namespace\":\"ix-home-assistant\",\"name\":\"home-assistant-cnpg-main\",\"reconcileID\":\"6aa12b26-3d85-4bce-ab55-393043955763\",\"uuid\":\"cab91103-871a-11ee-a73a-569df65b937e\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"currentPrimary\":\"home-assistant-cnpg-main-2\",\"targetPrimary\":\"home-assistant-cnpg-main-2\"}\r\n2023-11-19T21:32:43.516209684+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Switchover completed\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"home-assistant-cnpg-main\",\"namespace\":\"ix-home-assistant\"},\"namespace\":\"ix-home-assistant\",\"name\":\"home-assistant-cnpg-main\",\"reconcileID\":\"6aa12b26-3d85-4bce-ab55-393043955763\",\"uuid\":\"cab91103-871a-11ee-a73a-569df65b937e\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"targetPrimary\":\"home-assistant-cnpg-main-2\",\"currentPrimary\":\"home-assistant-cnpg-main-2\"}\r\n2023-11-19T21:32:43.516214754+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Waiting for the new primary to be available\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"primaryConnInfo\":\"host=home-assistant-cnpg-main-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=home-assistant-cnpg-main-1 sslmode=verify-ca dbname=postgres connect_timeout=5\"}\r\n2023-11-19T21:32:43.517024583+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:32:48.518729154+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:48Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:32:53.522523437+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:53Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:32:58.524717990+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:58Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:33:03.525932349+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:03Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:33:07.729019395+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Received termination signal\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"signal\":\"interrupt\",\"smartShutdownTimeout\":180}\r\n2023-11-19T21:33:07.729053389+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Ignoring maxStopDelay <= smartShutdownTimeout\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"smartShutdownTimeout\":180,\"maxStopDelay\":30}\r\n2023-11-19T21:33:07.729057484+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Requesting fast shutdown of the PostgreSQL instance\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:33:07.729748539+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"logger\":\"pg_ctl\",\"msg\":\"pg_ctl: no server running\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:33:07.729772340+01:00 {\"level\":\"error\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Error while shutting down the PostgreSQL instance\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"error\":\"instance is not running\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*Instance).TryShuttingDownSmartFast\\n\\tpkg/management/postgres/instance.go:467\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:128\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/manager/runnable_group.go:223\"}\r\n2023-11-19T21:33:07.729782924+01:00 {\"level\":\"error\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"error while shutting down instance, proceeding\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"error\":\"instance is not running\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:129\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/manager/runnable_group.go:223\"}\r\n2023-11-19T21:33:07.729830286+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\r\n2023-11-19T21:33:07.729842408+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Stopping and waiting for leader election runnables\"}\r\n2023-11-19T21:33:07.729857975+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n2023-11-19T21:33:07.729861345+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:33:07.729867151+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:33:07.729869784+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:33:07.729873597+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:33:07.729901980+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"address\":\"localhost:8010\"}\r\n2023-11-19T21:33:07.729922227+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"address\":\":8000\"}\r\n2023-11-19T21:33:07.729926202+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"address\":\":9187\"}\r\n2023-11-19T21:33:08.530726633+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:08Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:33:13.532147143+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:13Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:33:18.534983261+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:18Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:33:23.539351478+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:23Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:33:28.541414600+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:28Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:33:33.542925224+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:33Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:33:37.730059137+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:37Z\",\"msg\":\"Stopping and waiting for caches\"}\r\n2023-11-19T21:33:37.730090271+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:37Z\",\"msg\":\"Stopping and waiting for webhooks\"}\r\n2023-11-19T21:33:37.730094734+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:37Z\",\"msg\":\"Stopping and waiting for HTTP servers\"}\r\n2023-11-19T21:33:37.730115772+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:37Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\r\n2023-11-19T21:33:37.730333973+01:00 {\"level\":\"error\",\"ts\":\"2023-11-19T20:33:37Z\",\"logger\":\"setup\",\"msg\":\"unable to run controller-runtime manager\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"error\":\"failed waiting for all runnables to end within grace period of 30s: context deadline exceeded\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.runSubCommand\\n\\tinternal/cmd/manager/instance/run/cmd.go:252\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.NewCmd.func2.1\\n\\tinternal/cmd/manager/instance/run/cmd.go:87\\nk8s.io/client-go/util/retry.OnError.func1\\n\\tpkg/mod/k8s.io/client-go@v0.28.3/util/retry/util.go:51\\nk8s.io/apimachinery/pkg/util/wait.runConditionWithCrashProtection\\n\\tpkg/mod/k8s.io/apimachinery@v0.28.3/pkg/util/wait/wait.go:145\\nk8s.io/apimachinery/pkg/util/wait.ExponentialBackoff\\n\\tpkg/mod/k8s.io/apimachinery@v0.28.3/pkg/util/wait/backoff.go:461\\nk8s.io/client-go/util/retry.OnError\\n\\tpkg/mod/k8s.io/client-go@v0.28.3/util/retry/util.go:50\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.NewCmd.func2\\n\\tinternal/cmd/manager/instance/run/cmd.go:86\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.3/x64/src/runtime/proc.go:267\"}\r\n2023-11-19T21:33:37.730354036+01:00 Error: unretryable: failed waiting for all runnables to end within grace period of 30s: context deadline exceeded\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\nkjeld@truecharts.org\r\n### Version\r\n1.21.1\r\n### What version of Kubernetes are you using?\r\n1.28\r\n### What is your Kubernetes environment?\r\nSelf-managed: k3s\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nWhen hibernate is set through label on the cluster resource, it gets ignored if the cluster is still in a not-ready state:\r\n```\r\n    - lastTransitionTime: \"2023-11-15T23:46:59Z\"\r\n      message: Cluster Is Not Ready\r\n      reason: ClusterIsNotReady\r\n      status: \"False\"\r\n      type: Ready\r\n```\r\nIt keeps trying to create a primary pod, even though it fails.\r\nIt would be prefered if all pods, even those not functional, would be stopped and kept-stopped when declarative hibernation is set.\r\n### Cluster resource\r\n```shell\r\nroot@truenas[~]# k get cluster -n ix-home-assistant -o yaml\r\napiVersion: v1\r\nitems:\r\n- apiVersion: postgresql.cnpg.io/v1\r\n  kind: Cluster\r\n  metadata:\r\n    annotations:\r\n      cnpg.io/hibernation: \"on\"\r\n      meta.helm.sh/release-name: home-assistant\r\n      meta.helm.sh/release-namespace: ix-home-assistant\r\n    creationTimestamp: \"2023-10-16T22:04:38Z\"\r\n    generation: 2\r\n    labels:\r\n      app: home-assistant-20.0.23\r\n      app.kubernetes.io/instance: home-assistant\r\n      app.kubernetes.io/managed-by: Helm\r\n      app.kubernetes.io/name: home-assistant\r\n      app.kubernetes.io/version: 2023.11.2\r\n      cnpg.io/reload: \"on\"\r\n      helm-revision: \"29\"\r\n      helm.sh/chart: home-assistant-20.0.23\r\n      release: home-assistant\r\n    name: home-assistant-cnpg-main\r\n    namespace: ix-home-assistant\r\n    resourceVersion: \"665128\"\r\n    uid: 51762b22-0f74-4bb7-ae11-73ee87a0b5b3\r\n  spec:\r\n    affinity:\r\n      podAntiAffinityType: preferred\r\n    bootstrap:\r\n      initdb:\r\n        database: home-assistant\r\n        encoding: UTF8\r\n        localeCType: C\r\n        localeCollate: C\r\n        owner: home-assistant\r\n        secret:\r\n          name: home-assistant-cnpg-main-user\r\n    enableSuperuserAccess: true\r\n    failoverDelay: 0\r\n    imageName: ghcr.io/cloudnative-pg/postgresql:15.3\r\n    instances: 2\r\n    logLevel: info\r\n    maxSyncReplicas: 0\r\n    minSyncReplicas: 0\r\n    monitoring:\r\n      customQueriesConfigMap:\r\n      - key: queries\r\n        name: cnpg-default-monitoring\r\n      disableDefaultQueries: false\r\n      enablePodMonitor: true\r\n    nodeMaintenanceWindow:\r\n      inProgress: false\r\n      reusePVC: true\r\n    postgresGID: 26\r\n    postgresUID: 26\r\n    postgresql:\r\n      parameters:\r\n        archive_mode: \"on\"\r\n        archive_timeout: 5min\r\n        dynamic_shared_memory_type: posix\r\n        log_destination: csvlog\r\n        log_directory: /controller/log\r\n        log_filename: postgres\r\n        log_rotation_age: \"0\"\r\n        log_rotation_size: \"0\"\r\n        log_truncate_on_rotation: \"false\"\r\n        logging_collector: \"on\"\r\n        max_parallel_workers: \"32\"\r\n        max_replication_slots: \"32\"\r\n        max_worker_processes: \"32\"\r\n        shared_memory_type: mmap\r\n        shared_preload_libraries: \"\"\r\n        wal_keep_size: 512MB\r\n        wal_receiver_timeout: 5s\r\n        wal_sender_timeout: 5s\r\n      syncReplicaElectionConstraint:\r\n        enabled: false\r\n    primaryUpdateMethod: restart\r\n    primaryUpdateStrategy: unsupervised\r\n    replicationSlots:\r\n      highAvailability:\r\n        enabled: true\r\n        slotPrefix: _cnpg_\r\n      updateInterval: 30\r\n    resources:\r\n      limits:\r\n        cpu: \"4\"\r\n        memory: 8Gi\r\n      requests:\r\n        cpu: 10m\r\n        memory: 50Mi\r\n    smartShutdownTimeout: 180\r\n    startDelay: 30\r\n    stopDelay: 30\r\n    storage:\r\n      pvcTemplate:\r\n        accessModes:\r\n        - ReadWriteOnce\r\n        resources:\r\n          requests:\r\n            storage: 256Gi\r\n        storageClassName: ix-storage-class-home-assistant\r\n      resizeInUseVolumes: true\r\n    switchoverDelay: 40000000\r\n    walStorage:\r\n      pvcTemplate:\r\n        accessModes:\r\n        - ReadWriteOnce\r\n        resources:\r\n          requests:\r\n            storage: 256Gi\r\n        storageClassName: ix-storage-class-home-assistant\r\n      resizeInUseVolumes: true\r\n  status:\r\n    certificates:\r\n      clientCASecret: home-assistant-cnpg-main-ca\r\n      expirations:\r\n        home-assistant-cnpg-main-ca: 2024-01-14 21:59:38 +0000 UTC\r\n        home-assistant-cnpg-main-replication: 2024-01-14 21:59:38 +0000 UTC\r\n        home-assistant-cnpg-main-server: 2024-01-14 21:59:38 +0000 UTC\r\n      replicationTLSSecret: home-assistant-cnpg-main-replication\r\n      serverAltDNSNames:\r\n      - home-assistant-cnpg-main-rw\r\n      - home-assistant-cnpg-main-rw.ix-home-assistant\r\n      - home-assistant-cnpg-main-rw.ix-home-assistant.svc\r\n      - home-assistant-cnpg-main-r\r\n      - home-assistant-cnpg-main-r.ix-home-assistant\r\n      - home-assistant-cnpg-main-r.ix-home-assistant.svc\r\n      - home-assistant-cnpg-main-ro\r\n      - home-assistant-cnpg-main-ro.ix-home-assistant\r\n      - home-assistant-cnpg-main-ro.ix-home-assistant.svc\r\n      serverCASecret: home-assistant-cnpg-main-ca\r\n      serverTLSSecret: home-assistant-cnpg-main-server\r\n    cloudNativePGCommitHash: 27f62cac\r\n    cloudNativePGOperatorHash: 4912e5eb808aca3bf134923625f2346d404a3c860cf24aca4266493846f8fc3b\r\n    conditions:\r\n    - lastTransitionTime: \"2023-11-15T23:46:59Z\"\r\n      message: Cluster Is Not Ready\r\n      reason: ClusterIsNotReady\r\n      status: \"False\"\r\n      type: Ready\r\n    - lastTransitionTime: \"2023-11-07T14:41:05Z\"\r\n      message: Continuous archiving is working\r\n      reason: ContinuousArchivingSuccess\r\n      status: \"True\"\r\n      type: ContinuousArchiving\r\n    configMapResourceVersion:\r\n      metrics:\r\n        cnpg-default-monitoring: \"586854\"\r\n    currentPrimary: home-assistant-cnpg-main-2\r\n    currentPrimaryTimestamp: \"2023-11-15T23:49:17.670117Z\"\r\n    danglingPVC:\r\n    - home-assistant-cnpg-main-2\r\n    - home-assistant-cnpg-main-2-wal\r\n    healthyPVC:\r\n    - home-assistant-cnpg-main-1\r\n    - home-assistant-cnpg-main-1-wal\r\n    instanceNames:\r\n    - home-assistant-cnpg-main-1\r\n    - home-assistant-cnpg-main-2\r\n    instances: 2\r\n    instancesReportedState:\r\n      home-assistant-cnpg-main-1:\r\n        isPrimary: false\r\n    instancesStatus:\r\n      replicating:\r\n      - home-assistant-cnpg-main-1\r\n    latestGeneratedNode: 2\r\n    managedRolesStatus: {}\r\n    phase: Waiting for the instances to become active\r\n    phaseReason: Some instances are not yet active. Please wait.\r\n    poolerIntegrations:\r\n      pgBouncerIntegration:\r\n        secrets:\r\n        - home-assistant-cnpg-main-pooler\r\n    pvcCount: 4\r\n    readService: home-assistant-cnpg-main-r\r\n    secretsResourceVersion:\r\n      applicationSecretVersion: \"665099\"\r\n      clientCaSecretVersion: \"13830\"\r\n      replicationSecretVersion: \"13835\"\r\n      serverCaSecretVersion: \"13830\"\r\n      serverSecretVersion: \"13832\"\r\n      superuserSecretVersion: \"13836\"\r\n    targetPrimary: home-assistant-cnpg-main-2\r\n    targetPrimaryTimestamp: \"2023-11-15T23:49:16.984189Z\"\r\n    timelineID: 2\r\n    topology:\r\n      instances:\r\n        home-assistant-cnpg-main-1: {}\r\n      nodesUsed: 1\r\n      successfullyExtracted: true\r\n    writeService: home-assistant-cnpg-main-rw\r\nkind: List\r\nmetadata:\r\n  resourceVersion: \"\"\r\n```\r\n### Relevant log output\r\n```\r\n2023-11-19T21:32:43.405985829+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"version\":\"1.21.1\",\"build\":{\"Version\":\"1.21.1\",\"Commit\":\"27f62cac\",\"Date\":\"2023-11-03\"}}\r\n2023-11-19T21:32:43.408711398+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:32:43.408873981+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n2023-11-19T21:32:43.408893264+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n2023-11-19T21:32:43.409709897+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"address\":\"localhost:8010\"}\r\n2023-11-19T21:32:43.411464114+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:32:43.411478511+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:32:43.411912702+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"address\":\":9187\"}\r\n2023-11-19T21:32:43.411925713+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"address\":\":8000\"}\r\n2023-11-19T21:32:43.509015899+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n2023-11-19T21:32:43.509183585+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"syncReplicas\":-1,\"minSyncReplicas\":0,\"maxSyncReplicas\":0}\r\n2023-11-19T21:32:43.516098948+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Ignore minSyncReplicas to enforce self-healing\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"syncReplicas\":-1,\"minSyncReplicas\":0,\"maxSyncReplicas\":0}\r\n2023-11-19T21:32:43.516191657+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Cluster status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"home-assistant-cnpg-main\",\"namespace\":\"ix-home-assistant\"},\"namespace\":\"ix-home-assistant\",\"name\":\"home-assistant-cnpg-main\",\"reconcileID\":\"6aa12b26-3d85-4bce-ab55-393043955763\",\"uuid\":\"cab91103-871a-11ee-a73a-569df65b937e\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"currentPrimary\":\"home-assistant-cnpg-main-2\",\"targetPrimary\":\"home-assistant-cnpg-main-2\"}\r\n2023-11-19T21:32:43.516200129+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"This is an old primary instance, waiting for the switchover to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"home-assistant-cnpg-main\",\"namespace\":\"ix-home-assistant\"},\"namespace\":\"ix-home-assistant\",\"name\":\"home-assistant-cnpg-main\",\"reconcileID\":\"6aa12b26-3d85-4bce-ab55-393043955763\",\"uuid\":\"cab91103-871a-11ee-a73a-569df65b937e\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"currentPrimary\":\"home-assistant-cnpg-main-2\",\"targetPrimary\":\"home-assistant-cnpg-main-2\"}\r\n2023-11-19T21:32:43.516209684+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Switchover completed\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"home-assistant-cnpg-main\",\"namespace\":\"ix-home-assistant\"},\"namespace\":\"ix-home-assistant\",\"name\":\"home-assistant-cnpg-main\",\"reconcileID\":\"6aa12b26-3d85-4bce-ab55-393043955763\",\"uuid\":\"cab91103-871a-11ee-a73a-569df65b937e\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"targetPrimary\":\"home-assistant-cnpg-main-2\",\"currentPrimary\":\"home-assistant-cnpg-main-2\"}\r\n2023-11-19T21:32:43.516214754+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"Waiting for the new primary to be available\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"primaryConnInfo\":\"host=home-assistant-cnpg-main-rw user=streaming_replica port=5432 sslkey=/controller/certificates/streaming_replica.key sslcert=/controller/certificates/streaming_replica.crt sslrootcert=/controller/certificates/server-ca.crt application_name=home-assistant-cnpg-main-1 sslmode=verify-ca dbname=postgres connect_timeout=5\"}\r\n2023-11-19T21:32:43.517024583+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:43Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:32:48.518729154+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:48Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:32:53.522523437+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:53Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:32:58.524717990+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:32:58Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:33:03.525932349+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:03Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:33:07.729019395+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Received termination signal\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"signal\":\"interrupt\",\"smartShutdownTimeout\":180}\r\n2023-11-19T21:33:07.729053389+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Ignoring maxStopDelay <= smartShutdownTimeout\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"smartShutdownTimeout\":180,\"maxStopDelay\":30}\r\n2023-11-19T21:33:07.729057484+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Requesting fast shutdown of the PostgreSQL instance\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:33:07.729748539+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"logger\":\"pg_ctl\",\"msg\":\"pg_ctl: no server running\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:33:07.729772340+01:00 {\"level\":\"error\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Error while shutting down the PostgreSQL instance\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"error\":\"instance is not running\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/postgres.(*Instance).TryShuttingDownSmartFast\\n\\tpkg/management/postgres/instance.go:467\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:128\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/manager/runnable_group.go:223\"}\r\n2023-11-19T21:33:07.729782924+01:00 {\"level\":\"error\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"error while shutting down instance, proceeding\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"error\":\"instance is not running\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:129\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/manager/runnable_group.go:223\"}\r\n2023-11-19T21:33:07.729830286+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\r\n2023-11-19T21:33:07.729842408+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Stopping and waiting for leader election runnables\"}\r\n2023-11-19T21:33:07.729857975+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n2023-11-19T21:33:07.729861345+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:33:07.729867151+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:33:07.729869784+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:33:07.729873597+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"home-assistant-cnpg-main-1\"}\r\n2023-11-19T21:33:07.729901980+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"address\":\"localhost:8010\"}\r\n2023-11-19T21:33:07.729922227+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"address\":\":8000\"}\r\n2023-11-19T21:33:07.729926202+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:07Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"address\":\":9187\"}\r\n2023-11-19T21:33:08.530726633+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:08Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:33:13.532147143+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:13Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:33:18.534983261+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:18Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:33:23.539351478+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:23Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:33:28.541414600+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:28Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:33:33.542925224+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:33Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"err\":\"failed to connect to `host=home-assistant-cnpg-main-rw user=streaming_replica database=postgres`: dial error (dial tcp 172.17.158.174:5432: connect: connection refused)\"}\r\n2023-11-19T21:33:37.730059137+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:37Z\",\"msg\":\"Stopping and waiting for caches\"}\r\n2023-11-19T21:33:37.730090271+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:37Z\",\"msg\":\"Stopping and waiting for webhooks\"}\r\n2023-11-19T21:33:37.730094734+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:37Z\",\"msg\":\"Stopping and waiting for HTTP servers\"}\r\n2023-11-19T21:33:37.730115772+01:00 {\"level\":\"info\",\"ts\":\"2023-11-19T20:33:37Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\r\n2023-11-19T21:33:37.730333973+01:00 {\"level\":\"error\",\"ts\":\"2023-11-19T20:33:37Z\",\"logger\":\"setup\",\"msg\":\"unable to run controller-runtime manager\",\"logging_pod\":\"home-assistant-cnpg-main-1\",\"error\":\"failed waiting for all runnables to end within grace period of 30s: context deadline exceeded\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.runSubCommand\\n\\tinternal/cmd/manager/instance/run/cmd.go:252\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.NewCmd.func2.1\\n\\tinternal/cmd/manager/instance/run/cmd.go:87\\nk8s.io/client-go/util/retry.OnError.func1\\n\\tpkg/mod/k8s.io/client-go@v0.28.3/util/retry/util.go:51\\nk8s.io/apimachinery/pkg/util/wait.runConditionWithCrashProtection\\n\\tpkg/mod/k8s.io/apimachinery@v0.28.3/pkg/util/wait/wait.go:145\\nk8s.io/apimachinery/pkg/util/wait.ExponentialBackoff\\n\\tpkg/mod/k8s.io/apimachinery@v0.28.3/pkg/util/wait/backoff.go:461\\nk8s.io/client-go/util/retry.OnError\\n\\tpkg/mod/k8s.io/client-go@v0.28.3/util/retry/util.go:50\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run.NewCmd.func2\\n\\tinternal/cmd/manager/instance/run/cmd.go:86\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.3/x64/src/runtime/proc.go:267\"}\r\n2023-11-19T21:33:37.730354036+01:00 Error: unretryable: failed waiting for all runnables to end within grace period of 30s: context deadline exceeded\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductHi @Ornias1993 \r\nThe declarative hibernation feature saves CPU power by removing database Pods while retaining the database PVCs. Its general use is for batch processing tasks; the instance only needs to be active when the task is triggered. Therefore, declarative hibernation is not intended as a solution for resolving running issues\n---\n> Hi @Ornias1993 The declarative hibernation feature saves CPU power by removing database Pods while retaining the database PVCs. Its general use is for batch processing tasks; the instance only needs to be active when the task is triggered. Therefore, declarative hibernation is not intended as a solution for resolving running issues\r\nCorrect ofcoarse, but we also use it to ensure (error or otherwise) all pods in a deployment are stopped to... save CPU/IOPS power. The only clean way to do this with CNPG (without wiping PVCs) is with hibernation.\r\nBasically we need to be able to use something to turn the pods of, to ensure we can \"look at this later\", regardless of weither there is an error or not.\r\nWith hibernation on, it shouldn't keep trying to start. Regardless of weither there is an error or not.\r\nSo, simply put, at this time it's the only tool you give us to do this and it's behavior is not as expected (hybernate should not require a start AFTER toggling hibernation before it starts hibernating, unless documented, the expected result is... it would try ti hibernate right away.)\n---\nCan you please try again with 1.21.1? Also, please fix the `stopDelay` value.\n---\n> Can you please try again with 1.21.1? Also, please fix the `stopDelay` value.\r\nI think OP made a typo on the version, logs indicate that `1.21.1` is already in place!\r\nNice catch on the stopDelay tho!\n---\n> > Can you please try again with 1.21.1? Also, please fix the `stopDelay` value.\r\n> \r\n> I think OP made a typo on the version, logs indicate that `1.21.1` is already in place! Nice catch on the stopDelay tho!\r\nIt's a third-party log, not mine...\r\nBut yes its 1.21.1...\r\nSame behavior across all versions though...\r\nstopDelay is not set by us...\r\nThat must be the operator doing."
    },
    {
        "title": "Copy edits to kubernetes upgrade and kubectl plugin topics",
        "id": 1999519942,
        "state": "no reaction",
        "first": "",
        "messages": ""
    },
    {
        "title": "[Bug]: graceful shutdown not being set on manager",
        "id": 1995466868,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njohn.long@enterprisedb.com\n### Version\n1.21.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nI noticed that the smart shutdown is not being applied to the controller manager. This could cause hard stops to happen as the controller manager will still default to 30s if this is not implemented. \n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\njohn.long@enterprisedb.com\n### Version\n1.21.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nI noticed that the smart shutdown is not being applied to the controller manager. This could cause hard stops to happen as the controller manager will still default to 30s if this is not implemented. \n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductThe fix for this issue has been reverted in PR #3590 because it introduces a regression.\r\nWe need to discuss more about what we need to do to fix this issue."
    },
    {
        "title": "feat: add support for VolumeGroupSnapshots",
        "id": 1994366848,
        "state": "open",
        "first": "VolumeGroupSnapshot is a Kubernetes API that allows users to take a crash consistent snapshot of multiple volumes together, at the sime point in time, with write order consistency.\r\nThis patch add support for the VolumeGroupSnapshots API when we are taking a volume snapshot backup.\r\nCloses #4680",
        "messages": "VolumeGroupSnapshot is a Kubernetes API that allows users to take a crash consistent snapshot of multiple volumes together, at the sime point in time, with write order consistency.\r\nThis patch add support for the VolumeGroupSnapshots API when we are taking a volume snapshot backup.\r\nCloses #4680E2e: https://github.com/EnterpriseDB/cloudnative-pg/actions/runs/7207784765\n---\nRelated to #4680"
    },
    {
        "title": "[Bug]: Node fails to start after reboot",
        "id": 1994187669,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.20.3\n### What version of Kubernetes are you using?\n1.26\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nAfter rebooting one of the nodes, it will not start.\r\nI tried to read something from the logs but I cannot find the reason (and a way to fix it).\n### Cluster resource\n_No response_\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"pgsql15-3\",\"version\":\"1.20.2\",\"build\":{\"Version\":\"1.20.2\",\"Commit\":\"6f7f10b7\",\"Date\":\"2023-07-27\"}}\r\n2023-11-15T08:03:42.742170040+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:42.742336360+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n2023-11-15T08:03:42.742711670+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"pgsql15-3\",\"address\":\":9187\"}\r\n2023-11-15T08:03:42.743284866+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:42.743308185+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"logger\":\"roles_reconciler\",\"msg\":\"skipping the RoleSynchronizer in replicas\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:42.743323992+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"pgsql15-3\",\"address\":\"localhost:8010\"}\r\n2023-11-15T08:03:42.743542146+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:42.743566374+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"pgsql15-3\",\"address\":\":8000\"}\r\n2023-11-15T08:03:42.845851217+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Found previous run flag\",\"logging_pod\":\"pgsql15-3\",\"filename\":\"/var/lib/postgresql/data/pgdata/cnpg_initialized-pgsql15-3\"}\r\n2023-11-15T08:03:42.932422176+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"pgsql15-3\",\"reason\":\"postmaster start up\"}\r\n2023-11-15T08:03:42.934574796+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202209061\\nDatabase system identifier:           7227586665997950999\\nDatabase cluster state:               shut down in recovery\\npg_control last modified:             Tue 14 Nov 2023 10:39:59 PM UTC\\nLatest checkpoint location:           CA/33010BB8\\nLatest checkpoint's REDO location:    CA/33010B80\\nLatest checkpoint's REDO WAL file:    0000000A000000CA00000033\\nLatest checkpoint's TimeLineID:       10\\nLatest checkpoint's PrevTimeLineID:   10\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:714194\\nLatest checkpoint's NextOID:          50629\\nLatest checkpoint's NextMultiXactId:  3144\\nLatest checkpoint's NextMultiOffset:  6295\\nLatest checkpoint's oldestXID:        716\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  714194\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Mon 13 Nov 2023 07:06:30 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     CA/35000000\\nMin recovery ending loc's timeline:   10\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes\r\n setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            1527c7c996593b2ba147d001a5fd50051eefe2ee208395d490d357f44029e3f1\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:42.934651561+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"The PID file content is wrong, deleting it and assuming it's stale\",\"file\":\"/var/lib/postgresql/data/pgdata/postmaster.pid\",\"logging_pod\":\"pgsql15-3\",\"err\":\"file does not exist\",\"pidFileContents\":\"\"}\r\n2023-11-15T08:03:42.996000544+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgsql15\",\"namespace\":\"services-postgresql\"},\"namespace\":\"services-postgresql\",\"name\":\"pgsql15\",\"reconcileID\":\"de892087-34d7-4b8f-a44f-ee40c57c1227\",\"uuid\":\"1c94aeca-8385-11ee-b613-5e21884c1f50\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:43.005810907+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"2023-11-15 08:03:43.005 CET [24] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:43.005862850+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"2023-11-15 08:03:43.005 CET [24] HINT:  Future log output will appear \r\nin directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:43.007300860+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.005 CET\",\"process_id\":\"24\",\"session_id\":\"65546d4e.18\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:42 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.007345782+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.005 CET\",\"process_id\":\"24\",\"session_id\":\"65546d4e.18\",\"session_line_num\":\"2\",\"session_start_time\":\"2023-11-15 08:03:42 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 15.4 (Debian 15.4-2.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.007357171+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.005 CET\",\"process_id\":\"24\",\"session_id\":\"65546d4e.18\",\"session_line_num\":\"3\",\"session_start_time\":\"2023-11-15 08:03:42 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.007366174+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.005 CET\",\"process_id\":\"24\",\"session_id\":\"65546d4e.18\",\"session_line_num\":\"4\",\"session_start_time\":\"2023-11-15 08:03:42 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listenin\r\ng on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.007455479+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"2023-11-15 08:03:43.005 CET [24] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:43.007503716+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"2023-11-15 08:03:43.005 CET [24] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:43.008916700+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.008 CET\",\"process_id\":\"24\",\"session_id\":\"65546d4e.18\",\"session_line_num\":\"5\",\"session_start_time\":\"2023-11-15 08:03:42 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.018218862+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.017 CET\",\"process_id\":\"28\",\"session_id\":\"65546d4f.1c\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was shut down in recovery at 2023-11-14 23:39:59 CET\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.438842275+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.438 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"42\",\"connection_from\":\"[local]\",\"session_id\":\"65546d4f.2a\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"th\r\ne database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:43.457027227+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.456 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"43\",\"connection_from\":\"[local]\",\"session_id\":\"65546d4f.2b\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.457071559+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"msg\":\"Readiness probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:43.719088641+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:43.719264298+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.718 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"44\",\"connection_from\":\"[local]\",\"session_id\":\"65546d4f.2c\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.733528651+01:00 {\"\r\nlevel\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:43.733631521+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.733 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"45\",\"connection_from\":\"[local]\",\"session_id\":\"65546d4f.2d\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.789723364+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:43.790056753+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.789 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"46\",\"connection_from\":\"[local]\",\"session_id\":\"65546d4f.2e\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.875876807+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000B.history\",\"startTime\":\"2023-11-15T07:03:43Z\",\"endTime\":\"2023-11-15T07:03:43Z\",\"elapsedWalTime\":0.661408141}\r\n2023-11-15T08:03:43.875926647+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"wa\r\nl-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000B.history\",\"maxParallel\":1,\"successfulWalRestore\":1,\"failedWalRestore\":0,\"endOfWALStream\":false,\"startTime\":\"2023-11-15T07:03:43Z\",\"downloadStartTime\":\"2023-11-15T07:03:43Z\",\"downloadTotalTime\":0.661714141,\"totalTime\":0.822255043}\r\n2023-11-15T08:03:43.879204398+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.878 CET\",\"process_id\":\"28\",\"session_id\":\"65546d4f.1c\",\"session_line_num\":\"2\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000B.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:44.050955884+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:44.051059686+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:44.050 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"59\",\"connection_from\":\"[local]\",\"session_id\":\"65546d50.3b\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:44 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:44.062130205+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"msg\":\"Readiness probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:44.062987695+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44\r\nZ\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:44.061 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"60\",\"connection_from\":\"[local]\",\"session_id\":\"65546d50.3c\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:44 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:44.134314867+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:44.133 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"63\",\"connection_from\":\"[local]\",\"session_id\":\"65546d50.3f\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:44 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:44.137014323+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgsql15\",\"namespace\":\"services-postgresql\"},\"namespace\":\"services-postgresql\",\"name\":\"pgsql15\",\"reconcileID\":\"87c09a80-919d-4198-8207-1028f570a1bf\",\"uuid\":\"1d4434ec-8385-11ee-b613-5e21884c1f50\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:44.137105474+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:44.136 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"64\",\"connection_from\":\"[local]\",\"session_id\":\"65546d50.40\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:44 CET\",\"transaction_id\":\"0\",\"error_sev\r\nerity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:44.392636792+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"msg\":\"Unable to collect metrics\",\"logging_pod\":\"pgsql15-3\",\"error\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:44.392751926+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:44.392 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"65\",\"connection_from\":\"[local]\",\"session_id\":\"65546d50.41\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:44 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:44.394269768+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:44.393 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"66\",\"connection_from\":\"[local]\",\"session_id\":\"65546d50.42\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:44 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"msg\":\"Readiness probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:44.462981406+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:44.462 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"67\",\"connection_from\":\"[local]\",\"session_id\":\"65546d50.43\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:44 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000C.history\",\"startTime\":\"2023-11-15T07:03:44Z\",\"endTime\":\"2023-11-15T07:03:44Z\",\"elapsedWalTime\":0.700228542}\r\n2023-11-15T08:03:44.779809781+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000C.history\",\"maxParallel\":1,\"successfulWalRestore\":1,\"failedWalRestore\":0,\"endOfWALStream\":false,\"startTime\":\"2023-11-15T07:03:43Z\",\"downloadStartTime\":\"2023-11-15T07:03:44Z\",\"downloadTotalTime\":0.700498522,\"totalTime\":0.855732874}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:44.783 CET\",\"process_id\":\"28\",\"session_id\":\"65546d4f.1c\",\"session_line_num\":\"3\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000C.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:45.254 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"82\",\"connection_from\":\"[local]\",\"session_id\":\"65546d51.52\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:45 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgsql15\",\"namespace\":\"services-postgresql\"},\"namespace\":\"services-postgresql\",\"name\":\"pgsql15\",\"reconcileID\":\"49ed8d65-a36d-48f6-91fd-ae55dde738d8\",\"uuid\":\"1df24ed2-8385-11ee-b613-5e21884c1f50\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:45.258140790+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:45.257 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"83\",\"connection_from\":\"[local]\",\"session_id\":\"65546d51.53\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:45 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:45.361506337+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:45.360 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"84\",\"connection_from\":\"[local]\",\"session_id\":\"65546d51.54\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:45 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000D.history\",\"options\":[\"--endpoint-url\",\"https://backup1.services-minio.svc:9000\",\"--cloud-provider\",\"aws-s3\",\"s3://pgsql15-backups\",\"pgsql15\"],\"startTime\":\"2023-11-15T07:03:44Z\",\"endTime\":\"2023-11-15T07:03:45Z\",\"elapsedWalTime\":0.620937751}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:45.645 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"85\",\"connection_from\":\"[local]\",\"session_id\":\"65546d51.55\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:45 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:45.663213320+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:45.662 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"86\",\"connection_from\":\"[local]\",\"session_id\":\"65546d51.56\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:45 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:45.695 CET\",\"process_id\":\"28\",\"session_id\":\"65546d4f.1c\",\"session_line_num\":\"4\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"entering standby mode\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:45.726016581+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:45.725 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"97\",\"connection_from\":\"[local]\",\"session_id\":\"65546d51.61\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:45 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:45.993841326+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:45.993 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"101\",\"connection_from\":\"[local]\",\"session_id\":\"65546d51.65\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:45 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:46Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:46.378 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"103\",\"connection_from\":\"[local]\",\"session_id\":\"65546d52.67\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:46 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:46Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgsql15\",\"namespace\":\"services-postgresql\"},\"namespace\":\"services-postgresql\",\"name\":\"pgsql15\",\"reconcileID\":\"f74cdc83-913f-4003-ba51-780e5e02b843\",\"uuid\":\"1e9d6b01-8385-11ee-b613-5e21884c1f50\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:46Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:46.381 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"104\",\"connection_from\":\"[local]\",\"session_id\":\"65546d52.68\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:46 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:46Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000C.history\",\"startTime\":\"2023-11-15T07:03:45Z\",\"endTime\":\"2023-11-15T07:03:46Z\",\"elapsedWalTime\":0.622316712}\r\n2023-11-15T08:03:46.494627026+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:46Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000C.history\",\"maxParallel\":1,\"successfulWalRestore\":1,\"failedWalRestore\":0,\"endOfWALStream\":false,\"startTime\":\"2023-11-15T07:03:45Z\",\"downloadStartTime\":\"2023-11-15T07:03:45Z\",\"downloadTotalTime\":0.622730058,\"totalTime\":0.76213108}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:46Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:46.497 CET\",\"process_id\":\"28\",\"session_id\":\"65546d4f.1c\",\"session_line_num\":\"5\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000C.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:47.349903448+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:47.349 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"118\",\"connection_from\":\"[local]\",\"session_id\":\"65546d53.76\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:47 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000A000000CA00000033\",\"startTime\":\"2023-11-15T07:03:46Z\",\"endTime\":\"2023-11-15T07:03:47Z\",\"elapsedWalTime\":0.783000987}\r\n2023-11-15T08:03:47.483302502+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000A000000CA00000033\",\"maxParallel\":1,\"successfulWalRestore\":1,\"failedWalRestore\":0,\"endOfWALStream\":false,\"startTime\":\"2023-11-15T07:03:46Z\",\"downloadStartTime\":\"2023-11-15T07:03:46Z\",\"downloadTotalTime\":0.783424532,\"totalTime\":0.940842584}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:47.486 CET\",\"process_id\":\"28\",\"session_id\":\"65546d4f.1c\",\"session_line_num\":\"6\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000A000000CA00000033\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:47.500 CET\",\"process_id\":\"28\",\"session_id\":\"65546d4f.1c\",\"session_line_num\":\"7\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"XX000\",\"message\":\"requested timeline 12 does not contain minimum recovery point CA/35000000 on timeline 10\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:47.503 CET\",\"process_id\":\"24\",\"session_id\":\"65546d4e.18\",\"session_line_num\":\"6\",\"session_start_time\":\"2023-11-15 08:03:42 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 28) exited with exit code 1\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:47.503399532+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:47.503 CET\",\"process_id\":\"24\",\"session_id\":\"65546d4e.18\",\"session_line_num\":\"7\",\"session_start_time\":\"2023-11-15 08:03:42 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"aborting startup due to startup process failure\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:47.507 CET\",\"process_id\":\"24\",\"session_id\":\"65546d4e.18\",\"session_line_num\":\"8\",\"session_start_time\":\"2023-11-15 08:03:42 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"pgsql15-3\",\"reason\":\"postmaster has exited\"}\r\n2023-11-15T08:03:47.519806666+01:00 {\"level\":\"error\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"PostgreSQL process exited with errors\",\"logging_pod\":\"pgsql15-3\",\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:99\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/manager/runnable_group.go:219\"}\r\n2023-11-15T08:03:47.519892689+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\r\n2023-11-15T08:03:47.519975746+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Stopping and waiting for leader election runnables\"}\r\n2023-11-15T08:03:47.520245001+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"Replicator\",\"msg\":\"Terminated slot Replicator loop\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:47.520508729+01:00 {\"level\":\"error\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"error received after stop sequence was engaged\",\"error\":\"exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).engageStopProcedure.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/manager/internal.go:555\"}\r\n{\"level\":\"error\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Error while getting cluster CA Server secret\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgsql15\",\"namespace\":\"services-postgresql\"},\"namespace\":\"services-postgresql\",\"name\":\"pgsql15\",\"reconcileID\":\"0c8d5946-aff0-4735-a388-ed324ba1a852\",\"uuid\":\"1f48fefc-8385-11ee-b613-5e21884c1f50\",\"logging_pod\":\"pgsql15-3\",\"error\":\"Get \\\"https://10.43.0.1:443/api/v1/namespaces/services-postgresql/secrets/pgsql15-server-tls\\\": context canceled\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).RefreshSecrets\\n\\tinternal/management/controller/instance_controller.go:861\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).Reconcile\\n\\tinternal/management/controller/instance_controller.go:130\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:118\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:314\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:265\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:226\"}\r\n2023-11-15T08:03:47.521223502+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:47.521258168+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:47.521270186+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:47.521283563+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n2023-11-15T08:03:47.521294907+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:47.521358877+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"pgsql15-3\",\"address\":\":9187\"}\r\n2023-11-15T08:03:47.521372841+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"pgsql15-3\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"pgsql15-3\",\"address\":\":8000\"}\r\n2023-11-15T08:03:47.521418984+01:00 {\"level\":\"error\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Error while getting barman endpoint CA secret\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgsql15\",\"namespace\":\"services-postgresql\"},\"namespace\":\"services-postgresql\",\"name\":\"pgsql15\",\"reconcileID\":\"0c8d5946-aff0-4735-a388-ed324ba1a852\",\"uuid\":\"1f48fefc-8385-11ee-b613-5e21884c1f50\",\"logging_pod\":\"pgsql15-3\",\"error\":\"client rate limiter Wait returned an error: context canceled\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).RefreshSecrets\\n\\tinternal/management/controller/instance_controller.go:868\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).Reconcile\\n\\tinternal/management/controller/instance_controller.go:130\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:118\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:314\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:265\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:226\"}\r\n2023-11-15T08:03:47.521803404+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:             \r\n  202209061\\nDatabase system identifier:           7227586665997950999\\nDatabase cluster state:               shut down in recovery\\npg_control last modified:             Tue 14 Nov 2023 10:39:59 PM UTC\\nLatest checkpoint location:           CA/33010BB8\\nLatest checkpoint's REDO location:    CA/33010B80\\nLatest checkpoint's REDO WAL file:    0000000A000000CA00000033\\nLatest checkpoint's TimeLineID:       10\\nLatest checkpoint's PrevTimeLineID:   10\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:714194\\nLatest checkpoint's NextOID:          50629\\nLatest checkpoint's NextMultiXactId:  3144\\nLatest checkpoint's NextMultiOffset:  6295\\nLatest checkpoint's oldestXID:        716\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  714194\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Mon 13 Nov 2023 07:06:30 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     CA/35000000\\nMin recovery ending loc's timeline:   10\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDat\r\ne/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            1527c7c996593b2ba147d001a5fd50051eefe2ee208395d490d357f44029e3f1\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"pgsql15-3\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgsql15\",\"namespace\":\"services-postgresql\"},\"namespace\":\"services-postgresql\",\"name\":\"pgsql15\",\"reconcileID\":\"0c8d5946-aff0-4735-a388-ed324ba1a852\",\"uuid\":\"1f48fefc-8385-11ee-b613-5e21884c1f50\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:47.565078624+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n2023-11-15T08:03:47.565093879+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Stopping and waiting for caches\"}\r\n2023-11-15T08:03:47.565127252+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Stopping and waiting for webhooks\"}\r\n2023-11-15T08:03:47.565139524+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.20.3\n### What version of Kubernetes are you using?\n1.26\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nYAML manifest\n### What happened?\nAfter rebooting one of the nodes, it will not start.\r\nI tried to read something from the logs but I cannot find the reason (and a way to fix it).\n### Cluster resource\n_No response_\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"pgsql15-3\",\"version\":\"1.20.2\",\"build\":{\"Version\":\"1.20.2\",\"Commit\":\"6f7f10b7\",\"Date\":\"2023-07-27\"}}\r\n2023-11-15T08:03:42.742170040+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:42.742336360+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n2023-11-15T08:03:42.742711670+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"pgsql15-3\",\"address\":\":9187\"}\r\n2023-11-15T08:03:42.743284866+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:42.743308185+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"logger\":\"roles_reconciler\",\"msg\":\"skipping the RoleSynchronizer in replicas\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:42.743323992+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"pgsql15-3\",\"address\":\"localhost:8010\"}\r\n2023-11-15T08:03:42.743542146+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:42.743566374+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"pgsql15-3\",\"address\":\":8000\"}\r\n2023-11-15T08:03:42.845851217+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Found previous run flag\",\"logging_pod\":\"pgsql15-3\",\"filename\":\"/var/lib/postgresql/data/pgdata/cnpg_initialized-pgsql15-3\"}\r\n2023-11-15T08:03:42.932422176+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"pgsql15-3\",\"reason\":\"postmaster start up\"}\r\n2023-11-15T08:03:42.934574796+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202209061\\nDatabase system identifier:           7227586665997950999\\nDatabase cluster state:               shut down in recovery\\npg_control last modified:             Tue 14 Nov 2023 10:39:59 PM UTC\\nLatest checkpoint location:           CA/33010BB8\\nLatest checkpoint's REDO location:    CA/33010B80\\nLatest checkpoint's REDO WAL file:    0000000A000000CA00000033\\nLatest checkpoint's TimeLineID:       10\\nLatest checkpoint's PrevTimeLineID:   10\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:714194\\nLatest checkpoint's NextOID:          50629\\nLatest checkpoint's NextMultiXactId:  3144\\nLatest checkpoint's NextMultiOffset:  6295\\nLatest checkpoint's oldestXID:        716\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  714194\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Mon 13 Nov 2023 07:06:30 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     CA/35000000\\nMin recovery ending loc's timeline:   10\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes\r\n setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            1527c7c996593b2ba147d001a5fd50051eefe2ee208395d490d357f44029e3f1\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:42.934651561+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"The PID file content is wrong, deleting it and assuming it's stale\",\"file\":\"/var/lib/postgresql/data/pgdata/postmaster.pid\",\"logging_pod\":\"pgsql15-3\",\"err\":\"file does not exist\",\"pidFileContents\":\"\"}\r\n2023-11-15T08:03:42.996000544+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:42Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgsql15\",\"namespace\":\"services-postgresql\"},\"namespace\":\"services-postgresql\",\"name\":\"pgsql15\",\"reconcileID\":\"de892087-34d7-4b8f-a44f-ee40c57c1227\",\"uuid\":\"1c94aeca-8385-11ee-b613-5e21884c1f50\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:43.005810907+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"2023-11-15 08:03:43.005 CET [24] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:43.005862850+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"2023-11-15 08:03:43.005 CET [24] HINT:  Future log output will appear \r\nin directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:43.007300860+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.005 CET\",\"process_id\":\"24\",\"session_id\":\"65546d4e.18\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:42 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.007345782+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.005 CET\",\"process_id\":\"24\",\"session_id\":\"65546d4e.18\",\"session_line_num\":\"2\",\"session_start_time\":\"2023-11-15 08:03:42 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 15.4 (Debian 15.4-2.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.007357171+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.005 CET\",\"process_id\":\"24\",\"session_id\":\"65546d4e.18\",\"session_line_num\":\"3\",\"session_start_time\":\"2023-11-15 08:03:42 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.007366174+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.005 CET\",\"process_id\":\"24\",\"session_id\":\"65546d4e.18\",\"session_line_num\":\"4\",\"session_start_time\":\"2023-11-15 08:03:42 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listenin\r\ng on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.007455479+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"2023-11-15 08:03:43.005 CET [24] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:43.007503716+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"2023-11-15 08:03:43.005 CET [24] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:43.008916700+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.008 CET\",\"process_id\":\"24\",\"session_id\":\"65546d4e.18\",\"session_line_num\":\"5\",\"session_start_time\":\"2023-11-15 08:03:42 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.018218862+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.017 CET\",\"process_id\":\"28\",\"session_id\":\"65546d4f.1c\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was shut down in recovery at 2023-11-14 23:39:59 CET\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.438842275+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.438 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"42\",\"connection_from\":\"[local]\",\"session_id\":\"65546d4f.2a\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"th\r\ne database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:43.457027227+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.456 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"43\",\"connection_from\":\"[local]\",\"session_id\":\"65546d4f.2b\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.457071559+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"msg\":\"Readiness probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:43.719088641+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:43.719264298+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.718 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"44\",\"connection_from\":\"[local]\",\"session_id\":\"65546d4f.2c\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.733528651+01:00 {\"\r\nlevel\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:43.733631521+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.733 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"45\",\"connection_from\":\"[local]\",\"session_id\":\"65546d4f.2d\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.789723364+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:43.790056753+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.789 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"46\",\"connection_from\":\"[local]\",\"session_id\":\"65546d4f.2e\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:43.875876807+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000B.history\",\"startTime\":\"2023-11-15T07:03:43Z\",\"endTime\":\"2023-11-15T07:03:43Z\",\"elapsedWalTime\":0.661408141}\r\n2023-11-15T08:03:43.875926647+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"wa\r\nl-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000B.history\",\"maxParallel\":1,\"successfulWalRestore\":1,\"failedWalRestore\":0,\"endOfWALStream\":false,\"startTime\":\"2023-11-15T07:03:43Z\",\"downloadStartTime\":\"2023-11-15T07:03:43Z\",\"downloadTotalTime\":0.661714141,\"totalTime\":0.822255043}\r\n2023-11-15T08:03:43.879204398+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:43.878 CET\",\"process_id\":\"28\",\"session_id\":\"65546d4f.1c\",\"session_line_num\":\"2\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000B.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:44.050955884+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:44.051059686+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:44.050 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"59\",\"connection_from\":\"[local]\",\"session_id\":\"65546d50.3b\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:44 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:44.062130205+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"msg\":\"Readiness probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:44.062987695+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44\r\nZ\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:44.061 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"60\",\"connection_from\":\"[local]\",\"session_id\":\"65546d50.3c\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:44 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:44.134314867+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:44.133 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"63\",\"connection_from\":\"[local]\",\"session_id\":\"65546d50.3f\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:44 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:44.137014323+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgsql15\",\"namespace\":\"services-postgresql\"},\"namespace\":\"services-postgresql\",\"name\":\"pgsql15\",\"reconcileID\":\"87c09a80-919d-4198-8207-1028f570a1bf\",\"uuid\":\"1d4434ec-8385-11ee-b613-5e21884c1f50\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:44.137105474+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:44.136 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"64\",\"connection_from\":\"[local]\",\"session_id\":\"65546d50.40\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:44 CET\",\"transaction_id\":\"0\",\"error_sev\r\nerity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:44.392636792+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"msg\":\"Unable to collect metrics\",\"logging_pod\":\"pgsql15-3\",\"error\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:44.392751926+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:44.392 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"65\",\"connection_from\":\"[local]\",\"session_id\":\"65546d50.41\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:44 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:44.394269768+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:44.393 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"66\",\"connection_from\":\"[local]\",\"session_id\":\"65546d50.42\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:44 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"msg\":\"Readiness probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:44.462981406+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:44.462 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"67\",\"connection_from\":\"[local]\",\"session_id\":\"65546d50.43\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:44 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000C.history\",\"startTime\":\"2023-11-15T07:03:44Z\",\"endTime\":\"2023-11-15T07:03:44Z\",\"elapsedWalTime\":0.700228542}\r\n2023-11-15T08:03:44.779809781+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000C.history\",\"maxParallel\":1,\"successfulWalRestore\":1,\"failedWalRestore\":0,\"endOfWALStream\":false,\"startTime\":\"2023-11-15T07:03:43Z\",\"downloadStartTime\":\"2023-11-15T07:03:44Z\",\"downloadTotalTime\":0.700498522,\"totalTime\":0.855732874}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:44.783 CET\",\"process_id\":\"28\",\"session_id\":\"65546d4f.1c\",\"session_line_num\":\"3\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000C.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:45.254 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"82\",\"connection_from\":\"[local]\",\"session_id\":\"65546d51.52\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:45 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgsql15\",\"namespace\":\"services-postgresql\"},\"namespace\":\"services-postgresql\",\"name\":\"pgsql15\",\"reconcileID\":\"49ed8d65-a36d-48f6-91fd-ae55dde738d8\",\"uuid\":\"1df24ed2-8385-11ee-b613-5e21884c1f50\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:45.258140790+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:45.257 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"83\",\"connection_from\":\"[local]\",\"session_id\":\"65546d51.53\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:45 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:45.361506337+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:45.360 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"84\",\"connection_from\":\"[local]\",\"session_id\":\"65546d51.54\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:45 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL file not found in the recovery object store\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000D.history\",\"options\":[\"--endpoint-url\",\"https://backup1.services-minio.svc:9000\",\"--cloud-provider\",\"aws-s3\",\"s3://pgsql15-backups\",\"pgsql15\"],\"startTime\":\"2023-11-15T07:03:44Z\",\"endTime\":\"2023-11-15T07:03:45Z\",\"elapsedWalTime\":0.620937751}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:45.645 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"85\",\"connection_from\":\"[local]\",\"session_id\":\"65546d51.55\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:45 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:45.663213320+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:45.662 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"86\",\"connection_from\":\"[local]\",\"session_id\":\"65546d51.56\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:45 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:45.695 CET\",\"process_id\":\"28\",\"session_id\":\"65546d4f.1c\",\"session_line_num\":\"4\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"entering standby mode\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:45.726016581+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:45.725 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"97\",\"connection_from\":\"[local]\",\"session_id\":\"65546d51.61\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:45 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:45.993841326+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:45.993 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"101\",\"connection_from\":\"[local]\",\"session_id\":\"65546d51.65\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:45 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:46Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:46.378 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"103\",\"connection_from\":\"[local]\",\"session_id\":\"65546d52.67\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:46 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:46Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgsql15\",\"namespace\":\"services-postgresql\"},\"namespace\":\"services-postgresql\",\"name\":\"pgsql15\",\"reconcileID\":\"f74cdc83-913f-4003-ba51-780e5e02b843\",\"uuid\":\"1e9d6b01-8385-11ee-b613-5e21884c1f50\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:46Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:46.381 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"104\",\"connection_from\":\"[local]\",\"session_id\":\"65546d52.68\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:46 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:46Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000C.history\",\"startTime\":\"2023-11-15T07:03:45Z\",\"endTime\":\"2023-11-15T07:03:46Z\",\"elapsedWalTime\":0.622316712}\r\n2023-11-15T08:03:46.494627026+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:46Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000C.history\",\"maxParallel\":1,\"successfulWalRestore\":1,\"failedWalRestore\":0,\"endOfWALStream\":false,\"startTime\":\"2023-11-15T07:03:45Z\",\"downloadStartTime\":\"2023-11-15T07:03:45Z\",\"downloadTotalTime\":0.622730058,\"totalTime\":0.76213108}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:46Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:46.497 CET\",\"process_id\":\"28\",\"session_id\":\"65546d4f.1c\",\"session_line_num\":\"5\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000C.history\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Instance status probe failing\",\"logging_pod\":\"pgsql15-3\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is starting up (SQLSTATE 57P03))\"}\r\n2023-11-15T08:03:47.349903448+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:47.349 CET\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"118\",\"connection_from\":\"[local]\",\"session_id\":\"65546d53.76\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-15 08:03:47 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is starting up\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"wal-restore\",\"msg\":\"Restored WAL file\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000A000000CA00000033\",\"startTime\":\"2023-11-15T07:03:46Z\",\"endTime\":\"2023-11-15T07:03:47Z\",\"elapsedWalTime\":0.783000987}\r\n2023-11-15T08:03:47.483302502+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"wal-restore\",\"msg\":\"WAL restore command completed (parallel)\",\"logging_pod\":\"pgsql15-3\",\"walName\":\"0000000A000000CA00000033\",\"maxParallel\":1,\"successfulWalRestore\":1,\"failedWalRestore\":0,\"endOfWALStream\":false,\"startTime\":\"2023-11-15T07:03:46Z\",\"downloadStartTime\":\"2023-11-15T07:03:46Z\",\"downloadTotalTime\":0.783424532,\"totalTime\":0.940842584}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:47.486 CET\",\"process_id\":\"28\",\"session_id\":\"65546d4f.1c\",\"session_line_num\":\"6\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restored log file \\\"0000000A000000CA00000033\\\" from archive\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:47.500 CET\",\"process_id\":\"28\",\"session_id\":\"65546d4f.1c\",\"session_line_num\":\"7\",\"session_start_time\":\"2023-11-15 08:03:43 CET\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"XX000\",\"message\":\"requested timeline 12 does not contain minimum recovery point CA/35000000 on timeline 10\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:47.503 CET\",\"process_id\":\"24\",\"session_id\":\"65546d4e.18\",\"session_line_num\":\"6\",\"session_start_time\":\"2023-11-15 08:03:42 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"startup process (PID 28) exited with exit code 1\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n2023-11-15T08:03:47.503399532+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:47.503 CET\",\"process_id\":\"24\",\"session_id\":\"65546d4e.18\",\"session_line_num\":\"7\",\"session_start_time\":\"2023-11-15 08:03:42 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"aborting startup due to startup process failure\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"pgsql15-3\",\"record\":{\"log_time\":\"2023-11-15 08:03:47.507 CET\",\"process_id\":\"24\",\"session_id\":\"65546d4e.18\",\"session_line_num\":\"8\",\"session_start_time\":\"2023-11-15 08:03:42 CET\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is shut down\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"pgsql15-3\",\"reason\":\"postmaster has exited\"}\r\n2023-11-15T08:03:47.519806666+01:00 {\"level\":\"error\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"PostgreSQL process exited with errors\",\"logging_pod\":\"pgsql15-3\",\"error\":\"exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:99\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).reconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/manager/runnable_group.go:219\"}\r\n2023-11-15T08:03:47.519892689+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Stopping and waiting for non leader election runnables\"}\r\n2023-11-15T08:03:47.519975746+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Stopping and waiting for leader election runnables\"}\r\n2023-11-15T08:03:47.520245001+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"Replicator\",\"msg\":\"Terminated slot Replicator loop\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:47.520508729+01:00 {\"level\":\"error\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"error received after stop sequence was engaged\",\"error\":\"exit status 1\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/manager.(*controllerManager).engageStopProcedure.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/manager/internal.go:555\"}\r\n{\"level\":\"error\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Error while getting cluster CA Server secret\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgsql15\",\"namespace\":\"services-postgresql\"},\"namespace\":\"services-postgresql\",\"name\":\"pgsql15\",\"reconcileID\":\"0c8d5946-aff0-4735-a388-ed324ba1a852\",\"uuid\":\"1f48fefc-8385-11ee-b613-5e21884c1f50\",\"logging_pod\":\"pgsql15-3\",\"error\":\"Get \\\"https://10.43.0.1:443/api/v1/namespaces/services-postgresql/secrets/pgsql15-server-tls\\\": context canceled\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).RefreshSecrets\\n\\tinternal/management/controller/instance_controller.go:861\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).Reconcile\\n\\tinternal/management/controller/instance_controller.go:130\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:118\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:314\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:265\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:226\"}\r\n2023-11-15T08:03:47.521223502+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:47.521258168+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:47.521270186+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:47.521283563+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n2023-11-15T08:03:47.521294907+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:47.521358877+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"pgsql15-3\",\"address\":\":9187\"}\r\n2023-11-15T08:03:47.521372841+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"pgsql15-3\",\"address\":\"localhost:8010\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Webserver exited\",\"logging_pod\":\"pgsql15-3\",\"address\":\":8000\"}\r\n2023-11-15T08:03:47.521418984+01:00 {\"level\":\"error\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Error while getting barman endpoint CA secret\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgsql15\",\"namespace\":\"services-postgresql\"},\"namespace\":\"services-postgresql\",\"name\":\"pgsql15\",\"reconcileID\":\"0c8d5946-aff0-4735-a388-ed324ba1a852\",\"uuid\":\"1f48fefc-8385-11ee-b613-5e21884c1f50\",\"logging_pod\":\"pgsql15-3\",\"error\":\"client rate limiter Wait returned an error: context canceled\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).RefreshSecrets\\n\\tinternal/management/controller/instance_controller.go:868\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/management/controller.(*InstanceReconciler).Reconcile\\n\\tinternal/management/controller/instance_controller.go:130\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:118\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:314\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:265\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:226\"}\r\n2023-11-15T08:03:47.521803404+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:             \r\n  202209061\\nDatabase system identifier:           7227586665997950999\\nDatabase cluster state:               shut down in recovery\\npg_control last modified:             Tue 14 Nov 2023 10:39:59 PM UTC\\nLatest checkpoint location:           CA/33010BB8\\nLatest checkpoint's REDO location:    CA/33010B80\\nLatest checkpoint's REDO WAL file:    0000000A000000CA00000033\\nLatest checkpoint's TimeLineID:       10\\nLatest checkpoint's PrevTimeLineID:   10\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:714194\\nLatest checkpoint's NextOID:          50629\\nLatest checkpoint's NextMultiXactId:  3144\\nLatest checkpoint's NextMultiOffset:  6295\\nLatest checkpoint's oldestXID:        716\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  714194\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Mon 13 Nov 2023 07:06:30 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     CA/35000000\\nMin recovery ending loc's timeline:   10\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDat\r\ne/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            1527c7c996593b2ba147d001a5fd50051eefe2ee208395d490d357f44029e3f1\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"pgsql15-3\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgsql15\",\"namespace\":\"services-postgresql\"},\"namespace\":\"services-postgresql\",\"name\":\"pgsql15\",\"reconcileID\":\"0c8d5946-aff0-4735-a388-ed324ba1a852\",\"uuid\":\"1f48fefc-8385-11ee-b613-5e21884c1f50\",\"logging_pod\":\"pgsql15-3\"}\r\n2023-11-15T08:03:47.565078624+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n2023-11-15T08:03:47.565093879+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Stopping and waiting for caches\"}\r\n2023-11-15T08:03:47.565127252+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Stopping and waiting for webhooks\"}\r\n2023-11-15T08:03:47.565139524+01:00 {\"level\":\"info\",\"ts\":\"2023-11-15T07:03:47Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductI think the reason is this\r\n> requested timeline 12 does not contain minimum recovery point CA/35000000 on timeline 10\r\nwe've hit this several times as well, a workaround is to delete the pvc and pod, cnpg will then create a new replica. Not sure what the cause is though, something about replication and failover, probably...\n---\nI know that deleting pvc and pod helps. But is there any other way?\r\nDelete only some files from PVC?\r\nOn a large database recreating nodes from scratch will take a lot of time :(\n---\nThis problem should have been fixed by several commits that landed in 1.20.4 and 1.21.1. Please update and try again.\n---\nOk, I'll try 1.21.1\n---\nHi guys! i have an OKD cluster with 3masters 2workers 1manager to which IP addr the api server, console, api-int dns are attached. After shoot down of my network, i restarted all the nodes and now my OKD cluster does not start, the console is no more working. I encountered this error message : Get \"https://api.okd-mz.example.com:6443/apis/config.openshift.io/v1/clusteroperators\": EOF\n---\nProblem persistens in 1.21.1\r\nAfter Restarting the new primary Pod the not syncing Pod is working again\r\nController Log: \r\n```\r\n{\"level\":\"info\",\"ts\":\"2023-12-23T16:51:43Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"9e5c7082-cd05-4185-bb20-a2388c0a1c23\",\"uuid\":\"8c4ace69-a1b3-11ee-9cb7-86f8d40bd790\",\"name\":\"bzs-pg-1\",\"error\":\"error status code: 500, body: failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\\n\"}\r\n{\"level\":\"info\",\"ts\":\"2023-12-23T16:51:58Z\",\"msg\":\"Cannot extract Pod status\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"9ce12239-b26d-41f0-a9fa-4cbd23976398\",\"uuid\":\"94f8d52a-a1b3-11ee-9cb7-86f8d40bd790\",\"name\":\"bzs-pg-1\",\"error\":\"error status code: 500, body: failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\\n\"}\r\n```\r\nPod Log: \r\n```\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"setup\",\"msg\":\"Starting CloudNativePG Instance Manager\",\"logging_pod\":\"bzs-pg-1\",\"version\":\"1.21.1\",\"build\":{\"Version\":\"1.21.1\",\"Commit\":\"27f62cac\",\"Date\":\"2023-11-03\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"setup\",\"msg\":\"starting controller-runtime manager\",\"logging_pod\":\"bzs-pg-1\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"msg\":\"Starting EventSource\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"source\":\"kind source: *v1.Cluster\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"msg\":\"Starting Controller\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"roles_reconciler\",\"msg\":\"starting up the runnable\",\"logging_pod\":\"bzs-pg-1\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"roles_reconciler\",\"msg\":\"skipping the RoleSynchronizer in replicas\",\"logging_pod\":\"bzs-pg-1\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"roles_reconciler\",\"msg\":\"setting up RoleSynchronizer loop\",\"logging_pod\":\"bzs-pg-1\"}\r\nbootstrap-controller {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:35Z\",\"msg\":\"Installing the manager executable\",\"destination\":\"/controller/manager\",\"version\":\"1.21.1\",\"build\":{\"Version\":\"1.21.1\",\"Commit\":\"27f62cac\",\"Date\":\"2023-11-03\"}}\r\nbootstrap-controller {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:36Z\",\"msg\":\"Setting 0750 permissions\"}\r\nbootstrap-controller {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:36Z\",\"msg\":\"Bootstrap completed\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"bzs-pg-1\",\"address\":\":9187\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"bzs-pg-1\",\"address\":\"localhost:8010\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"msg\":\"Starting webserver\",\"logging_pod\":\"bzs-pg-1\",\"address\":\":8000\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"msg\":\"Starting workers\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"worker count\":1}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"bbdebc96-a42d-45af-9a01-5227b74eae14\",\"uuid\":\"41dc4ca8-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"filename\":\"/controller/certificates/server.crt\",\"secret\":\"bzs-pg-server\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"bbdebc96-a42d-45af-9a01-5227b74eae14\",\"uuid\":\"41dc4ca8-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"filename\":\"/controller/certificates/server.key\",\"secret\":\"bzs-pg-server\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"bbdebc96-a42d-45af-9a01-5227b74eae14\",\"uuid\":\"41dc4ca8-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"filename\":\"/controller/certificates/streaming_replica.crt\",\"secret\":\"bzs-pg-replication\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"bbdebc96-a42d-45af-9a01-5227b74eae14\",\"uuid\":\"41dc4ca8-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"filename\":\"/controller/certificates/streaming_replica.key\",\"secret\":\"bzs-pg-replication\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"bbdebc96-a42d-45af-9a01-5227b74eae14\",\"uuid\":\"41dc4ca8-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"filename\":\"/controller/certificates/client-ca.crt\",\"secret\":\"bzs-pg-ca\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"msg\":\"Refreshed configuration file\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"bbdebc96-a42d-45af-9a01-5227b74eae14\",\"uuid\":\"41dc4ca8-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"filename\":\"/controller/certificates/server-ca.crt\",\"secret\":\"bzs-pg-ca\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"msg\":\"Found previous run flag\",\"logging_pod\":\"bzs-pg-1\",\"filename\":\"/var/lib/postgresql/data/pgdata/cnpg_initialized-bzs-pg-1\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"bzs-pg-1\",\"reason\":\"postmaster start up\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202209061\\nDatabase system identifier:           7242590037021855768\\nDatabase cluster state:               shut down in recovery\\npg_control last modified:             Sat 23 Dec 2023 04:49:14 PM UTC\\nLatest checkpoint location:           0/4F000060\\nLatest checkpoint's REDO location:    0/4E00CE20\\nLatest checkpoint's REDO WAL file:    00000007000000000000004E\\nLatest checkpoint's TimeLineID:       7\\nLatest checkpoint's PrevTimeLineID:   7\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:827\\nLatest checkpoint's NextOID:          16497\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        716\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  827\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Sat 23 Dec 2023 03:27:11 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/50000028\\nMin recovery ending loc's timeline:   7\\nBackup start location:                0/4C001370\\nBackup end location:                  0/50000028\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            e644065f8d39cc30eeedc477c722fd7165efb917c247c9ddbf253d1e2f3a34b0\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"bzs-pg-1\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"msg\":\"Instance is still down, will retry in 1 second\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"bbdebc96-a42d-45af-9a01-5227b74eae14\",\"uuid\":\"41dc4ca8-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"postgres\",\"msg\":\"2023-12-23 16:49:37.280 UTC [21] LOG:  redirecting log output to logging collector process\",\"pipe\":\"stderr\",\"logging_pod\":\"bzs-pg-1\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"postgres\",\"msg\":\"2023-12-23 16:49:37.280 UTC [21] HINT:  Future log output will appear in directory \\\"/controller/log\\\".\",\"pipe\":\"stderr\",\"logging_pod\":\"bzs-pg-1\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:37.280 UTC\",\"process_id\":\"21\",\"session_id\":\"65870fa1.15\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"ending log output to stderr\",\"hint\":\"Future log output will go to log destination \\\"csvlog\\\".\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:37.280 UTC\",\"process_id\":\"21\",\"session_id\":\"65870fa1.15\",\"session_line_num\":\"2\",\"session_start_time\":\"2023-12-23 16:49:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"starting PostgreSQL 15.1 (Debian 15.1-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"postgres\",\"msg\":\"2023-12-23 16:49:37.280 UTC [21] LOG:  ending log output to stderr\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"bzs-pg-1\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"postgres\",\"msg\":\"2023-12-23 16:49:37.280 UTC [21] HINT:  Future log output will go to log destination \\\"csvlog\\\".\",\"source\":\"/controller/log/postgres\",\"logging_pod\":\"bzs-pg-1\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:37.281 UTC\",\"process_id\":\"21\",\"session_id\":\"65870fa1.15\",\"session_line_num\":\"3\",\"session_start_time\":\"2023-12-23 16:49:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv4 address \\\"0.0.0.0\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:37.281 UTC\",\"process_id\":\"21\",\"session_id\":\"65870fa1.15\",\"session_line_num\":\"4\",\"session_start_time\":\"2023-12-23 16:49:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on IPv6 address \\\"::\\\", port 5432\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:37.293 UTC\",\"process_id\":\"21\",\"session_id\":\"65870fa1.15\",\"session_line_num\":\"5\",\"session_start_time\":\"2023-12-23 16:49:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"listening on Unix socket \\\"/controller/run/.s.PGSQL.5432\\\"\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:37.319 UTC\",\"process_id\":\"25\",\"session_id\":\"65870fa1.19\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system was shut down in recovery at 2023-12-23 16:49:14 UTC\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"wal-restore\",\"msg\":\"tried restoring WALs, but no backup was configured\",\"logging_pod\":\"bzs-pg-1\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:37.459 UTC\",\"process_id\":\"25\",\"session_id\":\"65870fa1.19\",\"session_line_num\":\"2\",\"session_start_time\":\"2023-12-23 16:49:37 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"entering standby mode\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"wal-restore\",\"msg\":\"tried restoring WALs, but no backup was configured\",\"logging_pod\":\"bzs-pg-1\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"wal-restore\",\"msg\":\"tried restoring WALs, but no backup was configured\",\"logging_pod\":\"bzs-pg-1\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"wal-restore\",\"msg\":\"tried restoring WALs, but no backup was configured\",\"logging_pod\":\"bzs-pg-1\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:37.933 UTC\",\"process_id\":\"25\",\"session_id\":\"65870fa1.19\",\"session_line_num\":\"3\",\"session_start_time\":\"2023-12-23 16:49:37 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"redo starts at 0/4E00CE20\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:37Z\",\"logger\":\"wal-restore\",\"msg\":\"tried restoring WALs, but no backup was configured\",\"logging_pod\":\"bzs-pg-1\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:38Z\",\"logger\":\"wal-restore\",\"msg\":\"tried restoring WALs, but no backup was configured\",\"logging_pod\":\"bzs-pg-1\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:38Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:38.213 UTC\",\"process_id\":\"25\",\"session_id\":\"65870fa1.19\",\"session_line_num\":\"4\",\"session_start_time\":\"2023-12-23 16:49:37 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"unexpected pageaddr 0/2C000000 in log segment 000000070000000000000050, offset 0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:38Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:38.213 UTC\",\"process_id\":\"25\",\"session_id\":\"65870fa1.19\",\"session_line_num\":\"5\",\"session_start_time\":\"2023-12-23 16:49:37 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"unexpected pageaddr 0/2C000000 in log segment 000000070000000000000050, offset 0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:38Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:38.235 UTC\",\"process_id\":\"68\",\"session_id\":\"65870fa2.44\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:38 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"started streaming WAL from primary at 0/50000000 on timeline 7\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:38Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:38.357 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"70\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa2.46\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:38 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:38Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:38.360 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"71\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa2.47\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:38 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:38Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"9ec1ea54-c7ba-4720-943b-d99d8f54ed33\",\"uuid\":\"428974a2-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:39Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:39.517 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"73\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa3.49\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:39Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:39.523 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"74\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa3.4a\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:39 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:39Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"109d16b1-fc1e-46f6-bf0a-589b4bab4000\",\"uuid\":\"4331e5f3-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:40Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:40.583 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"76\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa4.4c\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:40 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:40Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"871acb5c-a5e1-42cd-8e28-88186c7ba2d2\",\"uuid\":\"43e38f91-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:40Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:40.584 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"77\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa4.4d\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:40 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:41Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:41.641 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"79\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa5.4f\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:41 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:41Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:41.643 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"80\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa5.50\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:41 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:41Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"819d54de-6949-4fda-9514-548f3453e71e\",\"uuid\":\"448529ef-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:42Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:42.694 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"82\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa6.52\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:42Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"d7c18a01-5d6f-4b2d-8817-809a066b4ae8\",\"uuid\":\"4526b6c6-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:42Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:42.696 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"83\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa6.53\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:42 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:43.135 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"84\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa7.54\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:43 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:43.149 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"85\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa7.55\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:43 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:43.205 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"86\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa7.56\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:43 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:43.356 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"88\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa7.58\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:43 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:43Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"53567b8a-2bb9-4e47-9347-9124b940b98f\",\"uuid\":\"457159f9-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:43.358 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"89\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa7.59\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:43 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:43.479 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"90\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa7.5a\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:43 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\nStream closed EOF for bzs/bzs-pg-1 (bootstrap-controller)\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:43.750 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"92\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa7.5c\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:43 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:43Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:43.752 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"93\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa7.5d\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:43 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:43Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"b59b5b82-7fd7-4f22-8127-7ea54dda5804\",\"uuid\":\"45c75e3c-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:44.818 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"95\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa8.5f\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:44 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:44.822 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"96\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa8.60\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:44 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:44.825 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"97\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa8.61\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:44 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:44Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"91feefd7-77de-47f0-9daf-bf13c14b343b\",\"uuid\":\"4668aa99-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:44.979 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"99\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa8.63\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:44 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:44Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"602f04bf-31a2-4372-8b75-a3975ab586f5\",\"uuid\":\"4681ee7c-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:44Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:44.983 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"100\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa8.64\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:44 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:45.284 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"102\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa9.66\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:45 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:45Z\",\"msg\":\"Unable to collect metrics\",\"logging_pod\":\"bzs-pg-1\",\"error\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:45.516 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"103\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa9.67\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:45 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:45.520 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"104\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa9.68\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:45 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:45.799 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"105\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa9.69\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:45 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:45.878 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"107\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa9.6b\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:45 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:45Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"a280291e-c093-4ef4-b45f-8d8c327763cf\",\"uuid\":\"470c6306-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:45Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:45.881 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"108\",\"connection_from\":\"[local]\",\"session_id\":\"65870fa9.6c\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:45 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:46Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:46.803 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"109\",\"connection_from\":\"[local]\",\"session_id\":\"65870faa.6d\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:46Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:46.929 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"111\",\"connection_from\":\"[local]\",\"session_id\":\"65870faa.6f\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:46Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"39f4603e-8d9d-4686-be37-7e194f641833\",\"uuid\":\"47ad6107-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:46Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:46.931 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"112\",\"connection_from\":\"[local]\",\"session_id\":\"65870faa.70\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:47.989 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"114\",\"connection_from\":\"[local]\",\"session_id\":\"65870fab.72\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:47 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:47Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"09b5f8d9-ac48-41c5-9fe6-188e1cc356f7\",\"uuid\":\"484db9b4-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:47.992 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"115\",\"connection_from\":\"[local]\",\"session_id\":\"65870fab.73\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:47 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:49Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:49.040 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"117\",\"connection_from\":\"[local]\",\"session_id\":\"65870fad.75\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:49 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:49Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"9d1c1965-377f-464f-bfec-e8ef0f6c5e40\",\"uuid\":\"48ef751e-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:49Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:49.042 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"118\",\"connection_from\":\"[local]\",\"session_id\":\"65870fad.76\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:49 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:50Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:50.096 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"120\",\"connection_from\":\"[local]\",\"session_id\":\"65870fae.78\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:50 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:50Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"b843f7c2-5e1f-4ffd-a0cd-0a97df615f98\",\"uuid\":\"498fd8f6-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:50Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:50.098 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"121\",\"connection_from\":\"[local]\",\"session_id\":\"65870fae.79\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:50 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:51Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:51.152 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"123\",\"connection_from\":\"[local]\",\"session_id\":\"65870faf.7b\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:51 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:51Z\",\"msg\":\"DB not available, will retry\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"19a0e2f1-e255-4020-b3bc-083368153f9a\",\"uuid\":\"4a30cd32-a1b3-11ee-a81c-4647014bd03f\",\"logging_pod\":\"bzs-pg-1\",\"err\":\"failed to connect to `host=/controller/run user=postgres database=postgres`: server error (FATAL: the database system is not yet accepting connections (SQLSTATE 57P03))\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T16:49:51Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-1\",\"record\":{\"log_time\":\"2023-12-23 16:49:51.155 UTC\",\"user_name\":\"postgres\",\"database_name\":\"postgres\",\"process_id\":\"124\",\"connection_from\":\"[local]\",\"session_id\":\"65870faf.7c\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 16:49:51 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P03\",\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\",\"backend_type\":\"client backend\",\"query_id\":\"0\"}}\r\n```\r\nPod Primary\r\n```\r\npostgres {\"level\":\"info\",\"ts\":\"2023-11-30T15:10:21Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-11-30 15:10:21.390 UTC\",\"process_id\":\"68\",\"session_id\":\"6568a5bc.44\",\"session_line_num\":\"2\",\"session_start_time\":\"2023-11-30 15:09:48 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"replication terminated by primary server\",\"detail\":\"End of WAL reached on timeline 6 at 0/4C0000A0.\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-11-30T15:10:21Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-11-30 15:10:21.390 UTC\",\"process_id\":\"68\",\"session_id\":\"6568a5bc.44\",\"session_line_num\":\"3\",\"session_start_time\":\"2023-11-30 15:09:48 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"08006\",\"message\":\"could not send end-of-streaming message to primary: SSL connection has been closed unexpectedly\\nno COPY in progress\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-11-30T15:10:21Z\",\"logger\":\"wal-restore\",\"msg\":\"tried restoring WALs, but no backup was configured\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-11-30T15:10:21Z\",\"logger\":\"wal-restore\",\"msg\":\"tried restoring WALs, but no backup was configured\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-11-30T15:10:21Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-11-30 15:10:21.666 UTC\",\"process_id\":\"24\",\"session_id\":\"6568a5ba.18\",\"session_line_num\":\"7\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"record with incorrect prev-link 0/28000028 at 0/4C0000A0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-11-30T15:10:21Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-11-30 15:10:21.673 UTC\",\"process_id\":\"139\",\"session_id\":\"6568a5dd.8b\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-30 15:10:21 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"08006\",\"message\":\"could not connect to the primary server: connection to server at \\\"bzs-pg-rw\\\" (10.43.15.25), port 5432 failed: Connection refused\\n\\tIs the server running on that host and accepting TCP/IP connections?\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-11-30T15:10:21Z\",\"logger\":\"wal-restore\",\"msg\":\"tried restoring WALs, but no backup was configured\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-11-30T15:10:21Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-11-30 15:10:21.824 UTC\",\"process_id\":\"24\",\"session_id\":\"6568a5ba.18\",\"session_line_num\":\"8\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"waiting for WAL to become available at 0/4C0000B8\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-11-30T15:10:26Z\",\"logger\":\"wal-restore\",\"msg\":\"tried restoring WALs, but no backup was configured\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-11-30T15:11:30Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-11-30 15:11:30.736 UTC\",\"process_id\":\"157\",\"session_id\":\"6568a5e2.9d\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-30 15:10:26 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"started streaming WAL from primary at 0/4C000000 on timeline 6\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-11-30T15:14:46Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-11-30 15:14:46.738 UTC\",\"process_id\":\"22\",\"session_id\":\"6568a5ba.16\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restartpoint starting: time\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-11-30T15:14:46Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-11-30 15:14:46.907 UTC\",\"process_id\":\"22\",\"session_id\":\"6568a5ba.16\",\"session_line_num\":\"2\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restartpoint complete: wrote 3 buffers (0.0%); 0 WAL file(s) added, 0 removed, 1 recycled; write=0.121 s, sync=0.009 s, total=0.170 s; sync files=2, longest=0.006 s, average=0.005 s; distance=32715 kB, estimate=32715 kB\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-11-30T15:14:46Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-11-30 15:14:46.907 UTC\",\"process_id\":\"22\",\"session_id\":\"6568a5ba.16\",\"session_line_num\":\"3\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"recovery restart point at 0/4C000028\",\"detail\":\"Last completed transaction was at log time 2023-11-30 15:10:50.59597+00.\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-11-30T15:19:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-11-30 15:19:47.005 UTC\",\"process_id\":\"22\",\"session_id\":\"6568a5ba.16\",\"session_line_num\":\"4\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restartpoint starting: time\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-11-30T15:19:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-11-30 15:19:47.063 UTC\",\"process_id\":\"22\",\"session_id\":\"6568a5ba.16\",\"session_line_num\":\"5\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"restartpoint complete: wrote 0 buffers (0.0%); 0 WAL file(s) added, 0 removed, 1 recycled; write=0.001 s, sync=0.001 s, total=0.059 s; sync files=0, longest=0.000 s, average=0.000 s; distance=4 kB, estimate=29444 kB\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-11-30T15:19:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-11-30 15:19:47.063 UTC\",\"process_id\":\"22\",\"session_id\":\"6568a5ba.16\",\"session_line_num\":\"6\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"recovery restart point at 0/4C001370\",\"detail\":\"Last completed transaction was at log time 2023-11-30 15:10:50.59597+00.\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:20:01Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-12-23 15:20:01.945 UTC\",\"process_id\":\"157\",\"session_id\":\"6568a5e2.9d\",\"session_line_num\":\"2\",\"session_start_time\":\"2023-11-30 15:10:26 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"08006\",\"message\":\"terminating walreceiver due to timeout\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:20:02Z\",\"logger\":\"wal-restore\",\"msg\":\"tried restoring WALs, but no backup was configured\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:20:02Z\",\"logger\":\"wal-restore\",\"msg\":\"tried restoring WALs, but no backup was configured\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:20:02Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-12-23 15:20:02.326 UTC\",\"process_id\":\"24\",\"session_id\":\"6568a5ba.18\",\"session_line_num\":\"9\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"unexpected pageaddr 0/2A000000 in log segment 00000006000000000000004E, offset 0\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:10Z\",\"msg\":\"Setting myself as primary\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"c4281f9b-67ee-4296-96e2-be6f3b15438a\",\"uuid\":\"0a9baaf3-a1a7-11ee-8e2e-5e4c9d2daf0c\",\"logging_pod\":\"bzs-pg-2\",\"phase\":\"Failing over\",\"currentTimestamp\":\"2023-12-23T15:22:10.518887Z\",\"targetPrimaryTimestamp\":\"2023-12-23T15:22:10.388303Z\",\"currentPrimaryTimestamp\":\"2023-11-25T11:13:29.949825Z\",\"msPassedSinceTargetPrimaryTimestamp\":130,\"msPassedSinceCurrentPrimaryTimestamp\":2434120569,\"msDifferenceBetweenCurrentAndTargetPrimary\":-2434120438}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:10Z\",\"msg\":\"I'm the target primary, wait for the wal_receiver to be terminated\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"c4281f9b-67ee-4296-96e2-be6f3b15438a\",\"uuid\":\"0a9baaf3-a1a7-11ee-8e2e-5e4c9d2daf0c\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:10Z\",\"msg\":\"I'm the target primary, applying WALs and promoting my instance\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"c4281f9b-67ee-4296-96e2-be6f3b15438a\",\"uuid\":\"0a9baaf3-a1a7-11ee-8e2e-5e4c9d2daf0c\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:10Z\",\"msg\":\"Extracting pg_controldata information\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"c4281f9b-67ee-4296-96e2-be6f3b15438a\",\"uuid\":\"0a9baaf3-a1a7-11ee-8e2e-5e4c9d2daf0c\",\"logging_pod\":\"bzs-pg-2\",\"reason\":\"promote\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:10Z\",\"logger\":\"pg_controldata\",\"msg\":\"pg_control version number:            1300\\nCatalog version number:               202209061\\nDatabase system identifier:           7242590037021855768\\nDatabase cluster state:               in archive recovery\\npg_control last modified:             Thu 30 Nov 2023 03:24:47 PM UTC\\nLatest checkpoint location:           0/4D000060\\nLatest checkpoint's REDO location:    0/4C001370\\nLatest checkpoint's REDO WAL file:    00000006000000000000004C\\nLatest checkpoint's TimeLineID:       6\\nLatest checkpoint's PrevTimeLineID:   6\\nLatest checkpoint's full_page_writes: on\\nLatest checkpoint's NextXID:          0:825\\nLatest checkpoint's NextOID:          16497\\nLatest checkpoint's NextMultiXactId:  1\\nLatest checkpoint's NextMultiOffset:  0\\nLatest checkpoint's oldestXID:        716\\nLatest checkpoint's oldestXID's DB:   1\\nLatest checkpoint's oldestActiveXID:  825\\nLatest checkpoint's oldestMultiXid:   1\\nLatest checkpoint's oldestMulti's DB: 1\\nLatest checkpoint's oldestCommitTsXid:0\\nLatest checkpoint's newestCommitTsXid:0\\nTime of latest checkpoint:            Thu 30 Nov 2023 03:15:49 PM UTC\\nFake LSN counter for unlogged rels:   0/3E8\\nMinimum recovery ending location:     0/4E000000\\nMin recovery ending loc's timeline:   6\\nBackup start location:                0/0\\nBackup end location:                  0/0\\nEnd-of-backup record required:        no\\nwal_level setting:                    logical\\nwal_log_hints setting:                on\\nmax_connections setting:              100\\nmax_worker_processes setting:         32\\nmax_wal_senders setting:              10\\nmax_prepared_xacts setting:           0\\nmax_locks_per_xact setting:           64\\ntrack_commit_timestamp setting:       off\\nMaximum data alignment:               8\\nDatabase block size:                  8192\\nBlocks per segment of large relation: 131072\\nWAL block size:                       8192\\nBytes per WAL segment:                16777216\\nMaximum length of identifiers:        64\\nMaximum columns in an index:          32\\nMaximum size of a TOAST chunk:        1996\\nSize of a large-object chunk:         2048\\nDate/time type storage:               64-bit integers\\nFloat8 argument passing:              by value\\nData page checksum version:           0\\nMock authentication nonce:            e644065f8d39cc30eeedc477c722fd7165efb917c247c9ddbf253d1e2f3a34b0\\n\",\"pipe\":\"stdout\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:10Z\",\"msg\":\"Promoting instance\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"c4281f9b-67ee-4296-96e2-be6f3b15438a\",\"uuid\":\"0a9baaf3-a1a7-11ee-8e2e-5e4c9d2daf0c\",\"logging_pod\":\"bzs-pg-2\",\"pgctl_options\":[\"-D\",\"/var/lib/postgresql/data/pgdata\",\"-w\",\"promote\",\"-t 40000000\"]}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:10Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-12-23 15:22:10.527 UTC\",\"process_id\":\"24\",\"session_id\":\"6568a5ba.18\",\"session_line_num\":\"10\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"received promote request\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:10Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-12-23 15:22:10.527 UTC\",\"process_id\":\"1725111\",\"session_id\":\"6586faa2.1a52b7\",\"session_line_num\":\"1\",\"session_start_time\":\"2023-12-23 15:20:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"FATAL\",\"sql_state_code\":\"57P01\",\"message\":\"terminating walreceiver process due to administrator command\",\"backend_type\":\"walreceiver\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:10Z\",\"logger\":\"wal-restore\",\"msg\":\"tried restoring WALs, but no backup was configured\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:10Z\",\"logger\":\"wal-restore\",\"msg\":\"tried restoring WALs, but no backup was configured\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:10Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-12-23 15:22:10.793 UTC\",\"process_id\":\"24\",\"session_id\":\"6568a5ba.18\",\"session_line_num\":\"11\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"redo done at 0/4D000110 system usage: CPU: user: 10.95 s, system: 17.91 s, elapsed: 1987943.19 s\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:10Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-12-23 15:22:10.793 UTC\",\"process_id\":\"24\",\"session_id\":\"6568a5ba.18\",\"session_line_num\":\"12\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"last completed transaction was at log time 2023-11-30 15:10:50.59597+00\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:10Z\",\"logger\":\"wal-restore\",\"msg\":\"tried restoring WALs, but no backup was configured\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:10Z\",\"logger\":\"wal-restore\",\"msg\":\"tried restoring WALs, but no backup was configured\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:11Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-12-23 15:22:11.086 UTC\",\"process_id\":\"24\",\"session_id\":\"6568a5ba.18\",\"session_line_num\":\"13\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"selected new timeline ID: 7\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:11Z\",\"logger\":\"wal-restore\",\"msg\":\"tried restoring WALs, but no backup was configured\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:11Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-12-23 15:22:11.347 UTC\",\"process_id\":\"24\",\"session_id\":\"6568a5ba.18\",\"session_line_num\":\"14\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"virtual_transaction_id\":\"1/0\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive recovery complete\",\"backend_type\":\"startup\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:11Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-12-23 15:22:11.380 UTC\",\"process_id\":\"22\",\"session_id\":\"6568a5ba.16\",\"session_line_num\":\"7\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"checkpoint starting: force\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:11Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-12-23 15:22:11.383 UTC\",\"process_id\":\"20\",\"session_id\":\"6568a5ba.14\",\"session_line_num\":\"7\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"database system is ready to accept connections\",\"backend_type\":\"postmaster\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:11Z\",\"logger\":\"wal-archive\",\"msg\":\"Backup not configured, skip WAL archiving\",\"logging_pod\":\"bzs-pg-2\",\"walName\":\"pg_wal/00000007.history\",\"currentPrimary\":\"bzs-pg-1\",\"targetPrimary\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:11Z\",\"logger\":\"pg_ctl\",\"msg\":\"waiting for server to promote.... done\",\"pipe\":\"stdout\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:11Z\",\"logger\":\"pg_ctl\",\"msg\":\"server promoted\",\"pipe\":\"stdout\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:11Z\",\"msg\":\"Requesting a checkpoint\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"c4281f9b-67ee-4296-96e2-be6f3b15438a\",\"uuid\":\"0a9baaf3-a1a7-11ee-8e2e-5e4c9d2daf0c\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:11Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-12-23 15:22:11.431 UTC\",\"process_id\":\"22\",\"session_id\":\"6568a5ba.16\",\"session_line_num\":\"8\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"checkpoint complete: wrote 2 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.009 s, sync=0.009 s, total=0.052 s; sync files=2, longest=0.008 s, average=0.005 s; distance=32763 kB, estimate=32763 kB\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:11Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-12-23 15:22:11.437 UTC\",\"process_id\":\"22\",\"session_id\":\"6568a5ba.16\",\"session_line_num\":\"9\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"checkpoint starting: immediate force wait\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:11Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-12-23 15:22:11.458 UTC\",\"process_id\":\"22\",\"session_id\":\"6568a5ba.16\",\"session_line_num\":\"10\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"checkpoint complete: wrote 0 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.001 s, sync=0.001 s, total=0.021 s; sync files=0, longest=0.000 s, average=0.000 s; distance=0 kB, estimate=29486 kB\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:11Z\",\"msg\":\"The PostgreSQL instance has been promoted successfully\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"c4281f9b-67ee-4296-96e2-be6f3b15438a\",\"uuid\":\"0a9baaf3-a1a7-11ee-8e2e-5e4c9d2daf0c\",\"logging_pod\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:11Z\",\"logger\":\"wal-archive\",\"msg\":\"Backup not configured, skip WAL archiving\",\"logging_pod\":\"bzs-pg-2\",\"walName\":\"pg_wal/00000002000000000000002D\",\"currentPrimary\":\"bzs-pg-1\",\"targetPrimary\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:11Z\",\"msg\":\"Finished setting myself as primary\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"bzs-pg\",\"namespace\":\"bzs\"},\"namespace\":\"bzs\",\"name\":\"bzs-pg\",\"reconcileID\":\"c4281f9b-67ee-4296-96e2-be6f3b15438a\",\"uuid\":\"0a9baaf3-a1a7-11ee-8e2e-5e4c9d2daf0c\",\"logging_pod\":\"bzs-pg-2\",\"phase\":\"Failing over\",\"currentTimestamp\":\"2023-12-23T15:22:11.503957Z\",\"targetPrimaryTimestamp\":\"2023-12-23T15:22:10.388303Z\",\"currentPrimaryTimestamp\":\"2023-12-23T15:22:11.458310Z\",\"msPassedSinceTargetPrimaryTimestamp\":1115,\"msPassedSinceCurrentPrimaryTimestamp\":45,\"msDifferenceBetweenCurrentAndTargetPrimary\":1070}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:22:29Z\",\"logger\":\"Replicator\",\"msg\":\"synchronizing replication slots\",\"logging_pod\":\"bzs-pg-2\",\"err\":\"getting replication slot status from primary: failed to connect to `host=bzs-pg-rw user=streaming_replica database=postgres`: dial error (timeout: dial tcp 10.43.15.25:5432: connect: connection timed out)\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:27:11Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-12-23 15:27:11.557 UTC\",\"process_id\":\"22\",\"session_id\":\"6568a5ba.16\",\"session_line_num\":\"11\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"checkpoint starting: time\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:27:11Z\",\"logger\":\"wal-archive\",\"msg\":\"Backup not configured, skip WAL archiving\",\"logging_pod\":\"bzs-pg-2\",\"walName\":\"pg_wal/00000007000000000000004E\",\"currentPrimary\":\"bzs-pg-2\",\"targetPrimary\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:27:12Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"bzs-pg-2\",\"record\":{\"log_time\":\"2023-12-23 15:27:12.755 UTC\",\"process_id\":\"22\",\"session_id\":\"6568a5ba.16\",\"session_line_num\":\"12\",\"session_start_time\":\"2023-11-30 15:09:46 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"checkpoint complete: wrote 11 buffers (0.1%); 0 WAL file(s) added, 1 removed, 1 recycled; write=1.127 s, sync=0.019 s, total=1.198 s; sync files=8, longest=0.016 s, average=0.003 s; distance=51 kB, estimate=26543 kB\",\"backend_type\":\"checkpointer\",\"query_id\":\"0\"}}\r\npostgres {\"level\":\"error\",\"ts\":\"2023-12-23T15:31:18Z\",\"msg\":\"pkg/mod/k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229: Failed to watch *v1.Cluster: the server is currently unable to handle the request (get clusters.postgresql.cnpg.io)\",\"stacktrace\":\"k8s.io/client-go/tools/cache.DefaultWatchErrorHandler\\n\\tpkg/mod/k8s.io/client-go@v0.28.3/tools/cache/reflector.go:147\\nk8s.io/client-go/tools/cache.(*Reflector).Run.func1\\n\\tpkg/mod/k8s.io/client-go@v0.28.3/tools/cache/reflector.go:292\\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.28.3/pkg/util/wait/backoff.go:226\\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\\n\\tpkg/mod/k8s.io/apimachinery@v0.28.3/pkg/util/wait/backoff.go:227\\nk8s.io/client-go/tools/cache.(*Reflector).Run\\n\\tpkg/mod/k8s.io/client-go@v0.28.3/tools/cache/reflector.go:290\\nk8s.io/client-go/tools/cache.(*controller).Run.(*Group).StartWithChannel.func2\\n\\tpkg/mod/k8s.io/apimachinery@v0.28.3/pkg/util/wait/wait.go:55\\nk8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.28.3/pkg/util/wait/wait.go:72\"}\r\npostgres {\"level\":\"info\",\"ts\":\"2023-12-23T15:32:12Z\",\"logger\":\"wal-archive\",\"msg\":\"Backup not configured, skip WAL archiving\",\"logging_pod\":\"bzs-pg-2\",\"walName\":\"pg_wal/00000007000000000000004F\",\"currentPrimary\":\"bzs-pg-2\",\"targetPrimary\":\"bzs-pg-2\"}\r\npostgres {\"level\":\"error\",\"ts\":\"2023-12-23T15:36:59Z\",\"msg\":\"pkg/mod/k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229: Failed to watch *v1.Cluster: the server is currently unable to handle the request (get clusters.postgresql.cnpg.io)\",\"stacktrace\":\"k8s.io/client-go/tools/cache.DefaultWatchErrorHandler\\n\\tpkg/mod/k8s.io/client-go@v0.28.3/tools/cache/reflector.go:147\\nk8s.io/client-go/tools/cache.(*Reflector).Run.func1\\n\\tpkg/mod/k8s.io/client-go@v0.28.3/tools/cache/reflector.go:292\\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.28.3/pkg/util/wait/backoff.go:226\\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\\n\\tpkg/mod/k8s.io/apimachinery@v0.28.3/pkg/util/wait/backoff.go:227\\nk8s.io/client-go/tools/cache.(*Reflector).Run\\n\\tpkg/mod/k8s.io/client-go@v0.28.3/tools/cache/reflector.go:290\\nk8s.io/client-go/tools/cache.(*controller).Run.(*Group).StartWithChannel.func2\\n\\tpkg/mod/k8s.io/apimachinery@v0.28.3/pkg/util/wait/wait.go:55\\nk8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1\\n\\tpkg/mod/k8s.io/apimachinery@v0.28.3/pkg/util/wait/wait.go:72\"}\r\nStream closed EOF for bzs/bzs-pg-2 (bootstrap-controller)\r\n```\n---\nany updates on this?\n---\nI have same issue"
    },
    {
        "title": "copy edits to installation and instance manager",
        "id": 1993490820,
        "state": "no reaction",
        "first": "",
        "messages": ""
    },
    {
        "title": "copy edits to fencing and index",
        "id": 1993173055,
        "state": "open",
        "first": "Please check query",
        "messages": "Please check query"
    },
    {
        "title": "added init container for createBootstrapContainer function,",
        "id": 1989291457,
        "state": "open",
        "first": "check if user exists and create it if not, for initdb to initialize the database along the UID GID passed in the spec successfully\r\nrelated issue: https://github.com/cloudnative-pg/cloudnative-pg/issues/3331",
        "messages": "check if user exists and create it if not, for initdb to initialize the database along the UID GID passed in the spec successfully\r\nrelated issue: https://github.com/cloudnative-pg/cloudnative-pg/issues/3331Hello @wibed, thanks for the PR. We will take a look at it ASAP.\n---\nHello @wibed, It seems that the PR depends on the presence of `getent` and `adduser`. Those binaries could not be available in the container. \r\nalso CC @leonardoce , @mnencia and @phisco  if you want to add your opinions\n---\nWouldn't these changes mean the container has to run as root?\n---\n> Hello @wibed, It seems that the PR depends on the presence of `getent` and `adduser`. Those binaries could not be available in the container.\r\n> \r\n> also CC @leonardoce , @mnencia and @phisco if you want to add your opinions\r\nas it is based on debian, those packages are indeed present.\r\nthe command i ran to test is:\r\n```shell\r\ndocker run -it -u 0 ghcr.io/cloudnative-pg/postgresql:15.2 bash\r\n```\r\ni did need root privileges to add a user or group to the system though.\n---\nHello, I don't think we should tie CNP with root permissions, but this is my opinion. I would love to hear more opinions from the community and the maintainers.\n---\n> Hello, I don't think we should tie CNP with root permissions, but this is my opinion. I would love to hear more opinions from the community and the maintainers.\r\nid love to have multiple images to choose from. it comes down to who will maintain those over the timeline though\n---\n@armru \r\nany update on this?\n---\n@wibed , \r\nHello, probably the fastest way to proceed is to bring this up at the community meeting to gather feedback from the community/ other maintainers. Would you be available to join one of the upcoming ones?\r\n@mnencia, @leonardoce any input on this patch?\n---\nWe are trying to solve the wrong problem here. Having the `postgres` user inside the Postgres container is the responsibility of whoever builds the container image. If the environment changes the user to something different (like open shift does with restricted SCC), it is responsible for ensuring that that user exists inside the /etc/password file, and our operator supports it.\r\nCNPG allows you to pass the desired user as part of the cluster configuration, which is all we need to do, IMHO.\n---\ni add my two cents here as well as i did in the issue last week.\r\ni see where you coming from, that any system related changes should not bleed into the responsibility of a postgres related deployment.\r\nyet does initdb, a fundamental part of postgres, do expect a user which can be set within the settings already available from within the repo. \r\ni ask myself if it wouldnt be smarter to exclude that option and just force uid/gid nobody on everyone as it should.\r\nwhat do you think?\n---\nPlease refer to https://github.com/cloudnative-pg/cloudnative-pg/issues/3331#issuecomment-1961476853 for my thoughts on this"
    },
    {
        "title": "[Bug]: postgres doesn't care what UID it runs in, but initdb does. user does not exist",
        "id": 1988892099,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.21.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\ni found a configuration item.\r\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/06871b2735823111a3f78ccc57c384447ee1cde5/config/crd/bases/postgresql.cnpg.io_clusters.yaml#L2517\r\nupon using postgresGID and postgresUID, the uid is not changed the user is:\r\n```shell\r\n# tl;dr: \"logger\":\"initdb\",\"msg\":\"initdb: could not look up effective user ID 501: user does not exist\\n\",\r\n# full log below:\r\n{\"level\":\"error\",\"ts\":\"2023-11-08T07:54:09Z\",\"msg\":\"Error while bootstrapping data directory\",\"logging_pod\":\"cluster0-1-initdb\",\"error\":\"error while creating the PostgreSQL instance: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.initSubCommand\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:151\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.NewCmd.func2\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:104\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.3/x64/src/runtime/proc.go:267\"}\r\nError: error while creating the PostgreSQL instance: exit status 1\r\n```\r\nthe doc states\r\n```\r\n--username=username \r\nSelects the user name of the database superuser. \r\nThis defaults to the name of the effective user running initdb. \r\nIt is really not important what the superuser's name is, \r\nbut one might choose to keep the customary name postgres, \r\neven if the operating system user's name is different.\r\n```\r\ntherefor to set uid/gid expects the system to create the user before starting the process. which it does not\r\nrelevant excerpt from: https://hub.docker.com/_/postgres\r\n```yaml\r\nThe main caveat to note is that postgres doesn't care what UID it runs as (as long as the owner of /var/lib/postgresql/data matches), but initdb does care (and needs the user to exist in /etc/passwd):\r\n```\r\nthe easiest way around this it to include a chmod operation before the pod starts.\r\nbitnami has an implementation of such a process.\r\nhttps://github.com/bitnami/charts/blob/main/bitnami/postgresql/templates/primary/statefulset.yaml\r\n### what do i expect\r\ni would like for the instance manager to create the user if its uid is missing.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\n_No response_\n### Version\n1.21.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: k3s\n### How did you install the operator?\nHelm\n### What happened?\ni found a configuration item.\r\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/06871b2735823111a3f78ccc57c384447ee1cde5/config/crd/bases/postgresql.cnpg.io_clusters.yaml#L2517\r\nupon using postgresGID and postgresUID, the uid is not changed the user is:\r\n```shell\r\n# tl;dr: \"logger\":\"initdb\",\"msg\":\"initdb: could not look up effective user ID 501: user does not exist\\n\",\r\n# full log below:\r\n{\"level\":\"error\",\"ts\":\"2023-11-08T07:54:09Z\",\"msg\":\"Error while bootstrapping data directory\",\"logging_pod\":\"cluster0-1-initdb\",\"error\":\"error while creating the PostgreSQL instance: exit status 1\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.initSubCommand\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:151\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/initdb.NewCmd.func2\\n\\tinternal/cmd/manager/instance/initdb/cmd.go:104\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.3/x64/src/runtime/proc.go:267\"}\r\nError: error while creating the PostgreSQL instance: exit status 1\r\n```\r\nthe doc states\r\n```\r\n--username=username \r\nSelects the user name of the database superuser. \r\nThis defaults to the name of the effective user running initdb. \r\nIt is really not important what the superuser's name is, \r\nbut one might choose to keep the customary name postgres, \r\neven if the operating system user's name is different.\r\n```\r\ntherefor to set uid/gid expects the system to create the user before starting the process. which it does not\r\nrelevant excerpt from: https://hub.docker.com/_/postgres\r\n```yaml\r\nThe main caveat to note is that postgres doesn't care what UID it runs as (as long as the owner of /var/lib/postgresql/data matches), but initdb does care (and needs the user to exist in /etc/passwd):\r\n```\r\nthe easiest way around this it to include a chmod operation before the pod starts.\r\nbitnami has an implementation of such a process.\r\nhttps://github.com/bitnami/charts/blob/main/bitnami/postgresql/templates/primary/statefulset.yaml\r\n### what do i expect\r\ni would like for the instance manager to create the user if its uid is missing.\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conducti found the exec command\r\nhttps://github.com/cloudnative-pg/cloudnative-pg/blob/8cceb710d459bf93da94a5ca115c63e2908afdc5/pkg/utils/exec.go#L42\r\nMaybe it is not necessary to use the wrapper and directly inject the user check.\r\ni would:\r\n- check for postgresUID, postgresGID are defined.  \r\n- check user with uid and gid exists\r\n- if not create user\r\n- i am not sure if the name matters to initdb as long as the user exists\r\n```go\r\nfunc addManagerLoggingOptions(cluster apiv1.Cluster, container *corev1.Container) {\r\n\tif cluster.Spec.LogLevel != \"\" {\r\n\t\tcontainer.Command = append(container.Command, fmt.Sprintf(\"--log-level=%s\", cluster.Spec.LogLevel))\r\n\t}\r\n\tcontainer.Command = append(container.Command, log.GetFieldsRemapFlags()...)\r\n}\r\n```\r\n- check for user command\r\n```shell\r\ngetent passwd POSTGRESUID &> /dev/null && echo true\r\n```\r\n- create command for user\r\n```shell\r\nadduser --uid $POSTGRESUID --gid POSTGRESGUID --no-create-home test --system --disabled-login --quiet && echo true\r\n```\r\n#### Optional\r\ni'd like to open a pr, check for the user in question, given the postgresuid and the postgresgid.\r\nwhat would you recommend? Are there any better approaches to this?\n---\nIt seems to be that ignoring `containerSecurityContext` is the root cause here, unless I don't understand the problem. Why doesn't  `runAsUser`, `runAsGroup` (and maybe `fsGroup`) override `postgresUID` and `postgresGID`?\n---\n@ndrwstn \r\nwell its in the title.\r\npg wont care if a user exists on the system but initdb, initializing the pg db does.\r\nso if you provide a uid/gid which does not exists your pods wont start up because of it.\n---\nI don't understand your response,then Isn't the point of `containerSecurityContext` to provide the user/group that the container is supposed to run as. I guess there is a scenario where someone is assigning that to a user that doesn't exist, but why? And if initdb respects the security context (and we trust the end-user not to put nonsense into security context), doesn't that solve the problem?\r\nI'm just looking for an alternative answer as it didn't seem like they're going to accept your PR due to the need for root access.\n---\nrunning the container with the gid/uid from nobody as sensible default might be the safest bet security wise and a valid reason. \r\niirc the securityContext in itself does not change the underlying system in itself. it just runs the container along the asigned gid/uid.  \r\nPS.: could you enlighten me on why rootprivileges within an initcontainer are so frowned upon?\n---\nI hadn't thought of that, but it's a good point.\n---\nThere seems to be a misunderstanding regarding the CNPG operator. It assumes that the container images it runs have a `postgres` user with UID 26 (which is the default in RHEL systems). However, if your image requires a different value, you can change the UIDs by using the `postgresUID` and `postgresGID` fields. It is the responsibility of the image creator to ensure that the user exists.\r\nI would like to highlight that `initdb` is not the only tool that requires an environment to have a line in `/etc/passwd` for the current UID. There are many other tools and extensions that also require this, so it is not safe to assume that a tool will work correctly in a misconfigured container.\r\nIt is the responsibility of the person who builds the container image to include the postgres user inside the Postgres container. If the environment changes the user to something different, such as OpenShift's restricted SCC, then it is up to the environment to make sure that the user exists within the /etc/password file, and our operator supports it.\n---\n@mnencia given id like to add such a user beforehand. How would i approach this? \r\nto shuffle the responsibility onto the user is easy, but if its not possible to solve it might be the wrong thing to do.\n---\nHello @wibed and @ndrwstn \r\nFrom a security point of view, this is not plausible since there's system that will not rely on the user provided and will \r\nchange it applying different security policies, so this will be blocking everything that may use a different user.\r\nFollowing on the security side, creating a UID or a GID requires super user privileges, the images are, and should always be, by default, run as non root, having a root user inside a PostgreSQL image, is, in all the possible meanings a really bad idea, and should never be the use case, now, going back to the patch, if that's required, this it's a fully no go for this.\r\nRelated to the image, presumes that a couple of commands are there, well, the major problem here will be with a PostgreSQL image using alpine or ubi, even worst, what if we use a distroless image? this for sure will not work.\r\nI think it's clear here that the problem shouldn't be handled at runtime level but when the image is created, but moreover, here is missing a way to reproduce the issue, I don't fully understand because we can't reproduce, can you add the steps to reproduce here so we can see more clearly how you reached this point?\r\nRegards,"
    },
    {
        "title": "Edits to failure_modes and faq topics",
        "id": 1986455730,
        "state": "open",
        "first": "Signed-off-by: Betsy Gitelman <betsy.gitelman@enterprisedb.com>",
        "messages": "Signed-off-by: Betsy Gitelman <betsy.gitelman@enterprisedb.com>"
    },
    {
        "title": "edits to e2e, expose_pg_services and failover topics",
        "id": 1986232392,
        "state": "open",
        "first": "Signed-off-by: Betsy Gitelman <betsy.gitelman@enterprisedb.com>",
        "messages": "Signed-off-by: Betsy Gitelman <betsy.gitelman@enterprisedb.com>"
    },
    {
        "title": "[Feature]: Add a command to cnpg plugin to schedule logical backups on PVCs",
        "id": 1983944820,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nSome users would like to simply take logical backups on a regular basis and store them on an existing PVC, in a provided folder.\n### Describe the solution you'd like\nThe user needs to have an existing PVC and a folder in which they can write. The idea is that they can execute:\r\n```bash\r\nkubectl cnpg pg_dump --name=JOBNAME --target=primary|replica --schedule=\"0 5 * * *\" CLUSTER DBNAME PVC FOLDER\r\n```\r\nIf schedule is passed, it will generate a cronjob with the provided name. Otherwise a job with the provided name.\r\nIt will connect to the requested database via the -rw service if target is primary or -ro if target is replica using the streaming replica credentials.\r\nThe process will run pg_dump -Fc and store it with the usual timestamp name in the provided folder.\r\nThis is just a generic idea, please expand.\n### Describe alternatives you've considered\nCurrently, the only explanation we have provided is this: https://cloudnative-pg.io/documentation/current/troubleshooting/#emergency-backup\n### Additional context\nN/A\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nSome users would like to simply take logical backups on a regular basis and store them on an existing PVC, in a provided folder.\n### Describe the solution you'd like\nThe user needs to have an existing PVC and a folder in which they can write. The idea is that they can execute:\r\n```bash\r\nkubectl cnpg pg_dump --name=JOBNAME --target=primary|replica --schedule=\"0 5 * * *\" CLUSTER DBNAME PVC FOLDER\r\n```\r\nIf schedule is passed, it will generate a cronjob with the provided name. Otherwise a job with the provided name.\r\nIt will connect to the requested database via the -rw service if target is primary or -ro if target is replica using the streaming replica credentials.\r\nThe process will run pg_dump -Fc and store it with the usual timestamp name in the provided folder.\r\nThis is just a generic idea, please expand.\n### Describe alternatives you've considered\nCurrently, the only explanation we have provided is this: https://cloudnative-pg.io/documentation/current/troubleshooting/#emergency-backup\n### Additional context\nN/A\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "edits to declarative hibernation and declarative role management",
        "id": 1982259340,
        "state": "no reaction",
        "first": "",
        "messages": ""
    },
    {
        "title": "Edits to container_images, controller, and database_import topics",
        "id": 1982161989,
        "state": "open",
        "first": "Please see several queries",
        "messages": "Please see several queries"
    },
    {
        "title": "[Bug]: existing PV causing: refusing to create the primary instance while the latest generated serial is not zero",
        "id": 1981103491,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.21.0\r\n### What version of Kubernetes are you using?\r\n1.26\r\n### What is your Kubernetes environment?\r\nOther\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nWhen I followed https://cloudnative-pg.io/documentation/1.20/storage/#static-provisioning-of-persistent-volumes, the cnpg operator could not bootstrap the cluster.\r\nWhen I've removed already existing PV and retries, the operator could bootstrap the cluster. \r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/last-applied-configuration: |\r\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"metadata\":{\"annotations\":{},\"name\":\"cnpg-testdb-db\",\"namespace\":\"dev\"},\"spec\":{\"bootstrap\":{\"initdb\":{\"database\":\"testdb\",\"owner\":\"testdb\",\"secret\":{\"name\":\"testdb-secret\"}}},\"imageName\":\"ghcr.io/cloudnative-pg/postgresql:13.7\",\"instances\":2,\"postgresql\":{\"parameters\":{\"effective_io_concurrency\":\"2\",\"hot_standby_feedback\":\"on\",\"maintenance_work_mem\":\"205MB\",\"max_connections\":\"1800\",\"shared_buffers\":\"614MB\",\"tcp_keepalives_count\":\"3\",\"tcp_keepalives_idle\":\"60\",\"tcp_keepalives_interval\":\"10\",\"temp_buffers\":\"12MB\",\"work_mem\":\"10MB\"},\"syncReplicaElectionConstraint\":{\"enabled\":true,\"nodeLabelsAntiAffinity\":[\"topology.kubernetes.io/region\"]}},\"primaryUpdateStrategy\":\"unsupervised\",\"replicationSlots\":{\"highAvailability\":{\"enabled\":true},\"updateInterval\":300},\"resources\":{\"limits\":{\"memory\":\"4Gi\"},\"requests\":{\"cpu\":\"2\",\"memory\":\"4Gi\"}},\"startDelay\":300,\"stopDelay\":300,\"storage\":{\"pvcTemplate\":{\"accessModes\":[\"ReadWriteOnce\"],\"resources\":{\"requests\":{\"storage\":\"100Gi\"}}}}}}\r\n  creationTimestamp: \"2023-11-07T10:50:02Z\"\r\n  generation: 1\r\n  name: cnpg-testdb-db\r\n  namespace: dev\r\n  resourceVersion: \"420923018\"\r\n  uid: 0aeac6be-b8ba-4ee0-bc92-2abc2928c705\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n  bootstrap:\r\n    initdb:\r\n      database: testdb\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: testdb\r\n      secret:\r\n        name: testdb-secret\r\n  enableSuperuserAccess: false\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:13.7\r\n  instances: 2\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: false\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      effective_io_concurrency: \"2\"\r\n      hot_standby_feedback: \"on\"\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      maintenance_work_mem: 205MB\r\n      max_connections: \"1800\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      shared_buffers: 614MB\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      tcp_keepalives_count: \"3\"\r\n      tcp_keepalives_idle: \"60\"\r\n      tcp_keepalives_interval: \"10\"\r\n      temp_buffers: 12MB\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n      work_mem: 10MB\r\n    syncReplicaElectionConstraint:\r\n      enabled: true\r\n      nodeLabelsAntiAffinity:\r\n      - topology.kubernetes.io/region\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 300\r\n  resources:\r\n    limits:\r\n      memory: 4Gi\r\n    requests:\r\n      cpu: \"2\"\r\n      memory: 4Gi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 300\r\n  stopDelay: 300\r\n  storage:\r\n    pvcTemplate:\r\n      accessModes:\r\n      - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 100Gi\r\n    resizeInUseVolumes: true\r\n  switchoverDelay: 3600\r\nstatus:\r\n  certificates:\r\n    clientCASecret: cnpg-testdb-db-ca\r\n    expirations:\r\n      cnpg-testdb-db-ca: 2024-02-05 10:45:03 +0000 UTC\r\n      cnpg-testdb-db-replication: 2024-02-05 10:45:03 +0000 UTC\r\n      cnpg-testdb-db-server: 2024-02-05 10:45:03 +0000 UTC\r\n    replicationTLSSecret: cnpg-testdb-db-replication\r\n    serverAltDNSNames:\r\n    - cnpg-testdb-db-rw\r\n    - cnpg-testdb-db-rw.dev\r\n    - cnpg-testdb-db-rw.dev.svc\r\n    - cnpg-testdb-db-r\r\n    - cnpg-testdb-db-r.dev\r\n    - cnpg-testdb-db-r.dev.svc\r\n    - cnpg-testdb-db-ro\r\n    - cnpg-testdb-db-ro.dev\r\n    - cnpg-testdb-db-ro.dev.svc\r\n    serverCASecret: cnpg-testdb-db-ca\r\n    serverTLSSecret: cnpg-testdb-db-server\r\n  cloudNativePGCommitHash: 27f62cac\r\n  cloudNativePGOperatorHash: 4912e5eb808aca3bf134923625f2346d404a3c860cf24aca4266493846f8fc3b\r\n  conditions:\r\n  - lastTransitionTime: \"2023-11-07T10:50:05Z\"\r\n    message: Cluster Is Not Ready\r\n    reason: ClusterIsNotReady\r\n    status: \"False\"\r\n    type: Ready\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"420922992\"\r\n  jobCount: 1\r\n  latestGeneratedNode: 1\r\n  managedRolesStatus: {}\r\n  phase: Setting up primary\r\n  phaseReason: Creating primary instance cnpg-testdb-db-1\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  readService: cnpg-testdb-db-r\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"420881610\"\r\n    clientCaSecretVersion: \"420922954\"\r\n    replicationSecretVersion: \"420922956\"\r\n    serverCaSecretVersion: \"420922954\"\r\n    serverSecretVersion: \"420922955\"\r\n  targetPrimary: cnpg-testdb-db-1\r\n  targetPrimaryTimestamp: \"2023-11-07T10:50:05.271532Z\"\r\n  topology:\r\n    successfullyExtracted: true\r\n  writeService: cnpg-testdb-db-rw\r\n```\r\n### Relevant log output\r\n```shell\r\n{\"level\":\"info\",\"ts\":\"2023-11-07T10:55:04Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-testdb-db\",\"namespace\":\"dev\"},\"namespace\":\"dev\",\"name\":\"cnpg-testdb-db\",\"reconcileID\":\"740270b9-92c2-47ae-8b82-972d53b8c1b9\",\"uuid\":\"1b3b0d20-7d5c-11ee-b048-bacd0350e280\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2023-11-07T10:55:06Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-testdb-db\",\"namespace\":\"dev\"},\"namespace\":\"dev\",\"name\":\"cnpg-testdb-db\",\"reconcileID\":\"d491d45f-3570-42e4-88b8-aad4c85e90a1\",\"uuid\":\"1c3e1d19-7d5c-11ee-b048-bacd0350e280\",\"latestGeneratedNode\":1}\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\r\n### I have read the troubleshooting guide\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### I am running a supported version of CloudNativePG\r\n- [X] I have read the troubleshooting guide and I think this is a new bug.\r\n### Contact Details\r\n_No response_\r\n### Version\r\n1.21.0\r\n### What version of Kubernetes are you using?\r\n1.26\r\n### What is your Kubernetes environment?\r\nOther\r\n### How did you install the operator?\r\nHelm\r\n### What happened?\r\nWhen I followed https://cloudnative-pg.io/documentation/1.20/storage/#static-provisioning-of-persistent-volumes, the cnpg operator could not bootstrap the cluster.\r\nWhen I've removed already existing PV and retries, the operator could bootstrap the cluster. \r\n### Cluster resource\r\n```shell\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/last-applied-configuration: |\r\n      {\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"metadata\":{\"annotations\":{},\"name\":\"cnpg-testdb-db\",\"namespace\":\"dev\"},\"spec\":{\"bootstrap\":{\"initdb\":{\"database\":\"testdb\",\"owner\":\"testdb\",\"secret\":{\"name\":\"testdb-secret\"}}},\"imageName\":\"ghcr.io/cloudnative-pg/postgresql:13.7\",\"instances\":2,\"postgresql\":{\"parameters\":{\"effective_io_concurrency\":\"2\",\"hot_standby_feedback\":\"on\",\"maintenance_work_mem\":\"205MB\",\"max_connections\":\"1800\",\"shared_buffers\":\"614MB\",\"tcp_keepalives_count\":\"3\",\"tcp_keepalives_idle\":\"60\",\"tcp_keepalives_interval\":\"10\",\"temp_buffers\":\"12MB\",\"work_mem\":\"10MB\"},\"syncReplicaElectionConstraint\":{\"enabled\":true,\"nodeLabelsAntiAffinity\":[\"topology.kubernetes.io/region\"]}},\"primaryUpdateStrategy\":\"unsupervised\",\"replicationSlots\":{\"highAvailability\":{\"enabled\":true},\"updateInterval\":300},\"resources\":{\"limits\":{\"memory\":\"4Gi\"},\"requests\":{\"cpu\":\"2\",\"memory\":\"4Gi\"}},\"startDelay\":300,\"stopDelay\":300,\"storage\":{\"pvcTemplate\":{\"accessModes\":[\"ReadWriteOnce\"],\"resources\":{\"requests\":{\"storage\":\"100Gi\"}}}}}}\r\n  creationTimestamp: \"2023-11-07T10:50:02Z\"\r\n  generation: 1\r\n  name: cnpg-testdb-db\r\n  namespace: dev\r\n  resourceVersion: \"420923018\"\r\n  uid: 0aeac6be-b8ba-4ee0-bc92-2abc2928c705\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n  bootstrap:\r\n    initdb:\r\n      database: testdb\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: testdb\r\n      secret:\r\n        name: testdb-secret\r\n  enableSuperuserAccess: false\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:13.7\r\n  instances: 2\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n    - key: queries\r\n      name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: false\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: \"on\"\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      effective_io_concurrency: \"2\"\r\n      hot_standby_feedback: \"on\"\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: \"0\"\r\n      log_rotation_size: \"0\"\r\n      log_truncate_on_rotation: \"false\"\r\n      logging_collector: \"on\"\r\n      maintenance_work_mem: 205MB\r\n      max_connections: \"1800\"\r\n      max_parallel_workers: \"32\"\r\n      max_replication_slots: \"32\"\r\n      max_worker_processes: \"32\"\r\n      shared_buffers: 614MB\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: \"\"\r\n      tcp_keepalives_count: \"3\"\r\n      tcp_keepalives_idle: \"60\"\r\n      tcp_keepalives_interval: \"10\"\r\n      temp_buffers: 12MB\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n      work_mem: 10MB\r\n    syncReplicaElectionConstraint:\r\n      enabled: true\r\n      nodeLabelsAntiAffinity:\r\n      - topology.kubernetes.io/region\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 300\r\n  resources:\r\n    limits:\r\n      memory: 4Gi\r\n    requests:\r\n      cpu: \"2\"\r\n      memory: 4Gi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 300\r\n  stopDelay: 300\r\n  storage:\r\n    pvcTemplate:\r\n      accessModes:\r\n      - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 100Gi\r\n    resizeInUseVolumes: true\r\n  switchoverDelay: 3600\r\nstatus:\r\n  certificates:\r\n    clientCASecret: cnpg-testdb-db-ca\r\n    expirations:\r\n      cnpg-testdb-db-ca: 2024-02-05 10:45:03 +0000 UTC\r\n      cnpg-testdb-db-replication: 2024-02-05 10:45:03 +0000 UTC\r\n      cnpg-testdb-db-server: 2024-02-05 10:45:03 +0000 UTC\r\n    replicationTLSSecret: cnpg-testdb-db-replication\r\n    serverAltDNSNames:\r\n    - cnpg-testdb-db-rw\r\n    - cnpg-testdb-db-rw.dev\r\n    - cnpg-testdb-db-rw.dev.svc\r\n    - cnpg-testdb-db-r\r\n    - cnpg-testdb-db-r.dev\r\n    - cnpg-testdb-db-r.dev.svc\r\n    - cnpg-testdb-db-ro\r\n    - cnpg-testdb-db-ro.dev\r\n    - cnpg-testdb-db-ro.dev.svc\r\n    serverCASecret: cnpg-testdb-db-ca\r\n    serverTLSSecret: cnpg-testdb-db-server\r\n  cloudNativePGCommitHash: 27f62cac\r\n  cloudNativePGOperatorHash: 4912e5eb808aca3bf134923625f2346d404a3c860cf24aca4266493846f8fc3b\r\n  conditions:\r\n  - lastTransitionTime: \"2023-11-07T10:50:05Z\"\r\n    message: Cluster Is Not Ready\r\n    reason: ClusterIsNotReady\r\n    status: \"False\"\r\n    type: Ready\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: \"420922992\"\r\n  jobCount: 1\r\n  latestGeneratedNode: 1\r\n  managedRolesStatus: {}\r\n  phase: Setting up primary\r\n  phaseReason: Creating primary instance cnpg-testdb-db-1\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  readService: cnpg-testdb-db-r\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"420881610\"\r\n    clientCaSecretVersion: \"420922954\"\r\n    replicationSecretVersion: \"420922956\"\r\n    serverCaSecretVersion: \"420922954\"\r\n    serverSecretVersion: \"420922955\"\r\n  targetPrimary: cnpg-testdb-db-1\r\n  targetPrimaryTimestamp: \"2023-11-07T10:50:05.271532Z\"\r\n  topology:\r\n    successfullyExtracted: true\r\n  writeService: cnpg-testdb-db-rw\r\n```\r\n### Relevant log output\r\n```shell\r\n{\"level\":\"info\",\"ts\":\"2023-11-07T10:55:04Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-testdb-db\",\"namespace\":\"dev\"},\"namespace\":\"dev\",\"name\":\"cnpg-testdb-db\",\"reconcileID\":\"740270b9-92c2-47ae-8b82-972d53b8c1b9\",\"uuid\":\"1b3b0d20-7d5c-11ee-b048-bacd0350e280\",\"latestGeneratedNode\":1}\r\n{\"level\":\"info\",\"ts\":\"2023-11-07T10:55:06Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"cnpg-testdb-db\",\"namespace\":\"dev\"},\"namespace\":\"dev\",\"name\":\"cnpg-testdb-db\",\"reconcileID\":\"d491d45f-3570-42e4-88b8-aad4c85e90a1\",\"uuid\":\"1c3e1d19-7d5c-11ee-b048-bacd0350e280\",\"latestGeneratedNode\":1}\r\n```\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of ConductI have a similar problem, my entire cluster crashed and after I removed all PVs and PVCs, the cluster could not be recreated and I am seeing this message in the operator's log.\n---\nEven re-installing the operator from scratch via helm uninstall / helm install did not help. The newly started operator pod cannot create the cluster with the following message:\r\n`{\"level\":\"info\",\"ts\":\"2023-11-14T11:50:24Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"my-cluster\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"my-cluster\",\"reconcileID\":\"3d01adc6-6156-4c88-a770-c57024e3633a\",\"uuid\":\"ff1ac4ab-82e3-11ee-aa18-4efa6df665a7\",\"latestGeneratedNode\":7}`\n---\nSame issue. \r\nDelete cluster.\r\nChecked it looked like PVC, secrets etc all perceivable state cleaned-up. \r\nRe-created the cluster, same name and getting the following errors in the logs\r\n```\r\n{\"level\":\"info\",\"ts\":\"2023-12-07T14:19:45Z\",\"msg\":\"refusing to create the primary instance while the latest generated serial is not zero\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"sentimental-pg-main\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"sentimental-pg-main\",\"reconcileID\":\"b8b3d2f3-2a2b-43de-adbc-c063fb8cf79a\",\"uuid\":\"ac163e95-950b-11ee-8141-4a42a479c272\",\"latestGeneratedNode\":1}\r\n{\"level\":\"debug\",\"ts\":\"2023-12-07T14:19:45Z\",\"msg\":\"object `default/sentimental-pg-main` has been reconciled\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"sentimental-pg-main\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"sentimental-pg-main\",\"reconcileID\":\"b8b3d2f3-2a2b-43de-adbc-c063fb8cf79a\",\"uuid\":\"ac077489-950b-11ee-8141-4a42a479c272\",\"caller\":\"controllers/cluster_controller.go:124\"}\r\n{\"level\":\"debug\",\"ts\":\"2023-12-07T14:19:46Z\",\"msg\":\"reconciling object `default/sentimental-pg-main`\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"sentimental-pg-main\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"sentimental-pg-main\",\"reconcileID\":\"eb9dd7f9-ceb7-4642-8ee1-16ad8344b160\",\"uuid\":\"acaef00f-950b-11ee-8141-4a42a479c272\",\"caller\":\"controllers/cluster_controller.go:121\"}\r\n{\"level\":\"debug\",\"ts\":\"2023-12-07T14:19:47Z\",\"msg\":\"haven't found any instance to create\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"sentimental-pg-main\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"sentimental-pg-main\",\"reconcileID\":\"eb9dd7f9-ceb7-4642-8ee1-16ad8344b160\",\"uuid\":\"acbaa37a-950b-11ee-8141-4a42a479c272\",\"caller\":\"controllers/cluster_create.go:1099\",\"instances\":[],\"dangling\":[],\"unusable\":[]}\r\n```\r\nThis is what the `cnpg status` command is showing\r\n```\r\nCluster Summary\r\nName:              sentimental-pg-main\r\nNamespace:         default\r\nPostgreSQL Image:  ghcr.io/cloudnative-pg/postgresql:15.2\r\nPrimary instance:\r\nStatus:\r\nInstances:         2\r\nReady instances:   0\r\nCertificates Status\r\nCertificate Name                 Expiration Date                Days Left Until Expiration\r\n----------------                 ---------------                --------------------------\r\nsentimental-pg-main-ca           2024-03-06 14:25:23 +0000 UTC  90.00\r\nsentimental-pg-main-replication  2024-03-06 14:25:23 +0000 UTC  90.00\r\nsentimental-pg-main-server       2024-03-06 14:25:23 +0000 UTC  90.00\r\nContinuous Backup status\r\nFirst Point of Recoverability:  Not Available\r\nNo Primary instance found\r\nStreaming Replication status\r\nPrimary instance not found\r\nUnmanaged Replication Slot Status\r\nNo unmanaged replication slots found\r\nInstances status\r\nName  Database Size  Current LSN  Replication role  Status  QoS  Manager Version  Node\r\n----  -------------  -----------  ----------------  ------  ---  ---------------  ----\r\n```\r\nEven tried deleting the S3 bucket where it was backing up to previously and still same errors in logs.\n---\nI'm also getting it, I'm trying to restore from a S3 backup\n---\nI'm getting this too.\r\ns3/minio backend & latest helm chart\n---\nI also have this. Completely removing the helm install and deleting the namespace,etc has no effect \r\nNFS backed storage provider\n---\nSame here \ud83d\ude22"
    },
    {
        "title": "[Bug]: Cluster does not reload secret when rotatet",
        "id": 1976904501,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nrichard.hagen@gmail.com\n### Version\n1.21.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nHelm\n### What happened?\nI used a broken SAS token to upload backups to a Azure Storage Account, this worked great for the first 5-6 hours, but then stopped working since the secret expired.\r\nI have recreated the secret with a updated longer lived token, but the backups are still failing, and the logs shows clearly that its using the old secret:\r\n```\r\nSignature not valid in the specified time frame: Start [Thu, 02 Nov 2023 18:12:38 GMT] - Expiry [Fri, 03 Nov 2023 03:12:38 GMT] - Current [Fri, 03 Nov 2023 20:34:49 GMT]\r\n```\r\nBut the updated secret is valid from this morning, until 1. january 2025. \r\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/restartedAt: '2023-11-01T21:08:27+01:00'\r\n    meta.helm.sh/release-name: happydogs\r\n    meta.helm.sh/release-namespace: happydogs-prod\r\n  creationTimestamp: '2023-10-29T11:49:06Z'\r\n  generation: 10\r\n  labels:\r\n    app.kubernetes.io/managed-by: Helm\r\n    helm.toolkit.fluxcd.io/name: happydogs-prod\r\n    helm.toolkit.fluxcd.io/namespace: happydogs-prod\r\n    k8slens-edit-resource-version: v1\r\n  managedFields:\r\n    - apiVersion: postgresql.cnpg.io/v1\r\n      fieldsType: FieldsV1\r\n      fieldsV1:\r\n        f:metadata:\r\n          f:annotations:\r\n            f:kubectl.kubernetes.io/restartedAt: {}\r\n      manager: kubectl-cnpg\r\n      operation: Update\r\n      time: '2023-11-01T20:08:27Z'\r\n    - apiVersion: postgresql.cnpg.io/v1\r\n      fieldsType: FieldsV1\r\n      fieldsV1:\r\n        f:spec:\r\n          f:backup:\r\n            f:barmanObjectStore:\r\n              f:destinationPath: {}\r\n      manager: kubectl-edit\r\n      operation: Update\r\n      time: '2023-11-02T19:20:35Z'\r\n    - apiVersion: postgresql.cnpg.io/v1\r\n      fieldsType: FieldsV1\r\n      fieldsV1:\r\n        f:metadata:\r\n          f:labels:\r\n            f:k8slens-edit-resource-version: {}\r\n        f:spec:\r\n          f:backup:\r\n            f:barmanObjectStore:\r\n              f:azureCredentials:\r\n                f:storageSasToken:\r\n                  .: {}\r\n                  f:key: {}\r\n                  f:name: {}\r\n      manager: node-fetch\r\n      operation: Update\r\n      time: '2023-11-02T19:42:43Z'\r\n    - apiVersion: postgresql.cnpg.io/v1\r\n      fieldsType: FieldsV1\r\n      fieldsV1:\r\n        f:metadata:\r\n          f:annotations:\r\n            .: {}\r\n            f:meta.helm.sh/release-name: {}\r\n            f:meta.helm.sh/release-namespace: {}\r\n          f:labels:\r\n            .: {}\r\n            f:app.kubernetes.io/managed-by: {}\r\n            f:helm.toolkit.fluxcd.io/name: {}\r\n            f:helm.toolkit.fluxcd.io/namespace: {}\r\n        f:spec:\r\n          .: {}\r\n          f:backup:\r\n            .: {}\r\n            f:barmanObjectStore:\r\n              .: {}\r\n              f:azureCredentials: {}\r\n              f:wal:\r\n                .: {}\r\n                f:compression: {}\r\n            f:target: {}\r\n          f:enableSuperuserAccess: {}\r\n          f:failoverDelay: {}\r\n          f:instances: {}\r\n          f:logLevel: {}\r\n          f:maxSyncReplicas: {}\r\n          f:minSyncReplicas: {}\r\n          f:postgresGID: {}\r\n          f:postgresUID: {}\r\n          f:primaryUpdateMethod: {}\r\n          f:primaryUpdateStrategy: {}\r\n          f:replicationSlots:\r\n            .: {}\r\n            f:highAvailability:\r\n              .: {}\r\n              f:enabled: {}\r\n              f:slotPrefix: {}\r\n            f:updateInterval: {}\r\n          f:resources:\r\n            .: {}\r\n            f:limits:\r\n              .: {}\r\n              f:memory: {}\r\n            f:requests:\r\n              .: {}\r\n              f:memory: {}\r\n          f:smartShutdownTimeout: {}\r\n          f:startDelay: {}\r\n          f:stopDelay: {}\r\n          f:storage:\r\n            .: {}\r\n            f:resizeInUseVolumes: {}\r\n            f:size: {}\r\n            f:storageClass: {}\r\n          f:switchoverDelay: {}\r\n      manager: helm-controller\r\n      operation: Update\r\n      time: '2023-11-02T19:56:05Z'\r\n    - apiVersion: postgresql.cnpg.io/v1\r\n      fieldsType: FieldsV1\r\n      fieldsV1:\r\n        f:status:\r\n          .: {}\r\n          f:certificates:\r\n            .: {}\r\n            f:clientCASecret: {}\r\n            f:expirations:\r\n              .: {}\r\n              f:happydogs-db-ca: {}\r\n              f:happydogs-db-replication: {}\r\n              f:happydogs-db-server: {}\r\n            f:replicationTLSSecret: {}\r\n            f:serverAltDNSNames: {}\r\n            f:serverCASecret: {}\r\n            f:serverTLSSecret: {}\r\n          f:cloudNativePGCommitHash: {}\r\n          f:cloudNativePGOperatorHash: {}\r\n          f:conditions: {}\r\n          f:configMapResourceVersion:\r\n            .: {}\r\n            f:metrics:\r\n              .: {}\r\n              f:cnpg-default-monitoring: {}\r\n          f:currentPrimary: {}\r\n          f:currentPrimaryTimestamp: {}\r\n          f:firstRecoverabilityPoint: {}\r\n          f:healthyPVC: {}\r\n          f:instanceNames: {}\r\n          f:instances: {}\r\n          f:instancesReportedState:\r\n            .: {}\r\n            f:happydogs-db-1:\r\n              .: {}\r\n              f:isPrimary: {}\r\n              f:timeLineID: {}\r\n            f:happydogs-db-2:\r\n              .: {}\r\n              f:isPrimary: {}\r\n              f:timeLineID: {}\r\n          f:instancesStatus:\r\n            .: {}\r\n            f:healthy: {}\r\n          f:lastFailedBackup: {}\r\n          f:lastSuccessfulBackup: {}\r\n          f:latestGeneratedNode: {}\r\n          f:managedRolesStatus: {}\r\n          f:phase: {}\r\n          f:poolerIntegrations:\r\n            .: {}\r\n            f:pgBouncerIntegration: {}\r\n          f:pvcCount: {}\r\n          f:readService: {}\r\n          f:readyInstances: {}\r\n          f:secretsResourceVersion:\r\n            .: {}\r\n            f:applicationSecretVersion: {}\r\n            f:clientCaSecretVersion: {}\r\n            f:replicationSecretVersion: {}\r\n            f:serverCaSecretVersion: {}\r\n            f:serverSecretVersion: {}\r\n            f:superuserSecretVersion: {}\r\n          f:targetPrimary: {}\r\n          f:targetPrimaryTimestamp: {}\r\n          f:timelineID: {}\r\n          f:topology:\r\n            .: {}\r\n            f:instances:\r\n              .: {}\r\n              f:happydogs-db-1: {}\r\n              f:happydogs-db-2: {}\r\n            f:nodesUsed: {}\r\n            f:successfullyExtracted: {}\r\n          f:writeService: {}\r\n      manager: manager\r\n      operation: Update\r\n      subresource: status\r\n      time: '2023-11-03T14:59:08Z'\r\n  name: happydogs-db\r\n  namespace: happydogs-prod\r\n  resourceVersion: '4869756'\r\n  uid: 0ba0cd3c-778b-45f3-ab9a-e3cebf131b61\r\n  selfLink: /apis/postgresql.cnpg.io/v1/namespaces/happydogs-prod/clusters/happydogs-db\r\nstatus:\r\n  certificates:\r\n    clientCASecret: happydogs-db-ca\r\n    expirations:\r\n      happydogs-db-ca: 2024-01-27 11:44:06 +0000 UTC\r\n      happydogs-db-replication: 2024-01-27 11:44:06 +0000 UTC\r\n      happydogs-db-server: 2024-01-27 11:44:06 +0000 UTC\r\n    replicationTLSSecret: happydogs-db-replication\r\n    serverAltDNSNames:\r\n      - happydogs-db-rw\r\n      - happydogs-db-rw.happydogs-prod\r\n      - happydogs-db-rw.happydogs-prod.svc\r\n      - happydogs-db-r\r\n      - happydogs-db-r.happydogs-prod\r\n      - happydogs-db-r.happydogs-prod.svc\r\n      - happydogs-db-ro\r\n      - happydogs-db-ro.happydogs-prod\r\n      - happydogs-db-ro.happydogs-prod.svc\r\n    serverCASecret: happydogs-db-ca\r\n    serverTLSSecret: happydogs-db-server\r\n  cloudNativePGCommitHash: 27f62cac\r\n  cloudNativePGOperatorHash: 4912e5eb808aca3bf134923625f2346d404a3c860cf24aca4266493846f8fc3b\r\n  conditions:\r\n    - lastTransitionTime: '2023-11-03T14:59:08Z'\r\n      message: Cluster is Ready\r\n      reason: ClusterIsReady\r\n      status: 'True'\r\n      type: Ready\r\n    - lastTransitionTime: '2023-11-02T20:11:43Z'\r\n      message: 'unexpected failure invoking barman-cloud-wal-archive: exit status 2'\r\n      reason: ContinuousArchivingFailing\r\n      status: 'False'\r\n      type: ContinuousArchiving\r\n    - lastTransitionTime: '2023-11-02T20:00:03Z'\r\n      message: >\r\n        command terminated with exit code 1 -\r\n        {\"level\":\"info\",\"ts\":\"2023-11-02T20:00:03Z\",\"msg\":\"Error while\r\n        requesting\r\n        backup\",\"logging_pod\":\"happydogs-db-2\",\"backupURL\":\"http://localhost:8010/pg/backup\",\"statusCode\":500,\"body\":\"error\r\n        while starting backup: cannot recover backup credentials: while getting\r\n        secret backup-credentials: secrets \\\"backup-credentials\\\" not found\\n\"}\r\n        Error: invalid status code: 500\r\n      reason: LastBackupFailed\r\n      status: 'False'\r\n      type: LastBackupSucceeded\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: '1602882'\r\n  currentPrimary: happydogs-db-2\r\n  currentPrimaryTimestamp: '2023-11-01T20:46:15.145392Z'\r\n  firstRecoverabilityPoint: '2023-11-02T19:22:32Z'\r\n  healthyPVC:\r\n    - happydogs-db-1\r\n    - happydogs-db-2\r\n  instanceNames:\r\n    - happydogs-db-1\r\n    - happydogs-db-2\r\n  instances: 2\r\n  instancesReportedState:\r\n    happydogs-db-1:\r\n      isPrimary: false\r\n      timeLineID: 2\r\n    happydogs-db-2:\r\n      isPrimary: true\r\n      timeLineID: 2\r\n  instancesStatus:\r\n    healthy:\r\n      - happydogs-db-1\r\n      - happydogs-db-2\r\n  lastFailedBackup: '2023-11-02T19:42:00Z'\r\n  lastSuccessfulBackup: '2023-11-02T19:51:02Z'\r\n  latestGeneratedNode: 2\r\n  managedRolesStatus: {}\r\n  phase: Cluster in healthy state\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 2\r\n  readService: happydogs-db-r\r\n  readyInstances: 2\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: '1602851'\r\n    clientCaSecretVersion: '1602846'\r\n    replicationSecretVersion: '1602848'\r\n    serverCaSecretVersion: '1602846'\r\n    serverSecretVersion: '1602847'\r\n    superuserSecretVersion: '1602849'\r\n  targetPrimary: happydogs-db-2\r\n  targetPrimaryTimestamp: '2023-11-01T20:46:10.999231Z'\r\n  timelineID: 2\r\n  topology:\r\n    instances:\r\n      happydogs-db-1: {}\r\n      happydogs-db-2: {}\r\n    nodesUsed: 2\r\n    successfullyExtracted: true\r\n  writeService: happydogs-db-rw\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n  backup:\r\n    barmanObjectStore:\r\n      azureCredentials:\r\n        storageSasToken:\r\n          key: sas-token\r\n          name: backup-credentials\r\n      destinationPath: https://<storage-account>.blob.core.windows.net/backup-prod/prod\r\n      wal:\r\n        compression: gzip\r\n    target: prefer-standby\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enableSuperuserAccess: true\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.0\r\n  instances: 2\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n      - key: queries\r\n        name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: false\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: 'on'\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: '0'\r\n      log_rotation_size: '0'\r\n      log_truncate_on_rotation: 'false'\r\n      logging_collector: 'on'\r\n      max_parallel_workers: '32'\r\n      max_replication_slots: '32'\r\n      max_worker_processes: '32'\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: ''\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      memory: 1512Mi\r\n    requests:\r\n      memory: 1512Mi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 50Gi\r\n    storageClass: managed-premium\r\n  switchoverDelay: 3600\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"happydogs-db-2\",\"record\":{\"log_time\":\"2023-11-03 20:34:47.562 UTC\",\"process_id\":\"27\",\"session_id\":\"65450ab6.1b\",\"session_line_num\":\"1266\",\"session_start_time\":\"2023-11-03 14:59:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/00000002000000000000003F\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"2023-11-03 20:34:49,072 [16233] ERROR: Can't connect to cloud provider: Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\",\"pipe\":\"stderr\",\"logging_pod\":\"happydogs-db-2\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"RequestId:8d10e3a4-501e-0069-3295-0e695d000000\",\"pipe\":\"stderr\",\"logging_pod\":\"happydogs-db-2\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"Time:2023-11-03T20:34:49.0712359Z\",\"pipe\":\"stderr\",\"logging_pod\":\"happydogs-db-2\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"ErrorCode:AuthenticationFailed\",\"pipe\":\"stderr\",\"logging_pod\":\"happydogs-db-2\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"authenticationerrordetail:Signature not valid in the specified time frame: Start [Thu, 02 Nov 2023 18:12:38 GMT] - Expiry [Fri, 03 Nov 2023 03:12:38 GMT] - Current [Fri, 03 Nov 2023 20:34:49 GMT]\",\"pipe\":\"stderr\",\"logging_pod\":\"happydogs-db-2\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"Content: <?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?><Error><Code>AuthenticationFailed</Code><Message>Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\",\"pipe\":\"stderr\",\"logging_pod\":\"happydogs-db-2\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"RequestId:8d10e3a4-501e-0069-3295-0e695d000000\",\"pipe\":\"stderr\",\"logging_pod\":\"happydogs-db-2\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"Time:2023-11-03T20:34:49.0712359Z</Message><AuthenticationErrorDetail>Signature not valid in the specified time frame: Start [Thu, 02 Nov 2023 18:12:38 GMT] - Expiry [Fri, 03 Nov 2023 03:12:38 GMT] - Current [Fri, 03 Nov 2023 20:34:49 GMT]</AuthenticationErrorDetail></Error>\",\"pipe\":\"stderr\",\"logging_pod\":\"happydogs-db-2\"}\r\n{\"level\":\"error\",\"ts\":\"2023-11-03T20:34:49Z\",\"msg\":\"Error invoking barman-cloud-wal-archive\",\"logging_pod\":\"happydogs-db-2\",\"walName\":\"pg_wal/00000002000000000000003F\",\"currentPrimary\":\"happydogs-db-2\",\"targetPrimary\":\"happydogs-db-2\",\"options\":[\"--gzip\",\"--cloud-provider\",\"azure-blob-storage\",\"https://happydogssaprod.blob.core.windows.net/backup-prod/happydogs-prod\",\"happydogs-db\",\"pg_wal/00000002000000000000003F\"],\"exitCode\":-1,\"error\":\"exit status 2\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/barman/archiver.(*WALArchiver).Archive\\n\\tpkg/management/barman/archiver/archiver.go:186\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/barman/archiver.(*WALArchiver).ArchiveList.func1\\n\\tpkg/management/barman/archiver/archiver.go:131\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"wal-archive\",\"msg\":\"Failed archiving WAL: PostgreSQL will retry\",\"logging_pod\":\"happydogs-db-2\",\"walName\":\"pg_wal/00000002000000000000003F\",\"startTime\":\"2023-11-03T20:34:48Z\",\"endTime\":\"2023-11-03T20:34:49Z\",\"elapsedWalTime\":0.391241473,\"error\":\"unexpected failure invoking barman-cloud-wal-archive: exit status 2\"}\r\n{\"level\":\"error\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"happydogs-db-2\",\"error\":\"unexpected failure invoking barman-cloud-wal-archive: exit status 2\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:95\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.3/x64/src/runtime/proc.go:267\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"happydogs-db-2\",\"record\":{\"log_time\":\"2023-11-03 20:34:49.123 UTC\",\"process_id\":\"27\",\"session_id\":\"65450ab6.1b\",\"session_line_num\":\"1267\",\"session_start_time\":\"2023-11-03 14:59:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/00000002000000000000003F\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"happydogs-db-2\",\"record\":{\"log_time\":\"2023-11-03 20:34:49.123 UTC\",\"process_id\":\"27\",\"session_id\":\"65450ab6.1b\",\"session_line_num\":\"1268\",\"session_start_time\":\"2023-11-03 14:59:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"WARNING\",\"sql_state_code\":\"01000\",\"message\":\"archiving write-ahead log file \\\"00000002000000000000003F\\\" failed too many times, will try again later\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nrichard.hagen@gmail.com\n### Version\n1.21.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nSelf-managed: kind (evaluation)\n### How did you install the operator?\nHelm\n### What happened?\nI used a broken SAS token to upload backups to a Azure Storage Account, this worked great for the first 5-6 hours, but then stopped working since the secret expired.\r\nI have recreated the secret with a updated longer lived token, but the backups are still failing, and the logs shows clearly that its using the old secret:\r\n```\r\nSignature not valid in the specified time frame: Start [Thu, 02 Nov 2023 18:12:38 GMT] - Expiry [Fri, 03 Nov 2023 03:12:38 GMT] - Current [Fri, 03 Nov 2023 20:34:49 GMT]\r\n```\r\nBut the updated secret is valid from this morning, until 1. january 2025. \r\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/restartedAt: '2023-11-01T21:08:27+01:00'\r\n    meta.helm.sh/release-name: happydogs\r\n    meta.helm.sh/release-namespace: happydogs-prod\r\n  creationTimestamp: '2023-10-29T11:49:06Z'\r\n  generation: 10\r\n  labels:\r\n    app.kubernetes.io/managed-by: Helm\r\n    helm.toolkit.fluxcd.io/name: happydogs-prod\r\n    helm.toolkit.fluxcd.io/namespace: happydogs-prod\r\n    k8slens-edit-resource-version: v1\r\n  managedFields:\r\n    - apiVersion: postgresql.cnpg.io/v1\r\n      fieldsType: FieldsV1\r\n      fieldsV1:\r\n        f:metadata:\r\n          f:annotations:\r\n            f:kubectl.kubernetes.io/restartedAt: {}\r\n      manager: kubectl-cnpg\r\n      operation: Update\r\n      time: '2023-11-01T20:08:27Z'\r\n    - apiVersion: postgresql.cnpg.io/v1\r\n      fieldsType: FieldsV1\r\n      fieldsV1:\r\n        f:spec:\r\n          f:backup:\r\n            f:barmanObjectStore:\r\n              f:destinationPath: {}\r\n      manager: kubectl-edit\r\n      operation: Update\r\n      time: '2023-11-02T19:20:35Z'\r\n    - apiVersion: postgresql.cnpg.io/v1\r\n      fieldsType: FieldsV1\r\n      fieldsV1:\r\n        f:metadata:\r\n          f:labels:\r\n            f:k8slens-edit-resource-version: {}\r\n        f:spec:\r\n          f:backup:\r\n            f:barmanObjectStore:\r\n              f:azureCredentials:\r\n                f:storageSasToken:\r\n                  .: {}\r\n                  f:key: {}\r\n                  f:name: {}\r\n      manager: node-fetch\r\n      operation: Update\r\n      time: '2023-11-02T19:42:43Z'\r\n    - apiVersion: postgresql.cnpg.io/v1\r\n      fieldsType: FieldsV1\r\n      fieldsV1:\r\n        f:metadata:\r\n          f:annotations:\r\n            .: {}\r\n            f:meta.helm.sh/release-name: {}\r\n            f:meta.helm.sh/release-namespace: {}\r\n          f:labels:\r\n            .: {}\r\n            f:app.kubernetes.io/managed-by: {}\r\n            f:helm.toolkit.fluxcd.io/name: {}\r\n            f:helm.toolkit.fluxcd.io/namespace: {}\r\n        f:spec:\r\n          .: {}\r\n          f:backup:\r\n            .: {}\r\n            f:barmanObjectStore:\r\n              .: {}\r\n              f:azureCredentials: {}\r\n              f:wal:\r\n                .: {}\r\n                f:compression: {}\r\n            f:target: {}\r\n          f:enableSuperuserAccess: {}\r\n          f:failoverDelay: {}\r\n          f:instances: {}\r\n          f:logLevel: {}\r\n          f:maxSyncReplicas: {}\r\n          f:minSyncReplicas: {}\r\n          f:postgresGID: {}\r\n          f:postgresUID: {}\r\n          f:primaryUpdateMethod: {}\r\n          f:primaryUpdateStrategy: {}\r\n          f:replicationSlots:\r\n            .: {}\r\n            f:highAvailability:\r\n              .: {}\r\n              f:enabled: {}\r\n              f:slotPrefix: {}\r\n            f:updateInterval: {}\r\n          f:resources:\r\n            .: {}\r\n            f:limits:\r\n              .: {}\r\n              f:memory: {}\r\n            f:requests:\r\n              .: {}\r\n              f:memory: {}\r\n          f:smartShutdownTimeout: {}\r\n          f:startDelay: {}\r\n          f:stopDelay: {}\r\n          f:storage:\r\n            .: {}\r\n            f:resizeInUseVolumes: {}\r\n            f:size: {}\r\n            f:storageClass: {}\r\n          f:switchoverDelay: {}\r\n      manager: helm-controller\r\n      operation: Update\r\n      time: '2023-11-02T19:56:05Z'\r\n    - apiVersion: postgresql.cnpg.io/v1\r\n      fieldsType: FieldsV1\r\n      fieldsV1:\r\n        f:status:\r\n          .: {}\r\n          f:certificates:\r\n            .: {}\r\n            f:clientCASecret: {}\r\n            f:expirations:\r\n              .: {}\r\n              f:happydogs-db-ca: {}\r\n              f:happydogs-db-replication: {}\r\n              f:happydogs-db-server: {}\r\n            f:replicationTLSSecret: {}\r\n            f:serverAltDNSNames: {}\r\n            f:serverCASecret: {}\r\n            f:serverTLSSecret: {}\r\n          f:cloudNativePGCommitHash: {}\r\n          f:cloudNativePGOperatorHash: {}\r\n          f:conditions: {}\r\n          f:configMapResourceVersion:\r\n            .: {}\r\n            f:metrics:\r\n              .: {}\r\n              f:cnpg-default-monitoring: {}\r\n          f:currentPrimary: {}\r\n          f:currentPrimaryTimestamp: {}\r\n          f:firstRecoverabilityPoint: {}\r\n          f:healthyPVC: {}\r\n          f:instanceNames: {}\r\n          f:instances: {}\r\n          f:instancesReportedState:\r\n            .: {}\r\n            f:happydogs-db-1:\r\n              .: {}\r\n              f:isPrimary: {}\r\n              f:timeLineID: {}\r\n            f:happydogs-db-2:\r\n              .: {}\r\n              f:isPrimary: {}\r\n              f:timeLineID: {}\r\n          f:instancesStatus:\r\n            .: {}\r\n            f:healthy: {}\r\n          f:lastFailedBackup: {}\r\n          f:lastSuccessfulBackup: {}\r\n          f:latestGeneratedNode: {}\r\n          f:managedRolesStatus: {}\r\n          f:phase: {}\r\n          f:poolerIntegrations:\r\n            .: {}\r\n            f:pgBouncerIntegration: {}\r\n          f:pvcCount: {}\r\n          f:readService: {}\r\n          f:readyInstances: {}\r\n          f:secretsResourceVersion:\r\n            .: {}\r\n            f:applicationSecretVersion: {}\r\n            f:clientCaSecretVersion: {}\r\n            f:replicationSecretVersion: {}\r\n            f:serverCaSecretVersion: {}\r\n            f:serverSecretVersion: {}\r\n            f:superuserSecretVersion: {}\r\n          f:targetPrimary: {}\r\n          f:targetPrimaryTimestamp: {}\r\n          f:timelineID: {}\r\n          f:topology:\r\n            .: {}\r\n            f:instances:\r\n              .: {}\r\n              f:happydogs-db-1: {}\r\n              f:happydogs-db-2: {}\r\n            f:nodesUsed: {}\r\n            f:successfullyExtracted: {}\r\n          f:writeService: {}\r\n      manager: manager\r\n      operation: Update\r\n      subresource: status\r\n      time: '2023-11-03T14:59:08Z'\r\n  name: happydogs-db\r\n  namespace: happydogs-prod\r\n  resourceVersion: '4869756'\r\n  uid: 0ba0cd3c-778b-45f3-ab9a-e3cebf131b61\r\n  selfLink: /apis/postgresql.cnpg.io/v1/namespaces/happydogs-prod/clusters/happydogs-db\r\nstatus:\r\n  certificates:\r\n    clientCASecret: happydogs-db-ca\r\n    expirations:\r\n      happydogs-db-ca: 2024-01-27 11:44:06 +0000 UTC\r\n      happydogs-db-replication: 2024-01-27 11:44:06 +0000 UTC\r\n      happydogs-db-server: 2024-01-27 11:44:06 +0000 UTC\r\n    replicationTLSSecret: happydogs-db-replication\r\n    serverAltDNSNames:\r\n      - happydogs-db-rw\r\n      - happydogs-db-rw.happydogs-prod\r\n      - happydogs-db-rw.happydogs-prod.svc\r\n      - happydogs-db-r\r\n      - happydogs-db-r.happydogs-prod\r\n      - happydogs-db-r.happydogs-prod.svc\r\n      - happydogs-db-ro\r\n      - happydogs-db-ro.happydogs-prod\r\n      - happydogs-db-ro.happydogs-prod.svc\r\n    serverCASecret: happydogs-db-ca\r\n    serverTLSSecret: happydogs-db-server\r\n  cloudNativePGCommitHash: 27f62cac\r\n  cloudNativePGOperatorHash: 4912e5eb808aca3bf134923625f2346d404a3c860cf24aca4266493846f8fc3b\r\n  conditions:\r\n    - lastTransitionTime: '2023-11-03T14:59:08Z'\r\n      message: Cluster is Ready\r\n      reason: ClusterIsReady\r\n      status: 'True'\r\n      type: Ready\r\n    - lastTransitionTime: '2023-11-02T20:11:43Z'\r\n      message: 'unexpected failure invoking barman-cloud-wal-archive: exit status 2'\r\n      reason: ContinuousArchivingFailing\r\n      status: 'False'\r\n      type: ContinuousArchiving\r\n    - lastTransitionTime: '2023-11-02T20:00:03Z'\r\n      message: >\r\n        command terminated with exit code 1 -\r\n        {\"level\":\"info\",\"ts\":\"2023-11-02T20:00:03Z\",\"msg\":\"Error while\r\n        requesting\r\n        backup\",\"logging_pod\":\"happydogs-db-2\",\"backupURL\":\"http://localhost:8010/pg/backup\",\"statusCode\":500,\"body\":\"error\r\n        while starting backup: cannot recover backup credentials: while getting\r\n        secret backup-credentials: secrets \\\"backup-credentials\\\" not found\\n\"}\r\n        Error: invalid status code: 500\r\n      reason: LastBackupFailed\r\n      status: 'False'\r\n      type: LastBackupSucceeded\r\n  configMapResourceVersion:\r\n    metrics:\r\n      cnpg-default-monitoring: '1602882'\r\n  currentPrimary: happydogs-db-2\r\n  currentPrimaryTimestamp: '2023-11-01T20:46:15.145392Z'\r\n  firstRecoverabilityPoint: '2023-11-02T19:22:32Z'\r\n  healthyPVC:\r\n    - happydogs-db-1\r\n    - happydogs-db-2\r\n  instanceNames:\r\n    - happydogs-db-1\r\n    - happydogs-db-2\r\n  instances: 2\r\n  instancesReportedState:\r\n    happydogs-db-1:\r\n      isPrimary: false\r\n      timeLineID: 2\r\n    happydogs-db-2:\r\n      isPrimary: true\r\n      timeLineID: 2\r\n  instancesStatus:\r\n    healthy:\r\n      - happydogs-db-1\r\n      - happydogs-db-2\r\n  lastFailedBackup: '2023-11-02T19:42:00Z'\r\n  lastSuccessfulBackup: '2023-11-02T19:51:02Z'\r\n  latestGeneratedNode: 2\r\n  managedRolesStatus: {}\r\n  phase: Cluster in healthy state\r\n  poolerIntegrations:\r\n    pgBouncerIntegration: {}\r\n  pvcCount: 2\r\n  readService: happydogs-db-r\r\n  readyInstances: 2\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: '1602851'\r\n    clientCaSecretVersion: '1602846'\r\n    replicationSecretVersion: '1602848'\r\n    serverCaSecretVersion: '1602846'\r\n    serverSecretVersion: '1602847'\r\n    superuserSecretVersion: '1602849'\r\n  targetPrimary: happydogs-db-2\r\n  targetPrimaryTimestamp: '2023-11-01T20:46:10.999231Z'\r\n  timelineID: 2\r\n  topology:\r\n    instances:\r\n      happydogs-db-1: {}\r\n      happydogs-db-2: {}\r\n    nodesUsed: 2\r\n    successfullyExtracted: true\r\n  writeService: happydogs-db-rw\r\nspec:\r\n  affinity:\r\n    podAntiAffinityType: preferred\r\n  backup:\r\n    barmanObjectStore:\r\n      azureCredentials:\r\n        storageSasToken:\r\n          key: sas-token\r\n          name: backup-credentials\r\n      destinationPath: https://<storage-account>.blob.core.windows.net/backup-prod/prod\r\n      wal:\r\n        compression: gzip\r\n    target: prefer-standby\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      encoding: UTF8\r\n      localeCType: C\r\n      localeCollate: C\r\n      owner: app\r\n  enableSuperuserAccess: true\r\n  failoverDelay: 0\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:16.0\r\n  instances: 2\r\n  logLevel: info\r\n  maxSyncReplicas: 0\r\n  minSyncReplicas: 0\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n      - key: queries\r\n        name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: false\r\n  postgresGID: 26\r\n  postgresUID: 26\r\n  postgresql:\r\n    parameters:\r\n      archive_mode: 'on'\r\n      archive_timeout: 5min\r\n      dynamic_shared_memory_type: posix\r\n      log_destination: csvlog\r\n      log_directory: /controller/log\r\n      log_filename: postgres\r\n      log_rotation_age: '0'\r\n      log_rotation_size: '0'\r\n      log_truncate_on_rotation: 'false'\r\n      logging_collector: 'on'\r\n      max_parallel_workers: '32'\r\n      max_replication_slots: '32'\r\n      max_worker_processes: '32'\r\n      shared_memory_type: mmap\r\n      shared_preload_libraries: ''\r\n      wal_keep_size: 512MB\r\n      wal_receiver_timeout: 5s\r\n      wal_sender_timeout: 5s\r\n    syncReplicaElectionConstraint:\r\n      enabled: false\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n  replicationSlots:\r\n    highAvailability:\r\n      enabled: true\r\n      slotPrefix: _cnpg_\r\n    updateInterval: 30\r\n  resources:\r\n    limits:\r\n      memory: 1512Mi\r\n    requests:\r\n      memory: 1512Mi\r\n  smartShutdownTimeout: 180\r\n  startDelay: 3600\r\n  stopDelay: 1800\r\n  storage:\r\n    resizeInUseVolumes: true\r\n    size: 50Gi\r\n    storageClass: managed-premium\r\n  switchoverDelay: 3600\n```\n### Relevant log output\n```shell\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:47Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"happydogs-db-2\",\"record\":{\"log_time\":\"2023-11-03 20:34:47.562 UTC\",\"process_id\":\"27\",\"session_id\":\"65450ab6.1b\",\"session_line_num\":\"1266\",\"session_start_time\":\"2023-11-03 14:59:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/00000002000000000000003F\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"2023-11-03 20:34:49,072 [16233] ERROR: Can't connect to cloud provider: Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\",\"pipe\":\"stderr\",\"logging_pod\":\"happydogs-db-2\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"RequestId:8d10e3a4-501e-0069-3295-0e695d000000\",\"pipe\":\"stderr\",\"logging_pod\":\"happydogs-db-2\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"Time:2023-11-03T20:34:49.0712359Z\",\"pipe\":\"stderr\",\"logging_pod\":\"happydogs-db-2\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"ErrorCode:AuthenticationFailed\",\"pipe\":\"stderr\",\"logging_pod\":\"happydogs-db-2\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"authenticationerrordetail:Signature not valid in the specified time frame: Start [Thu, 02 Nov 2023 18:12:38 GMT] - Expiry [Fri, 03 Nov 2023 03:12:38 GMT] - Current [Fri, 03 Nov 2023 20:34:49 GMT]\",\"pipe\":\"stderr\",\"logging_pod\":\"happydogs-db-2\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"Content: <?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?><Error><Code>AuthenticationFailed</Code><Message>Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\",\"pipe\":\"stderr\",\"logging_pod\":\"happydogs-db-2\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"RequestId:8d10e3a4-501e-0069-3295-0e695d000000\",\"pipe\":\"stderr\",\"logging_pod\":\"happydogs-db-2\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"barman-cloud-wal-archive\",\"msg\":\"Time:2023-11-03T20:34:49.0712359Z</Message><AuthenticationErrorDetail>Signature not valid in the specified time frame: Start [Thu, 02 Nov 2023 18:12:38 GMT] - Expiry [Fri, 03 Nov 2023 03:12:38 GMT] - Current [Fri, 03 Nov 2023 20:34:49 GMT]</AuthenticationErrorDetail></Error>\",\"pipe\":\"stderr\",\"logging_pod\":\"happydogs-db-2\"}\r\n{\"level\":\"error\",\"ts\":\"2023-11-03T20:34:49Z\",\"msg\":\"Error invoking barman-cloud-wal-archive\",\"logging_pod\":\"happydogs-db-2\",\"walName\":\"pg_wal/00000002000000000000003F\",\"currentPrimary\":\"happydogs-db-2\",\"targetPrimary\":\"happydogs-db-2\",\"options\":[\"--gzip\",\"--cloud-provider\",\"azure-blob-storage\",\"https://happydogssaprod.blob.core.windows.net/backup-prod/happydogs-prod\",\"happydogs-db\",\"pg_wal/00000002000000000000003F\"],\"exitCode\":-1,\"error\":\"exit status 2\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/barman/archiver.(*WALArchiver).Archive\\n\\tpkg/management/barman/archiver/archiver.go:186\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/barman/archiver.(*WALArchiver).ArchiveList.func1\\n\\tpkg/management/barman/archiver/archiver.go:131\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"wal-archive\",\"msg\":\"Failed archiving WAL: PostgreSQL will retry\",\"logging_pod\":\"happydogs-db-2\",\"walName\":\"pg_wal/00000002000000000000003F\",\"startTime\":\"2023-11-03T20:34:48Z\",\"endTime\":\"2023-11-03T20:34:49Z\",\"elapsedWalTime\":0.391241473,\"error\":\"unexpected failure invoking barman-cloud-wal-archive: exit status 2\"}\r\n{\"level\":\"error\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"happydogs-db-2\",\"error\":\"unexpected failure invoking barman-cloud-wal-archive: exit status 2\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:95\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.21.3/x64/src/runtime/proc.go:267\"}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"happydogs-db-2\",\"record\":{\"log_time\":\"2023-11-03 20:34:49.123 UTC\",\"process_id\":\"27\",\"session_id\":\"65450ab6.1b\",\"session_line_num\":\"1267\",\"session_start_time\":\"2023-11-03 14:59:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/00000002000000000000003F\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-11-03T20:34:49Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"happydogs-db-2\",\"record\":{\"log_time\":\"2023-11-03 20:34:49.123 UTC\",\"process_id\":\"27\",\"session_id\":\"65450ab6.1b\",\"session_line_num\":\"1268\",\"session_start_time\":\"2023-11-03 14:59:02 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"WARNING\",\"sql_state_code\":\"01000\",\"message\":\"archiving write-ahead log file \\\"00000002000000000000003F\\\" failed too many times, will try again later\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductHi @Richard87, can you run the plugin command `kubectl cnpg reload <cluster_name>`, it can reload resources the operator does not watch\r\nReference: https://cloudnative-pg.io/documentation/1.20/kubectl-plugin/#reload\n---\nThanks, i will try that next time! Is there any wish to have this handled transparantlt by the operator?\n---\nperhaps setting the label `cnpg.io/reload` could help? Source: https://cloudnative-pg.io/documentation/1.20/labels_annotations/\r\n```\r\ncnpg.io/reload\r\nAvailable on ConfigMap and Secret resources. When set to true, a change in the resource will be automatically reloaded by the operator.\r\n```\n---\nI ran into the same issue. Using `kubectl cnpg reload <cluster_name>` worked for me but using the label `cnpg.io/reload` doesn't do anything.\r\nAlso, updating the cluster with a new secret reference with the new token gives an error like this:\r\n```\"level\":\"error\",\"ts\":\"2023-11-15T10:54:49Z\",\"msg\":\"while getting recover credentials\",\"logging_pod\":\"<cluster-name>-1\",\"error\":\"while getting secret <new-secret-name>: secrets \\\"<new-secret-name>\\\" is forbidden: User \\\"system:serviceaccount:<ns>:<sa>\\\" cannot get resource \\\"secrets\\\" in API group \\\"\\\" in the namespace \\\"<ns>\\\"\",\"stacktrace\":\"github.com/cloudnative-pg...```"
    },
    {
        "title": "[Bug]: Setting a priorityClassName requires manual restart of the pods.",
        "id": 1975600440,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nkarl.bohlmark@netinsight.net\n### Version\nolder in 1.20.x\n### What version of Kubernetes are you using?\n1.25\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nI configured a priorityClassName for the cluster. It did not take effect without manual restart of the pods.\r\nI have\r\n```\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n```\r\nI expected the operator reconciliation process to ensure that the change propagated to the pods.\r\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\nkarl.bohlmark@netinsight.net\n### Version\nolder in 1.20.x\n### What version of Kubernetes are you using?\n1.25\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nHelm\n### What happened?\nI configured a priorityClassName for the cluster. It did not take effect without manual restart of the pods.\r\nI have\r\n```\r\n  primaryUpdateMethod: restart\r\n  primaryUpdateStrategy: unsupervised\r\n```\r\nI expected the operator reconciliation process to ensure that the change propagated to the pods.\r\n### Cluster resource\n_No response_\n### Relevant log output\n_No response_\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Implement peering support in PgBouncer",
        "id": 1971568676,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nWe configure PgBouncer behind a `Service` that automatically load balances between all available pods. Thus a cancellation request from a client may end up in a different PgBouncer instance than the one the client was initially connected to. Postgres will ignore the cancellation request because the cancel key won't match what Postgres has. Thus under such a deployment and heavy load, Postgres might have many idle open connections.\r\n### Describe the solution you'd like\r\nWe should be able to have a headless service for each PgBouncer instance so that all of them can talk to one another. The operator already manages PgBouncer config. Thus the operator can add a `[peers]` section to the config keyed on each PgBouncer instance. This section should be managed by the operator.\r\n### Describe alternatives you've considered\r\nKilling idle connections on Postgres is an option, using config like `statement_timeout`, `idle_in_transaction_session_timeout`, and `idle_session_timeout`. But all that adds more operational burden on the admin. Having peering enabled is a more transparent solution.\r\n### Additional context\r\nThe connection cancellation problem is described in [PgBouncer docs](https://www.pgbouncer.org/config.html#section-peers). And in this [video](https://www.youtube.com/watch?v=M585FfbboNA). Also https://github.com/pgbouncer/pgbouncer/pull/666\r\n### Backport?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\r\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\r\n### What problem is this feature going to solve? Why should it be added?\r\nWe configure PgBouncer behind a `Service` that automatically load balances between all available pods. Thus a cancellation request from a client may end up in a different PgBouncer instance than the one the client was initially connected to. Postgres will ignore the cancellation request because the cancel key won't match what Postgres has. Thus under such a deployment and heavy load, Postgres might have many idle open connections.\r\n### Describe the solution you'd like\r\nWe should be able to have a headless service for each PgBouncer instance so that all of them can talk to one another. The operator already manages PgBouncer config. Thus the operator can add a `[peers]` section to the config keyed on each PgBouncer instance. This section should be managed by the operator.\r\n### Describe alternatives you've considered\r\nKilling idle connections on Postgres is an option, using config like `statement_timeout`, `idle_in_transaction_session_timeout`, and `idle_session_timeout`. But all that adds more operational burden on the admin. Having peering enabled is a more transparent solution.\r\n### Additional context\r\nThe connection cancellation problem is described in [PgBouncer docs](https://www.pgbouncer.org/config.html#section-peers). And in this [video](https://www.youtube.com/watch?v=M585FfbboNA). Also https://github.com/pgbouncer/pgbouncer/pull/666\r\n### Backport?\r\nNo\r\n### Code of Conduct\r\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: volume snapshot cold backup of an hibernated cluster",
        "id": 1970353408,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nFor maintenance, it could be useful to hibernate a cluster and then take a snapshot, to be on the safe side (as a former DBA I would appreciate this a lot).\n### Describe the solution you'd like\nIf a cluster is hibernated, enable taking an offline/cold volume snapshot backup on the requested target instance.\n### Describe alternatives you've considered\nN/A\n### Additional context\nBackport to 1.21\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nFor maintenance, it could be useful to hibernate a cluster and then take a snapshot, to be on the safe side (as a former DBA I would appreciate this a lot).\n### Describe the solution you'd like\nIf a cluster is hibernated, enable taking an offline/cold volume snapshot backup on the requested target instance.\n### Describe alternatives you've considered\nN/A\n### Additional context\nBackport to 1.21\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct"
    },
    {
        "title": "[Feature]: Cluster without default user",
        "id": 1968522542,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWe use the new declarative role management to create an admin user:\r\n```\r\n...\r\n  managed:\r\n    roles:\r\n    - connectionLimit: -1\r\n      createdb: true\r\n      createrole: true\r\n      ensure: present\r\n      inherit: true\r\n      login: true\r\n      name: admin\r\n      passwordSecret:\r\n        name: cnpg-admin-secret\r\n```\r\nThe user of the DB is supposed to handle user/db creation them self hence the default app user/db created by bootstrap.initdb isn't needed.\n### Describe the solution you'd like\nWe like to see an option where none of the bootstrap options (initdb/pg_basebackup/recovery) is used e.g. by configuring `bootstrap: false`.\n### Describe alternatives you've considered\nI've checked, but couldn't find any option to disable the `app` db/user creation.\n### Additional context\n_No response_\n### Backport?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWe use the new declarative role management to create an admin user:\r\n```\r\n...\r\n  managed:\r\n    roles:\r\n    - connectionLimit: -1\r\n      createdb: true\r\n      createrole: true\r\n      ensure: present\r\n      inherit: true\r\n      login: true\r\n      name: admin\r\n      passwordSecret:\r\n        name: cnpg-admin-secret\r\n```\r\nThe user of the DB is supposed to handle user/db creation them self hence the default app user/db created by bootstrap.initdb isn't needed.\n### Describe the solution you'd like\nWe like to see an option where none of the bootstrap options (initdb/pg_basebackup/recovery) is used e.g. by configuring `bootstrap: false`.\n### Describe alternatives you've considered\nI've checked, but couldn't find any option to disable the `app` db/user creation.\n### Additional context\n_No response_\n### Backport?\nNo\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct> We like to see an option where none of the bootstrap options (initdb/pg_basebackup/recovery)\r\nWhat does that even mean?\n---\nOh ... I get it now. We cannot skip `initdb` as that's required by Postgres to create a cluster. You simply want, as part of the initdb bootstrap, to avoid creating an app user and database. Is this correct?\n---\nThat's correct, yes. Sorry for being inaccurate about this!\n---\nWe are having the same problem with clusters bootstrapped from external non-CNPG databases. The operator keeps trying to alter the app role after booting the base and fails with `ERROR: role \\\"app\\\" does not exist`.\r\nWe don't use the app database and role so we would like them to be togglable.\n---\nI also get this error after using initdb.import from a non-cnpg deployment of cnpg that doesn't have an \"app\" anything. I'm importing one database and one role, which works flawlessly. The database, owner, and password all appear correct and work. However, the <cluster-name>-app secret gets created nevertheless with the username as \"app\" and the Reconciler Error regularly appearing stating that <\"error\":\"while updating database owner password: ERROR: role \\\"app\\\" does not exist (SQLSTATE 42704)\"> . \nIs there a known documented solution to this that I'm missing, or some magic workaround other than manually creating the \"app\" role to make cnpg happy once again?\nThanks a lot for you help!"
    },
    {
        "title": "[Bug]: cnpg gets upset if a mutating admission controller changes the image to a pull-through proxy",
        "id": 1966248500,
        "state": "open",
        "first": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ndluke@geeklair.net\n### Version\n1.21.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nI recently installed habor + imageswap-webhook to use a pull-through caching proxy for images from ghcr.io wherever applicable.\r\nThis works just fine for my configured pooler (it runs a pgbouncer image pulled from my local cache). My cnpg 2-node Cluster kept trying to recover one of the nodes because it didn't think the version of the image was correct.\r\nI was able to work around it by specifying an imageName that references my local proxy in my Cluster definition.  This works, but means I have to remember to modify the cluster definition if I ever turn this all off (instead of just removing the mutating webhook or disabling it from the namespace my cluster is running in).\r\nI haven't looked through the code yet, but I suspect something is parsing the image name, which ideally wouldn't.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: foodb\r\n  namespace: foo\r\nspec:\r\n  imageName: harbor.k8s.geeklair.net/ghcr.io/cloudnative-pg/postgresql:15.3\r\n  instances: 2\r\n  primaryUpdateStrategy: unsupervised\r\n  storage:\r\n    size: 10Gi\r\n  backup:\r\n    retentionPolicy: \"7d\"\r\n    barmanObjectStore:\r\n      wal:\r\n        compression: bzip2\r\n      data:\r\n        compression: bzip2\r\n      endpointCA:\r\n        name:\r\n        key: \r\n      destinationPath: \"s3://cnpgbackups\"\r\n      endpointURL: \"http://minio.lintilla-minio\"\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: minio-creds\r\n          key: ACCESS_KEY_ID\r\n        secretAccessKey:\r\n          name: minio-creds\r\n          key: ACCESS_SECRET_KEY\n```\n### Relevant log output\n```shell\nsorry, I didn't save this, I can recreate it pretty easily if that would be desirable, though.\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this bug?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new bug.\n### I have read the troubleshooting guide\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### I am running a supported version of CloudNativePG\n- [X] I have read the troubleshooting guide and I think this is a new bug.\n### Contact Details\ndluke@geeklair.net\n### Version\n1.21.0\n### What version of Kubernetes are you using?\n1.28\n### What is your Kubernetes environment?\nOther\n### How did you install the operator?\nYAML manifest\n### What happened?\nI recently installed habor + imageswap-webhook to use a pull-through caching proxy for images from ghcr.io wherever applicable.\r\nThis works just fine for my configured pooler (it runs a pgbouncer image pulled from my local cache). My cnpg 2-node Cluster kept trying to recover one of the nodes because it didn't think the version of the image was correct.\r\nI was able to work around it by specifying an imageName that references my local proxy in my Cluster definition.  This works, but means I have to remember to modify the cluster definition if I ever turn this all off (instead of just removing the mutating webhook or disabling it from the namespace my cluster is running in).\r\nI haven't looked through the code yet, but I suspect something is parsing the image name, which ideally wouldn't.\n### Cluster resource\n```shell\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: foodb\r\n  namespace: foo\r\nspec:\r\n  imageName: harbor.k8s.geeklair.net/ghcr.io/cloudnative-pg/postgresql:15.3\r\n  instances: 2\r\n  primaryUpdateStrategy: unsupervised\r\n  storage:\r\n    size: 10Gi\r\n  backup:\r\n    retentionPolicy: \"7d\"\r\n    barmanObjectStore:\r\n      wal:\r\n        compression: bzip2\r\n      data:\r\n        compression: bzip2\r\n      endpointCA:\r\n        name:\r\n        key: \r\n      destinationPath: \"s3://cnpgbackups\"\r\n      endpointURL: \"http://minio.lintilla-minio\"\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: minio-creds\r\n          key: ACCESS_KEY_ID\r\n        secretAccessKey:\r\n          name: minio-creds\r\n          key: ACCESS_SECRET_KEY\n```\n### Relevant log output\n```shell\nsorry, I didn't save this, I can recreate it pretty easily if that would be desirable, though.\n```\n### Code of Conduct\n- [X] I agree to follow this project's Code of ConductThere's very little information here, what was happening, what does\r\n > cnpg gets upset\r\n or\r\n > My cnpg 2-node Cluster kept trying to recover one of the nodes because it didn't think the version of the image was correct.\r\n mean concretely? \r\n I'm also using a mutating webhook to use harbor as a pull-through cache, so I'm interested in this.\n---\nDid you try it? cnpg thinks the version of the image pulled through harbor is not correct and terminates the pod (which then happens over and over).\n---\nThis information is important to provide:\r\n```\r\n  phase: Primary instance is being restarted without a switchover\r\n  phaseReason: 'the instance is using an old image: harbor.mydomain/ghcr/cloudnative-pg/postgresql:16.0\r\n    -> ghcr.io/cloudnative-pg/postgresql:16.0'\r\n```\n---\nHow about better - the problem is here: https://github.com/cloudnative-pg/cloudnative-pg/blob/faec6d7097b72900e12d88c1e30982e4e11b3c54/controllers/cluster_upgrade.go#L419 cnpg expects pgCurrentImageName == targetImageName (which means you can't use a mutating webhook to change the image name)\n---\n:+1: on this issue. I'm currently using [k8s-image-swapper](https://github.com/estahn/k8s-image-swapper) and I'm facing the same problem.\r\nIt's a good production practice to have container images cached locally.\r\nWhat do you think about checking the sha256 of the image instead ? Actually the name doesn't mean the image is the same :-)\r\nI'll be pleased to open a PR if it suites everyone :-)\n---\nFWIW I can also confirm the same is happening when using cloudnative-pg along with [kuik](https://github.com/enix/kube-image-keeper)\n---\nKuik users (likely same applies to similar systems) can bypass that by prefixing their manifest with localhost:7439 both on the operator (required by the init-container) and the clusters. However I would be curious to hear what is the recommended way to deal with that from the maintainers."
    },
    {
        "title": "[Feature]: Pooler does not create secret for JDBC-URI",
        "id": 1962968486,
        "state": "open",
        "first": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWhen we deploy cluster along with pool, cloudnative-pg create secret that include JDBC-URI that is pointing to primary RW service,\r\n`jdbc:postgresql://prod-rw:5432/data?password=zMoUxzv&user=data`\r\nThere is no secret for Pooler service\n### Describe the solution you'd like\nWhen any one deploy Pooler it should create secret that point to Pool-service like this\r\n```\r\nhost: pooler-prod\r\nJDBC-URI: jdbc:postgresql://pooler-prod:5432/data?password=zMoUxzv&user=data\r\n```\r\nOne more important thing is that pooler should wait for cluster up & running then pooler pods will be deploy.\r\n### Describe alternatives you've considered\nManual way to create secret but that is not highly recomended.\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conduct",
        "messages": "### Is there an existing issue already for this feature request/idea?\n- [X] I have searched for an existing issue, and could not find anything. I believe this is a new feature request to be evaluated.\n### What problem is this feature going to solve? Why should it be added?\nWhen we deploy cluster along with pool, cloudnative-pg create secret that include JDBC-URI that is pointing to primary RW service,\r\n`jdbc:postgresql://prod-rw:5432/data?password=zMoUxzv&user=data`\r\nThere is no secret for Pooler service\n### Describe the solution you'd like\nWhen any one deploy Pooler it should create secret that point to Pool-service like this\r\n```\r\nhost: pooler-prod\r\nJDBC-URI: jdbc:postgresql://pooler-prod:5432/data?password=zMoUxzv&user=data\r\n```\r\nOne more important thing is that pooler should wait for cluster up & running then pooler pods will be deploy.\r\n### Describe alternatives you've considered\nManual way to create secret but that is not highly recomended.\n### Additional context\n_No response_\n### Backport?\nYes\n### Code of Conduct\n- [X] I agree to follow this project's Code of Conducton JDBC-URI also add the `prepareThreshold` as well, like this\r\n```\r\nJDBC-URI: jdbc:postgresql://pooler-prod:5432/data?password=zMoUxzv&prepareThreshold=0&user=data\r\n```"
    },
    {
        "title": "CloudNativePG applications, architectures, bootstrapping, and backporting topics",
        "id": 1953093484,
        "state": "open",
        "first": "Rebased to resolve merge conflicts and signed off everything properly.\r\nFirst twelve topics in CloudNativePG doc.",
        "messages": "Rebased to resolve merge conflicts and signed off everything properly.\r\nFirst twelve topics in CloudNativePG doc.@ebgitelman Could you please work on smaller batches? Or do you prefer you first complete your work, exit draft mode, and we review it? Consider we are changing some of the docs in the meantime.\n---\n@ebgitelman can you please sign off your commits?\r\nMore info: https://github.com/cloudnative-pg/cloudnative-pg/blob/main/contribute/README.md#sign-your-work\n---\n> @ebgitelman can you please sign off your commits? More info: https://github.com/cloudnative-pg/cloudnative-pg/blob/main/contribute/README.md#sign-your-work\r\nYes, I will do going forward. Thank you for letting me know.\n---\n> @ebgitelman Could you please work on smaller batches? Or do you prefer you first complete your work, exit draft mode, and we review it? Consider we are changing some of the docs in the meantime.\r\n@gbartolini I can work however you'd like. However, sometimes I do commit things when they've only been read once. I will always comment on these, as I do need to read everything two times. Please let me know how you'd like this to go but note that I don't like to read the same topic two times in a row (without something intervening) as it's hard to have fresh eyes that way. So probably at a minimum I would prefer to commit two topics at a time. Thank you!\n---\n> > @ebgitelman Could you please work on smaller batches? Or do you prefer you first complete your work, exit draft mode, and we review it? Consider we are changing some of the docs in the meantime.\r\n> \r\n> @gbartolini I can work however you'd like. However, sometimes I do commit things when they've only been read once. I will always comment on these, as I do need to read everything two times. Please let me know how you'd like this to go but note that I don't like to read the same topic two times in a row (without something intervening) as it's hard to have fresh eyes that way. So probably at a minimum I would prefer to commit two topics at a time. Thank you!\r\nApologies for this large pull request. Have been working in smaller increments going forward"
    },
    {
        "title": "JDBC with Connection Pooler (pgbouncer)",
        "id": 1949729959,
        "state": "open",
        "first": "I have deployed Cluster along with Pooler, Our application is using JDBC, CLoudnative-PG cluster created secret that have jdbc-uri, When I have checked the value the, it's point to primary RW service, like below,\r\n`jdbc:postgresql://prod-rw:5432/telemetry?password=zMoUxiSpxpeOdzv&user=telemetry`\r\nNow my question is that how can we use JDBC URI with Pooler?\r\n@gbartolini",
        "messages": "I have deployed Cluster along with Pooler, Our application is using JDBC, CLoudnative-PG cluster created secret that have jdbc-uri, When I have checked the value the, it's point to primary RW service, like below,\r\n`jdbc:postgresql://prod-rw:5432/telemetry?password=zMoUxiSpxpeOdzv&user=telemetry`\r\nNow my question is that how can we use JDBC URI with Pooler?\r\n@gbartoliniI also have an issue regarding the app secret when using a Pooler, A few apps I have expect only one secret containing the connection string. It would be great if a cluster is using a Pooler that the pgbouncer-uri and pgbouncer-jdbc-uri is added to the current app secret to make it easy to use. I am happy to put in a PR if there are any pointers to the best way to achieve this.\n---\n@gbartolini any ideas on how I can implement this in the existing secret? This would be very helpful for us to migrate away from Crunchy Postgres."
    },
    {
        "title": "lack of primary instance in the postgres cluster after the incident with ceph",
        "id": 1942401674,
        "state": "open",
        "first": "I'm using a near-default two-instance postgres cluster configuration. After the incident with ceph, after restoring the ceph cluster, I discovered that the primary is missing, and there is only a replica that cannot start or change role to primary.\r\nHere are the replica latest logs. pvc are present.\r\n`{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"postgres-cluster-1\"}\r\n{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"postgres-cluster-1\"}\r\n{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"postgres-cluster-1\"}\r\n{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"msg\":\"Stopping and waiting for caches\"}\r\n{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"postgres-cluster-1\"}\r\n{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"msg\":\"Stopping and waiting for webhooks\"}\r\n{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\r\n`\r\ncnpg-controller-manager\r\n```{\"level\":\"info\",\"ts\":\"2023-10-13T18:36:25Z\",\"msg\":\"Cannot update target primary: operation cannot be fulfilled. An immediate retry will be scheduled\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres-cluster\",\"namespace\":\"postgres-ha\"},\"namespace\":\"postgres-ha\",\"name\":\"postgres-cluster\",\"reconcileID\":\"1276b184-eaf0-4f5d-924a-b45ed50f938e\",\"uuid\":\"6965c8a2-69f7-11ee-b686-ae5270612cc6\",\"cluster\":\"postgres-cluster\"}```\r\nThis is my configuration. \r\n```apiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: postgres-cluster\r\n  namespace: postgres-ha\r\nspec:\r\n  instances: 2\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:13\r\n  postgresql:\r\n    parameters:\r\n      max_worker_processes: \"60\"\r\n      max_connections: \"1500\"\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      owner: test\r\n      secret:\r\n        name: testuser       \r\n  superuserSecret:\r\n    name: superuser\r\n  primaryUpdateStrategy: unsupervised\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  resources:\r\n    requests:\r\n      memory: \"1Gi\"\r\n      cpu: \"1\"\r\n    limits: \r\n      memory: \"2Gi\"\r\n  storage:\r\n    size: \"10Gi\"\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: true\r\n```\r\nI tried to increase the number of instances in the Postgres cluster, but in fact this did not happen.",
        "messages": "I'm using a near-default two-instance postgres cluster configuration. After the incident with ceph, after restoring the ceph cluster, I discovered that the primary is missing, and there is only a replica that cannot start or change role to primary.\r\nHere are the replica latest logs. pvc are present.\r\n`{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.json\",\"logging_pod\":\"postgres-cluster-1\"}\r\n{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"msg\":\"Shutdown signal received, waiting for all workers to finish\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"msg\":\"All workers finished\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\"}\r\n{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres.csv\",\"logging_pod\":\"postgres-cluster-1\"}\r\n{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"msg\":\"Exited log pipe\",\"fileName\":\"/controller/log/postgres\",\"logging_pod\":\"postgres-cluster-1\"}\r\n{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"msg\":\"Stopping and waiting for caches\"}\r\n{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"logger\":\"roles_reconciler\",\"msg\":\"Terminated RoleSynchronizer loop\",\"logging_pod\":\"postgres-cluster-1\"}\r\n{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"msg\":\"Stopping and waiting for webhooks\"}\r\n{\"level\":\"info\",\"ts\":\"2023-10-13T17:46:22Z\",\"msg\":\"Wait completed, proceeding to shutdown the manager\"}\r\n`\r\ncnpg-controller-manager\r\n```{\"level\":\"info\",\"ts\":\"2023-10-13T18:36:25Z\",\"msg\":\"Cannot update target primary: operation cannot be fulfilled. An immediate retry will be scheduled\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"postgres-cluster\",\"namespace\":\"postgres-ha\"},\"namespace\":\"postgres-ha\",\"name\":\"postgres-cluster\",\"reconcileID\":\"1276b184-eaf0-4f5d-924a-b45ed50f938e\",\"uuid\":\"6965c8a2-69f7-11ee-b686-ae5270612cc6\",\"cluster\":\"postgres-cluster\"}```\r\nThis is my configuration. \r\n```apiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: postgres-cluster\r\n  namespace: postgres-ha\r\nspec:\r\n  instances: 2\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:13\r\n  postgresql:\r\n    parameters:\r\n      max_worker_processes: \"60\"\r\n      max_connections: \"1500\"\r\n  bootstrap:\r\n    initdb:\r\n      database: app\r\n      owner: test\r\n      secret:\r\n        name: testuser       \r\n  superuserSecret:\r\n    name: superuser\r\n  primaryUpdateStrategy: unsupervised\r\n  monitoring:\r\n    enablePodMonitor: true\r\n  resources:\r\n    requests:\r\n      memory: \"1Gi\"\r\n      cpu: \"1\"\r\n    limits: \r\n      memory: \"2Gi\"\r\n  storage:\r\n    size: \"10Gi\"\r\n  nodeMaintenanceWindow:\r\n    inProgress: false\r\n    reusePVC: true\r\n```\r\nI tried to increase the number of instances in the Postgres cluster, but in fact this did not happen.Having this same issue. Weirdly it looks like the PVCs have labels which show the primary/replica are the opposite as what the `status` field of the CNPG `Cluster` resource has.\r\nEven when scaling the instances down to 1 and removing the replica PVC, CNPG would not start the pod up correctly and continued to configure it to look for a primary that does not exist.\n---\nRan into the same issue here. Caused by draining a node which apparently must have been the primary node. The work-around was to manually relabel the PVCs and deleting the wrong db pod.\r\n![image](https://github.com/cloudnative-pg/cloudnative-pg/assets/500754/50a10a28-785c-486c-bf9a-62e640f67743)\n---\n> Ran into the same issue here. Caused by draining a node which apparently must have been the primary node. The work-around was to manually relabel the PVCs and deleting the wrong db pod. ![image](https://private-user-images.githubusercontent.com/500754/282505800-50a10a28-785c-486c-bf9a-62e640f67743.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDM5MzE4NTEsIm5iZiI6MTcwMzkzMTU1MSwicGF0aCI6Ii81MDA3NTQvMjgyNTA1ODAwLTUwYTEwYTI4LTc4NWMtNDg2Yy1iZjlhLTYyZTY0MGY2Nzc0My5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjMxMjMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIzMTIzMFQxMDE5MTFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT03MDcwZGY3ZjQzOTg5OGFjYmNkZGMwZTI4NjUwZjI5MmM1OTJkNTQyZGY1NjA5Yzk2YTM3ZDI4NDdkYjljNzQ1JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.NJfRWtq5cuxpKtXhxvs9fTKIhLBumJbJfrRHsb7R72Y)\r\nIt works!!!\n---\nHad the same issue here, but little luck with trying to reliable PVCs/delete pods, etc.\n---\nI've been able to reproduce this issue in #3810\n---\n@mnencia my understanding is that this was already fixed, is that right ? I think we can close this issue right ?\n---\nIt's not fixed yet. Will be handled in #4049"
    },
    {
        "title": "Embed the version number in the header of the docs",
        "id": 1927690248,
        "state": "open",
        "first": "The current docs do not indicate what version the docs were created for (outside release notes). This small change must be manually updated. It embeds that information in the page header via the site_name configuration variable, which is only (according to the docs) used as the main title.",
        "messages": "The current docs do not indicate what version the docs were created for (outside release notes). This small change must be manually updated. It embeds that information in the page header via the site_name configuration variable, which is only (according to the docs) used as the main title.Thanks @djw-m ! Adding the release version to the docs is very valuable.\r\nLet's have a look at possibly scripting this.\r\nNice, no? @gbartolini\n---\nThis needs to be automated though as part of the release process.\n---\nOk, noted on the automation so reworked to simplify things\r\n1) No more embedded HTML - Version number is displayed through the readthedocs theme's extra.version support\r\n2) The setting of extra.version is done via an environment variable CNPG_VERSION (adjust to taste/standards in mkdocs.yml). If not set, no version is rendered. This should allow any release process to set the env var and run generating the appropriate docs set for that version.\r\n3) The version number styling is tuned in override.css - I've made it a little more prominent yet smaller over the default theme (which is very low contrast).\r\nHopefully this makes things easier to integrate.\n---\nGoing to try this out sometime this morning. Thanks @djw-m\n---\nI think we need to iterate on this.\r\nOn one hand, the automation aspect - env vars vs something else.\r\nOn the other hand, the version number is ensconced in a corner.\r\nDefinitely better than not having it, but while we're mucking here, we could try\r\nto make it more prominent. Perhaps with a link back to the version list page https://cloudnative-pg.io/docs/\r\n![Screenshot 2023-10-06 at 15 24 27](https://github.com/cloudnative-pg/cloudnative-pg/assets/423864/3a1abf4b-a498-4a09-8ae0-d95f7b598c6e)\n---\nOk well, I've been trying to avoid diving into tweaking the theme itself or\r\nadding some dependencies....\r\nTheoretically it should be easy to override a block in the readthedocs\r\ntemplate but.... the thing we'd want to override is not in a block. I may\r\nsubmit a fix upstream to mkdocs to resolve that as its very irritating :)\r\nSo anyway, I'm pushing a defaulted CSS setting and offering this...\r\nRun mkdocs with\r\nVERSION=1.20.2 CNPG_VERSION=\"<a\r\nhref='https://cloudnative-pg.io/docs/'>$VERSION</a>\"\r\nmkdocs serve -a localhost:6900\r\nAnd it will build with a link on the version number. It's a bit kludgy, but\r\nit does work and if its being automated, almost invisible.\r\nDj\r\nOn Fri, Oct 6, 2023 at 2:26\u202fPM Jaime Silvela ***@***.***>\r\nwrote:\r\n> I think we need to iterate on this.\r\n> On one hand, the automation aspect - env vars vs something else.\r\n>\r\n> On the other hand, the version number is ensconced in a corner.\r\n> Definitely better than not having it, but while we're mucking here, we\r\n> could try\r\n> to make it more prominent. Perhaps with a link back to the version list\r\n> page https://cloudnative-pg.io/docs/\r\n> [image: Screenshot 2023-10-06 at 15 24 27]\r\n> <https://user-images.githubusercontent.com/423864/273221343-3a1abf4b-a498-4a09-8ae0-d95f7b598c6e.png>\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/cloudnative-pg/cloudnative-pg/pull/2995#issuecomment-1750680788>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/A6E5CB5B6W4W7T3TH6CS4XDX6ABI7AVCNFSM6AAAAAA5T3O3YOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTONJQGY4DANZYHA>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\n---\nHi! CNPG_VERSION it's not an environment variable that exists when building the documentation so this patch will not work on a standard build, am I right @djw-m ? if I'm not wrong, when the documentation is built there's a variable with the version with the name `VERSION`, probably we can try and test with that one ? What do you think @jsilvela and @gbartolini ?\n---\nThis is  a valuable change to make for the documentation. I don't think\r\nwe're ready to merge this for today (releasing 1.21).\r\nHowever, it should go in soon and we can amend the documentation in\r\nhttps://cloudnative-pg.io without cutting a new release.\r\nAlso, @djw-m , the automation is a bit more involved than I had told you,\r\nas there is a separate repo used to house the documents for the website.\r\nWe need to have an approach that works with the commands used to\r\nexport the documents to that repo.\r\nThis ticket merits a good look from several of the CNPG core team members.\n---\n> Hi! CNPG_VERSION it's not an environment variable that exists when building the documentation so this patch will not work on a standard build, am I right @djw-m ? \r\nYes, I was assuming it would get mated with an appropriate environment variable in review as this is more a proof of concept."
    },
    {
        "title": "Use backup end time in volume snapshot labels",
        "id": 1926183975,
        "state": "open",
        "first": "At the moment, `backupYear`, `backupMonth` and `backupDate` are determined based on the current time. We should use the end time of the backup instead.",
        "messages": "At the moment, `backupYear`, `backupMonth` and `backupDate` are determined based on the current time. We should use the end time of the backup instead.Related to #2983"
    },
    {
        "title": "Log messages should give a better idea of instance state",
        "id": 1917149964,
        "state": "open",
        "first": "Debugging based on logs can often be confusing, as there are lots of messages, and many of them offer little information.\r\nWe should have a reasonable collection of logs that clearly allow us to distinguish, e.g.:\r\n- Instance is fenced\r\n- Instance restarted due to failover / switchover\r\n- Instance operating normally\r\n- Postgres process cannot start up\r\n- Postgres process has had a fatal failure\r\nInvestigating #2859, for instance, I'm finding the Readiness probe log message is not very smart.\r\nNote the contrast between the `readiness probe failing` message, and the `metrics collection skipped`, in two different scenarios.\r\n1. Instance is fenced:\r\n``` json\r\n{\"level\":\"info\",\"ts\":\"2023-09-24T00:00:12Z\",\r\n  \"msg\":\"Readiness probe failing\",\"logging_pod\":\"kubecon-na-23-eu-central-1-2\",\r\n  \"err\":\"instance is not ready yet\"}\r\n{\"level\":\"info\",\"ts\":\"2023-09-24T00:00:17Z\",\r\n  \"msg\":\"metrics collection skipped due to fencing\",\"logging_pod\":\"kubecon-na-23-eu-central-1-2\"}\r\n```\r\n2. Postgres startup has a FATAL failure\r\n``` json\r\n{\"level\":\"info\",\"ts\":\"2023-09-25T04:52:12Z\",\r\n  \"msg\":\"Readiness probe failing\",\"logging_pod\":\"kubecon-na-23-eu-central-1-2\",\r\n  \"err\":\"instance is not ready yet\"}\r\n{\"level\":\"info\",\"ts\":\"2023-09-25T04:52:17Z\",\r\n  \"msg\":\"metrics collection skipped due to instance still being down\",\"logging_pod\":\"kubecon-na-23-eu-central-1-2\"}\r\n```",
        "messages": "Debugging based on logs can often be confusing, as there are lots of messages, and many of them offer little information.\r\nWe should have a reasonable collection of logs that clearly allow us to distinguish, e.g.:\r\n- Instance is fenced\r\n- Instance restarted due to failover / switchover\r\n- Instance operating normally\r\n- Postgres process cannot start up\r\n- Postgres process has had a fatal failure\r\nInvestigating #2859, for instance, I'm finding the Readiness probe log message is not very smart.\r\nNote the contrast between the `readiness probe failing` message, and the `metrics collection skipped`, in two different scenarios.\r\n1. Instance is fenced:\r\n``` json\r\n{\"level\":\"info\",\"ts\":\"2023-09-24T00:00:12Z\",\r\n  \"msg\":\"Readiness probe failing\",\"logging_pod\":\"kubecon-na-23-eu-central-1-2\",\r\n  \"err\":\"instance is not ready yet\"}\r\n{\"level\":\"info\",\"ts\":\"2023-09-24T00:00:17Z\",\r\n  \"msg\":\"metrics collection skipped due to fencing\",\"logging_pod\":\"kubecon-na-23-eu-central-1-2\"}\r\n```\r\n2. Postgres startup has a FATAL failure\r\n``` json\r\n{\"level\":\"info\",\"ts\":\"2023-09-25T04:52:12Z\",\r\n  \"msg\":\"Readiness probe failing\",\"logging_pod\":\"kubecon-na-23-eu-central-1-2\",\r\n  \"err\":\"instance is not ready yet\"}\r\n{\"level\":\"info\",\"ts\":\"2023-09-25T04:52:17Z\",\r\n  \"msg\":\"metrics collection skipped due to instance still being down\",\"logging_pod\":\"kubecon-na-23-eu-central-1-2\"}\r\n```"
    },
    {
        "title": "Pooler don't works with custom clientCASecret",
        "id": 1907084043,
        "state": "open",
        "first": "Hello!\r\nWhen I create a Cluster using a `certificates.clientCASecret` generated using cert-manager (like in the [doc](https://cloudnative-pg.io/documentation/1.20/certificates/#cert-manager-example_1)), the controller fails if the Cluster has an attached Pooler.\r\n<details><summary>Logs</summary>\r\n```json\r\n{\r\n  \"level\": \"error\",\r\n  \"ts\": \"2023-09-21T14:03:45Z\",\r\n  \"msg\": \"Reconciler error\",\r\n  \"controller\": \"cluster\",\r\n  \"controllerGroup\": \"postgresql.cnpg.io\",\r\n  \"controllerKind\": \"Cluster\",\r\n  \"Cluster\": {\r\n    \"name\": \"barcnpg-tloubiou-test\",\r\n    \"namespace\": \"barcnpg-tloubiou-test\"\r\n  },\r\n  \"namespace\": \"barcnpg-tloubiou-test\",\r\n  \"name\": \"barcnpg-tloubiou-test\",\r\n  \"reconcileID\": \"8f4e007b-5505-47c6-a73c-c1095994f489\",\r\n  \"error\": \"cannot create Cluster auxiliary objects: missing ca.key secret data\",\r\n  \"stacktrace\": \"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:324\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:265\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:226\"\r\n}\r\n```\r\n</details> \r\nAfter looking the source code, I've found the error is raised by the [ensureLeafCertificate](https://github.com/cloudnative-pg/cloudnative-pg/blob/6cc81803aefd73d9db30f6f185046d9a7a5477c4/controllers/cluster_create.go#L259C19-L259C19) function which call `generateCertificateFromCA` which calls `certs.ParseCASecret` using the clientCASecret generated by cert-manager.\r\nThis certificate only contains `ca.crt`, `tls.crt` and `tls.key`.\r\nI think I can use the `Pooler.spec.pgbouncer.authQuerySecret` field as a workaround, but it would be nice to at least document this behavior.\r\nRemoving the pooler fixes the problem.",
        "messages": "Hello!\r\nWhen I create a Cluster using a `certificates.clientCASecret` generated using cert-manager (like in the [doc](https://cloudnative-pg.io/documentation/1.20/certificates/#cert-manager-example_1)), the controller fails if the Cluster has an attached Pooler.\r\n<details><summary>Logs</summary>\r\n```json\r\n{\r\n  \"level\": \"error\",\r\n  \"ts\": \"2023-09-21T14:03:45Z\",\r\n  \"msg\": \"Reconciler error\",\r\n  \"controller\": \"cluster\",\r\n  \"controllerGroup\": \"postgresql.cnpg.io\",\r\n  \"controllerKind\": \"Cluster\",\r\n  \"Cluster\": {\r\n    \"name\": \"barcnpg-tloubiou-test\",\r\n    \"namespace\": \"barcnpg-tloubiou-test\"\r\n  },\r\n  \"namespace\": \"barcnpg-tloubiou-test\",\r\n  \"name\": \"barcnpg-tloubiou-test\",\r\n  \"reconcileID\": \"8f4e007b-5505-47c6-a73c-c1095994f489\",\r\n  \"error\": \"cannot create Cluster auxiliary objects: missing ca.key secret data\",\r\n  \"stacktrace\": \"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:324\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:265\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/controller/controller.go:226\"\r\n}\r\n```\r\n</details> \r\nAfter looking the source code, I've found the error is raised by the [ensureLeafCertificate](https://github.com/cloudnative-pg/cloudnative-pg/blob/6cc81803aefd73d9db30f6f185046d9a7a5477c4/controllers/cluster_create.go#L259C19-L259C19) function which call `generateCertificateFromCA` which calls `certs.ParseCASecret` using the clientCASecret generated by cert-manager.\r\nThis certificate only contains `ca.crt`, `tls.crt` and `tls.key`.\r\nI think I can use the `Pooler.spec.pgbouncer.authQuerySecret` field as a workaround, but it would be nice to at least document this behavior.\r\nRemoving the pooler fixes the problem.Up for a PR? :)\n---\nI could reproduce the issue.\r\n<details>\r\n  <summary>cert-manager-cluster-certificates.yaml</summary>\r\n```yaml\r\n---\r\napiVersion: cert-manager.io/v1\r\nkind: Issuer\r\nmetadata:\r\n  name: selfsigned-issuer\r\nspec:\r\n  selfSigned: {}\r\n---\r\napiVersion: v1\r\nkind: Secret\r\nmetadata:\r\n  name: my-postgres-server-cert\r\n  labels:\r\n    cnpg.io/reload: \"\"\r\n---\r\napiVersion: cert-manager.io/v1\r\nkind: Certificate\r\nmetadata:\r\n  name: my-postgres-server-cert\r\nspec:\r\n  secretName: my-postgres-server-cert\r\n  usages:\r\n    - server auth\r\n  dnsNames:\r\n    - cluster-example-lb.internal.mydomain.net\r\n    - cluster-example-rw\r\n    - cluster-example-rw.default\r\n    - cluster-example-rw.default.svc\r\n    - cluster-example-r\r\n    - cluster-example-r.default\r\n    - cluster-example-r.default.svc\r\n    - cluster-example-ro\r\n    - cluster-example-ro.default\r\n    - cluster-example-ro.default.svc\r\n  issuerRef:\r\n    name: selfsigned-issuer\r\n    kind: Issuer\r\n    group: cert-manager.io\r\n---\r\napiVersion: v1\r\nkind: Secret\r\nmetadata:\r\n  name: my-postgres-client-cert\r\n  labels:\r\n    cnpg.io/reload: \"\"\r\n---\r\napiVersion: cert-manager.io/v1\r\nkind: Certificate\r\nmetadata:\r\n  name: my-postgres-client-cert\r\nspec:\r\n  secretName: my-postgres-client-cert\r\n  usages:\r\n    - client auth\r\n  commonName: streaming_replica\r\n  issuerRef:\r\n    name: selfsigned-issuer\r\n    kind: Issuer\r\n    group: cert-manager.io\r\n```\r\n</details>\r\nThen a Cluster using both the secrets above.\r\n<details>\r\n  <summary>cluster-example-using-certs.yaml</summary>\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-example\r\nspec:\r\n  instances: 3\r\n  certificates:\r\n    serverTLSSecret: my-postgres-server-cert\r\n    serverCASecret: my-postgres-server-cert\r\n    clientCASecret: my-postgres-client-cert\r\n    replicationTLSSecret: my-postgres-client-cert\r\n  storage:\r\n    size: 1Gi\r\n```\r\n</details>\r\nEverything works fine  as expected.\r\nThen deploying a Pooler:\r\n<details>\r\n  <summary>pooler.yaml</summary>\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Pooler\r\nmetadata:\r\n  name: pooler-example-rw\r\nspec:\r\n  cluster:\r\n    name: cluster-example\r\n  instances: 3\r\n  type: rw\r\n  pgbouncer:\r\n    poolMode: session\r\n    parameters:\r\n      max_client_conn: \"1000\"\r\n      default_pool_size: \"10\"\r\n```\r\n</details>\r\nResults in the same error as in the description.\r\n<details>\r\n  <summary>error.json</summary>\r\n```json\r\n{\r\n  \"level\": \"error\",\r\n  \"ts\": \"2023-12-11T13:47:46Z\",\r\n  \"msg\": \"Reconciler error\",\r\n  \"controller\": \"cluster\",\r\n  \"controllerGroup\": \"postgresql.cnpg.io\",\r\n  \"controllerKind\": \"Cluster\",\r\n  \"Cluster\": {\r\n    \"name\": \"cluster-example\",\r\n    \"namespace\": \"default\"\r\n  },\r\n  \"namespace\": \"default\",\r\n  \"name\": \"cluster-example\",\r\n  \"reconcileID\": \"b9357cf3-9dc0-4ab0-ac3b-2b46d4efea98\",\r\n  \"error\": \"cannot create Cluster auxiliary objects: missing ca.key secret data\",\r\n  \"stacktrace\": \"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227\"\r\n}\r\n```\r\n</details>\r\nThis error comes from the Cluster reconciler trying to generate a Client certificate `cluster-example-pooler` for the Pooler, as specified at `status.poolerIntegrations.secrets`.\r\n<details>\r\n  <summary>kubect get cluster cluster-example -o yaml</summary>\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-example\r\n  namespace: default\r\nspec:\r\n  # ... omitted for brevity ...\r\nstatus:\r\n  certificates:\r\n    clientCASecret: my-postgres-client-cert\r\n    expirations:\r\n      my-postgres-client-cert: 2024-03-10 13:30:55 +0000 UTC\r\n      my-postgres-server-cert: 2024-03-10 13:25:46 +0000 UTC\r\n    replicationTLSSecret: my-postgres-client-cert\r\n    serverAltDNSNames:\r\n    - cluster-example-rw\r\n    - cluster-example-rw.default\r\n    - cluster-example-rw.default.svc\r\n    - cluster-example-r\r\n    - cluster-example-r.default\r\n    - cluster-example-r.default.svc\r\n    - cluster-example-ro\r\n    - cluster-example-ro.default\r\n    - cluster-example-ro.default.svc\r\n    serverCASecret: my-postgres-server-cert\r\n    serverTLSSecret: my-postgres-server-cert\r\n  phase: Cluster in healthy state\r\n  poolerIntegrations:\r\n    pgBouncerIntegration:\r\n      secrets:\r\n      - cluster-example-pooler                           # <= the operator tries to create this secret\r\n  secretsResourceVersion:\r\n    applicationSecretVersion: \"4335\"\r\n    clientCaSecretVersion: \"2187\"\r\n    replicationSecretVersion: \"2187\"\r\n    serverCaSecretVersion: \"1568\"\r\n    serverSecretVersion: \"1568\"\r\n  # ... omitted for brevity ...\r\n```\r\n</details>\r\n## Not really a bug\r\nThe operator can not indeed properly generate the required client certificate, as it doesn't have access to the CA private key.\r\nThe fix would be to manually create a client certificate for the Pooler and set it as `spec.pgbouncer.authQuerySecret`, however that will mean the Pooler is in manual certificates mode as specified in the [docs](https://cloudnative-pg.io/documentation/1.20/connection_pooling/#authentication).\r\n### Possible Improvements\r\nThere is nothing the operator could do in this case, so this is not a bug, however we could improve its behaviour:\r\n- the error message could be more explicit, e.g. mention why we are trying to read the client ca.key\r\n- the operator could avoid trying to setup automatic pooler integration if we know we are not handling the CA\r\n- if possible, the pooler should be rejected if the target cluster has certificates not handled by the operator itself\r\n- we could relax the condition on the pooler, and still automatically configure or try configuring everything if `authQuerySecret` is provided without `authQuery` which would be equivalent to creating a secret with the same name expected by the operator `cluster-example-pooler` without actually configuring it as `authQuerySecret`, see manifest below.\r\n```yaml\r\n---\r\napiVersion: v1\r\nkind: Secret\r\nmetadata:\r\n  name: cluster-example-pooler\r\n  labels:\r\n    cnpg.io/reload: \"\"\r\n---\r\napiVersion: cert-manager.io/v1\r\nkind: Certificate\r\nmetadata:\r\n  name: cluster-example-pooler\r\nspec:\r\n  secretName: cluster-example-pooler\r\n  usages:\r\n    - client auth\r\n  commonName: cnpg_pooler_pgbouncer\r\n  issuerRef:\r\n    name: selfsigned-issuer\r\n    kind: Issuer\r\n    group: cert-manager.io\r\n```\n---\nThis indeed would allow to configure assuming that a secret with the Client CA is available, e.g.:\r\n```yaml\r\n---\r\napiVersion: v1\r\nkind: Namespace\r\nmetadata:\r\n  name: sandbox\r\n---\r\napiVersion: cert-manager.io/v1\r\nkind: ClusterIssuer\r\nmetadata:\r\n  name: selfsigned-issuer\r\nspec:\r\n  selfSigned: {}\r\n---\r\napiVersion: cert-manager.io/v1\r\nkind: Certificate\r\nmetadata:\r\n  name: my-selfsigned-ca\r\n  namespace: sandbox\r\nspec:\r\n  isCA: true\r\n  commonName: my-selfsigned-ca\r\n  secretName: root-secret\r\n  privateKey:\r\n    algorithm: ECDSA\r\n    size: 256\r\n  issuerRef:\r\n    name: selfsigned-issuer\r\n    kind: ClusterIssuer\r\n    group: cert-manager.io\r\n---\r\napiVersion: cert-manager.io/v1\r\nkind: Issuer\r\nmetadata:\r\n  name: my-ca-issuer\r\n  namespace: sandbox\r\nspec:\r\n  ca:\r\n    secretName: root-secret\r\n---\r\napiVersion: v1\r\nkind: Secret\r\nmetadata:\r\n  name: my-postgres-server-cert\r\n  labels:\r\n    cnpg.io/reload: \"\"\r\n---\r\napiVersion: cert-manager.io/v1\r\nkind: Certificate\r\nmetadata:\r\n  name: my-postgres-server-cert\r\nspec:\r\n  secretName: my-postgres-server-cert\r\n  usages:\r\n    - server auth\r\n  dnsNames:\r\n    - cluster-example-lb.internal.mydomain.net\r\n    - cluster-example-rw\r\n    - cluster-example-rw.default\r\n    - cluster-example-rw.default.svc\r\n    - cluster-example-r\r\n    - cluster-example-r.default\r\n    - cluster-example-r.default.svc\r\n    - cluster-example-ro\r\n    - cluster-example-ro.default\r\n    - cluster-example-ro.default.svc\r\n  issuerRef:\r\n    name: my-ca-issuer\r\n    kind: Issuer\r\n    group: cert-manager.io\r\n---\r\napiVersion: v1\r\nkind: Secret\r\nmetadata:\r\n  name: my-postgres-client-cert\r\n  labels:\r\n    cnpg.io/reload: \"\"\r\n---\r\napiVersion: cert-manager.io/v1\r\nkind: Certificate\r\nmetadata:\r\n  name: my-postgres-client-cert\r\nspec:\r\n  secretName: my-postgres-client-cert\r\n  usages:\r\n    - client auth\r\n  commonName: streaming_replica\r\n  issuerRef:\r\n    name: my-ca-issuer\r\n    kind: Issuer\r\n    group: cert-manager.io\r\n```\r\nand then:\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-example\r\nspec:\r\n  instances: 3\r\n  certificates:\r\n    serverTLSSecret: my-postgres-server-cert\r\n    serverCASecret: my-postgres-server-cert\r\n    clientCASecret: root-secret                             # <= the secret containing the ca.key\r\n    replicationTLSSecret: my-postgres-client-cert\r\n  storage:\r\n    size: 1Gi\r\n```\r\nNote that, this is an additional use-case w.r.t. the ones explained in the [docs](https://cloudnative-pg.io/documentation/1.21/certificates/#user-provided-certificates-mode), as what we would like here is for the operator to be able to adopt a CA provided by cert-manager and therefore work in a hybrid mode, while what we support atm is client and/or server certificates fully handled either by cert-manager or cnpg.\n---\nIs there any sort of workaround/alternative for this in the meantime?"
    },
    {
        "title": "Cannot rollback from resizing a volume when not allowed",
        "id": 1904827057,
        "state": "open",
        "first": "While doing some tests on volume snapshot backups, I ran out of disk on the WAL volume and tried to increase the size of it using `kubectl edit cluster`, passing the webhook check.\r\nHowever, the underlying storage class did not support expansion - the underlying PVC confirmed that the size was still the original one. I tried to rollback the change by decreasing the size to the one of the current PVC, but the webhook prevented that with: `can't shrink existing storage`.\r\nI was able to exit this catch22 situation by deleting the webhook, changing the cluster spec, and restoring the webhook. However ideally the operator should allow restoring the size to the previous value, provided that none of the related PVCs in the cluster have been resized in the meantime.",
        "messages": "While doing some tests on volume snapshot backups, I ran out of disk on the WAL volume and tried to increase the size of it using `kubectl edit cluster`, passing the webhook check.\r\nHowever, the underlying storage class did not support expansion - the underlying PVC confirmed that the size was still the original one. I tried to rollback the change by decreasing the size to the one of the current PVC, but the webhook prevented that with: `can't shrink existing storage`.\r\nI was able to exit this catch22 situation by deleting the webhook, changing the cluster spec, and restoring the webhook. However ideally the operator should allow restoring the size to the previous value, provided that none of the related PVCs in the cluster have been resized in the meantime."
    },
    {
        "title": "add support for `securityContext` in postgres containers",
        "id": 1904256057,
        "state": "open",
        "first": "Hello,\r\nI've noticed that the `postgres` container of the pods created by the operator have a very basic `securityContext`:\r\n```yaml\r\n    securityContext:\r\n      runAsUser: 1001\r\n```\r\nI saw there's a way to set the `seccompProfile` but I was hoping one can also set the full `securityContext` of the container, which doesn't seem to be the case. This is a problem when the restricted PSS is enforced.\r\nThis could become part of the `ClusterSpec` object - e.g. keeping the current value as defaults, but adding the option to override if required.\r\nI don't know how much work this would represent, and I would be happy to help if I can - I would just need some guidance to get started.\r\nFred",
        "messages": "Hello,\r\nI've noticed that the `postgres` container of the pods created by the operator have a very basic `securityContext`:\r\n```yaml\r\n    securityContext:\r\n      runAsUser: 1001\r\n```\r\nI saw there's a way to set the `seccompProfile` but I was hoping one can also set the full `securityContext` of the container, which doesn't seem to be the case. This is a problem when the restricted PSS is enforced.\r\nThis could become part of the `ClusterSpec` object - e.g. keeping the current value as defaults, but adding the option to override if required.\r\nI don't know how much work this would represent, and I would be happy to help if I can - I would just need some guidance to get started.\r\nFred@fad3t hi!\r\nAre you talking about restricted-scc ? which it's something from openshift right ?\r\nRegards,\n---\nHi @sxd, no I'm referring to the pod security standards (cf. https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted).\r\nWe're enforcing it with Kyverno in our case, although I suppose this can be done otherwise.\n---\nHi @sxd, I see you have assigned yourself to this issue. Does this mean you're working on it?\r\nOtherwise I would be happy to contribute!\n---\nHi, any update? Thanks\n---\nOne more reason to have this feature is that large database pods might take up to 30 minutes to mount due to this issue: https://github.com/kubernetes/kubernetes/issues/69699\r\nusing\r\n```\r\nsecurityContext:\r\n  fsGroupChangePolicy: \"OnRootMismatch\"\r\n```\r\nwould fix this.\n---\nlast I ran across this setting the namespace to restricted prevent the init pods from starting entirely\n---\n@sxd, could you please review the PR I opened that is solving this issue?\n---\n@x0ddf, this for this essential feature and your PR!"
    },
    {
        "title": "docs: introduce sections in the left bar",
        "id": 1892894477,
        "state": "open",
        "first": "Closes #2742",
        "messages": "Closes #2742@gbartolini should we merge this?\n---\n@gbartolini why we don't merge this one? it looks really nice to me to merge it"
    },
    {
        "title": "Organise the documentation left bar in sections",
        "id": 1892556006,
        "state": "open",
        "first": "The documentation of CloudNativePG has grown over time in scar tissue mode. The left bar now looks like an infinite list of sections. While we find better ways to improve the overall content (maybe have tutorials, references, admin guides formats), let's organise the left bar in sections.",
        "messages": "The documentation of CloudNativePG has grown over time in scar tissue mode. The left bar now looks like an infinite list of sections. While we find better ways to improve the overall content (maybe have tutorials, references, admin guides formats), let's organise the left bar in sections."
    },
    {
        "title": "volumeSnapshots: kind: PersistentVolumeClaim recovery not work",
        "id": 1889847103,
        "state": "open",
        "first": "Hi All.\r\nI attempted to perform a recovery of a previously deployed cluster using the cluster-restore-pvc.yaml sample file. However, the recovery process failed. The previous cluster's PVs had a Reclaim Policy of \"Retain\" and were in the \"Available\" state. I modified the cluster-restore-pvc.yaml file by changing the cluster name to something different and deployed it. However, the recovery did not succeed, and the process kept repeating in the \"Restart\" state until it eventually crashed.\r\n-- pod log --\r\n{\"level\":\"info\",\"ts\":\"2023-09-11T07:21:00Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"mycluster-restore-1\",\"err\":\"failed to connect to `host=mycluster-restore-rw user=streaming_replica database=postgres`: dial error (dial tcp 10.0.228.180:5432: connect: connection refused)\"}\r\n\"Help, please. :)\"",
        "messages": "Hi All.\r\nI attempted to perform a recovery of a previously deployed cluster using the cluster-restore-pvc.yaml sample file. However, the recovery process failed. The previous cluster's PVs had a Reclaim Policy of \"Retain\" and were in the \"Available\" state. I modified the cluster-restore-pvc.yaml file by changing the cluster name to something different and deployed it. However, the recovery did not succeed, and the process kept repeating in the \"Restart\" state until it eventually crashed.\r\n-- pod log --\r\n{\"level\":\"info\",\"ts\":\"2023-09-11T07:21:00Z\",\"msg\":\"DB not available, will retry\",\"logging_pod\":\"mycluster-restore-1\",\"err\":\"failed to connect to `host=mycluster-restore-rw user=streaming_replica database=postgres`: dial error (dial tcp 10.0.228.180:5432: connect: connection refused)\"}\r\n\"Help, please. :)\"> \r\n## before deploy `cluster-restore-pvc.yaml`\r\n$ kubectl get pv\r\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE\r\npvc-b8a722b1-2021-468a-92c7-d1f060c65c56   1Gi        RWO            Retain           Available           default                 16m\r\n## after deploy `cluster-restore-pvc.yaml`\r\n$ kubectl apply -f cluster-restore-pvc.yaml\r\n$ kubectl get pv,pvc\r\nNAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                      STORAGECLASS   REASON   AGE\r\npersistentvolume/pvc-b8a722b1-2021-468a-92c7-d1f060c65c56   1Gi        RWO            Retain           Bound    joan/mycluster-restore-1   default                 30m\r\nNAME                                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\r\npersistentvolumeclaim/mycluster-restore-1   Bound    pvc-b8a722b1-2021-468a-92c7-d1f060c65c56   1Gi        RWO            default        14m\r\n$ kubect get pod \r\nNAME                  READY   STATUS             RESTARTS      AGE\r\nmycluster-restore-1   0/1     CrashLoopBackOff   7 (65s ago)   14m"
    },
    {
        "title": "It should be possible to add a backup section to an existing cluster",
        "id": 1878046526,
        "state": "open",
        "first": "If you create a cluster without a backup section, and later add one (and an annotation to the serviceaccount template), nothing happens to the cluster as far as I can tell. Backups do not start showing up in the object store, the pods don't get restarted to pick up the new config, etc.\r\nThe Service Account will get a new annotation though.\r\nI killed one of the replica pods to force a reload and then created a backup object, which seemed to work, but the resulting backup didn't successfully load into a new cluster. Maybe it would if I let the cluster live long enough.\r\nIn any case, it would be nice if the operator would detect that it should now start taking backups, and go through whatever steps required to make that happen.",
        "messages": "If you create a cluster without a backup section, and later add one (and an annotation to the serviceaccount template), nothing happens to the cluster as far as I can tell. Backups do not start showing up in the object store, the pods don't get restarted to pick up the new config, etc.\r\nThe Service Account will get a new annotation though.\r\nI killed one of the replica pods to force a reload and then created a backup object, which seemed to work, but the resulting backup didn't successfully load into a new cluster. Maybe it would if I let the cluster live long enough.\r\nIn any case, it would be nice if the operator would detect that it should now start taking backups, and go through whatever steps required to make that happen.Hi @tculp \r\nCan you provide a set of steps to reproduce this issue? looks like a bug to me but not sure how to reproduce it\r\nRegards!\n---\nHang on, the backup section does not take any backup. You need to set a `ScheduledBackup` object for that if you want to regularly take them, or create a `Backup` object for on-demand backups.\n---\nBy backup section, I mean for example:\r\n```\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: \"s3://example-bucket\"\r\n      s3Credentials:\r\n        inheritFromIAMRole: true\r\n    retentionPolicy: \"30d\"\r\n ```\n---\nStep-by-step:\r\n1. Apply the following manifest:\r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: example-cluster-1\r\nspec:\r\n  instances: 3\r\n  storage:\r\n    size: 1Gi\r\n  bootstrap:\r\n    initdb:\r\n      database: customdb\r\n```\r\nA cluster is created, without performing backups.\r\n2. Update the manifest to include `serviceAccountTemplate` and `backup` sections:\r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: example-cluster-1\r\nspec:\r\n  instances: 3\r\n  storage:\r\n    size: 1Gi\r\n  serviceAccountTemplate:\r\n    metadata:\r\n      annotations:\r\n        eks.amazonaws.com/role-arn: \"arn:aws:iam::<redacted>\"\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: \"s3://example-<redacted>\"\r\n      s3Credentials:\r\n        inheritFromIAMRole: true\r\n  bootstrap:\r\n    initdb:\r\n      database: customdb\r\n```\r\nThe ServiceAccount is updated with the new annotation, as expected.\r\nNo pods are restarted to pick up the new `backup` config, etc.\r\nNo backups appear in the specified bucket.\r\nThis is the point where I would expect the controller to realize that the manifest has changed, restart pods,\r\nand do whatever is needed to get object storage backups working.\r\nHowever, to investigate behavior, I'll proceed.\r\n3. Update the manifest to include a Backup.\r\n```\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: example-cluster-1\r\nspec:\r\n  instances: 3\r\n  storage:\r\n    size: 1Gi\r\n  serviceAccountTemplate:\r\n    metadata:\r\n      annotations:\r\n        eks.amazonaws.com/role-arn: \"arn:aws:iam::<redacted>\"\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: \"s3://example-<redacted>\"\r\n      s3Credentials:\r\n        inheritFromIAMRole: true\r\n  bootstrap:\r\n    initdb:\r\n      database: customdb\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Backup\r\nmetadata:\r\n  name: backup-example-1\r\nspec:\r\n  cluster:\r\n    name: example-cluster-1\r\n```\r\nA Backup is created, which stays in running for a while, but eventually gets to walArchivingFailing.\r\n4. Kill the cluster pods one at a time\r\nObjects appear in the bucket in the expected location (bucket/exmaple-cluster-1/wals).\r\n5. Create a new backup:\r\n```\r\n...\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Backup\r\nmetadata:\r\n  name: backup-example-2\r\nspec:\r\n  cluster:\r\n    name: example-cluster-1\r\n```\r\nThe new backup gets to phase `completed`.\r\n6. Create a new cluster, using the objectStore of the previous one:\r\n```\r\n...\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-example-2\r\nspec:\r\n  instances: 3\r\n  storage:\r\n    size: 1Gi\r\n  serviceAccountTemplate:\r\n    metadata:\r\n      annotations:\r\n        eks.amazonaws.com/role-arn: \"arn:aws:iam::<redacted>\"\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: \"s3://example-<redacted>\"\r\n      s3Credentials:\r\n        inheritFromIAMRole: true\r\n    retentionPolicy: \"30d\"\r\n  bootstrap:\r\n    recovery:\r\n      source: example-cluster-1\r\n  externalClusters:\r\n    - name: example-cluster-1\r\n      barmanObjectStore:\r\n        destinationPath: \"s3://example-<redacted>\"\r\n        serverName: example-cluster-1\r\n        s3Credentials:\r\n          inheritFromIAMRole: true\r\n```\r\nI think this last step failed with an error last time, but this time it seemed to work fine. One potential difference may be that last time I only killed some of the pods before WALs showed up in object storage, and then the new cluster off of a backup failed. Also the first time I was making ScheduledBackups instead of Backups directly, but I don't see why that would have made a difference.\n---\nI have a similar issue. \r\nHow to reproduce:\r\nCreate a cluster using kubectl apply -f on yaml file like this\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-example\r\n  namespace: cluster1\r\nspec:\r\n  instances: 2\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:14.9\r\n  primaryUpdateStrategy: unsupervised\r\n  bootstrap:\r\n    recovery:\r\n      source: cluster-example\r\n  externalClusters:\r\n    - name: cluster-example\r\n      barmanObjectStore:\r\n        serverName: test-backup-3\r\n        destinationPath: s3://lab-cnpg\r\n        endpointURL: https://[REDACTED]\r\n        s3Credentials:\r\n          accessKeyId:\r\n            name: s3-creds\r\n            key: ACCESS_KEY_ID\r\n          secretAccessKey:\r\n            name: s3-creds\r\n            key: ACCESS_SECRET_KEY\r\n        wal:\r\n          maxParallel: 8\r\n  storage:\r\n    size: 5Gi\r\n```\r\nThe cluster goes up ad loads the backup correctly\r\nNow edit the definition and add a ScheduledBackup object too. Reapply with kubectl  apply -f edited_file.yaml\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: cluster-example\r\n  namespace: cluster1\r\nspec:\r\n  instances: 2\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:14.9\r\n  primaryUpdateStrategy: unsupervised\r\n  backup:\r\n    retentionPolicy: \"3d\"\r\n    barmanObjectStore:\r\n      serverName: test-backup-4\r\n      destinationPath: s3://lab-cnpg\r\n      endpointURL: https://[REDACTED]\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: s3-creds\r\n          key: ACCESS_KEY_ID\r\n        secretAccessKey:\r\n          name: s3-creds\r\n          key: ACCESS_SECRET_KEY\r\n  bootstrap:\r\n    recovery:\r\n      source: cluster-example\r\n  externalClusters:\r\n    - name: cluster-example\r\n      barmanObjectStore:\r\n        serverName: test-backup-3\r\n        destinationPath: s3://lab-cnpg\r\n        endpointURL: https://[REDACTED]\r\n        s3Credentials:\r\n          accessKeyId:\r\n            name: s3-creds\r\n            key: ACCESS_KEY_ID\r\n          secretAccessKey:\r\n            name: s3-creds\r\n            key: ACCESS_SECRET_KEY\r\n        wal:\r\n          maxParallel: 8\r\n  storage:\r\n    size: 5Gi\r\n---\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: ScheduledBackup\r\nmetadata:\r\n  name: backup-example\r\n  namespace: cluster1\r\nspec:\r\n  immediate: true\r\n  schedule: \"0 0 0 * * *\"\r\n  backupOwnerReference: self\r\n  cluster:\r\n    name: cluster-example\r\n```\r\nAfter a while, the test-backup-4 is created into the object store.\r\nNow try to recovery from the test-backup-4 will fail with\r\n```\r\n\"Error while restoring a backup\",\"logging_pod\":\"cluster-example-1-full-recovery\",\"error\":\"encountered an error while checking the presence of first needed WAL in the archive: file not found 000000040000000000000016: WAL not found\"\r\n```\n---\nWhen I got an error with loading I also got an `encountered an error while checking the presence of first needed WAL in the archive` message\n---\nI have also noticed the same problem:\r\nIf the database was created without the barmanObjectStore section, it is not possible to restore the database from backup even by adding it later.\r\nIf WALs were generated between the creation of the cluster and the addition of the barmanObjectStore section they will be lost and this will prevent a restore via bootstrap.recovery.source with error:\r\n ```\r\nencountered an error while checking the presence of first needed WAL in the archive: file not found 000000010000000000000010: WAL not found\r\n```\r\nOn source database, the missing file is logged as skipped due to backup not configured:\r\n```\r\n{\"level\":\"info\",\"ts\":\"2023-12-04T14:17:33Z\",\"logger\":\"wal-archive\",\"msg\":\"Backup not configured, skip WAL archiving\",\"logging_pod\":\"[cluster-name]-1\",\"walName\":\"pg_wal/000000010000000000000010\",\"currentPrimary\":\"[cluster-name]-1\",\"targetPrimary\":\"[cluster-name]1\"}\r\n```\r\nOnly the [emergency backup procedure](https://cloudnative-pg.io/documentation/1.20/troubleshooting/#emergency-backup) allowed me to complete the restore.\n---\n@sxd This is a potentially huge data security issue, can this get a tad more attention please?\n---\nRestarting the cluster to start wal archiving works consistently for me. I don't understand though why it's not possible to restore from a base backup alone (if I had no wals)."
    },
    {
        "title": "fix: check image before reconciling the pg config",
        "id": 1874056223,
        "state": "open",
        "first": "Skip pg configuration reconciliation if the pod image differs from the one declared in the cluster spec. During an image upgrade, the pg configuration of the cluster may be tight to the new image declared in the spec. So if the instance manager applies the pg configuration to the old image, it might cause Postgres to start failing.\r\nIssue #2530",
        "messages": "Skip pg configuration reconciliation if the pod image differs from the one declared in the cluster spec. During an image upgrade, the pg configuration of the cluster may be tight to the new image declared in the spec. So if the instance manager applies the pg configuration to the old image, it might cause Postgres to start failing.\r\nIssue #2530/test limit=local\n---\nHello @GabriFedi97, the proposed PR is not passing the E2E tests\n---\nHey @armru, I didn't get to check the E2E tests logs before the retention period. Any chances to run them again?\n---\n/test limit=local\n---\n@GabriFedi97 rebased and started test\n---\n/test limit=local\n---\n@GabriFedi97 \u261d\ufe0f  ran an E2E test suite\n---\nThanks @armru.\r\nLooks like e2e are not passing for the `Online Upgrade` and `Declarative Volume Snapshot` features. \r\nI haven't investigated the second one yet, but the first one is failing because the proposed patch makes the operator changing the environment of the operands, adding the `Image` variable. In that case the pods get upgraded with a restart anyway and not through the binary replace."
    },
    {
        "title": "S3 backup: Invalid endpoint with environment variable usage",
        "id": 1873987806,
        "state": "open",
        "first": "I setup my backup like that:\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  {{- with .Values.additionalAnnotations }}\r\n  annotations:\r\n  {{ toYaml . | indent 4 }}\r\n  {{- end }}\r\n  labels:\r\n    {{- include \"authentik.labels\" (index .Subcharts \"authentik\") | nindent 4 }}\r\n  name: {{ .Values.cloudnativepg.name }}\r\n  namespace: {{ .Release.Namespace }}\r\nspec:\r\n  ... \r\n  envFrom:\r\n  - secretRef:\r\n      name: authentik-cloudflare-credentials\r\n  ...\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3://{{ .Values.cloudnativepg.backup.s3.bucket }}\r\n      endpointURL: \"${AWS_ENDPOINT_URL}\"\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: authentik-cloudflare-credentials\r\n          key: ACCESS_KEY_ID\r\n        secretAccessKey:\r\n          name: authentik-cloudflare-credentials\r\n          key: ACCESS_SECRET_KEY\r\n      wal:\r\n        compression: bzip2\r\n        maxParallel: 8\r\n```\r\nin the cluster pod, i've got the environment variable set:\r\n```\r\nnobody@authentik-1:/$ env|grep AWS\r\nAWS_ENDPOINT_URL=https://xxxxxxx.r2.cloudflarestorage.com\r\n```\r\nBut backup logs says it is invalid:\r\n```\r\n{\"level\":\"error\",\"ts\":\"2023-08-30T16:11:51Z\",\"msg\":\"while barman-cloud-check-wal-archive\",\"logging_pod\":\"authentik-1\",\"error\":\"unexpected failure invoking barman-cloud-wal-archive: exit status 4\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.checkWalArchive\\n\\tinternal/cmd/manager/walarchive/cmd.go:384\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.run\\n\\tinternal/cmd/manager/walarchive/cmd.go:169\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:81\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.20.6/x64/src/runtime/proc.go:250\"}\r\n{\"level\":\"error\",\"ts\":\"2023-08-30T16:11:51Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"authentik-1\",\"error\":\"unexpected failure invoking barman-cloud-wal-archive: exit status 4\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:83\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.20.6/x64/src/runtime/proc.go:250\"}\r\n{\"level\":\"info\",\"ts\":\"2023-08-30T16:11:51Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"authentik-1\",\"record\":{\"log_time\":\"2023-08-30 16:11:51.194 UTC\",\"process_id\":\"28\",\"session_id\":\"64ef6585.1c\",\"session_line_num\":\"65\",\"session_start_time\":\"2023-08-30 15:51:33 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-08-30T16:11:53Z\",\"logger\":\"wal-archive\",\"msg\":\"barman-cloud-check-wal-archive checking the first wal\",\"logging_pod\":\"authentik-1\"}\r\n{\"level\":\"info\",\"ts\":\"2023-08-30T16:11:56Z\",\"logger\":\"barman-cloud-check-wal-archive\",\"msg\":\"2023-08-30 16:11:56,897 [971] ERROR: Barman cloud WAL archive check exception: Invalid endpoint: ${AWS_ENDPOINT_URL}\",\"pipe\":\"stderr\",\"logging_pod\":\"authentik-1\"}\r\n{\"level\":\"error\",\"ts\":\"2023-08-30T16:11:57Z\",\"logger\":\"wal-archive\",\"msg\":\"Error invoking barman-cloud-check-wal-archive\",\"logging_pod\":\"authentik-1\",\"currentPrimary\":\"authentik-1\",\"targetPrimary\":\"authentik-1\",\"options\":[\"--endpoint-url\",\"${AWS_ENDPOINT_URL}\",\"--cloud-provider\",\"aws-s3\",\"s3://portefaix-homelab-authentik\",\"authentik\"],\"exitCode\":-1,\"error\":\"exit status 4\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/barman/archiver.(*WALArchiver).CheckWalArchiveDestination\\n\\tpkg/management/barman/archiver/archiver.go:257\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.checkWalArchive\\n\\tinternal/cmd/manager/walarchive/cmd.go:383\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.run\\n\\tinternal/cmd/manager/walarchive/cmd.go:169\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:81\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.20.6/x64/src/runtime/proc.go:250\"}\r\n```",
        "messages": "I setup my backup like that:\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  {{- with .Values.additionalAnnotations }}\r\n  annotations:\r\n  {{ toYaml . | indent 4 }}\r\n  {{- end }}\r\n  labels:\r\n    {{- include \"authentik.labels\" (index .Subcharts \"authentik\") | nindent 4 }}\r\n  name: {{ .Values.cloudnativepg.name }}\r\n  namespace: {{ .Release.Namespace }}\r\nspec:\r\n  ... \r\n  envFrom:\r\n  - secretRef:\r\n      name: authentik-cloudflare-credentials\r\n  ...\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: s3://{{ .Values.cloudnativepg.backup.s3.bucket }}\r\n      endpointURL: \"${AWS_ENDPOINT_URL}\"\r\n      s3Credentials:\r\n        accessKeyId:\r\n          name: authentik-cloudflare-credentials\r\n          key: ACCESS_KEY_ID\r\n        secretAccessKey:\r\n          name: authentik-cloudflare-credentials\r\n          key: ACCESS_SECRET_KEY\r\n      wal:\r\n        compression: bzip2\r\n        maxParallel: 8\r\n```\r\nin the cluster pod, i've got the environment variable set:\r\n```\r\nnobody@authentik-1:/$ env|grep AWS\r\nAWS_ENDPOINT_URL=https://xxxxxxx.r2.cloudflarestorage.com\r\n```\r\nBut backup logs says it is invalid:\r\n```\r\n{\"level\":\"error\",\"ts\":\"2023-08-30T16:11:51Z\",\"msg\":\"while barman-cloud-check-wal-archive\",\"logging_pod\":\"authentik-1\",\"error\":\"unexpected failure invoking barman-cloud-wal-archive: exit status 4\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/log.Error\\n\\tpkg/management/log/log.go:166\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.checkWalArchive\\n\\tinternal/cmd/manager/walarchive/cmd.go:384\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.run\\n\\tinternal/cmd/manager/walarchive/cmd.go:169\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:81\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.20.6/x64/src/runtime/proc.go:250\"}\r\n{\"level\":\"error\",\"ts\":\"2023-08-30T16:11:51Z\",\"logger\":\"wal-archive\",\"msg\":\"failed to run wal-archive command\",\"logging_pod\":\"authentik-1\",\"error\":\"unexpected failure invoking barman-cloud-wal-archive: exit status 4\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:83\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.20.6/x64/src/runtime/proc.go:250\"}\r\n{\"level\":\"info\",\"ts\":\"2023-08-30T16:11:51Z\",\"logger\":\"postgres\",\"msg\":\"record\",\"logging_pod\":\"authentik-1\",\"record\":{\"log_time\":\"2023-08-30 16:11:51.194 UTC\",\"process_id\":\"28\",\"session_id\":\"64ef6585.1c\",\"session_line_num\":\"65\",\"session_start_time\":\"2023-08-30 15:51:33 UTC\",\"transaction_id\":\"0\",\"error_severity\":\"LOG\",\"sql_state_code\":\"00000\",\"message\":\"archive command failed with exit code 1\",\"detail\":\"The failed archive command was: /controller/manager wal-archive --log-destination /controller/log/postgres.json pg_wal/000000010000000000000001\",\"backend_type\":\"archiver\",\"query_id\":\"0\"}}\r\n{\"level\":\"info\",\"ts\":\"2023-08-30T16:11:53Z\",\"logger\":\"wal-archive\",\"msg\":\"barman-cloud-check-wal-archive checking the first wal\",\"logging_pod\":\"authentik-1\"}\r\n{\"level\":\"info\",\"ts\":\"2023-08-30T16:11:56Z\",\"logger\":\"barman-cloud-check-wal-archive\",\"msg\":\"2023-08-30 16:11:56,897 [971] ERROR: Barman cloud WAL archive check exception: Invalid endpoint: ${AWS_ENDPOINT_URL}\",\"pipe\":\"stderr\",\"logging_pod\":\"authentik-1\"}\r\n{\"level\":\"error\",\"ts\":\"2023-08-30T16:11:57Z\",\"logger\":\"wal-archive\",\"msg\":\"Error invoking barman-cloud-check-wal-archive\",\"logging_pod\":\"authentik-1\",\"currentPrimary\":\"authentik-1\",\"targetPrimary\":\"authentik-1\",\"options\":[\"--endpoint-url\",\"${AWS_ENDPOINT_URL}\",\"--cloud-provider\",\"aws-s3\",\"s3://portefaix-homelab-authentik\",\"authentik\"],\"exitCode\":-1,\"error\":\"exit status 4\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/log.go:128\\ngithub.com/cloudnative-pg/cloudnative-pg/pkg/management/barman/archiver.(*WALArchiver).CheckWalArchiveDestination\\n\\tpkg/management/barman/archiver/archiver.go:257\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.checkWalArchive\\n\\tinternal/cmd/manager/walarchive/cmd.go:383\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.run\\n\\tinternal/cmd/manager/walarchive/cmd.go:169\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/walarchive.NewCmd.func1\\n\\tinternal/cmd/manager/walarchive/cmd.go:81\\ngithub.com/spf13/cobra.(*Command).execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\tpkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\\nmain.main\\n\\tcmd/manager/main.go:64\\nruntime.main\\n\\t/opt/hostedtoolcache/go/1.20.6/x64/src/runtime/proc.go:250\"}\r\n```Where do You set this AWS_ENDPOINT_URL variable ?\r\nLog shows that operator cannot resolve it to an address.\r\nIf You try to do it in the cluster's manifest, than I'm affraid it won't work that way.\n---\n@MichaluxPL the environment variable comes from the `envFrom` field and set into the corresponding secret. So it's actually present in the pod `cluster` but not in the pod operator.\r\nI can set that variable into the Operator pod, but with that, i've got only one possible endpoint ...\r\ndo you think it could possible to add a variable into the `s3credentials` ?\r\nlike that:\r\n```yaml\r\n     s3Credentials:\r\n        accessKeyId:\r\n          name: authentik-cloudflare-credentials\r\n          key: ACCESS_KEY_ID\r\n        secretAccessKey:\r\n          name: authentik-cloudflare-credentials\r\n          key: ACCESS_SECRET_KEY\r\n       endpointUrl:\r\n          name: authentik-cloudflare-credentials\r\n          key: ENDPOINT_URL\r\n```\n---\nFrom the api reference:\r\n<html>\r\n<body>\r\n<!--StartFragment-->\r\nName | Description | Type\r\n-- | -- | --\r\nendpointURL | Endpoint to be used to upload data to the cloud, overriding the automatic endpoint discovery | string\r\n<!--EndFragment-->\r\n</body>\r\n</html>\r\nso not really.\r\nendpointUrl needs to point to a correct url address, not a variable.\n---\nWe have a similar problem.\r\nThe use case is:\r\n1) we create the S3 bucket by asking rook-ceph a new bucket through a OBC clain that generates the bucket AND the secret and config map that contains the all the info for AWS_secrets and bucket name endpoint etc.\r\n2) we'd like to be able to have our chart to assign the OBC as part of the PG cluster creation and thus composing the strings needed in the barmanObjectStore through the strings we have inside the config maps and secret generated by the OBC without a human to write anything else.\r\n3) these Cm and secrets belong to the NS where the cluster will be placed\r\n4) having the option to bring these content in some sort of \"environment\" usable in the Cluster context could be useful"
    },
    {
        "title": "How we can reduce latency between Primary Postgres cluster and Replica Cluster in CNPG for Kubernetes environment?",
        "id": 1868376856,
        "state": "open",
        "first": "Currently, we have a setup running with Primary and multiple Replica clusters. We are also using S3 object storage to store WAL archives and base backups. Our Replica clusters are pointing to this storage to replicate the data but we have a data latency of approximately ~5 minutes and data is not getting replicated immediately.\r\nIs there any way to reduce the latency or could you please suggest parameters to tune it?",
        "messages": "Currently, we have a setup running with Primary and multiple Replica clusters. We are also using S3 object storage to store WAL archives and base backups. Our Replica clusters are pointing to this storage to replicate the data but we have a data latency of approximately ~5 minutes and data is not getting replicated immediately.\r\nIs there any way to reduce the latency or could you please suggest parameters to tune it?Those 5 minutes sound like it's the `archive_timeout` closing and shipping the WALs every 5 minutes because your instance generates little WAL traffic (there's a caveat explaining that in the [WAL Archiving section](https://cloudnative-pg.io/documentation/current/backup_recovery/#wal-archiving) of the doc). Could that be the case?\r\nCNPG sets a default of 5 minutes which you can tune, but as explained also in the [PostgreSQL doc](https://www.postgresql.org/docs/current/runtime-config-wal.html#GUC-ARCHIVE-LIBRARY) a value in the range of minutes is usually recommended. It's a tradeoff in the end, you don't want to set it too high because that way you'll have unarchived data for a long time if your server has a low workload, but you also don't want to set it too low otherwise you'll end up generating a lot of WAL files (which will also impact the recovery time).\r\nIf you want the replica clusters to replicate the data immediately you should rather use streaming replication, which you can setup via the `externalClusters` section (assuming you can grant the access between your primary cluster and your replica cluster)\n---\n> Those 5 minutes sound like it's the `archive_timeout` closing and shipping the WALs every 5 minutes because your instance generates little WAL traffic (there's a caveat explaining that in the [WAL Archiving section](https://cloudnative-pg.io/documentation/current/backup_recovery/#wal-archiving) of the doc). Could that be the case?\r\n> \r\n> CNPG sets a default of 5 minutes which you can tune, but as explained also in the [PostgreSQL doc](https://www.postgresql.org/docs/current/runtime-config-wal.html#GUC-ARCHIVE-LIBRARY) a value in the range of minutes is usually recommended. It's a tradeoff in the end, you don't want to set it too high because that way you'll have unarchived data for a long time if your server has a low workload, but you also don't want to set it too low otherwise you'll end up generating a lot of WAL files (which will also impact the recovery time).\r\n> \r\n> If you want the replica clusters to replicate the data immediately you should rather use streaming replication, which you can setup via the `externalClusters` section (assuming you can grant the access between your primary cluster and your replica cluster)\r\n@NiccoloFei  Thanks for the Reply. it is not exactly 5 minutes for each transaction but sometimes data gets replicated within 5 minutes or even faster. This is basically randomly taking the time to replicate the data to Replica clusters. But still, I will try to tweak this parameter. \r\nWe can not use streaming replication because we want to use multiple Kubernetes clusters(i.e. minimum 4 Kubernetes clusters). On a single Kubernetes Cluster, we would set up a Primary Postgres Cluster, and the other three Kubernetes clusters would be running the replica clusters and eventually, DB failover operation would happen across the sites. \r\ne.g we will promote Replica to Primary and Primary to Replica after failover. That's why we are using object storage as \"externalClusters\" for this solution. We will not use this solution for disaster recovery but we are trying to use this solution for GeoRedundancy. \r\nThere will also be a role for Network latency and hardware performance because the Primary Postgres cluster and Replica cluster are in different clusters or regions.\n---\nHi @oberai07,\r\n> @NiccoloFei Thanks for the Reply. it is not exactly 5 minutes for each transaction but sometimes data gets replicated within 5 minutes or even faster. This is basically randomly taking the time to replicate the data to Replica clusters. But still, I will try to tweak this parameter.\r\nMhh ... it is not randomly. It is actually deterministic. It is called RPO and we are capping it to 5 minutes. So RPO <= 5 minutes by default for backup/disaster recovery purposes.\r\n> We can not use streaming replication because we want to use multiple Kubernetes clusters(i.e. minimum 4 Kubernetes clusters).\r\nCan you please help me understand the correlation between streaming replication and the number of clusters? Postgres has cascading replication, if needed, so you could have a serialized architecture:\r\nK8s1 -> K8s2 -> K8s3 -> K8s4\r\nYou could have a replica cluster in K8s2 streamin from K8s1, another replica cluster in K8s3 streaming from K8s2, and K8s4 streaming from K8s3.\r\nI assume each Kubernetes cluster is with a single availability zone. Is that correct?\r\n> On a single Kubernetes Cluster, we would set up a Primary Postgres Cluster, and the other three Kubernetes clusters\r\n> would be running the replica clusters and eventually, DB failover operation would happen across the sites.\r\nJust to be clear: failover across Kubernetes cluster needs to be done manually at the moment.\r\n> e.g we will promote Replica to Primary and Primary to Replica after failover.\r\nAnother clarification: at the moment, CloudNativePG does not allow you to resync a former primary as a standby. You need to destroy it and recreate it. We are planning to improve replica clusters' failover and switchover management in 2024.\r\n> We will not use this solution for disaster recovery but we are trying to use this solution for GeoRedundancy.\r\nInterested in knowing the differences as to me geo redundancy is still part of the broader disaster recovery strategies.\r\n> There will also be a role for Network latency and hardware performance because the Primary Postgres cluster and Replica cluster are in different clusters or regions.\r\nYep ... this was a pre-requisite. Normally, multiple Kubernetes clusters are created only when you have only 2 \"availability zones\" in the same region, otherwise if you have more than 2 you should just create a single Kubernetes cluster with multiple availability zones. That way you can fully exploit Kubernetes.\r\nKeep us posted. Let us know if we can close this ticket.\r\nThanks,\r\nGabriele\n---\n> Mhh ... it is not randomly. It is actually deterministic. It is called RPO and we are capping it to 5 minutes. So RPO <= 5 minutes by default for backup/disaster recovery purposes.\r\nI tried to tweak this value in the Postgres configuration and set it to 2 minutes. But it still did not reduce the latency. \r\n> Can you please help me understand the correlation between streaming replication and the number of clusters? Postgres has cascading replication, if needed, so you could have a serialized architecture:\r\n> K8s1 -> K8s2 -> K8s3 -> K8s4\r\n> You could have a replica cluster in K8s2 streamin from K8s1, another replica cluster in K8s3 streaming from K8s2, and K8s4 streaming from K8s3.\r\n> `I assume each Kubernetes cluster is with a single availability zone. Is that correct?`\r\nWe have four Kubernetes clusters running on four different availability zones and for each K8s cluster we have a Postgres cluster running.  (i.e. on k8s1 - Primary Postgres cluster deployed and the other three k8s clusters (i.e. k8s2, k8s3, and k8s3) are running with Replica clusters (Designated Primary Postgres clusters). All Postgres clusters are pointing to BarmanObjectStore for backup and restoring the backup purposes. This design was proposed by the Enterprise DB Postgres(EDB) team to fulfill our requirements. \r\nMy other concern is how the Replica cluster which is running on different k8s clusters in different availability zones will make the connection to the Primary Postgres cluster. Because the Postgres cluster always exposes the K8s cluster-ip service, then we cannot communicate externally with the Cluster-ip service. \r\n> Another clarification: at the moment, CloudNativePG does not allow you to resync a former primary as a standby. You need to destroy it and recreate it. We are planning to improve replica clusters' failover and switchover management in 2024.\r\nWe are exactly doing the same i.e. destroying the  Primary cluster and creating the replica cluster after failover.\r\n> `Interested in knowing the differences as to me geo-redundancy is still part of the broader disaster recovery strategies.`\r\nwe have some limitations that forced us to use this design because on each Kubernetes cluster, we have the same applications running and after failover, the application deployed on that K8s cluster will read and write to the new Prmary Postgres db cluster.(i.e. DB Cluster after Promoting the Replica cluster).\n---\nHi oberai07,\r\nI lead EDB solution engineer team in EMEA, can we have a quick chat to understand what replication solution you are looking for, and how is your configuration looks like.\r\nYou have mentioned also specific design on the architecture, based on the limitation.   Curious to know more about that too. Perhaps we have the solution allows you to implement replica data to different AZ or regions.  Since you are already working with the EnterpriseDB team, I am interested to understand the proposed solution .  You can reach out to me kevin.li@enterprisedb.com I look forward to connecting with you.\r\nCheers, Kevin\n---\n> Hi oberai07,\r\n> \r\n> I lead EDB solution engineer team in EMEA, can we have a quick chat to understand what replication solution you are looking for, and how is your configuration looks like. You have mentioned also specific design on the architecture, based on the limitation. Curious to know more about that too. Perhaps we have the solution allows you to implement replica data to different AZ or regions. Since you are already working with the EnterpriseDB team, I am interested to understand the proposed solution . You can reach out to me [kevin.li@enterprisedb.com](mailto:kevin.li@enterprisedb.com) I look forward to connecting with you.\r\n> \r\n> Cheers, Kevin\r\nSure @Kevinckli, I will send you an email with all details and design document."
    },
    {
        "title": "Request: Add connection URI to the basic-auth secrets",
        "id": 1867953344,
        "state": "open",
        "first": "A lot of applications that use PostgreSQL take a connection URI containing the connection parameters (https://www.postgresql.org/docs/current/libpq-connect.html#id-1.7.3.8.3.6).\r\nIt would be great if CNPG could add such a URI to the `basic-auth` secrets, next to the username, password and pgpass file. That way, applications that want to be given such a URI could be configured with just an environment variable and `secretKeyRef`, rather than the current way of either manually constructing the URI or using variable expansions like:\r\n```yaml\r\n          - name: PGUSER\r\n            valueFrom:\r\n              secretKeyRef:\r\n                name: db-app\r\n                key: username\r\n          - name: PGPASSWORD\r\n            valueFrom:\r\n              secretKeyRef:\r\n                name: db-app\r\n                key: password\r\n          - name: DATABASE_URI\r\n            value: postgresql://$(PGUSER):$(PGPASSWORD)@db-r/app\r\n```\r\nInstead, I'd want just:\r\n```yaml\r\n          - name: DATABASE_URI\r\n            valueFrom:\r\n              secretKeyRef:\r\n                name: db-app\r\n                key: uri\r\n```\r\nIt's certainly not the end of the world, but feels like a relatively small change for a decent improvement in DX.",
        "messages": "A lot of applications that use PostgreSQL take a connection URI containing the connection parameters (https://www.postgresql.org/docs/current/libpq-connect.html#id-1.7.3.8.3.6).\r\nIt would be great if CNPG could add such a URI to the `basic-auth` secrets, next to the username, password and pgpass file. That way, applications that want to be given such a URI could be configured with just an environment variable and `secretKeyRef`, rather than the current way of either manually constructing the URI or using variable expansions like:\r\n```yaml\r\n          - name: PGUSER\r\n            valueFrom:\r\n              secretKeyRef:\r\n                name: db-app\r\n                key: username\r\n          - name: PGPASSWORD\r\n            valueFrom:\r\n              secretKeyRef:\r\n                name: db-app\r\n                key: password\r\n          - name: DATABASE_URI\r\n            value: postgresql://$(PGUSER):$(PGPASSWORD)@db-r/app\r\n```\r\nInstead, I'd want just:\r\n```yaml\r\n          - name: DATABASE_URI\r\n            valueFrom:\r\n              secretKeyRef:\r\n                name: db-app\r\n                key: uri\r\n```\r\nIt's certainly not the end of the world, but feels like a relatively small change for a decent improvement in DX.+1 for this"
    },
    {
        "title": "fixing messed up cloudnative cluster",
        "id": 1864660528,
        "state": "open",
        "first": "Any way we could scale down the cluster; if in case of postgres issues in HA.\r\nOnce in a while we get issues in HA postgres pods like waiting for master to come up or instance not ready.\r\nAny way to scale down cluster and scale it  up again and get it back running.\r\nSimply editing the cluster instances does not seem to work",
        "messages": "Any way we could scale down the cluster; if in case of postgres issues in HA.\r\nOnce in a while we get issues in HA postgres pods like waiting for master to come up or instance not ready.\r\nAny way to scale down cluster and scale it  up again and get it back running.\r\nSimply editing the cluster instances does not seem to work"
    },
    {
        "title": "Feature request: set labels/annotations on generated services",
        "id": 1861036099,
        "state": "open",
        "first": "Current: cannot set custom labels or annotations on the generated k8s service objects\r\nWant: ability to specify custom labels/annotations on service objects. \r\nBeing able to do something like this would be great:\r\n```\r\nservice-r:\r\n    metadata:\r\n      annotations:\r\n        external-dns.alpha.kubernetes.io/hostname: <hostname>\r\n        metallb.universe.tf/address-pool: <pool>\r\n```\r\nMotivation: We want some services to be picked up by metallb. For this, we need to be able to set a custom label to a specific services ( e.g. the -r service). Currently, it seems we can only add the annotation with some post deployment. This is not ideal.",
        "messages": "Current: cannot set custom labels or annotations on the generated k8s service objects\r\nWant: ability to specify custom labels/annotations on service objects. \r\nBeing able to do something like this would be great:\r\n```\r\nservice-r:\r\n    metadata:\r\n      annotations:\r\n        external-dns.alpha.kubernetes.io/hostname: <hostname>\r\n        metallb.universe.tf/address-pool: <pool>\r\n```\r\nMotivation: We want some services to be picked up by metallb. For this, we need to be able to set a custom label to a specific services ( e.g. the -r service). Currently, it seems we can only add the annotation with some post deployment. This is not ideal.This is our TODO list. Ideally, we would like to add a service template but don't have an ETA yet.\n---\nWe're using tailscale to expose our k8s services to our VPN and this would be very useful for us."
    },
    {
        "title": "Misleading documentation regarding Azure Blob Storage backups",
        "id": 1850028955,
        "state": "open",
        "first": "The docs on backing up into Azure Blob Storage currently say the following:\r\n> Given the previous secret, the provided credentials can be injected inside the cluster configuration:\r\n ```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\n[...]\r\nspec:\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: \"<destination path here>\"\r\n      azureCredentials:\r\n        connectionString:\r\n          name: azure-creds\r\n          key: AZURE_CONNECTION_STRING\r\n        storageAccount:\r\n          name: azure-creds\r\n          key: AZURE_STORAGE_ACCOUNT\r\n        storageKey:\r\n          name: azure-creds\r\n          key: AZURE_STORAGE_KEY\r\n        storageSasToken:\r\n          name: azure-creds\r\n          key: AZURE_STORAGE_SAS_TOKEN\r\n```\r\n(https://cloudnative-pg.io/documentation/1.18/backup_recovery/#azure-blob-storage)\r\nThe first part of the docs correctly mentions that only specific combinations of these values are required.\r\nHowever, specifying more than the required values actually makes cloudnative-pg reject the resource with an error like\r\n`when connection string is not specified, one and only one of storage key and storage SAS token is allowed` or similar.\r\nThis should either be explicitly specified or the example changed such that specifying all 4 parameters isn't presented as a valid configuration.",
        "messages": "The docs on backing up into Azure Blob Storage currently say the following:\r\n> Given the previous secret, the provided credentials can be injected inside the cluster configuration:\r\n ```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\n[...]\r\nspec:\r\n  backup:\r\n    barmanObjectStore:\r\n      destinationPath: \"<destination path here>\"\r\n      azureCredentials:\r\n        connectionString:\r\n          name: azure-creds\r\n          key: AZURE_CONNECTION_STRING\r\n        storageAccount:\r\n          name: azure-creds\r\n          key: AZURE_STORAGE_ACCOUNT\r\n        storageKey:\r\n          name: azure-creds\r\n          key: AZURE_STORAGE_KEY\r\n        storageSasToken:\r\n          name: azure-creds\r\n          key: AZURE_STORAGE_SAS_TOKEN\r\n```\r\n(https://cloudnative-pg.io/documentation/1.18/backup_recovery/#azure-blob-storage)\r\nThe first part of the docs correctly mentions that only specific combinations of these values are required.\r\nHowever, specifying more than the required values actually makes cloudnative-pg reject the resource with an error like\r\n`when connection string is not specified, one and only one of storage key and storage SAS token is allowed` or similar.\r\nThis should either be explicitly specified or the example changed such that specifying all 4 parameters isn't presented as a valid configuration."
    },
    {
        "title": "[Feature Request] Support the scale sub-resource for Clusters",
        "id": 1848073150,
        "state": "open",
        "first": "We would like to utilize the vertical pod autoscaler to automatically scale the PG pods in the Cluster. The VPA can support managing pods of custom resources as long as they implement the scale sub-resource. It would be great to have that for the Cluster CRD.\r\n## Example VPA Manifest\r\n```yaml\r\napiVersion: autoscaling.k8s.io/v1\r\nkind: VerticalPodAutoscaler\r\nmetadata:\r\n  creationTimestamp: \"2023-08-11T19:53:06Z\"\r\n  generation: 2\r\n  name: primary-api-pg\r\n  namespace: primary-api\r\n  resourceVersion: \"5833222\"\r\n  uid: 307a843d-9891-4a0c-8a4c-b6c05827697e\r\nspec:\r\n  targetRef:\r\n    apiVersion: postgresql.cnpg.io/v1\r\n    kind: Cluster\r\n    name: primary-api-pg\r\n  updatePolicy:\r\n    updateMode: Auto\r\n```\r\n## Current Error\r\n```\r\nCannot get target selector from VPA's targetRef. Reason: Unhandled targetRef postgresql.cnpg.io/v1 / Cluster / primary-api-pg, last error Resource primary-api/primary-api-pg has an empty selector for scale sub-resource\r\n```",
        "messages": "We would like to utilize the vertical pod autoscaler to automatically scale the PG pods in the Cluster. The VPA can support managing pods of custom resources as long as they implement the scale sub-resource. It would be great to have that for the Cluster CRD.\r\n## Example VPA Manifest\r\n```yaml\r\napiVersion: autoscaling.k8s.io/v1\r\nkind: VerticalPodAutoscaler\r\nmetadata:\r\n  creationTimestamp: \"2023-08-11T19:53:06Z\"\r\n  generation: 2\r\n  name: primary-api-pg\r\n  namespace: primary-api\r\n  resourceVersion: \"5833222\"\r\n  uid: 307a843d-9891-4a0c-8a4c-b6c05827697e\r\nspec:\r\n  targetRef:\r\n    apiVersion: postgresql.cnpg.io/v1\r\n    kind: Cluster\r\n    name: primary-api-pg\r\n  updatePolicy:\r\n    updateMode: Auto\r\n```\r\n## Current Error\r\n```\r\nCannot get target selector from VPA's targetRef. Reason: Unhandled targetRef postgresql.cnpg.io/v1 / Cluster / primary-api-pg, last error Resource primary-api/primary-api-pg has an empty selector for scale sub-resource\r\n```More info on the implementation spec [here](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource).\n---\nGood point! Looks like the only thing we are missing is the `selectorpath`\n---\nNeed this also. Can I propose a PR ?\n---\nHere is my workaround:\r\n- disable cnpg webhooks, here are the values passed to the official cnpg chart\r\n```yaml\r\nwebhook:\r\n    mutating:\r\n      create: false\r\n    validating:\r\n      create: false\r\n````\r\n- add a kyverno rule to mutate the cluster crd and the clusters\r\n```yaml\r\napiVersion: kyverno.io/v1\r\nkind: ClusterPolicy\r\nmetadata:\r\n  name: cnpg-cluster-scale-selector\r\nspec:\r\n  background: true\r\n  generateExisting: true\r\n  mutateExistingOnPolicyUpdate: true\r\n  useServerSideApply: true\r\n  rules:\r\n    - name: add-spec-selector-to-crd\r\n      match:\r\n        resources:\r\n          kinds:\r\n            - CustomResourceDefinition\r\n          names:\r\n            - clusters.postgresql.cnpg.io\r\n      mutate:\r\n        targets:\r\n          - apiVersion: apiextensions.k8s.io/v1\r\n            kind: CustomResourceDefinition\r\n            name: clusters.postgresql.cnpg.io\r\n        patchStrategicMerge:\r\n          spec:\r\n            versions:\r\n              - (name): v1\r\n                schema:\r\n                  openAPIV3Schema:\r\n                    type: object\r\n                    served: true\r\n                    storage: true\r\n                    properties:\r\n                      spec:\r\n                        properties:\r\n                          selector: \r\n                            type: string\r\n                subresources:\r\n                  scale:\r\n                    labelSelectorPath: .spec.selector\r\n                    specReplicasPath: .spec.instances\r\n                    statusReplicasPath: .status.instances\r\n    - name: add-spec-selector-to-clusters\r\n      match:\r\n        resources:\r\n          kinds:\r\n            - postgresql.cnpg.io/v1/Cluster\r\n      mutate:\r\n        targets:\r\n          - apiVersion: postgresql.cnpg.io/v1\r\n            kind: Cluster\r\n            name: {{ request.object.metadata.name }}\r\n        patchStrategicMerge:\r\n          spec:\r\n            selector: \"cnpg.io/cluster={{ request.object.metadata.name }}\"\r\n```\r\nand my vpa:\r\n```yaml\r\napiVersion: autoscaling.k8s.io/v1\r\nkind: VerticalPodAutoscaler\r\nmetadata:\r\n  name: my-vpa\r\nspec:\r\n  targetRef:\r\n    apiVersion: postgresql.cnpg.io/v1\r\n    kind: Cluster\r\n    name: my-pg-cluster\r\n  updatePolicy:\r\n    updateMode: \"Off\"\r\n```\r\nand I can see the provided recommendations from the vpa\r\ntada !!!\n---\nI finally had problem caused by this issue: https://github.com/cloudnative-pg/cloudnative-pg/issues/3753 so I reactived webhook and removed a part using the following manual patch\r\nand after upgraded cnpg to latest version the kyverno policy part that update the crd didn't work anymore, so\r\nhere is my tricky solution for now\r\n```sh\r\nkubectl -n cnpg patch crd clusters.postgresql.cnpg.io --type json --patch-file patches/cluster-crd.patch.yaml\r\nkubectl patch mutatingwebhookconfiguration cnpg-mutating-webhook-configuration --type=json  --patch-file patches/webhook.patch.yaml\r\n```\r\n`patches/cluster-crd.patch.yaml`:\r\n```json\r\n[\r\n  {\r\n    \"op\": \"add\",\r\n    \"path\": \"/spec/versions/0/schema/openAPIV3Schema/properties/spec/properties/selector\",\r\n    \"value\": { \"type\": \"string\" },\r\n  },\r\n  {\r\n    \"op\": \"add\",\r\n    \"path\": \"/spec/versions/0/subresources/scale/labelSelectorPath\",\r\n    \"value\": \".spec.selector\",\r\n  },\r\n]\r\n```\r\n`patches/webhook.patch.yaml`:\r\n```json\r\n[{ \"op\": \"remove\", \"path\": \"/webhooks/1\" }]\r\n```\r\nand in argocd applicationset set.templatate.spec:\r\n```yaml\r\nignoreDifferences:\r\n        - group: admissionregistration.k8s.io\r\n          kind: MutatingWebhookConfiguration\r\n          jsonPointers:\r\n            - /webhooks\r\n        - group: apiextensions.k8s.io\r\n          kind: CustomResourceDefinition\r\n          jsonPointers:\r\n            - /spec/versions/0/schema/openAPIV3Schema/properties/spec/properties/selector\r\n            - /spec/versions/0/subresources/scale/labelSelectorPath\r\n````\r\nand the new kyverno policy:\r\n```yaml\r\napiVersion: kyverno.io/v1\r\nkind: ClusterPolicy\r\nmetadata:\r\n  name: cnpg-cluster-scale-selector\r\nspec:\r\n  background: true\r\n  generateExisting: true\r\n  mutateExistingOnPolicyUpdate: true\r\n  useServerSideApply: true\r\n  rules:\r\n    - name: add-spec-selector-to-clusters\r\n      match:\r\n        resources:\r\n          kinds:\r\n            - postgresql.cnpg.io/v1/Cluster\r\n      mutate:\r\n        targets:\r\n          - apiVersion: postgresql.cnpg.io/v1\r\n            kind: Cluster\r\n            name: {{ request.object.metadata.name }}\r\n        patchStrategicMerge:\r\n          spec:\r\n            selector: \"cnpg.io/cluster={{`{{ request.object.metadata.name }}`}}\"\r\n```\n---\n@gbartolini Are you open to a PR here? I think above demonstrates this is just a small config change.\n---\nTo follow up on @devthejo 's solution, you can use `patchesJson6902` in the kyverno policy instead of using `kubectl` to patch. I wasn't able to get `patchStrategicMerge` to work.\nAdditionally, as @devthejo has called out, removing the second webhook from `cnpg-mutating-webhook-configuration` is required. Otherwise, the selector will not show up on the Cluster spec (the CNPG webhook overwrites it).\nFinally, using `\"cnpg.io/cluster={{`{{ request.object.metadata.name }}`}}\"` as the selector is bad as this will also select any Pooler pods you are running. The role of the pod should be added also.\nI have included our Kyverno policy to work around this limitation below:\n```yaml\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"kyverno.io/v1\",\"kind\":\"ClusterPolicy\",\"metadata\":{\"annotations\":{},\"labels\":{\"panfactum.com/environment\":\"production\",\"panfactum.com/local\":\"false\",\"panfactum.com/module\":\"kube_cloudnative_pg\",\"panfactum.com/region\":\"us-east-2\",\"panfactum.com/root-module\":\"kube_cloudnative_pg\",\"panfactum.com/stack-commit\":\"local\",\"panfactum.com/stack-version\":\"local\",\"test.1/2.3.4.5\":\"test.1.2.3.4.5\",\"test1\":\"foo\",\"test2\":\"bar\",\"test3\":\"baz\",\"test4\":\"42\"},\"name\":\"cnpg-scale-selector\"},\"spec\":{\"generateExisting\":true,\"mutateExistingOnPolicyUpdate\":true,\"rules\":[{\"match\":{\"resources\":{\"kinds\":[\"CustomResourceDefinition\"],\"names\":[\"clusters.postgresql.cnpg.io\"]}},\"mutate\":{\"patchesJson6902\":\"- \\\"op\\\": \\\"replace\\\"\\n  \\\"path\\\": \\\"/spec/versions/0/schema/openAPIV3Schema/properties/spec/properties/selector\\\"\\n  \\\"value\\\":\\n    \\\"type\\\": \\\"string\\\"\\n- \\\"op\\\": \\\"replace\\\"\\n  \\\"path\\\": \\\"/spec/versions/0/subresources/scale/labelSelectorPath\\\"\\n  \\\"value\\\": \\\".spec.selector\\\"\\n\",\"targets\":[{\"apiVersion\":\"apiextensions.k8s.io/v1\",\"kind\":\"CustomResourceDefinition\",\"name\":\"clusters.postgresql.cnpg.io\"}]},\"name\":\"add-spec-selector-to-crd\"},{\"match\":{\"resources\":{\"kinds\":[\"MutatingWebhookConfiguration\"],\"names\":[\"cnpg-mutating-webhook-configuration\"]}},\"mutate\":{\"patchesJson6902\":\"- \\\"op\\\": \\\"remove\\\"\\n  \\\"path\\\": \\\"/webhooks/1\\\"\\n\",\"targets\":[{\"apiVersion\":\"admissionregistration.k8s.io/v1\",\"kind\":\"MutatingWebhookConfiguration\",\"name\":\"cnpg-mutating-webhook-configuration\"}]},\"name\":\"remove-cnpg-mutating-webhook\"},{\"match\":{\"resources\":{\"kinds\":[\"postgresql.cnpg.io/v1/Cluster\"]}},\"mutate\":{\"patchStrategicMerge\":{\"spec\":{\"selector\":\"cnpg.io/cluster={{ request.object.metadata.name }},cnpg.io/podRole=instance\"}},\"targets\":[{\"apiVersion\":\"postgresql.cnpg.io/v1\",\"kind\":\"Cluster\",\"name\":\"{{ request.object.metadata.name }}\"}]},\"name\":\"add-spec-selector-to-clusters\"}],\"useServerSideApply\":true}}\n  creationTimestamp: \"2024-11-13T15:41:04Z\"\n  generation: 18\n  labels:\n    panfactum.com/environment: production\n    panfactum.com/local: \"false\"\n    panfactum.com/module: kube_cloudnative_pg\n    panfactum.com/region: us-east-2\n    panfactum.com/root-module: kube_cloudnative_pg\n    panfactum.com/stack-commit: local\n    panfactum.com/stack-version: local\n    test.1/2.3.4.5: test.1.2.3.4.5\n    test1: foo\n    test2: bar\n    test3: baz\n    test4: \"42\"\n  name: cnpg-scale-selector\n  resourceVersion: \"288754720\"\n  uid: 020d360d-16f6-40c4-b781-ead13871b980\nspec:\n  admission: true\n  background: true\n  emitWarning: false\n  generateExisting: true\n  mutateExistingOnPolicyUpdate: true\n  rules:\n  - match:\n      resources:\n        kinds:\n        - CustomResourceDefinition\n        names:\n        - clusters.postgresql.cnpg.io\n    mutate:\n      patchesJson6902: |\n        - \"op\": \"replace\"\n          \"path\": \"/spec/versions/0/schema/openAPIV3Schema/properties/spec/properties/selector\"\n          \"value\":\n            \"type\": \"string\"\n        - \"op\": \"replace\"\n          \"path\": \"/spec/versions/0/subresources/scale/labelSelectorPath\"\n          \"value\": \".spec.selector\"\n      targets:\n      - apiVersion: apiextensions.k8s.io/v1\n        kind: CustomResourceDefinition\n        name: clusters.postgresql.cnpg.io\n    name: add-spec-selector-to-crd\n    skipBackgroundRequests: true\n  - match:\n      resources:\n        kinds:\n        - MutatingWebhookConfiguration\n        names:\n        - cnpg-mutating-webhook-configuration\n    mutate:\n      patchesJson6902: |\n        - \"op\": \"remove\"\n          \"path\": \"/webhooks/1\"\n      targets:\n      - apiVersion: admissionregistration.k8s.io/v1\n        kind: MutatingWebhookConfiguration\n        name: cnpg-mutating-webhook-configuration\n    name: remove-cnpg-mutating-webhook\n    skipBackgroundRequests: true\n  - match:\n      resources:\n        kinds:\n        - postgresql.cnpg.io/v1/Cluster\n    mutate:\n      patchStrategicMerge:\n        spec:\n          selector: cnpg.io/cluster={{ request.object.metadata.name }},cnpg.io/podRole=instance\n      targets:\n      - apiVersion: postgresql.cnpg.io/v1\n        kind: Cluster\n        name: '{{ request.object.metadata.name }}'\n    name: add-spec-selector-to-clusters\n    skipBackgroundRequests: true\n  useServerSideApply: true\n  validationFailureAction: Audit\nstatus:\n  autogen: {}\n  conditions:\n  - lastTransitionTime: \"2024-11-13T15:41:04Z\"\n    message: Ready\n    reason: Succeeded\n    status: \"True\"\n    type: Ready\n  rulecount:\n    generate: 0\n    mutate: 3\n    validate: 0\n    verifyimages: 0\n  validatingadmissionpolicy:\n    generated: false\n    message: \"\"\n```\n---\n@gbartolini I want to present a case for getting this into the roadmap for 1.25:\n- Vertical autoscaling is a really powerful feature, especially for databases where horizontal scaling isn't possible / practical. This would help CNPG reach parity with other operators and managed postgres services. This issue is in the top 10 most requested features (by \ud83d\udc4d\ud83c\udffb ) for CNPG currently.\n- The level of effort for the change should be low. This really only impacts the CRDs (though some logic will need to be added to keep the `.spec.selector` in sync).\n- This update is 100% backwards compatible.\n- Both @devthejo and I have verified that with the changes outlined above, the VPA integrates well. \nHappy to take a crack at it but also want to get directional sign-off from a maintainer first.\n---\n@gbartolini Wanted to circle back here to see if there were any thoughts here from the project maintainers.\nI am happy to implement this if given the OK.\n---\n@fullykubed I'd say just open a PR and reference this issue. I'm pretty sure the maintainers will be happy with the contribution \ud83d\ude0a And if they require any change you can always address it in the PR."
    },
    {
        "title": "Proposal: Rename cluster label to cnpg_cluster",
        "id": 1822535697,
        "state": "open",
        "first": "I would like to discuss a breaking change on observability. Our problem (and I will argue in a moment why it would not only affect us): Metrics from cnpg expose the `cluster` label (for example `cnpg_collector_up` ), and the [Grafana dashboard](https://github.com/cloudnative-pg/cloudnative-pg/blob/main/docs/src/samples/monitoring/grafana-dashboard.json) builds upon that. However, the `cluster` label is a commonly-used general label often referring to the Kubernetes cluster the data originates from, and used by many as an external label when pushing metrics etc.. Those two concept cannot co-exist, and we decided to relabel the cnpg `cluster` label to `app_cluster`, but then we of course cannot use your upstream dashboard anymore (might also affect alerts at some point in time). My point is, with the current naming, the support for multi-cluster scenarios is quite restricted, and I am afraid we are not the only ones running cnpg in multi-cluster scenarios. My proposal is to rename the `cluster` label to `cnpg_cluster`, knowing that it will break existing alerts & dashboards for users. What do you think?\r\nFeel free to also add to the Slack Thread: https://cloudnativepg.slack.com/archives/C03AX0J5P29/p1690284881972749",
        "messages": "I would like to discuss a breaking change on observability. Our problem (and I will argue in a moment why it would not only affect us): Metrics from cnpg expose the `cluster` label (for example `cnpg_collector_up` ), and the [Grafana dashboard](https://github.com/cloudnative-pg/cloudnative-pg/blob/main/docs/src/samples/monitoring/grafana-dashboard.json) builds upon that. However, the `cluster` label is a commonly-used general label often referring to the Kubernetes cluster the data originates from, and used by many as an external label when pushing metrics etc.. Those two concept cannot co-exist, and we decided to relabel the cnpg `cluster` label to `app_cluster`, but then we of course cannot use your upstream dashboard anymore (might also affect alerts at some point in time). My point is, with the current naming, the support for multi-cluster scenarios is quite restricted, and I am afraid we are not the only ones running cnpg in multi-cluster scenarios. My proposal is to rename the `cluster` label to `cnpg_cluster`, knowing that it will break existing alerts & dashboards for users. What do you think?\r\nFeel free to also add to the Slack Thread: https://cloudnativepg.slack.com/archives/C03AX0J5P29/p1690284881972749I am concerned about the introduction of this breaking change. Can we do something like what we did for the log field names?\r\nhttps://github.com/cloudnative-pg/cloudnative-pg/commit/936b930eaca7d0c85d571a7eac3fc6f401105551\r\nAnd then get the plugin to control that in the generate command as part of the options for the deployment of the operator. See:\r\nhttps://github.com/cloudnative-pg/cloudnative-pg/commit/91efc27e1a514ea1f4e0d003f3296dd611a304a5\r\nAny comments?\r\nThat way we won't be breaking existing installations, but give the possibility for users to change the mapping in the installation and update their dashboard. In a few releases we can change the default.\n---\nI'd suggest adding both for a while, there should be no issue in doing so and the cluster label would be overridden if needed. We can even make the label to use a parameter in the dashboard, to ease the transition.\n---\nHi Philippe,\r\n> I'd suggest adding both for a while, there should be no issue in doing so and the cluster label would be overridden if needed. We can even make the label to use a parameter in the dashboard, to ease the transition.\r\nI am probably missing something here. But wouldn't that mean that the exporter everytime sends the information twice?\n---\n@gbartolini Nono, labels are just additional metadata for each metrics.\n```\ncnpg_collector_up{cluster=\"cluster-example\"} 1\n```\nShould be reported as:\n```\ncnpg_collector_up{cluster=\"cluster-example\" cnpg_cluster=\"cluster-example\"} 1\n```\nBecause \"cluster\" in multi cluster monitoring systems can be used by other tools to show which Kubernetes cluster the metric is coming from, and therefore can overwrite the one we set.\n---\noh ... and then we change the dashboard immediately and communicate the change. Got it. Thanks\n---\nThanks for following up on this!\r\nTwo concerns/notes from my end (as an extensive user of the Prometheus ecosystem):\r\n- Quite often people are relying on using external labels to add the `cluster` label. However, those actually do not overwrite existent labels, so having `cnpg_collector_up{cluster=\"cluster-example\" cnpg_cluster=\"cluster-example\"} 1` would be a bit unfortunate still, with `cluster` label being set. For the setups I maintain, I rely on using relabeling instead of external labels to force-set them, but as mentioned, this is rather uncommon. Considering this, I would prefer having either `cluster` (\"old\" way) or `cnpg_cluster` (the \"new\" standard), but not both at the same time.\r\n- I guess I see most effort with the dashboard that is maintained (`docs/src/samples/monitoring/grafana-dashboard.json`). Many projects do rely on using [Monitoring mixins](https://monitoring.mixins.dev/) to allow for certain configurability of dashboards + alerts, and the name of the cluster label could be one of the configuration options, as for example in the kube-state-metrics mixins: https://github.com/kubernetes/kube-state-metrics/blob/main/jsonnet/kube-state-metrics-mixin/config.libsonnet#L5 . However, the `grafana-dashboard.json` you currently maintain is not generated from anything, right? Might be worth the effort, especially, as with this you could configure it to support multi-cluster scenarios - so the user can drop-down for both `cluster` (to select the Kubernetes cluster, resp. source infra, where the cnpg cluster(s) reside(s)), and then the `cnpg_cluster` for the actual postgres cluster to observe. We forked your current dashboard to support exactly this, and would love to see this being supported upstream (can contribute necessary changes back of course) :-)\r\n@phisco I am not sure what you mean by \r\n> We can even make the label to use a parameter in the dashboard, to ease the transition.\n---\n> @phisco I am not sure what you mean by\nI meant that we could add another parameter to the dashboard to let people choose which label they want to use, cluster or cnpg_cluster.\nHowever, I was assuming it would have been overwritten, if that's not the case I don't see any other way than changing it, updating the dashboard and clearly stating the breaking change in the next minor release.\n---\n@phisco I am more worried about automatic alerts that would break, not that worried for the dashboard.\n---\nAs I just implemented prom federation I feel like this change is needed.\nI think if the change is communicated clearly in an upcoming release, and then actually changed in the next version after that people should have enough heads up that this is changing. I honestly can't imagine anyone upgrading cnpg without reading the changelogs, giving people enough warning a head of time is ample enough IMO\n---\n@onedr0p, https://github.com/cloudnative-pg/cloudnative-pg/pull/3075 was just merged, as per the comment [here](https://github.com/cloudnative-pg/cloudnative-pg/pull/3075#issuecomment-1793802677) this is not a complete fix as the real issue is with the grafana dashboard lacking the ability to change the label being used, but using that and manually fixing the label being used by the dashboard you should still be able to have everything working. Changing the label by default could mean user's custom alerts are not going to trigger, so we should do that with extreme care and we couldn't come up with a safe migration path for it.\n---\n> Changing the label by default could mean user's custom alerts are not going to trigger, so we should do that with extreme care and we couldn't come up with a safe migration path for it.\nI saw that mentioned earlier which is why I suggested to give people a heads up this will change in a future release, and then change it in a release sometime after that.  Communication is key here... basically keep mentioning this change will happen on every release until it happens:\n> [!IMPORTANT]\n> Breaking changes to the prometheus metrics will be happening in v1.x.x, this will affect alerts. Please read here for more information.\nIf you all don't want to change this label at all maybe it's worth it to support two grafana dashboards one additional one with the cnpg_cluster label in this repo?\n---\nI tried to setup those relabelings but it's not coming thru, the cluster label just appears to be dropped instead...\r\n## Prom Query returned\r\n```\r\ncnpg_backends_max_tx_duration_seconds{application_name=\"cnpg_metrics_exporter\",\u00a0container=\"postgres\",\u00a0datname=\"app\",\u00a0endpoint=\"metrics\",\u00a0instance=\"10.42.1.245:9187\",\u00a0job=\"database/postgres16\",\u00a0namespace=\"database\",\u00a0pod=\"postgres16-3\",\u00a0state=\"active\",\u00a0usename=\"postgres\"}\r\n```\r\n## Cluster config\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: postgres16\r\nspec:\r\n...\r\n  monitoring:\r\n    enablePodMonitor: true\r\n    podMonitorMetricRelabelings:\r\n      - { sourceLabels: [\"cluster\"], targetLabel: cnpg_cluster, action: replace }\r\n      - { regex: cluster, action: labeldrop }\r\n...\r\n```\r\n## Rendered podMonitor\r\n```yaml\r\napiVersion: monitoring.coreos.com/v1\r\nkind: PodMonitor\r\nmetadata:\r\n  annotations:\r\n    cnpg.io/operatorVersion: 1.22.1\r\n  labels:\r\n    cnpg.io/cluster: postgres16\r\n  name: postgres16\r\n  namespace: database\r\nspec:\r\n  namespaceSelector: {}\r\n  podMetricsEndpoints:\r\n  - bearerTokenSecret:\r\n      key: \"\"\r\n    metricRelabelings:\r\n    - action: replace\r\n      sourceLabels:\r\n      - cluster\r\n      targetLabel: cnpg_cluster\r\n    - action: labeldrop\r\n      regex: cluster\r\n    port: metrics\r\n  selector:\r\n    matchLabels:\r\n      cnpg.io/cluster: postgres16\r\n```\r\n## Prom rendered config\r\nI am seeing it applied in prometheus but the label is not being changed...\r\n```yaml\r\n- job_name: podMonitor/database/postgres16/0\r\n  honor_timestamps: true\r\n  track_timestamps_staleness: false\r\n  scrape_interval: 30s\r\n  scrape_timeout: 10s\r\n  scrape_protocols:\r\n  - OpenMetricsText1.0.0\r\n  - OpenMetricsText0.0.1\r\n  - PrometheusText0.0.4\r\n  metrics_path: /metrics\r\n  scheme: http\r\n  enable_compression: true\r\n  follow_redirects: true\r\n  enable_http2: true\r\n  relabel_configs:\r\n  - source_labels: [job]\r\n    separator: ;\r\n    regex: (.*)\r\n    target_label: __tmp_prometheus_job_name\r\n    replacement: $1\r\n    action: replace\r\n  - source_labels: [__meta_kubernetes_pod_phase]\r\n    separator: ;\r\n    regex: (Failed|Succeeded)\r\n    replacement: $1\r\n    action: drop\r\n  - source_labels: [__meta_kubernetes_pod_label_cnpg_io_cluster, __meta_kubernetes_pod_labelpresent_cnpg_io_cluster]\r\n    separator: ;\r\n    regex: (postgres16);true\r\n    replacement: $1\r\n    action: keep\r\n  - source_labels: [__meta_kubernetes_pod_container_port_name]\r\n    separator: ;\r\n    regex: metrics\r\n    replacement: $1\r\n    action: keep\r\n  - source_labels: [__meta_kubernetes_namespace]\r\n    separator: ;\r\n    regex: (.*)\r\n    target_label: namespace\r\n    replacement: $1\r\n    action: replace\r\n  - source_labels: [__meta_kubernetes_pod_container_name]\r\n    separator: ;\r\n    regex: (.*)\r\n    target_label: container\r\n    replacement: $1\r\n    action: replace\r\n  - source_labels: [__meta_kubernetes_pod_name]\r\n    separator: ;\r\n    regex: (.*)\r\n    target_label: pod\r\n    replacement: $1\r\n    action: replace\r\n  - separator: ;\r\n    regex: (.*)\r\n    target_label: job\r\n    replacement: database/postgres16\r\n    action: replace\r\n  - separator: ;\r\n    regex: (.*)\r\n    target_label: endpoint\r\n    replacement: metrics\r\n    action: replace\r\n  - source_labels: [__address__]\r\n    separator: ;\r\n    regex: (.*)\r\n    modulus: 1\r\n    target_label: __tmp_hash\r\n    replacement: $1\r\n    action: hashmod\r\n  - source_labels: [__tmp_hash]\r\n    separator: ;\r\n    regex: \"0\"\r\n    replacement: $1\r\n    action: keep\r\n  metric_relabel_configs:\r\n  - source_labels: [cluster]\r\n    separator: ;\r\n    regex: (.*)\r\n    target_label: cnpg_cluster\r\n    replacement: $1\r\n    action: replace\r\n  - separator: ;\r\n    regex: cluster\r\n    replacement: $1\r\n    action: labeldrop\r\n  kubernetes_sd_configs:\r\n  - role: pod\r\n    kubeconfig_file: \"\"\r\n    follow_redirects: true\r\n    enable_http2: true\r\n    namespaces:\r\n      own_namespace: false\r\n      names:\r\n      - database\r\n```\r\nNot sure what to do here, anyone have any ideas?\n---\nAccording to some docs..  (we're using victoriametrics/vmagent here), if your adding 'cluster' to all your metrics as a global label, prometheus is supposed to rename the metric label from 'cluster' to 'exported_cluster', and then add your over-riding cluster label..\r\nie, in a grafana explorer, look for cnpg_collector_up, and see if you have an 'exported_cluster' label (which in our case, we do have..)\r\nThis should not be considered a long term solution..  it's just how a current Prometheus/VMagent handles conflicting labels.\n---\nWould you consider calling the label: `clusters.postgresql.cnpg.io/name` instead?\n---\nHi @onedr0p , JFYI I managed to configure:\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nspec:\r\n  monitoring:\r\n    customQueriesConfigMap:\r\n      - key: queries\r\n        name: cnpg-default-monitoring\r\n    disableDefaultQueries: false\r\n    enablePodMonitor: true\r\n    podMonitorMetricRelabelings:\r\n      - action: replace\r\n        sourceLabels:\r\n          - cluster\r\n        targetLabel: cnpg_cluster\r\n      - action: labeldrop\r\n        regex: cluster\r\n```\r\nand in Thanos I see properly:\r\n```\r\n{__name__=\"cnpg_collector_postgres_version\", cluster=\"mycluster\", cnpg_cluster=\"cloud-cluster\", container=\"postgres\", endpoint=\"metrics\", environment=\"myenv\", full=\"16.4\", instance=\"10.2.177.35:9187\", job=\"cloud-cluster/cloud-cluster\", namespace=\"cloud-cluster\", pod=\"cloud-cluster-2\", tenant=\"mytenant\"}\r\n{__name__=\"cnpg_collector_postgres_version\", cluster=\"mycluster\", cnpg_cluster=\"cloud-cluster\", container=\"postgres\", endpoint=\"metrics\", environment=\"myenv\", full=\"16.4\", instance=\"10.2.53.241:9187\", job=\"cloud-cluster/cloud-cluster\", namespace=\"cloud-cluster\", pod=\"cloud-cluster-3\", tenant=\"mytenant\"}\r\n{__name__=\"cnpg_collector_postgres_version\", cluster=\"mycluster\", cnpg_cluster=\"cloud-cluster\", container=\"postgres\", endpoint=\"metrics\", environment=\"myenv\", full=\"16.4\", instance=\"10.2.58.115:9187\", job=\"cloud-cluster/cloud-cluster\", namespace=\"cloud-cluster\", pod=\"cloud-cluster-1\", tenant=\"mytenant\"}\r\n```\r\nI think this is just workaround, and better not use `cluster` label at exporter and use `cnpg_cluster` label by default. Even with this workaround we or will have to deploy custom Grafana dashboard, or allow template existing one.\r\nAlso we do not need to statically set `cnpg_cluster` label in Prometheus alerts, it will be anyway populated from metrics we are querying in all cases except someone will use `sum` and would forget to add `by (cnpg_cluster)` \ud83d\ude0a.\r\nP.s. I deploy my monitoring with `kube-prometheus-stack` and `bitnami/thanos`, and use (real values are changed):\r\n```\r\nprometheusSpec:\r\n  ## External labels to add to any time series or alerts when communicating with external systems\r\n  ## Required by https://thanos.io/tip/components/sidecar.md/\r\n  externalLabels:\r\n    cluster: mycluster\r\n    tenant: mytenant\r\n    environment: myenv\r\n```\n---\nYeah the rub of all these changes comes down the dashboard. Remapping the metrics as you've demonstrated isn't too much of a fuss.\nRemapping all the dashboard queries to filter for cluster, then namespace, then cnpg_cluster then instance is quite the breaking change.\nNot sure maintaining two hard forks of the same chart is that joyful, especially given if you aren't running Prometheus federation or another flavour of multi cluster collection you probably won't get a working dashboard with a missing cluster metric either. \nNot sure what the precedent has been on this for other projects who have to deal with single cluster oriented metrics collection vs multi cluster, I'm primarily using the pre-configured federation patched dashboards grafana distributed with their cloud offering.\n---\nHi @danielloader, honestly to patch current dashboard we need change `/\\bcnpg_cluster\\b=\"(?<text>[^\"]+)/g` to `/\\bcnpg_cluster\\b=\"(?<text>[^\"]+)/g`, so allow to do template as: `/\\b{{ .Value.grafanaDashboard.clusterLabel }}\\b=\"(?<text>[^\"]+)/g` as workaround solution will do the trick. Question is: is it worth to do such workarounds instead of proper fix? I think - no.\n---\nP.s. if speak about Grafana dashboards, I would split Operator metrics fully from Postgres metrics. It better to be separate dashboard.\r\nAlso `sum(kube_pod_status_ready{namespace=\"$operatorNamespace\"} * on (pod) group_left( label_app_kubernetes_io_name ) kube_pod_labels{label_app_kubernetes_io_name=~\"cloudnative-pg\"})` will not work for most of defualt setups.\r\n`kube_pod_labels` is empty by default, and to get it working `kube-state-metrics` should be deployed  with:\r\n```\r\nkube-state-metrics:\r\n  metricLabelsAllowlist:\r\n    - pods=[app.kubernetes.io/name,app.kubernetes.io/instance,app.kubernetes.io/version,app.kubernetes.io/component,app.kubernetes.io/part-of,app.kubernetes.io/managed-by]\r\n```\r\nThere should be notice about such requirement to work in chart notes near `grafanaDashboard.create`, or we can do simpler:\r\n```\r\ncount(\r\n  sum(kube_pod_status_ready{namespace=\"$operatorNamespace\"} == 1)\r\n  by (pod,namespace)\r\n  * on (pod) group_left()\r\n  count(controller_runtime_webhook_requests_total{namespace=\"$operatorNamespace\",webhook=\"/mutate-postgresql-cnpg-io-v1-cluster\"}) by (pod)\r\n)\r\n```\r\nI created PR to fix this with controller_runtime_webhook_requests_total \ud83d\ude0a\n---\nWell again that relies on your dashboard being installed via helm I guess.\nIf you aren't hosting grafana yourself you're more likely to import the json as is or import it via id on the dashboard marketplace (which has the cnpg dashboard published to it and where I found it)\n---\nI see that the `cluster` label is used only in for these two default metrics:\r\n- `cnpg_collector_up`\r\n- `cnpg_collector_postgres_version`\r\nThen, the `cnpg_collector_up` metric is the base from which the dashboard extracts the namespace, cluster and instance variables. But we don't really need the `cluster` label there. We could change the dashboard to get the cluster variable not from the `cluster` or `cnpg_cluster` label, but from the `pod` one, moving from\r\n```\r\n/\\bcnpg_cluster\\b=\"(?<text>[^\"]+)/g\r\n```\r\nto\r\n```\r\n/\\bpod\\b=\\\"(.*)-[1-9][0-9]*\\\"/g\r\n```\r\nIt would still extract the same value, but won't require a label defined in the metric. People would be able to directly use the json. \r\nThen we could drop the `cluster` label in a future cnpg release, giving proper notification to the users. They could easily set relabelling if needed.\n---\n@fcanovai I think this is a bad idea, better to not loose cluster information, it easily can become needed at any time. Also if you deploy multiply clusters in one namespace - it will work incorrectly.\n---\nYeah I'd keep a label for differentiating between CNPG clusters. Adding a cluster selector to every single panel sounds like the more difficult to square issue - just because if you run a single kubernetes cluster self hosted prometheus/grafana stack you may not have the config akin to above setup:\r\n```\r\nprometheusSpec:\r\n  ## External labels to add to any time series or alerts when communicating with external systems\r\n  ## Required by https://thanos.io/tip/components/sidecar.md/\r\n  externalLabels:\r\n    cluster: mycluster\r\n    tenant: mytenant\r\n    environment: myenv\r\n```\r\nSo either you'd need to support single/multi cluster deployments and put the associated additional filter on each query on each dashboard chart, or you'd have to mandate in the documentation your prometheus setup _has_ to include a cluster value. \r\nNot an enviable decision but in a selfish personal perspective, I'd support anything that can support this federated metrics grafana dashboard to work - currently I'm having to suffix the cluster name to the CNPG cluster names in each environment just to be able to tell them apart in the dashboard.\n---\nI'd like to have a solution that works for Prometheus with and without `externalLabels` being defined.\r\nAlso, right now setting `cluster` in an external label would break the dashboard, since it will override the `cluster` label that `cnpg_collector_up` exports and the dashboard uses to find the instances.\r\nBy the way, that label was added exactly for this: having a working dropdown menu. If it was really important as a label it would have been added to all the default metrics, instead of just two of them. But we don't need it for that, we just need the pod name and the namespace to identify the existing clusters, and Prometheus gets those labels automatically. @danielloader : this works for multiple clusters in the same namespace. In the following example I have two clusters in the default namespace.\r\n![cluster_query](https://github.com/user-attachments/assets/a327d767-6683-4852-a743-de8cf14789cf)\r\nThis would give us a chance to have an updated dashboard with the following variables:\r\n1. namespace (from label)\r\n2. cnpg_cluster (regexp)\r\n3. instance (label again)\r\nNow, we want to add a k8s `cluster` to this set of identifiers, to support multicluster metrics collection. We can write in the documentation that multicluster Prometheus setups have to define an `externalLabels` `cluster` label. This will add the k8s cluster name, unless there is a `cluster` label already, according to the Prometheus doc. Which is annoying because that label exists for `cnpg_collector_up` and `cnpg_collector_postgres_version`. I consider this a point in favour of removing the label. There is a [Thanos sidecar issue](https://github.com/thanos-io/thanos/issues/1579) about it always overriding the value, and I've seen it happening in my test, but I'm not convinced we should rely on it.\r\nIn a single k8s cluster scenario, instead, I don't think we should force users to define `externalLabels`. In this case, most of the metrics won't have the `cluster` label, but it will exist for `cnpg_collector_up` and `cnpg_collector_postgres_version`. If we add to the dashboard the logic to filter on what we believe a k8s cluster, the label would break the logic. Again, I'd remove the `cluster` label from there.\r\nRemoving the label, however, could break some logic for customers that have their own dashboards, or use that label in some ways. We could rename the to `cnpg_cluster` for a while and document how to relabel it to `cluster` if needed.\r\nThen we could do one of the following:\r\n* Add an info metric with value 1 and informational labels, telling people that they can find the value there, if needed\r\n* Add the `cnpg_cluster` label to all the extracted metrics, if we want to associate the metric directly to the CNPG cluster\r\nI'd go with the first and leave the second approach to the future if it comes up as needed.\r\nSo, in a single k8s cluster scenario where `cluster` is not set in the `externalLabels`, Grafana would not find `cluster` in the metrics, will evaluate that as empty string and would be able to match `cluster=''`, getting all the metrics for which cluster is not set.\r\nWe would need to support the new `cluster` variable  as a predicate in all the queries of the dashboard. Maybe it would be possible to template (jsonnet?) the dashboard to allow users to define their own predicates and build the dashboard, in case `cluster` cannot be used or it is not enough to correctly identify where the metrics are coming from.\r\nSo, the steps for me:\r\n* make the dashboard (CNPG) `cluster` label agnostic\r\n* drop/rename/move the (CNPG) `cluster` label. Inform users and document alternatives.\r\n* add the (k8s) `cluster` label support in the dashboard\n---\nHi @fcanovai, I not get why we need to rely on `cluster` label, sorry.\r\nMulticluster can be easy out of this, main reason from my view here - not get label `cluster` conflicts and solution just move to `cnpg_cluster`.\r\nIf users have multiply clusters and global view - they would name their `cnpg_cluster` in uniq way across different clusters, so they will not mix together - this will be enough.\r\n> Add an info metric with value 1 and informational labels, telling people that they can find the value there, if needed\r\n> I'd go with the first and leave the second approach to the future if it comes up as needed.\r\nI don't think it is good idea, to get `cnpg_cluster` from such metric - it will require group_left queries in both Grafana dashboard and Prom alert, they always so bad in maintaining... Such solution can be accepted if there would be tongs of labels on tons of metrics, but this not the case. Also group_left in badly written queries (which why they hard to maintain) results in many-to-many correlation errors, and problem of this errors that they not always instantly visible - in someone's setup they will appears and in another is not, or they will appears only when pods will be recreated, etc...\n---\n> Hi @fcanovai, I not get why we need to rely on cluster label, sorry.\r\nWe don't. My idea was adding support for k8s clusters inside the dashboard. It would make sense to me to have a variable there, so we can select just the cnpg clusters in a single, user-specified k8s cluster. Right now there is no way to clearly identify the source of the metrics beside namespace and instance, since most of the metrics don't export anything else.\r\n> Multicluster can be easy out of this, main reason from my view here - not get label cluster conflicts and solution just move to cnpg_cluster.\r\n> If users have multiply clusters and global view - they would name their cnpg_cluster in uniq way across different clusters, so they will not mix together - this will be enough.\r\nThis is not 100% transparent. It requires a (minimal, alright) amount of housekeeping user-side. It would be a quick win, but it doesn't generalise the dashboard.\r\n> I don't think it is good idea, to get cnpg_cluster from such metric - it will require group_left queries in both Grafana dashboard and Prom alert, they always so bad in maintaining... Such solution can be accepted if there would be tongs of labels on tons of metrics, but this not the case. Also group_left in badly written queries (which why they hard to maintain) results in many-to-many correlation errors, and problem of this errors that they not always instantly visible - in someone's setup they will appears and in another is not, or they will appears only when pods will be recreated, etc...\r\nI'm not sure I understand your position here. What we currently have doesn't seem different: two metrics are the only ones exporting `cluster`. E.g.: a snippet of the `/metrics` endpoint\r\n```\r\n[...]\r\n# HELP cnpg_collector_sync_replicas Number of requested synchronous replicas (synchronous_standby_names)\r\n# TYPE cnpg_collector_sync_replicas gauge\r\ncnpg_collector_sync_replicas{value=\"expected\"} 0\r\ncnpg_collector_sync_replicas{value=\"max\"} 0\r\ncnpg_collector_sync_replicas{value=\"min\"} 0\r\ncnpg_collector_sync_replicas{value=\"observed\"} 0\r\n# HELP cnpg_collector_up 1 if PostgreSQL is up, 0 otherwise.\r\n# TYPE cnpg_collector_up gauge\r\ncnpg_collector_up{cluster=\"cluster-with-metrics\"} 1\r\n[...]\r\n```\r\nUnless the idea is adding the label to all metrics?\n---\n@fcanovai if you would see distributed topology you will also see the best practices for naming of the clusters :)\nThey must have different naming. This is my view and view that written in docs.\nhttps://cloudnative-pg.io/documentation/current/replica_cluster/#distributed-topology\n---\nSure but what if you have multiple kubernetes environments that are stages like production, staging and test. I wouldn't want to link my clusters across kubernetes environments and it complicates referencing the databases if the object itself has different names in different kubernetes clusters.\n---\n> Sure but what if you have multiple kubernetes environments that are stages like production, staging and test. I wouldn't want to link my clusters across kubernetes environments and it complicates referencing the databases if the object itself has different names in different kubernetes clusters.\nUsually when people have multiple environments for production and staging they have independent monitoring for them as well which is not connected between each other. Different environments should not be connected in any way. This is best practice always.\nWe speak here not about different environments, but about same one with multiple clusters.\nLike thanos for prod and staging and dedicated grafanas. If you still connected them, well cluster will not help you here, you will anyway to manage own dashboard that would have dedicated \"environment filter\". And you will face such issues with any premade dashboard.\nI run for example grafana and open search with dark scheme for prod and light for non prod. Not only urls are different, but color too. Because when you have incident you can have tons of tabs open and by incident look at wrong environment which will create even more issues...\n---\nAgain while I agree that's the opposite of how Prometheus federation works and it's the opposite of how grafana cloud works. There's an expectation that you bring your kubernetes clusters to the same backend and switch clusters using dashboard variables.\nhttps://grafana.com/docs/grafana-cloud/monitor-infrastructure/kubernetes-monitoring/configuration/helm-chart-config/\nCan't confirm for sure but Datadog and new relic also angle towards this model.\nIf I were hosting my own monitoring stack I'd agree with you because the monitoring stack would be isolated internally to the cluster and thus naturally cluster scoped, but are you confident this is how it will work with anyone who's using a SaaS observability solution?\nPersonally I'm just asking in terms of making it a soft request to support anyone using Prometheus  federation where you need a unique metric label per federated instance to define the source cluster of the metric, that's basically all.\nIf that's not a model that's going to be considered that's fine just wanted to bring it up.\n---\nYes, there truly possible chances that monitoring could be done this way. But you can't filter by everything possible option, otherwise you will have force users to set this label even they not needed it. I by myself have environment label even when my monitoring stacks are not connected. But this should be noted in some obvious place (values comments?), with examples hot to put this labels statically on the pod monitor if users not have them by design.\nIf this is what wanted: cluster, environment & tenant is a way to go.\nThere's another options - to add free to adjust filters on dashboard?"
    },
    {
        "title": "Feature Request: provide cluster status metrics for monitoring",
        "id": 1814281447,
        "state": "open",
        "first": "Hi folks,\r\nIt would help to have monitoring for the status of a cluster to be informed asap. If a cluster is\r\nin failover or upgrade status for too long, so it's stuck in the process, then I would like to be able to raise an alarm. It would also help to know if all instances in a cluster are in \"ready\" status.\r\nI tried to find available metrics which might help and thought about a custom query based on pg_* metrics but didn't\r\nfind any working approach. Neither with pod nor with operator metrics.\r\nI hope this is relevant for others too. If I oversaw something then please give advice :)",
        "messages": "Hi folks,\r\nIt would help to have monitoring for the status of a cluster to be informed asap. If a cluster is\r\nin failover or upgrade status for too long, so it's stuck in the process, then I would like to be able to raise an alarm. It would also help to know if all instances in a cluster are in \"ready\" status.\r\nI tried to find available metrics which might help and thought about a custom query based on pg_* metrics but didn't\r\nfind any working approach. Neither with pod nor with operator metrics.\r\nI hope this is relevant for others too. If I oversaw something then please give advice :)Still would like to monitor more about the status of the cluster like described"
    },
    {
        "title": "Operator automatically deletes any manually created podMonitor during cluster creation",
        "id": 1647358608,
        "state": "open",
        "first": "During cluster creation, when `.spec.monitoring.enablePodMonitor: false` (the default value) the operator will delete any manually created podMonitor resource. Only after cluster creation has completed, can I create the podMonitor and the operator will not interfere.\r\nThe logic is at [controllers/cluster_create.go](https://github.com/cloudnative-pg/cloudnative-pg/blob/main/controllers/cluster_create.go#L806-L814):\r\n```\r\ncase !cluster.IsPodMonitorEnabled() && podMonitor != nil:\r\n\t\tcontextLogger.Info(\"Deleting PodMonitor\")\r\n```\r\nWhen deploying clusters, I would like to deploy all resources (secrets, podmonitors, poolers) at the same time and not implement some seperate delay logic for podMonitors to be only applied after the cluster creation has completed.\r\nMaybe there is a way to distinguish between which podMonitor is managed by the operator and which one is managed by the user?\r\nFor example, a label the operator looks for to know this podMonitor is managed by it. Also, the opposite would work, a general label the user can apply to a resource which tells the operator \"hands off\".",
        "messages": "During cluster creation, when `.spec.monitoring.enablePodMonitor: false` (the default value) the operator will delete any manually created podMonitor resource. Only after cluster creation has completed, can I create the podMonitor and the operator will not interfere.\r\nThe logic is at [controllers/cluster_create.go](https://github.com/cloudnative-pg/cloudnative-pg/blob/main/controllers/cluster_create.go#L806-L814):\r\n```\r\ncase !cluster.IsPodMonitorEnabled() && podMonitor != nil:\r\n\t\tcontextLogger.Info(\"Deleting PodMonitor\")\r\n```\r\nWhen deploying clusters, I would like to deploy all resources (secrets, podmonitors, poolers) at the same time and not implement some seperate delay logic for podMonitors to be only applied after the cluster creation has completed.\r\nMaybe there is a way to distinguish between which podMonitor is managed by the operator and which one is managed by the user?\r\nFor example, a label the operator looks for to know this podMonitor is managed by it. Also, the opposite would work, a general label the user can apply to a resource which tells the operator \"hands off\".Just to be clear, using a name for the podmonitor different from the default one is not an option? Or isn't that working?\n---\n> Just to be clear, using a name for the podmonitor different from the default one is not an option? Or isn't that working?\r\nIndeed, changing the podMonitor name is a viable workaround for this issue. I missed the fact the operator only looks for a podMonitor with the same name as the cluster. Even though I would prefer to name the resources of a single cluster the same way, it is not a hard blocker for me and I can use the workaround.\r\nThanks @phisco !\n---\n@phisco this behavior is still present and is not very transparent to the user, it is unexpected. I use gitops to manage my cluster and I only recently noticed that the podmonitor was constantly being reconciled. I don't think it's correct that cnpg tries to manage podmonitors (or any other resource for that matter) that it does not own.\n---\nTotally agree @rouke-broersma, reopening this. I would argue it's not the operator's job to manage these resources altogether actually, I'll propose deprecating this in the future, too much effort for something users can define on their own.\n---\nI propose to mark this as deprecated and start to disable this piece of code in the next few versions unless is explicitly marked as enable or disabled, what do you think @phisco @leonardoce  ?\n---\nTotally agree, but let's open a dedicated issue for it, @sxd, I can do that by EOD.\n---\nI don't necessarily agree that it's not the operator's responsibility to manage these resources, because the operator is best suited to have the knowledge of how to interact with the resource. If you look at the different operator maturity levels it's expected that an operator manages nearly all operational tasks for the highest maturity level. \r\nBut I understand that a tradeoff must be made between features and their cost. Managing this resource ourselves isn't that big of a deal so deprecating and removing is fine with me. \r\nAnd Prometheus is also not the only monitoring tool available for Kubernetes so deciding to support one over one of the others is also a choice this project may not want to make.\n---\nThe operator deletes my custom pooler's podmonitors, regardless the name.\nI have the following parameter in my custom CR chart.\n```yaml\n  monitoring:\n    enablePodMonitor: false\n```\nAnd I have the custom podmonitor templates for db and poolers pods.\nTemplate for db podmonitor, where `instanceName` is a cluster name and the podmonitor name will be \"users\", for exampe (I guess it not duplicates the operator's default one)\n```yaml\n---\napiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: {{ $instanceName }}\n  labels:\n    {{- include \"core.labels\" $ | nindent 4 }}\n    {{- toYaml $.Values.exporter.labels | nindent 4 }}\n    name: {{ $instanceName }}\nspec:\n*****\n```\nTemplate for pooler podmonitor, where `pooler.suffix` is \"pgpool\" `.` is \"rw\" or \"ro\" and the podmonitor name will be \"users-rw-pgpool\"\n```yaml\n{{- $poolerName := printf \"%s-%s-%s\" $instanceName . $.Values.pooler.suffix }}\n---\napiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  labels:\n    cnpg.io/poolerName: {{ $poolerName }}\n    {{- toYaml $.Values.exporter.labels | nindent 4 }}\n  name: {{ $poolerName }}\n****\n```\nHelm creates both podmonitors for db pods and pooler pods, during the installation.\nBut operator deletes only the pooler's podmonitors after some time and does not touch the db podmonitors.\nThe other solution is implement the custom labels for podmonitors, in this case no need to redefine the whole podmonitor via chart's template."
    },
    {
        "title": "out of disk space - refusing to create the primary instance while the latest generated serial is not zero ",
        "id": 1639219587,
        "state": "open",
        "first": "Kubernetes 1.25 5 \r\nCNPG 1.19\r\nReproduce: create pgsql instans, run out of disk space = game over.\r\nAfter data disk runs full the CNPG operator refuses to create a pod again leaving the installation defunct. Resizing a persistent volume cannot take place until the pod is provisioned again.\r\n{\"level\":\"info\",\"ts\":\"2023-03-24T11:19:44Z\",\"msg\":\"No primary instance found for this cluster\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgsql\",\"namespace\":\"lala-staging\"},\"namespace\":\"lala-staging\",\"name\":\"pgsql\",\"reconcileID\":\"c67ecf51-3b9a-4b62-bf5a-c128f5d305d5\",\"uuid\":\"c70e5a04-ca35-11ed-aad4-46acb41cf6fd\"}\r\nAny idea how to fix?",
        "messages": "Kubernetes 1.25 5 \r\nCNPG 1.19\r\nReproduce: create pgsql instans, run out of disk space = game over.\r\nAfter data disk runs full the CNPG operator refuses to create a pod again leaving the installation defunct. Resizing a persistent volume cannot take place until the pod is provisioned again.\r\n{\"level\":\"info\",\"ts\":\"2023-03-24T11:19:44Z\",\"msg\":\"No primary instance found for this cluster\",\"controller\":\"cluster\",\"controllerGroup\":\"postgresql.cnpg.io\",\"controllerKind\":\"Cluster\",\"Cluster\":{\"name\":\"pgsql\",\"namespace\":\"lala-staging\"},\"namespace\":\"lala-staging\",\"name\":\"pgsql\",\"reconcileID\":\"c67ecf51-3b9a-4b62-bf5a-c128f5d305d5\",\"uuid\":\"c70e5a04-ca35-11ed-aad4-46acb41cf6fd\"}\r\nAny idea how to fix?@Tipsmark you need to delete the cluster and create it again, this may be a valid issue we will look into this one\n---\n@Tipsmark sorry didn't mean to close the issue, please let us know if you still having this issue\n---\nstill an issue - or rather, we had to delete and re-deploy and restore data... but certainly still a problem that a simple but rare case of disk full caused us an unrecoverable instance - happy to hear how others fixed this issue if anyone did.\n---\nHi @Tipsmark \r\nChecking on this, can you add a way to reproduce this issue? also, did you tried to resize the disk before?\r\nRegards!\n---\nI \"reproduced\" the same issue today with cloudnative-pg:1.20.1\n---\n@jouve  Did you manage to recover w/o restore from backup?\n---\nnope, I did not try much and used your methodology (delete/create/restore)\n---\nAnd it just happened again - few hundred GB came in, disk full - cluster DEAD... it's becoming a real showstopper that one cannot recover from such a simple problem.\n---\nSplitting WAL volume to a separate disk didn't help - again database DEAD, gone - ciao... and since backup ALWAYS enables WAL archiving it's rather useless - in our case a simple restore of 500GB db takes 9-12 hours... vs. 15 min. using Kasten, so the network is not the problem... but alas a snapshot based backup doesn't always work when restored... so essentially CNPG is going out the window and being replaced by another solution. It's too fragile and has too few and weak backup options.\n---\nHi Tipsmark,\r\nI am slowly but steadily introducing CNPG in multiple projects. And your case is interesting for me, since you've got way bigger DBs than me. But this is likely to change in the near future.\r\nI understand you are frustrated - and frankly I can relate to that. If I had to restore a DB in production environment, 9-12 hours for a restore would bring me into quite some trouble.\r\nI would rather avoid making the same experience ... and I think this is a valid issue which the operator should be able to handle, at least without having to delete a cluster!\r\nSo my question is, would it help to use an autoscaler in your situation?\r\nI found this: [[Kubernetes-Volume-Autoscaler](https://github.com/DevOps-Nirvana/Kubernetes-Volume-Autoscaler)]\r\nFrom my understanding, you cannot scale down your PVCs afterwards without maintenance - but a sudden spike in disk usage could be cushioned.\r\nAnd from what I tested, during restore (all?) WALs are beeing processed. There is a lot of random IO involved which is time consuming. Restoring a PVC from a snapshot will always be faster.\r\nThere is volume snapshot available since cnpg 1.20.1 https://github.com/cloudnative-pg/cloudnative-pg/releases/tag/v1.20.1 , but I didn't look into that yet...\n---\n@quercus-carsten Not a bad idea, but it will likely not work when using VMWare CSI since we snapshot the FCD's and expansion of a VMDK is not possible when it has a snapshot.  We don't see much IOPS, actually very little, we have 100k+++ available, we see maybe 3k-5k... Even if it was twice or triple the speed it'd still be too slow. We need a 10x improvement to consider it ready for production. Not sure why it can't utilize full bandwidth/cpu/disk... it's just slow with no obvious bottleneck... now we're on 14th hour, restore still running... ingesting wal's at 20-30 Mbit/sec, IOPS 2k, CPU 0.4 of a core, memory a few GB... underlying host near idle, storage near idle - just agonizingly useless. \r\nNot exactly apples to apples, but when we run nightly restores of MSSQL it takes 15 minutes flat for 600+GB... that's the speed we need for Postgresql. Exactly same underlying infrastructure.\n---\n@quercus-carsten so probably last comment there - the restore finally completed, a bit over 60 HOURS! the default config of barman is, at best... useless. I have no intention of trying to tweak it further, instead we've setup workflow to grab the db using pg_dump and the entire dump + upload to S3 takes 46 minutes... a little better than barman's 10 hours and 58 minutes to backup the same database (base backup) - I presume a restore using pg_dump is also going to orders of magnitude faster than restoring the base backup from barman. Whatever the problem is, it needs some serious work before barman can be relied on in a real-world scenario.\n---\nHi @Tipsmark , thanks for sharing. I'm planning to do heavier tests on backup and restore now, too. Some months ago I did prepare workflows and documentation in preparation for the migration of the first projects. But I didn't get to test with databases of dozens of GBs in size - let alone hundreds...\r\nI have zero experience with MSSQL. But roughly 600GB of data in approx 15 min - I mean this can't involve applying WALs or \"redo logs\" or whatever this may be called in this case, or can it? I mean does this also include PITR? I can only image that MS does this snapshot based without any catching up to latest timeline.\r\nAnyway, I find this to be extremely fast  - and from the metrics you gave and the underlying infrastructure I guess one has to look closer on optimizing barman cloud restore.\r\nI first stumbled upon bandwidth limitation settings regarding barman - but this does not apply to barman-cloud-backup and barman-cloud-restore / barman-wal-restore. The only setting I found which I will test is \"--parallel\". Defaults to 2 if I'm not mistaken.\r\nAlso interesting to hear, that pg_dump works this fast... But PITR is essential for me, so I really am dependent on it.\r\nAs soon as I did my own tests, I will share the results here.\n---\n> still an issue - or rather, we had to delete and re-deploy and restore data... but certainly still a problem that a simple but rare case of disk full caused us an unrecoverable instance - happy to hear how others fixed this issue if anyone did.\r\nCan you please help us understand why you ran out of disk? Was it because of a spike in the WAL production, WAL archiving to be lagging, or increase in the database size?\n---\nIt's still happening in v1.22.1\r\nI used pgbench to add dummy data beyond the capacity of underlying storage. \r\nIt made the whole setup and wal archives useless. Couldn't recover into new instance either.\n---\nI find this particular issue somewhat odd. It is not standard for Postgres to become irrecoverable when disk space is exceeded. At worst, in-flight transactions cannot be written and the database halts. Once disk storage is increased, it should be able to resume following a short recovery phase. is there something the operator itself is doing to prevent this from occurring?\n---\nThis sounds like a deficiency in the CSI you are using. I hit this problem this week, adjusted the volume size, and my CSI driver (democratic-csi) did the resize of the volume and the partition on the volume, and I was back up and running.\r\nIf your CSI driver doesn't resize the partition (hence the partition needs to be online part) then:\r\n- Apply the manual intevention step from https://cloudnative-pg.io/documentation/current/failure_modes/#failure-modes_1 to stop the self healing loop.\r\n- Delete any db instance pods that are deployed\r\n- Craft a Pod deployment that mounts the volumes of the DBs using a container image that contains resize2fs as a cluster admin so you have the appropriate rights, the container will need to run as root.\r\n- Use resizesf2 to scale the partitions to the newly adjusted volume size\r\n- Delete your crafted pod to release the volumes\r\n- Remove the manual intervention property to allow the self-healing loop to resume\r\n- CNPG will take back over and start the db pods, and you'll be able to recover.\n---\nSo I just hit this again today and this time it's just bonkers...\r\n* WAL file backups and archives are working and all on S3, however db-1 thought it was failing to upload the history file for some reason (it wasn't - those were there too) and as a result it was not deleting WALs.\r\n* Of course, this lead to a full disk\r\n* For some reason, even though all the WALs were on S3, db-2 was not applying them, and now it's primary and broken.\r\nSo from this state - I am trying to recover. I deleted the already backed up WALs from db-1's PVC, and I am struggling to get CNPG to force it back to primary, because it keeps trying to copy the garbage data from db-2 which I killed with fire since it's state was inconsistent and it should never have been promoted in the first place... \r\nAnd I'm going to have to likely wipe and restore from backup to get out of this..  meanwhile - db-1's database is perfectly fine and if CNPG would just start it - I would be fine.\n---\nI also have this issue. 100MB of actual data, 10GB of disk space, and CNPG randomly stops deleting WALs even though they appear to be in S3, and completely takes down a service. Insane. This makes CNPG completely unsuitable for any production usage, as the only \"solution\" is to not have backups enabled.\n---\nNot stale, not solved."
    },
    {
        "title": "Add temporary volume for import function to store pg_dump",
        "id": 1638822386,
        "state": "open",
        "first": "Hi,\r\nI was recently testing import function to prepare some upgrade procedures for our databases, and I have one thought. Currently we need to provide additional space in data volume to temporarily store pg_dump. The problem is, not every storage type accepts storage size decrease (also I'm not sure CNPG supports storage downsizing at all ?). So after the operation, we're left with possibly considerable amout of unsused and somehow unreclaimable storage. It would be good to have additional temporary storage volume for such purposes. It could be defined for instance in the import section and mounted only for the time of the import operation is in progress.\r\nProposed spec:\r\nimport:\r\n  storage:\r\n    storageClass: XXXXXX\r\n    size: XXGi",
        "messages": "Hi,\r\nI was recently testing import function to prepare some upgrade procedures for our databases, and I have one thought. Currently we need to provide additional space in data volume to temporarily store pg_dump. The problem is, not every storage type accepts storage size decrease (also I'm not sure CNPG supports storage downsizing at all ?). So after the operation, we're left with possibly considerable amout of unsused and somehow unreclaimable storage. It would be good to have additional temporary storage volume for such purposes. It could be defined for instance in the import section and mounted only for the time of the import operation is in progress.\r\nProposed spec:\r\nimport:\r\n  storage:\r\n    storageClass: XXXXXX\r\n    size: XXGiDuplicate of:\n---\nDuplicate of: #1669\n---\nNo this is not a duplicate ! Mentioned one was for general purpose and has nothing to do with import function.\r\nThis one is only for the import function purposes.\n---\nAs @sxd suggested in #1669, the best course of action for such scenarios is to launch another pod with your storage requirements and use that to perform your import. If you need, you can even launch said pod on the same node as the primary database instance (to avoid additional network latency), although it's not 100% guaranteed the primary won't change.\r\nMy preferred choice for an image is: `alpine:3.17` with: `apk add postgresql15-client`.\n---\n@itay-grudev I feal You do not understand the case. We are talking about CPNG function that enables doing automatic import od data from another database. This function is available from the code/cluster manifest level. During its action it requires enough data storage to fit both imported data and temporary pg_dump that is being transfered from the source DB. So after the operation compeltes, we have cluster that has much more data storage that is actually required. This storage is unreclaimable. So much wiser it would be to have a temp storage volume just to store pg_dump there only for that operation to complete. Actually Gabrielle agreed to such an approach on slack channel , so I've made this issue to get this option developed.\n---\nGot it. You're correct.\n---\nI'm unsure if my use case is relevant to this issue, let's find out!\r\nI want to leverage Secret Store CSI / Vault CSI Provider to create the secrets related to the DB. \r\ne.g.  [Using Secret Provider Classes](https://developer.hashicorp.com/vault/docs/platform/k8s/csi#using-secret-provider-classes)\r\nThe ability to mount additional volumes would enable to dynamically create secrets base on the pod creation.\r\nLet me know if I need to open another issue.\r\nThanks!\n---\n> I'm unsure if my use case is relevant to this issue, let's find out!\r\n> \r\n> I want to leverage Secret Store CSI / Vault CSI Provider to create the secrets related to the DB.\r\n> \r\n> e.g. [Using Secret Provider Classes](https://developer.hashicorp.com/vault/docs/platform/k8s/csi#using-secret-provider-classes)\r\n> \r\n> The ability to mount additional volumes would enable to dynamically create secrets base on the pod creation.\r\n> \r\n> Let me know if I need to open another issue.\r\n> \r\n> Thanks!\r\nThat's a totally different case. This one is only for the import functionality.\n---\nnot stale"
    },
    {
        "title": "Dependency Dashboard",
        "id": 1585798131,
        "state": "open",
        "first": "This issue lists Renovate updates and detected dependencies. Read the [Dependency Dashboard](https://docs.renovatebot.com/key-concepts/dashboard/) docs to learn more.\n## Rate-Limited\nThese updates are currently rate-limited. Click on a checkbox below to force their creation now.\n - [ ] <!-- unlimit-branch=renovate/main-backup-test-tools -->chore(deps): update dependency vmware-tanzu/velero to v1.15.2 (main)\n - [ ] <!-- unlimit-branch=renovate/release-1.22-github.com-cloudnative-pg-machinery-digest -->fix(deps): update github.com/cloudnative-pg/machinery digest to d15e1d1 (release-1.22)\n - [ ] <!-- unlimit-branch=renovate/release-1.22-backup-test-tools -->chore(deps): update dependency vmware-tanzu/velero to v1.15.2 (release-1.22)\n - [ ] <!-- unlimit-branch=renovate/release-1.22-sigs.k8s.io-controller-tools-0.x -->chore(deps): update module sigs.k8s.io/controller-tools to v0.17.2 (release-1.22)\n - [ ] <!-- unlimit-branch=renovate/release-1.24-github.com-cloudnative-pg-barman-cloud-digest -->fix(deps): update github.com/cloudnative-pg/barman-cloud digest to 10ef19b (release-1.24)\n - [ ] <!-- unlimit-branch=renovate/release-1.24-backup-test-tools -->chore(deps): update dependency vmware-tanzu/velero to v1.15.2 (release-1.24)\n - [ ] <!-- unlimit-branch=renovate/release-1.24-sigs.k8s.io-controller-tools-0.x -->chore(deps): update module sigs.k8s.io/controller-tools to v0.17.2 (release-1.24)\n - [ ] <!-- unlimit-branch=renovate/release-1.25-github.com-cloudnative-pg-barman-cloud-digest -->fix(deps): update github.com/cloudnative-pg/barman-cloud digest to 10ef19b (release-1.25)\n - [ ] <!-- unlimit-branch=renovate/release-1.25-github.com-cloudnative-pg-machinery-digest -->fix(deps): update github.com/cloudnative-pg/machinery digest to d15e1d1 (release-1.25)\n - [ ] <!-- unlimit-branch=renovate/release-1.25-backup-test-tools -->chore(deps): update dependency vmware-tanzu/velero to v1.15.2 (release-1.25)\n - [ ] <!-- unlimit-branch=renovate/release-1.25-sigs.k8s.io-controller-tools-0.x -->chore(deps): update module sigs.k8s.io/controller-tools to v0.17.2 (release-1.25)\n - [ ] <!-- create-all-rate-limited-prs -->\ud83d\udd10 **Create all rate-limited PRs at once** \ud83d\udd10\n## Open\nThese updates have all been created already. Click a checkbox below to force a retry/rebase of any.\n - [ ] <!-- rebase-branch=renovate/main-github.com-cloudnative-pg-barman-cloud-digest -->[fix(deps): update github.com/cloudnative-pg/barman-cloud digest to 10ef19b (main)](../pull/6798)\n - [ ] <!-- rebase-branch=renovate/main-github.com-cloudnative-pg-machinery-digest -->[fix(deps): update github.com/cloudnative-pg/machinery digest to d15e1d1 (main)](../pull/6795)\n - [ ] <!-- rebase-branch=renovate/main-sigs.k8s.io-controller-tools-0.x -->[chore(deps): update module sigs.k8s.io/controller-tools to v0.17.2 (main)](../pull/6591)\n - [ ] <!-- rebase-branch=renovate/release-1.22-github.com-cloudnative-pg-barman-cloud-digest -->[fix(deps): update github.com/cloudnative-pg/barman-cloud digest to 10ef19b (release-1.22)](../pull/6799)\n - [ ] <!-- rebase-branch=renovate/release-1.24-github.com-cloudnative-pg-machinery-digest -->[fix(deps): update github.com/cloudnative-pg/machinery digest to d15e1d1 (release-1.24)](../pull/6803)\n - [ ] <!-- rebase-all-open-prs -->**Click on this checkbox to rebase all open PRs at once**\n## Ignored or Blocked\nThese are blocked by an existing closed PR and will not be recreated unless you click a checkbox below.\n - [ ] <!-- recreate-branch=renovate/main-github.com-cloudnative-pg-cnpg-i-digest -->[fix(deps): update github.com/cloudnative-pg/cnpg-i digest to cd31008 (main)](../pull/6791)\n - [ ] <!-- recreate-branch=renovate/release-1.24-github.com-cloudnative-pg-cnpg-i-digest -->[fix(deps): update github.com/cloudnative-pg/cnpg-i digest to cd31008 (release-1.24)](../pull/6792)\n - [ ] <!-- recreate-branch=renovate/release-1.25-github.com-cloudnative-pg-cnpg-i-digest -->[fix(deps): update github.com/cloudnative-pg/cnpg-i digest to cd31008 (release-1.25)](../pull/6793)\n## Detected dependencies\n> [!NOTE]\n> Detected dependencies section has been truncated\n<details><summary>Branch main</summary>\n<blockquote>\n<details><summary>devcontainer</summary>\n<blockquote>\n<details><summary>.devcontainer/devcontainer.json</summary>\n - `mcr.microsoft.com/devcontainers/go 1-bookworm`\n - `ghcr.io/devcontainers/features/docker-in-docker 2`\n - `ghcr.io/rio/features/k3d 1`\n - `ghcr.io/mpriscella/features/kind 1`\n - `ghcr.io/rjfmachado/devcontainer-features/cloud-native 1`\n - `ghcr.io/guiyomh/features/golangci-lint 0`\n - `ghcr.io/devcontainers-contrib/features/kubectx-kubens 1`\n - `ghcr.io/dhoeric/features/stern 1`\n</details>\n</blockquote>\n</details>\n<details><summary>dockerfile</summary>\n<blockquote>\n<details><summary>Dockerfile</summary>\n</details>\n<details><summary>Dockerfile-ubi8</summary>\n</details>\n<details><summary>Dockerfile-ubi9</summary>\n</details>\n</blockquote>\n</details>\n<details><summary>github-actions</summary>\n<blockquote>\n<details><summary>.github/workflows/backport.yml</summary>\n - `actions-ecosystem/action-add-labels v1`\n - `peter-evans/create-or-update-comment v4`\n - `actions-ecosystem/action-remove-labels v1`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `dacbd/create-issue-action v2`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/chatops.yml</summary>\n - `actions-cool/check-user-permission v2`\n - `actions-ecosystem/action-add-labels v1.1.3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/close-inactive-issues.yml</summary>\n - `actions/stale v9`\n</details>\n<details><summary>.github/workflows/codeql-analysis.yml</summary>\n - `fkirc/skip-duplicate-actions v5.3.1`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `github/codeql-action v3`\n - `github/codeql-action v3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `benc-uk/workflow-dispatch v1`\n - `xt0rted/slash-command-action v2`\n - `xt0rted/pull-request-comment-branch v3`\n - `peter-evans/create-or-update-comment v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `goreleaser/goreleaser-action v6`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/upload-artifact v4`\n - `goreleaser/goreleaser-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `ad-m/github-push-action v0.8.0`\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `azure/login v2.2.0`\n - `nick-fields/retry v3`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `docker/login-action v3`\n - `azure/login v2.2.0`\n - `azure/setup-kubectl v4`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `azure/login v2.2.0`\n - `nick-fields/retry v3`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `aws-actions/configure-aws-credentials v4`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `google-github-actions/auth v2`\n - `google-github-actions/setup-gcloud v2`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `redhat-actions/openshift-tools-installer v1`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/download-artifact v4`\n - `cloudnative-pg/ciclops v1.3.1`\n - `actions/upload-artifact v4`\n - `rtCamp/action-slack-notify v2`\n - `actions-ecosystem/action-add-labels v1.1.3`\n - `actions-ecosystem/action-remove-labels v1.3.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `benc-uk/workflow-dispatch v1`\n - `fkirc/skip-duplicate-actions v5.3.1`\n - `actions/checkout v4`\n - `dorny/paths-filter v3.0.2`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `golangci/golangci-lint-action v6`\n - `actions/checkout v4`\n - `golang/govulncheck-action v1`\n - `actions/checkout v4`\n - `ludeeus/action-shellcheck 2.0.0`\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `goreleaser/goreleaser-action v6`\n - `rtCamp/action-slack-notify v2`\n - `goreleaser/goreleaser-action v6`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `github/codeql-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `docker/setup-qemu-action v3`\n - `actions/setup-go v5`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `helm/kind-action v1.12.0`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `redhat-actions/podman-login v1`\n - `actions/download-artifact v4`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/k8s-versions-check.yml</summary>\n - `actions/checkout v4`\n - `azure/login v2.2.0`\n - `google-github-actions/auth v2`\n - `google-github-actions/setup-gcloud v2`\n - `frenck/action-setup-yq v1`\n - `peter-evans/create-pull-request v7`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/latest-postgres-version-check.yml</summary>\n - `actions/checkout v4`\n - `actions/setup-python v5`\n - `peter-evans/create-pull-request v7`\n - `peter-evans/create-pull-request v7`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/pr_verify_linked_issue.yml</summary>\n - `hattan/verify-linked-issue-action v1.1.5`\n</details>\n<details><summary>.github/workflows/registry-clean.yml</summary>\n - `snok/container-retention-policy v3.0.0`\n - `snok/container-retention-policy v3.0.0`\n</details>\n<details><summary>.github/workflows/release-pr.yml</summary>\n - `actions/checkout v4`\n - `repo-sync/pull-request v2.12`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/release-publish.yml</summary>\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `softprops/action-gh-release v2`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `crazy-max/ghaction-import-gpg v6`\n - `goreleaser/goreleaser-action v6`\n - `rajatjindal/krew-release-bot v0.0.47`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `docker/setup-qemu-action v3`\n - `actions/setup-go v5`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `peter-evans/create-pull-request v7`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `ad-m/github-push-action v0.8.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/release-tag.yml</summary>\n - `actions/checkout v4`\n - `christophebedard/tag-version-commit v1.7.0`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/require-labels.yml</summary>\n - `agilepathway/pull-request-label-checker v1.6.61`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/snyk.yml</summary>\n - `actions/checkout v4`\n - `snyk/actions 0.4.0`\n - `github/codeql-action v3`\n - `snyk/actions 0.4.0`\n - `github/codeql-action v3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/spellcheck.yml</summary>\n - `actions/checkout v4`\n - `get-woke/woke-action v0`\n - `actions/checkout v4`\n - `rojopolis/spellcheck-github-actions 0.46.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/sync-api.yml</summary>\n - `peter-evans/repository-dispatch v3`\n</details>\n</blockquote>\n</details>\n<details><summary>gomod</summary>\n<blockquote>\n<details><summary>go.mod</summary>\n - `go 1.23.5`\n - `github.com/DATA-DOG/go-sqlmock v1.5.2`\n - `github.com/Masterminds/semver/v3 v3.3.1`\n - `github.com/avast/retry-go/v4 v4.6.0`\n - `github.com/blang/semver v3.5.1+incompatible`\n - `github.com/cheynewallace/tabby v1.1.1`\n - `github.com/cloudnative-pg/barman-cloud v0.0.0-20250104195650-c1472628b450@c1472628b450`\n - `github.com/cloudnative-pg/cnpg-i v0.0.0-20241224161104-7e2cfa59debc@7e2cfa59debc`\n - `github.com/cloudnative-pg/machinery v0.0.0-20250102082645-95c37fe624d0@95c37fe624d0`\n - `github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc@d8f796af33cc`\n - `github.com/evanphx/json-patch/v5 v5.9.11`\n - `github.com/go-logr/logr v1.4.2`\n - `github.com/google/shlex v0.0.0-20191202100458-e7afc7fbc510@e7afc7fbc510`\n - `github.com/grpc-ecosystem/go-grpc-middleware/v2 v2.2.0`\n - `github.com/jackc/pgx/v5 v5.7.2`\n - `github.com/jackc/puddle/v2 v2.2.2`\n - `github.com/kballard/go-shellquote v0.0.0-20180428030007-95032a82bc51@95032a82bc51`\n - `github.com/kubernetes-csi/external-snapshotter/client/v8 v8.2.0`\n - `github.com/lib/pq v1.10.9`\n - `github.com/logrusorgru/aurora/v4 v4.0.0`\n - `github.com/mitchellh/go-ps v1.0.0`\n - `github.com/onsi/ginkgo/v2 v2.22.2`\n - `github.com/onsi/gomega v1.36.2`\n - `github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring v0.80.0`\n - `github.com/prometheus/client_golang v1.20.5`\n - `github.com/robfig/cron v1.2.0`\n - `github.com/sethvargo/go-password v0.3.1`\n - `github.com/spf13/cobra v1.8.1`\n - `github.com/stern/stern v1.32.0`\n - `github.com/thoas/go-funk v0.9.3`\n - `go.uber.org/atomic v1.11.0`\n - `go.uber.org/multierr v1.11.0`\n - `go.uber.org/zap v1.27.0`\n - `golang.org/x/term v0.29.0`\n - `google.golang.org/grpc v1.70.0`\n - `gopkg.in/yaml.v3 v3.0.1`\n - `k8s.io/api v0.32.1`\n - `k8s.io/apiextensions-apiserver v0.32.1`\n - `k8s.io/apimachinery v0.32.1`\n - `k8s.io/cli-runtime v0.32.1`\n - `k8s.io/client-go v0.32.1`\n - `k8s.io/utils v0.0.0-20241210054802-24370beab758@24370beab758`\n - `sigs.k8s.io/controller-runtime v0.20.1`\n - `sigs.k8s.io/yaml v1.4.0`\n</details>\n</blockquote>\n</details>\n<details><summary>regex</summary>\n<blockquote>\n<details><summary>Makefile</summary>\n - `sigs.k8s.io/kustomize/kustomize/v5 v5.6.0`\n</details>\n<details><summary>Makefile</summary>\n - `sigs.k8s.io/controller-tools v0.16.5`\n</details>\n<details><summary>Makefile</summary>\n - `github.com/goreleaser/goreleaser v2.6.1`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `kubernetes-csi/external-snapshotter v8.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-snapshotter v8.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-provisioner v5.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-resizer v1.13.1`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-attacher v4.8.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/csi-driver-host-path v1.15.0`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `rook/rook v1.16.3`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `kubernetes-sigs/kind v0.26.0`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `kubernetes-sigs/kind v0.26.0`\n</details>\n<details><summary>hack/e2e/run-e2e-kind.sh</summary>\n - `kindest/node v1.32.1`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kindest/node v1.32.1`\n</details>\n<details><summary>hack/e2e/run-e2e-k3d.sh</summary>\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n</details>\n<details><summary>Makefile</summary>\n - `jonasbn/github-action-spellcheck 0.46.0`\n</details>\n<details><summary>Makefile</summary>\n - `getwoke/woke 0.19.0`\n</details>\n<details><summary>Makefile</summary>\n - `operator-framework/operator-sdk v1.39.1`\n</details>\n<details><summary>Makefile</summary>\n - `operator-framework/operator-registry v1.50.0`\n</details>\n<details><summary>Makefile</summary>\n - `redhat-openshift-ecosystem/openshift-preflight 1.11.1`\n</details>\n<details><summary>config/olm-scorecard/patches/basic.config.yaml</summary>\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n</details>\n<details><summary>config/olm-scorecard/patches/olm.config.yaml</summary>\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n</details>\n<details><summary>pkg/specs/pgbouncer/deployments.go</summary>\n - `ghcr.io/cloudnative-pg/pgbouncer 1.24.0`\n</details>\n<details><summary>.github/workflows/backport.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/codeql-analysis.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/release-publish.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `golangci/golangci-lint 1.63.4`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `vmware-tanzu/velero 1.15.1`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `vmware-tanzu/velero-plugin-for-aws 1.11.1`\n</details>\n</blockquote>\n</details>\n</blockquote>\n</details>\n<details><summary>Branch release-1.22</summary>\n<blockquote>\n<details><summary>devcontainer</summary>\n<blockquote>\n<details><summary>.devcontainer/devcontainer.json</summary>\n - `mcr.microsoft.com/devcontainers/go 1-bookworm`\n - `ghcr.io/devcontainers/features/docker-in-docker 2`\n - `ghcr.io/rio/features/k3d 1`\n - `ghcr.io/mpriscella/features/kind 1`\n - `ghcr.io/rjfmachado/devcontainer-features/cloud-native 1`\n - `ghcr.io/guiyomh/features/golangci-lint 0`\n - `ghcr.io/devcontainers-contrib/features/kubectx-kubens 1`\n - `ghcr.io/dhoeric/features/stern 1`\n</details>\n</blockquote>\n</details>\n<details><summary>dockerfile</summary>\n<blockquote>\n<details><summary>Dockerfile</summary>\n</details>\n<details><summary>Dockerfile-ubi8</summary>\n</details>\n<details><summary>Dockerfile-ubi9</summary>\n</details>\n</blockquote>\n</details>\n<details><summary>github-actions</summary>\n<blockquote>\n<details><summary>.github/workflows/backport.yml</summary>\n - `actions-ecosystem/action-add-labels v1`\n - `peter-evans/create-or-update-comment v4`\n - `actions-ecosystem/action-remove-labels v1`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `dacbd/create-issue-action v2`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/chatops.yml</summary>\n - `actions-cool/check-user-permission v2`\n - `actions-ecosystem/action-add-labels v1.1.3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/codeql-analysis.yml</summary>\n - `fkirc/skip-duplicate-actions v5.3.1`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `github/codeql-action v3`\n - `github/codeql-action v3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `benc-uk/workflow-dispatch v1`\n - `xt0rted/slash-command-action v2`\n - `xt0rted/pull-request-comment-branch v3`\n - `peter-evans/create-or-update-comment v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `goreleaser/goreleaser-action v6`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/upload-artifact v4`\n - `goreleaser/goreleaser-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `ad-m/github-push-action v0.8.0`\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `azure/login v2.2.0`\n - `nick-fields/retry v3`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `docker/login-action v3`\n - `azure/login v2.2.0`\n - `azure/setup-kubectl v4`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `azure/login v2.2.0`\n - `nick-fields/retry v3`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `aws-actions/configure-aws-credentials v4`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `google-github-actions/auth v2`\n - `google-github-actions/setup-gcloud v2`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `redhat-actions/openshift-tools-installer v1`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/download-artifact v4`\n - `cloudnative-pg/ciclops v1.3.1`\n - `actions/upload-artifact v4`\n - `rtCamp/action-slack-notify v2`\n - `actions-ecosystem/action-add-labels v1.1.3`\n - `actions-ecosystem/action-remove-labels v1.3.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `benc-uk/workflow-dispatch v1`\n - `fkirc/skip-duplicate-actions v5.3.1`\n - `actions/checkout v4`\n - `dorny/paths-filter v3.0.2`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `golangci/golangci-lint-action v6`\n - `actions/checkout v4`\n - `golang/govulncheck-action v1`\n - `actions/checkout v4`\n - `ludeeus/action-shellcheck 2.0.0`\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `goreleaser/goreleaser-action v6`\n - `rtCamp/action-slack-notify v2`\n - `goreleaser/goreleaser-action v6`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `github/codeql-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `docker/setup-qemu-action v3`\n - `actions/setup-go v5`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `helm/kind-action v1.12.0`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `redhat-actions/podman-login v1`\n - `actions/download-artifact v4`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/k8s-versions-check.yml</summary>\n - `actions/checkout v4`\n - `azure/login v2.2.0`\n - `google-github-actions/auth v2`\n - `google-github-actions/setup-gcloud v2`\n - `frenck/action-setup-yq v1`\n - `peter-evans/create-pull-request v7`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/latest-postgres-version-check.yml</summary>\n - `actions/checkout v4`\n - `actions/setup-python v5`\n - `peter-evans/create-pull-request v7`\n - `peter-evans/create-pull-request v7`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/pr_verify_linked_issue.yml</summary>\n - `hattan/verify-linked-issue-action v1.1.5`\n</details>\n<details><summary>.github/workflows/registry-clean.yml</summary>\n - `snok/container-retention-policy v3.0.0`\n - `snok/container-retention-policy v3.0.0`\n</details>\n<details><summary>.github/workflows/release-pr.yml</summary>\n - `actions/checkout v4`\n - `repo-sync/pull-request v2.12`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/release-publish.yml</summary>\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `softprops/action-gh-release v2`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `crazy-max/ghaction-import-gpg v6`\n - `goreleaser/goreleaser-action v6`\n - `rajatjindal/krew-release-bot v0.0.47`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `docker/setup-qemu-action v3`\n - `actions/setup-go v5`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `peter-evans/create-pull-request v7`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `ad-m/github-push-action v0.8.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/release-tag.yml</summary>\n - `actions/checkout v4`\n - `christophebedard/tag-version-commit v1.7.0`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/require-labels.yml</summary>\n - `agilepathway/pull-request-label-checker v1.6.61`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/snyk.yml</summary>\n - `actions/checkout v4`\n - `snyk/actions 0.4.0`\n - `github/codeql-action v3`\n - `snyk/actions 0.4.0`\n - `github/codeql-action v3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/spellcheck.yml</summary>\n - `actions/checkout v4`\n - `get-woke/woke-action v0`\n - `actions/checkout v4`\n - `rojopolis/spellcheck-github-actions 0.46.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n</blockquote>\n</details>\n<details><summary>gomod</summary>\n<blockquote>\n<details><summary>go.mod</summary>\n - `go 1.23.5`\n - `github.com/DATA-DOG/go-sqlmock v1.5.2`\n - `github.com/Masterminds/semver/v3 v3.3.1`\n - `github.com/avast/retry-go/v4 v4.6.0`\n - `github.com/blang/semver v3.5.1+incompatible`\n - `github.com/cheynewallace/tabby v1.1.1`\n - `github.com/cloudnative-pg/barman-cloud v0.0.0-20250104195650-c1472628b450@c1472628b450`\n - `github.com/cloudnative-pg/machinery v0.0.0-20250102082645-95c37fe624d0@95c37fe624d0`\n - `github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc@d8f796af33cc`\n - `github.com/go-logr/logr v1.4.2`\n - `github.com/google/shlex v0.0.0-20191202100458-e7afc7fbc510@e7afc7fbc510`\n - `github.com/jackc/pgx/v5 v5.7.2`\n - `github.com/kballard/go-shellquote v0.0.0-20180428030007-95032a82bc51@95032a82bc51`\n - `github.com/kubernetes-csi/external-snapshotter/client/v8 v8.2.0`\n - `github.com/lib/pq v1.10.9`\n - `github.com/logrusorgru/aurora/v4 v4.0.0`\n - `github.com/mitchellh/go-ps v1.0.0`\n - `github.com/onsi/ginkgo/v2 v2.22.2`\n - `github.com/onsi/gomega v1.36.2`\n - `github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring v0.80.0`\n - `github.com/prometheus/client_golang v1.20.5`\n - `github.com/robfig/cron v1.2.0`\n - `github.com/sethvargo/go-password v0.3.1`\n - `github.com/spf13/cobra v1.8.1`\n - `github.com/stern/stern v1.32.0`\n - `github.com/thoas/go-funk v0.9.3`\n - `go.uber.org/atomic v1.11.0`\n - `go.uber.org/zap v1.27.0`\n - `golang.org/x/term v0.29.0`\n - `gopkg.in/yaml.v3 v3.0.1`\n - `k8s.io/api v0.32.1`\n - `k8s.io/apiextensions-apiserver v0.32.1`\n - `k8s.io/apimachinery v0.32.1`\n - `k8s.io/cli-runtime v0.32.1`\n - `k8s.io/client-go v0.32.1`\n - `k8s.io/utils v0.0.0-20241210054802-24370beab758@24370beab758`\n - `sigs.k8s.io/controller-runtime v0.20.1`\n - `sigs.k8s.io/yaml v1.4.0`\n</details>\n</blockquote>\n</details>\n<details><summary>regex</summary>\n<blockquote>\n<details><summary>Makefile</summary>\n - `sigs.k8s.io/kustomize/kustomize/v5 v5.6.0`\n</details>\n<details><summary>Makefile</summary>\n - `sigs.k8s.io/controller-tools v0.16.5`\n</details>\n<details><summary>Makefile</summary>\n - `github.com/goreleaser/goreleaser v2.6.1`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `kubernetes-csi/external-snapshotter v8.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-snapshotter v8.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-provisioner v5.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-resizer v1.13.1`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-attacher v4.8.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/csi-driver-host-path v1.15.0`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `rook/rook v1.16.3`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `kubernetes-sigs/kind v0.26.0`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `kubernetes-sigs/kind v0.26.0`\n</details>\n<details><summary>hack/e2e/run-e2e-kind.sh</summary>\n - `kindest/node v1.32.1`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kindest/node v1.32.1`\n</details>\n<details><summary>hack/e2e/run-e2e-k3d.sh</summary>\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n</details>\n<details><summary>Makefile</summary>\n - `jonasbn/github-action-spellcheck 0.46.0`\n</details>\n<details><summary>Makefile</summary>\n - `getwoke/woke 0.19.0`\n</details>\n<details><summary>Makefile</summary>\n - `operator-framework/operator-sdk v1.39.1`\n</details>\n<details><summary>Makefile</summary>\n - `operator-framework/operator-registry v1.50.0`\n</details>\n<details><summary>Makefile</summary>\n - `redhat-openshift-ecosystem/openshift-preflight 1.11.1`\n</details>\n<details><summary>config/olm-scorecard/patches/basic.config.yaml</summary>\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n</details>\n<details><summary>config/olm-scorecard/patches/olm.config.yaml</summary>\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n</details>\n<details><summary>pkg/specs/pgbouncer/deployments.go</summary>\n - `ghcr.io/cloudnative-pg/pgbouncer 1.24.0`\n</details>\n<details><summary>.github/workflows/backport.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/codeql-analysis.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/release-publish.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `golangci/golangci-lint 1.63.4`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `vmware-tanzu/velero 1.15.1`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `vmware-tanzu/velero-plugin-for-aws 1.11.1`\n</details>\n</blockquote>\n</details>\n</blockquote>\n</details>\n<details><summary>Branch release-1.24</summary>\n<blockquote>\n<details><summary>devcontainer</summary>\n<blockquote>\n<details><summary>.devcontainer/devcontainer.json</summary>\n - `mcr.microsoft.com/devcontainers/go 1-bookworm`\n - `ghcr.io/devcontainers/features/docker-in-docker 2`\n - `ghcr.io/rio/features/k3d 1`\n - `ghcr.io/mpriscella/features/kind 1`\n - `ghcr.io/rjfmachado/devcontainer-features/cloud-native 1`\n - `ghcr.io/guiyomh/features/golangci-lint 0`\n - `ghcr.io/devcontainers-contrib/features/kubectx-kubens 1`\n - `ghcr.io/dhoeric/features/stern 1`\n</details>\n</blockquote>\n</details>\n<details><summary>dockerfile</summary>\n<blockquote>\n<details><summary>Dockerfile</summary>\n</details>\n<details><summary>Dockerfile-ubi8</summary>\n</details>\n<details><summary>Dockerfile-ubi9</summary>\n</details>\n</blockquote>\n</details>\n<details><summary>github-actions</summary>\n<blockquote>\n<details><summary>.github/workflows/backport.yml</summary>\n - `actions-ecosystem/action-add-labels v1`\n - `peter-evans/create-or-update-comment v4`\n - `actions-ecosystem/action-remove-labels v1`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `dacbd/create-issue-action v2`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/chatops.yml</summary>\n - `actions-cool/check-user-permission v2`\n - `actions-ecosystem/action-add-labels v1.1.3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/codeql-analysis.yml</summary>\n - `fkirc/skip-duplicate-actions v5.3.1`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `github/codeql-action v3`\n - `github/codeql-action v3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `benc-uk/workflow-dispatch v1`\n - `xt0rted/slash-command-action v2`\n - `xt0rted/pull-request-comment-branch v3`\n - `peter-evans/create-or-update-comment v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `goreleaser/goreleaser-action v6`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/upload-artifact v4`\n - `goreleaser/goreleaser-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `ad-m/github-push-action v0.8.0`\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `azure/login v2.2.0`\n - `nick-fields/retry v3`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `docker/login-action v3`\n - `azure/login v2.2.0`\n - `azure/setup-kubectl v4`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `azure/login v2.2.0`\n - `nick-fields/retry v3`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `aws-actions/configure-aws-credentials v4`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `google-github-actions/auth v2`\n - `google-github-actions/setup-gcloud v2`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `redhat-actions/openshift-tools-installer v1`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/download-artifact v4`\n - `cloudnative-pg/ciclops v1.3.1`\n - `actions/upload-artifact v4`\n - `rtCamp/action-slack-notify v2`\n - `actions-ecosystem/action-add-labels v1.1.3`\n - `actions-ecosystem/action-remove-labels v1.3.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `benc-uk/workflow-dispatch v1`\n - `fkirc/skip-duplicate-actions v5.3.1`\n - `actions/checkout v4`\n - `dorny/paths-filter v3.0.2`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `golangci/golangci-lint-action v6`\n - `actions/checkout v4`\n - `golang/govulncheck-action v1`\n - `actions/checkout v4`\n - `ludeeus/action-shellcheck 2.0.0`\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `goreleaser/goreleaser-action v6`\n - `rtCamp/action-slack-notify v2`\n - `goreleaser/goreleaser-action v6`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `github/codeql-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `docker/setup-qemu-action v3`\n - `actions/setup-go v5`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `helm/kind-action v1.12.0`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `redhat-actions/podman-login v1`\n - `actions/download-artifact v4`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/k8s-versions-check.yml</summary>\n - `actions/checkout v4`\n - `azure/login v2.2.0`\n - `google-github-actions/auth v2`\n - `google-github-actions/setup-gcloud v2`\n - `frenck/action-setup-yq v1`\n - `peter-evans/create-pull-request v7`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/latest-postgres-version-check.yml</summary>\n - `actions/checkout v4`\n - `actions/setup-python v5`\n - `peter-evans/create-pull-request v7`\n - `peter-evans/create-pull-request v7`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/pr_verify_linked_issue.yml</summary>\n - `hattan/verify-linked-issue-action v1.1.5`\n</details>\n<details><summary>.github/workflows/registry-clean.yml</summary>\n - `snok/container-retention-policy v3.0.0`\n - `snok/container-retention-policy v3.0.0`\n</details>\n<details><summary>.github/workflows/release-pr.yml</summary>\n - `actions/checkout v4`\n - `repo-sync/pull-request v2.12`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/release-publish.yml</summary>\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `softprops/action-gh-release v2`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `crazy-max/ghaction-import-gpg v6`\n - `goreleaser/goreleaser-action v6`\n - `rajatjindal/krew-release-bot v0.0.47`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `docker/setup-qemu-action v3`\n - `actions/setup-go v5`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `peter-evans/create-pull-request v7`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `ad-m/github-push-action v0.8.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/release-tag.yml</summary>\n - `actions/checkout v4`\n - `christophebedard/tag-version-commit v1.7.0`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/require-labels.yml</summary>\n - `agilepathway/pull-request-label-checker v1.6.61`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/snyk.yml</summary>\n - `actions/checkout v4`\n - `snyk/actions 0.4.0`\n - `github/codeql-action v3`\n - `snyk/actions 0.4.0`\n - `github/codeql-action v3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/spellcheck.yml</summary>\n - `actions/checkout v4`\n - `get-woke/woke-action v0`\n - `actions/checkout v4`\n - `rojopolis/spellcheck-github-actions 0.46.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/sync-api.yml</summary>\n - `peter-evans/repository-dispatch v3`\n</details>\n</blockquote>\n</details>\n<details><summary>gomod</summary>\n<blockquote>\n<details><summary>go.mod</summary>\n - `go 1.23.5`\n - `github.com/DATA-DOG/go-sqlmock v1.5.2`\n - `github.com/Masterminds/semver/v3 v3.3.1`\n - `github.com/avast/retry-go/v4 v4.6.0`\n - `github.com/blang/semver v3.5.1+incompatible`\n - `github.com/cheynewallace/tabby v1.1.1`\n - `github.com/cloudnative-pg/barman-cloud v0.0.0-20250104195650-c1472628b450@c1472628b450`\n - `github.com/cloudnative-pg/cnpg-i v0.0.0-20241224161104-7e2cfa59debc@7e2cfa59debc`\n - `github.com/cloudnative-pg/machinery v0.0.0-20250102082645-95c37fe624d0@95c37fe624d0`\n - `github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc@d8f796af33cc`\n - `github.com/evanphx/json-patch/v5 v5.9.11`\n - `github.com/go-logr/logr v1.4.2`\n - `github.com/google/shlex v0.0.0-20191202100458-e7afc7fbc510@e7afc7fbc510`\n - `github.com/grpc-ecosystem/go-grpc-middleware/v2 v2.2.0`\n - `github.com/jackc/pgx/v5 v5.7.2`\n - `github.com/jackc/puddle/v2 v2.2.2`\n - `github.com/kballard/go-shellquote v0.0.0-20180428030007-95032a82bc51@95032a82bc51`\n - `github.com/kubernetes-csi/external-snapshotter/client/v8 v8.2.0`\n - `github.com/lib/pq v1.10.9`\n - `github.com/logrusorgru/aurora/v4 v4.0.0`\n - `github.com/mitchellh/go-ps v1.0.0`\n - `github.com/onsi/ginkgo/v2 v2.22.2`\n - `github.com/onsi/gomega v1.36.2`\n - `github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring v0.80.0`\n - `github.com/prometheus/client_golang v1.20.5`\n - `github.com/robfig/cron v1.2.0`\n - `github.com/sethvargo/go-password v0.3.1`\n - `github.com/spf13/cobra v1.8.1`\n - `github.com/stern/stern v1.32.0`\n - `github.com/thoas/go-funk v0.9.3`\n - `go.uber.org/atomic v1.11.0`\n - `go.uber.org/multierr v1.11.0`\n - `go.uber.org/zap v1.27.0`\n - `golang.org/x/term v0.29.0`\n - `google.golang.org/grpc v1.70.0`\n - `gopkg.in/yaml.v3 v3.0.1`\n - `k8s.io/api v0.32.1`\n - `k8s.io/apiextensions-apiserver v0.32.1`\n - `k8s.io/apimachinery v0.32.1`\n - `k8s.io/cli-runtime v0.32.1`\n - `k8s.io/client-go v0.32.1`\n - `k8s.io/utils v0.0.0-20241210054802-24370beab758@24370beab758`\n - `sigs.k8s.io/controller-runtime v0.20.1`\n - `sigs.k8s.io/yaml v1.4.0`\n</details>\n</blockquote>\n</details>\n<details><summary>regex</summary>\n<blockquote>\n<details><summary>Makefile</summary>\n - `sigs.k8s.io/kustomize/kustomize/v5 v5.6.0`\n</details>\n<details><summary>Makefile</summary>\n - `sigs.k8s.io/controller-tools v0.16.5`\n</details>\n<details><summary>Makefile</summary>\n - `github.com/goreleaser/goreleaser v2.6.1`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `kubernetes-csi/external-snapshotter v8.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-snapshotter v8.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-provisioner v5.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-resizer v1.13.1`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-attacher v4.8.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/csi-driver-host-path v1.15.0`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `rook/rook v1.16.3`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `kubernetes-sigs/kind v0.26.0`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `kubernetes-sigs/kind v0.26.0`\n</details>\n<details><summary>hack/e2e/run-e2e-kind.sh</summary>\n - `kindest/node v1.32.1`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kindest/node v1.32.1`\n</details>\n<details><summary>hack/e2e/run-e2e-k3d.sh</summary>\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n</details>\n<details><summary>Makefile</summary>\n - `jonasbn/github-action-spellcheck 0.46.0`\n</details>\n<details><summary>Makefile</summary>\n - `getwoke/woke 0.19.0`\n</details>\n<details><summary>Makefile</summary>\n - `operator-framework/operator-sdk v1.39.1`\n</details>\n<details><summary>Makefile</summary>\n - `operator-framework/operator-registry v1.50.0`\n</details>\n<details><summary>Makefile</summary>\n - `redhat-openshift-ecosystem/openshift-preflight 1.11.1`\n</details>\n<details><summary>config/olm-scorecard/patches/basic.config.yaml</summary>\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n</details>\n<details><summary>config/olm-scorecard/patches/olm.config.yaml</summary>\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n</details>\n<details><summary>pkg/specs/pgbouncer/deployments.go</summary>\n - `ghcr.io/cloudnative-pg/pgbouncer 1.24.0`\n</details>\n<details><summary>.github/workflows/backport.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/codeql-analysis.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/release-publish.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `golangci/golangci-lint 1.63.4`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `vmware-tanzu/velero 1.15.1`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `vmware-tanzu/velero-plugin-for-aws 1.11.1`\n</details>\n</blockquote>\n</details>\n</blockquote>\n</details>\n<details><summary>Branch release-1.25</summary>\n<blockquote>\n<details><summary>devcontainer</summary>\n<blockquote>\n<details><summary>.devcontainer/devcontainer.json</summary>\n - `mcr.microsoft.com/devcontainers/go 1-bookworm`\n - `ghcr.io/devcontainers/features/docker-in-docker 2`\n - `ghcr.io/rio/features/k3d 1`\n - `ghcr.io/mpriscella/features/kind 1`\n - `ghcr.io/rjfmachado/devcontainer-features/cloud-native 1`\n - `ghcr.io/guiyomh/features/golangci-lint 0`\n - `ghcr.io/devcontainers-contrib/features/kubectx-kubens 1`\n - `ghcr.io/dhoeric/features/stern 1`\n</details>\n</blockquote>\n</details>\n<details><summary>dockerfile</summary>\n<blockquote>\n<details><summary>Dockerfile</summary>\n</details>\n<details><summary>Dockerfile-ubi8</summary>\n</details>\n<details><summary>Dockerfile-ubi9</summary>\n</details>\n</blockquote>\n</details>\n<details><summary>github-actions</summary>\n<blockquote>\n<details><summary>.github/workflows/backport.yml</summary>\n - `actions-ecosystem/action-add-labels v1`\n - `peter-evans/create-or-update-comment v4`\n - `actions-ecosystem/action-remove-labels v1`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `dacbd/create-issue-action v2`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/chatops.yml</summary>\n - `actions-cool/check-user-permission v2`\n - `actions-ecosystem/action-add-labels v1.1.3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/codeql-analysis.yml</summary>\n - `fkirc/skip-duplicate-actions v5.3.1`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `github/codeql-action v3`\n - `github/codeql-action v3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `benc-uk/workflow-dispatch v1`\n - `xt0rted/slash-command-action v2`\n - `xt0rted/pull-request-comment-branch v3`\n - `peter-evans/create-or-update-comment v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `goreleaser/goreleaser-action v6`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/upload-artifact v4`\n - `goreleaser/goreleaser-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `ad-m/github-push-action v0.8.0`\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `azure/login v2.2.0`\n - `nick-fields/retry v3`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `docker/login-action v3`\n - `azure/login v2.2.0`\n - `azure/setup-kubectl v4`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `azure/login v2.2.0`\n - `nick-fields/retry v3`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `aws-actions/configure-aws-credentials v4`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `google-github-actions/auth v2`\n - `google-github-actions/setup-gcloud v2`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `redhat-actions/openshift-tools-installer v1`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/download-artifact v4`\n - `cloudnative-pg/ciclops v1.3.1`\n - `actions/upload-artifact v4`\n - `rtCamp/action-slack-notify v2`\n - `actions-ecosystem/action-add-labels v1.1.3`\n - `actions-ecosystem/action-remove-labels v1.3.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `benc-uk/workflow-dispatch v1`\n - `fkirc/skip-duplicate-actions v5.3.1`\n - `actions/checkout v4`\n - `dorny/paths-filter v3.0.2`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `golangci/golangci-lint-action v6`\n - `actions/checkout v4`\n - `golang/govulncheck-action v1`\n - `actions/checkout v4`\n - `ludeeus/action-shellcheck 2.0.0`\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `goreleaser/goreleaser-action v6`\n - `rtCamp/action-slack-notify v2`\n - `goreleaser/goreleaser-action v6`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `github/codeql-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `docker/setup-qemu-action v3`\n - `actions/setup-go v5`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `helm/kind-action v1.12.0`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `redhat-actions/podman-login v1`\n - `actions/download-artifact v4`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/k8s-versions-check.yml</summary>\n - `actions/checkout v4`\n - `azure/login v2.2.0`\n - `google-github-actions/auth v2`\n - `google-github-actions/setup-gcloud v2`\n - `frenck/action-setup-yq v1`\n - `peter-evans/create-pull-request v7`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/latest-postgres-version-check.yml</summary>\n - `actions/checkout v4`\n - `actions/setup-python v5`\n - `peter-evans/create-pull-request v7`\n - `peter-evans/create-pull-request v7`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/pr_verify_linked_issue.yml</summary>\n - `hattan/verify-linked-issue-action v1.1.5`\n</details>\n<details><summary>.github/workflows/registry-clean.yml</summary>\n - `snok/container-retention-policy v3.0.0`\n - `snok/container-retention-policy v3.0.0`\n</details>\n<details><summary>.github/workflows/release-pr.yml</summary>\n - `actions/checkout v4`\n - `repo-sync/pull-request v2.12`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/release-publish.yml</summary>\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `softprops/action-gh-release v2`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `crazy-max/ghaction-import-gpg v6`\n - `goreleaser/goreleaser-action v6`\n - `rajatjindal/krew-release-bot v0.0.47`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `docker/setup-qemu-action v3`\n - `actions/setup-go v5`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `peter-evans/create-pull-request v7`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `ad-m/github-push-action v0.8.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/release-tag.yml</summary>\n - `actions/checkout v4`\n - `christophebedard/tag-version-commit v1.7.0`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/require-labels.yml</summary>\n - `agilepathway/pull-request-label-checker v1.6.61`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/snyk.yml</summary>\n - `actions/checkout v4`\n - `snyk/actions 0.4.0`\n - `github/codeql-action v3`\n - `snyk/actions 0.4.0`\n - `github/codeql-action v3`\n - `ubuntu 24.04`\n</details>\n</blockquote>\n</details>\n</blockquote>\n</details>\n---\n- [ ] <!-- manual job -->Check this box to trigger a request for Renovate to run again on this repository",
        "messages": "This issue lists Renovate updates and detected dependencies. Read the [Dependency Dashboard](https://docs.renovatebot.com/key-concepts/dashboard/) docs to learn more.\n## Rate-Limited\nThese updates are currently rate-limited. Click on a checkbox below to force their creation now.\n - [ ] <!-- unlimit-branch=renovate/main-backup-test-tools -->chore(deps): update dependency vmware-tanzu/velero to v1.15.2 (main)\n - [ ] <!-- unlimit-branch=renovate/release-1.22-github.com-cloudnative-pg-machinery-digest -->fix(deps): update github.com/cloudnative-pg/machinery digest to d15e1d1 (release-1.22)\n - [ ] <!-- unlimit-branch=renovate/release-1.22-backup-test-tools -->chore(deps): update dependency vmware-tanzu/velero to v1.15.2 (release-1.22)\n - [ ] <!-- unlimit-branch=renovate/release-1.22-sigs.k8s.io-controller-tools-0.x -->chore(deps): update module sigs.k8s.io/controller-tools to v0.17.2 (release-1.22)\n - [ ] <!-- unlimit-branch=renovate/release-1.24-github.com-cloudnative-pg-barman-cloud-digest -->fix(deps): update github.com/cloudnative-pg/barman-cloud digest to 10ef19b (release-1.24)\n - [ ] <!-- unlimit-branch=renovate/release-1.24-backup-test-tools -->chore(deps): update dependency vmware-tanzu/velero to v1.15.2 (release-1.24)\n - [ ] <!-- unlimit-branch=renovate/release-1.24-sigs.k8s.io-controller-tools-0.x -->chore(deps): update module sigs.k8s.io/controller-tools to v0.17.2 (release-1.24)\n - [ ] <!-- unlimit-branch=renovate/release-1.25-github.com-cloudnative-pg-barman-cloud-digest -->fix(deps): update github.com/cloudnative-pg/barman-cloud digest to 10ef19b (release-1.25)\n - [ ] <!-- unlimit-branch=renovate/release-1.25-github.com-cloudnative-pg-machinery-digest -->fix(deps): update github.com/cloudnative-pg/machinery digest to d15e1d1 (release-1.25)\n - [ ] <!-- unlimit-branch=renovate/release-1.25-backup-test-tools -->chore(deps): update dependency vmware-tanzu/velero to v1.15.2 (release-1.25)\n - [ ] <!-- unlimit-branch=renovate/release-1.25-sigs.k8s.io-controller-tools-0.x -->chore(deps): update module sigs.k8s.io/controller-tools to v0.17.2 (release-1.25)\n - [ ] <!-- create-all-rate-limited-prs -->\ud83d\udd10 **Create all rate-limited PRs at once** \ud83d\udd10\n## Open\nThese updates have all been created already. Click a checkbox below to force a retry/rebase of any.\n - [ ] <!-- rebase-branch=renovate/main-github.com-cloudnative-pg-barman-cloud-digest -->[fix(deps): update github.com/cloudnative-pg/barman-cloud digest to 10ef19b (main)](../pull/6798)\n - [ ] <!-- rebase-branch=renovate/main-github.com-cloudnative-pg-machinery-digest -->[fix(deps): update github.com/cloudnative-pg/machinery digest to d15e1d1 (main)](../pull/6795)\n - [ ] <!-- rebase-branch=renovate/main-sigs.k8s.io-controller-tools-0.x -->[chore(deps): update module sigs.k8s.io/controller-tools to v0.17.2 (main)](../pull/6591)\n - [ ] <!-- rebase-branch=renovate/release-1.22-github.com-cloudnative-pg-barman-cloud-digest -->[fix(deps): update github.com/cloudnative-pg/barman-cloud digest to 10ef19b (release-1.22)](../pull/6799)\n - [ ] <!-- rebase-branch=renovate/release-1.24-github.com-cloudnative-pg-machinery-digest -->[fix(deps): update github.com/cloudnative-pg/machinery digest to d15e1d1 (release-1.24)](../pull/6803)\n - [ ] <!-- rebase-all-open-prs -->**Click on this checkbox to rebase all open PRs at once**\n## Ignored or Blocked\nThese are blocked by an existing closed PR and will not be recreated unless you click a checkbox below.\n - [ ] <!-- recreate-branch=renovate/main-github.com-cloudnative-pg-cnpg-i-digest -->[fix(deps): update github.com/cloudnative-pg/cnpg-i digest to cd31008 (main)](../pull/6791)\n - [ ] <!-- recreate-branch=renovate/release-1.24-github.com-cloudnative-pg-cnpg-i-digest -->[fix(deps): update github.com/cloudnative-pg/cnpg-i digest to cd31008 (release-1.24)](../pull/6792)\n - [ ] <!-- recreate-branch=renovate/release-1.25-github.com-cloudnative-pg-cnpg-i-digest -->[fix(deps): update github.com/cloudnative-pg/cnpg-i digest to cd31008 (release-1.25)](../pull/6793)\n## Detected dependencies\n> [!NOTE]\n> Detected dependencies section has been truncated\n<details><summary>Branch main</summary>\n<blockquote>\n<details><summary>devcontainer</summary>\n<blockquote>\n<details><summary>.devcontainer/devcontainer.json</summary>\n - `mcr.microsoft.com/devcontainers/go 1-bookworm`\n - `ghcr.io/devcontainers/features/docker-in-docker 2`\n - `ghcr.io/rio/features/k3d 1`\n - `ghcr.io/mpriscella/features/kind 1`\n - `ghcr.io/rjfmachado/devcontainer-features/cloud-native 1`\n - `ghcr.io/guiyomh/features/golangci-lint 0`\n - `ghcr.io/devcontainers-contrib/features/kubectx-kubens 1`\n - `ghcr.io/dhoeric/features/stern 1`\n</details>\n</blockquote>\n</details>\n<details><summary>dockerfile</summary>\n<blockquote>\n<details><summary>Dockerfile</summary>\n</details>\n<details><summary>Dockerfile-ubi8</summary>\n</details>\n<details><summary>Dockerfile-ubi9</summary>\n</details>\n</blockquote>\n</details>\n<details><summary>github-actions</summary>\n<blockquote>\n<details><summary>.github/workflows/backport.yml</summary>\n - `actions-ecosystem/action-add-labels v1`\n - `peter-evans/create-or-update-comment v4`\n - `actions-ecosystem/action-remove-labels v1`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `dacbd/create-issue-action v2`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/chatops.yml</summary>\n - `actions-cool/check-user-permission v2`\n - `actions-ecosystem/action-add-labels v1.1.3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/close-inactive-issues.yml</summary>\n - `actions/stale v9`\n</details>\n<details><summary>.github/workflows/codeql-analysis.yml</summary>\n - `fkirc/skip-duplicate-actions v5.3.1`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `github/codeql-action v3`\n - `github/codeql-action v3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `benc-uk/workflow-dispatch v1`\n - `xt0rted/slash-command-action v2`\n - `xt0rted/pull-request-comment-branch v3`\n - `peter-evans/create-or-update-comment v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `goreleaser/goreleaser-action v6`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/upload-artifact v4`\n - `goreleaser/goreleaser-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `ad-m/github-push-action v0.8.0`\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `azure/login v2.2.0`\n - `nick-fields/retry v3`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `docker/login-action v3`\n - `azure/login v2.2.0`\n - `azure/setup-kubectl v4`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `azure/login v2.2.0`\n - `nick-fields/retry v3`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `aws-actions/configure-aws-credentials v4`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `google-github-actions/auth v2`\n - `google-github-actions/setup-gcloud v2`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `redhat-actions/openshift-tools-installer v1`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/download-artifact v4`\n - `cloudnative-pg/ciclops v1.3.1`\n - `actions/upload-artifact v4`\n - `rtCamp/action-slack-notify v2`\n - `actions-ecosystem/action-add-labels v1.1.3`\n - `actions-ecosystem/action-remove-labels v1.3.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `benc-uk/workflow-dispatch v1`\n - `fkirc/skip-duplicate-actions v5.3.1`\n - `actions/checkout v4`\n - `dorny/paths-filter v3.0.2`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `golangci/golangci-lint-action v6`\n - `actions/checkout v4`\n - `golang/govulncheck-action v1`\n - `actions/checkout v4`\n - `ludeeus/action-shellcheck 2.0.0`\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `goreleaser/goreleaser-action v6`\n - `rtCamp/action-slack-notify v2`\n - `goreleaser/goreleaser-action v6`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `github/codeql-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `docker/setup-qemu-action v3`\n - `actions/setup-go v5`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `helm/kind-action v1.12.0`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `redhat-actions/podman-login v1`\n - `actions/download-artifact v4`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/k8s-versions-check.yml</summary>\n - `actions/checkout v4`\n - `azure/login v2.2.0`\n - `google-github-actions/auth v2`\n - `google-github-actions/setup-gcloud v2`\n - `frenck/action-setup-yq v1`\n - `peter-evans/create-pull-request v7`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/latest-postgres-version-check.yml</summary>\n - `actions/checkout v4`\n - `actions/setup-python v5`\n - `peter-evans/create-pull-request v7`\n - `peter-evans/create-pull-request v7`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/pr_verify_linked_issue.yml</summary>\n - `hattan/verify-linked-issue-action v1.1.5`\n</details>\n<details><summary>.github/workflows/registry-clean.yml</summary>\n - `snok/container-retention-policy v3.0.0`\n - `snok/container-retention-policy v3.0.0`\n</details>\n<details><summary>.github/workflows/release-pr.yml</summary>\n - `actions/checkout v4`\n - `repo-sync/pull-request v2.12`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/release-publish.yml</summary>\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `softprops/action-gh-release v2`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `crazy-max/ghaction-import-gpg v6`\n - `goreleaser/goreleaser-action v6`\n - `rajatjindal/krew-release-bot v0.0.47`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `docker/setup-qemu-action v3`\n - `actions/setup-go v5`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `peter-evans/create-pull-request v7`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `ad-m/github-push-action v0.8.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/release-tag.yml</summary>\n - `actions/checkout v4`\n - `christophebedard/tag-version-commit v1.7.0`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/require-labels.yml</summary>\n - `agilepathway/pull-request-label-checker v1.6.61`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/snyk.yml</summary>\n - `actions/checkout v4`\n - `snyk/actions 0.4.0`\n - `github/codeql-action v3`\n - `snyk/actions 0.4.0`\n - `github/codeql-action v3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/spellcheck.yml</summary>\n - `actions/checkout v4`\n - `get-woke/woke-action v0`\n - `actions/checkout v4`\n - `rojopolis/spellcheck-github-actions 0.46.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/sync-api.yml</summary>\n - `peter-evans/repository-dispatch v3`\n</details>\n</blockquote>\n</details>\n<details><summary>gomod</summary>\n<blockquote>\n<details><summary>go.mod</summary>\n - `go 1.23.5`\n - `github.com/DATA-DOG/go-sqlmock v1.5.2`\n - `github.com/Masterminds/semver/v3 v3.3.1`\n - `github.com/avast/retry-go/v4 v4.6.0`\n - `github.com/blang/semver v3.5.1+incompatible`\n - `github.com/cheynewallace/tabby v1.1.1`\n - `github.com/cloudnative-pg/barman-cloud v0.0.0-20250104195650-c1472628b450@c1472628b450`\n - `github.com/cloudnative-pg/cnpg-i v0.0.0-20241224161104-7e2cfa59debc@7e2cfa59debc`\n - `github.com/cloudnative-pg/machinery v0.0.0-20250102082645-95c37fe624d0@95c37fe624d0`\n - `github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc@d8f796af33cc`\n - `github.com/evanphx/json-patch/v5 v5.9.11`\n - `github.com/go-logr/logr v1.4.2`\n - `github.com/google/shlex v0.0.0-20191202100458-e7afc7fbc510@e7afc7fbc510`\n - `github.com/grpc-ecosystem/go-grpc-middleware/v2 v2.2.0`\n - `github.com/jackc/pgx/v5 v5.7.2`\n - `github.com/jackc/puddle/v2 v2.2.2`\n - `github.com/kballard/go-shellquote v0.0.0-20180428030007-95032a82bc51@95032a82bc51`\n - `github.com/kubernetes-csi/external-snapshotter/client/v8 v8.2.0`\n - `github.com/lib/pq v1.10.9`\n - `github.com/logrusorgru/aurora/v4 v4.0.0`\n - `github.com/mitchellh/go-ps v1.0.0`\n - `github.com/onsi/ginkgo/v2 v2.22.2`\n - `github.com/onsi/gomega v1.36.2`\n - `github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring v0.80.0`\n - `github.com/prometheus/client_golang v1.20.5`\n - `github.com/robfig/cron v1.2.0`\n - `github.com/sethvargo/go-password v0.3.1`\n - `github.com/spf13/cobra v1.8.1`\n - `github.com/stern/stern v1.32.0`\n - `github.com/thoas/go-funk v0.9.3`\n - `go.uber.org/atomic v1.11.0`\n - `go.uber.org/multierr v1.11.0`\n - `go.uber.org/zap v1.27.0`\n - `golang.org/x/term v0.29.0`\n - `google.golang.org/grpc v1.70.0`\n - `gopkg.in/yaml.v3 v3.0.1`\n - `k8s.io/api v0.32.1`\n - `k8s.io/apiextensions-apiserver v0.32.1`\n - `k8s.io/apimachinery v0.32.1`\n - `k8s.io/cli-runtime v0.32.1`\n - `k8s.io/client-go v0.32.1`\n - `k8s.io/utils v0.0.0-20241210054802-24370beab758@24370beab758`\n - `sigs.k8s.io/controller-runtime v0.20.1`\n - `sigs.k8s.io/yaml v1.4.0`\n</details>\n</blockquote>\n</details>\n<details><summary>regex</summary>\n<blockquote>\n<details><summary>Makefile</summary>\n - `sigs.k8s.io/kustomize/kustomize/v5 v5.6.0`\n</details>\n<details><summary>Makefile</summary>\n - `sigs.k8s.io/controller-tools v0.16.5`\n</details>\n<details><summary>Makefile</summary>\n - `github.com/goreleaser/goreleaser v2.6.1`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `kubernetes-csi/external-snapshotter v8.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-snapshotter v8.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-provisioner v5.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-resizer v1.13.1`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-attacher v4.8.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/csi-driver-host-path v1.15.0`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `rook/rook v1.16.3`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `kubernetes-sigs/kind v0.26.0`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `kubernetes-sigs/kind v0.26.0`\n</details>\n<details><summary>hack/e2e/run-e2e-kind.sh</summary>\n - `kindest/node v1.32.1`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kindest/node v1.32.1`\n</details>\n<details><summary>hack/e2e/run-e2e-k3d.sh</summary>\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n</details>\n<details><summary>Makefile</summary>\n - `jonasbn/github-action-spellcheck 0.46.0`\n</details>\n<details><summary>Makefile</summary>\n - `getwoke/woke 0.19.0`\n</details>\n<details><summary>Makefile</summary>\n - `operator-framework/operator-sdk v1.39.1`\n</details>\n<details><summary>Makefile</summary>\n - `operator-framework/operator-registry v1.50.0`\n</details>\n<details><summary>Makefile</summary>\n - `redhat-openshift-ecosystem/openshift-preflight 1.11.1`\n</details>\n<details><summary>config/olm-scorecard/patches/basic.config.yaml</summary>\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n</details>\n<details><summary>config/olm-scorecard/patches/olm.config.yaml</summary>\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n</details>\n<details><summary>pkg/specs/pgbouncer/deployments.go</summary>\n - `ghcr.io/cloudnative-pg/pgbouncer 1.24.0`\n</details>\n<details><summary>.github/workflows/backport.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/codeql-analysis.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/release-publish.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `golangci/golangci-lint 1.63.4`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `vmware-tanzu/velero 1.15.1`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `vmware-tanzu/velero-plugin-for-aws 1.11.1`\n</details>\n</blockquote>\n</details>\n</blockquote>\n</details>\n<details><summary>Branch release-1.22</summary>\n<blockquote>\n<details><summary>devcontainer</summary>\n<blockquote>\n<details><summary>.devcontainer/devcontainer.json</summary>\n - `mcr.microsoft.com/devcontainers/go 1-bookworm`\n - `ghcr.io/devcontainers/features/docker-in-docker 2`\n - `ghcr.io/rio/features/k3d 1`\n - `ghcr.io/mpriscella/features/kind 1`\n - `ghcr.io/rjfmachado/devcontainer-features/cloud-native 1`\n - `ghcr.io/guiyomh/features/golangci-lint 0`\n - `ghcr.io/devcontainers-contrib/features/kubectx-kubens 1`\n - `ghcr.io/dhoeric/features/stern 1`\n</details>\n</blockquote>\n</details>\n<details><summary>dockerfile</summary>\n<blockquote>\n<details><summary>Dockerfile</summary>\n</details>\n<details><summary>Dockerfile-ubi8</summary>\n</details>\n<details><summary>Dockerfile-ubi9</summary>\n</details>\n</blockquote>\n</details>\n<details><summary>github-actions</summary>\n<blockquote>\n<details><summary>.github/workflows/backport.yml</summary>\n - `actions-ecosystem/action-add-labels v1`\n - `peter-evans/create-or-update-comment v4`\n - `actions-ecosystem/action-remove-labels v1`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `dacbd/create-issue-action v2`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/chatops.yml</summary>\n - `actions-cool/check-user-permission v2`\n - `actions-ecosystem/action-add-labels v1.1.3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/codeql-analysis.yml</summary>\n - `fkirc/skip-duplicate-actions v5.3.1`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `github/codeql-action v3`\n - `github/codeql-action v3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `benc-uk/workflow-dispatch v1`\n - `xt0rted/slash-command-action v2`\n - `xt0rted/pull-request-comment-branch v3`\n - `peter-evans/create-or-update-comment v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `goreleaser/goreleaser-action v6`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/upload-artifact v4`\n - `goreleaser/goreleaser-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `ad-m/github-push-action v0.8.0`\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `azure/login v2.2.0`\n - `nick-fields/retry v3`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `docker/login-action v3`\n - `azure/login v2.2.0`\n - `azure/setup-kubectl v4`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `azure/login v2.2.0`\n - `nick-fields/retry v3`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `aws-actions/configure-aws-credentials v4`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `google-github-actions/auth v2`\n - `google-github-actions/setup-gcloud v2`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `redhat-actions/openshift-tools-installer v1`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/download-artifact v4`\n - `cloudnative-pg/ciclops v1.3.1`\n - `actions/upload-artifact v4`\n - `rtCamp/action-slack-notify v2`\n - `actions-ecosystem/action-add-labels v1.1.3`\n - `actions-ecosystem/action-remove-labels v1.3.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `benc-uk/workflow-dispatch v1`\n - `fkirc/skip-duplicate-actions v5.3.1`\n - `actions/checkout v4`\n - `dorny/paths-filter v3.0.2`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `golangci/golangci-lint-action v6`\n - `actions/checkout v4`\n - `golang/govulncheck-action v1`\n - `actions/checkout v4`\n - `ludeeus/action-shellcheck 2.0.0`\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `goreleaser/goreleaser-action v6`\n - `rtCamp/action-slack-notify v2`\n - `goreleaser/goreleaser-action v6`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `github/codeql-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `docker/setup-qemu-action v3`\n - `actions/setup-go v5`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `helm/kind-action v1.12.0`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `redhat-actions/podman-login v1`\n - `actions/download-artifact v4`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/k8s-versions-check.yml</summary>\n - `actions/checkout v4`\n - `azure/login v2.2.0`\n - `google-github-actions/auth v2`\n - `google-github-actions/setup-gcloud v2`\n - `frenck/action-setup-yq v1`\n - `peter-evans/create-pull-request v7`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/latest-postgres-version-check.yml</summary>\n - `actions/checkout v4`\n - `actions/setup-python v5`\n - `peter-evans/create-pull-request v7`\n - `peter-evans/create-pull-request v7`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/pr_verify_linked_issue.yml</summary>\n - `hattan/verify-linked-issue-action v1.1.5`\n</details>\n<details><summary>.github/workflows/registry-clean.yml</summary>\n - `snok/container-retention-policy v3.0.0`\n - `snok/container-retention-policy v3.0.0`\n</details>\n<details><summary>.github/workflows/release-pr.yml</summary>\n - `actions/checkout v4`\n - `repo-sync/pull-request v2.12`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/release-publish.yml</summary>\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `softprops/action-gh-release v2`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `crazy-max/ghaction-import-gpg v6`\n - `goreleaser/goreleaser-action v6`\n - `rajatjindal/krew-release-bot v0.0.47`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `docker/setup-qemu-action v3`\n - `actions/setup-go v5`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `peter-evans/create-pull-request v7`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `ad-m/github-push-action v0.8.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/release-tag.yml</summary>\n - `actions/checkout v4`\n - `christophebedard/tag-version-commit v1.7.0`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/require-labels.yml</summary>\n - `agilepathway/pull-request-label-checker v1.6.61`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/snyk.yml</summary>\n - `actions/checkout v4`\n - `snyk/actions 0.4.0`\n - `github/codeql-action v3`\n - `snyk/actions 0.4.0`\n - `github/codeql-action v3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/spellcheck.yml</summary>\n - `actions/checkout v4`\n - `get-woke/woke-action v0`\n - `actions/checkout v4`\n - `rojopolis/spellcheck-github-actions 0.46.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n</blockquote>\n</details>\n<details><summary>gomod</summary>\n<blockquote>\n<details><summary>go.mod</summary>\n - `go 1.23.5`\n - `github.com/DATA-DOG/go-sqlmock v1.5.2`\n - `github.com/Masterminds/semver/v3 v3.3.1`\n - `github.com/avast/retry-go/v4 v4.6.0`\n - `github.com/blang/semver v3.5.1+incompatible`\n - `github.com/cheynewallace/tabby v1.1.1`\n - `github.com/cloudnative-pg/barman-cloud v0.0.0-20250104195650-c1472628b450@c1472628b450`\n - `github.com/cloudnative-pg/machinery v0.0.0-20250102082645-95c37fe624d0@95c37fe624d0`\n - `github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc@d8f796af33cc`\n - `github.com/go-logr/logr v1.4.2`\n - `github.com/google/shlex v0.0.0-20191202100458-e7afc7fbc510@e7afc7fbc510`\n - `github.com/jackc/pgx/v5 v5.7.2`\n - `github.com/kballard/go-shellquote v0.0.0-20180428030007-95032a82bc51@95032a82bc51`\n - `github.com/kubernetes-csi/external-snapshotter/client/v8 v8.2.0`\n - `github.com/lib/pq v1.10.9`\n - `github.com/logrusorgru/aurora/v4 v4.0.0`\n - `github.com/mitchellh/go-ps v1.0.0`\n - `github.com/onsi/ginkgo/v2 v2.22.2`\n - `github.com/onsi/gomega v1.36.2`\n - `github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring v0.80.0`\n - `github.com/prometheus/client_golang v1.20.5`\n - `github.com/robfig/cron v1.2.0`\n - `github.com/sethvargo/go-password v0.3.1`\n - `github.com/spf13/cobra v1.8.1`\n - `github.com/stern/stern v1.32.0`\n - `github.com/thoas/go-funk v0.9.3`\n - `go.uber.org/atomic v1.11.0`\n - `go.uber.org/zap v1.27.0`\n - `golang.org/x/term v0.29.0`\n - `gopkg.in/yaml.v3 v3.0.1`\n - `k8s.io/api v0.32.1`\n - `k8s.io/apiextensions-apiserver v0.32.1`\n - `k8s.io/apimachinery v0.32.1`\n - `k8s.io/cli-runtime v0.32.1`\n - `k8s.io/client-go v0.32.1`\n - `k8s.io/utils v0.0.0-20241210054802-24370beab758@24370beab758`\n - `sigs.k8s.io/controller-runtime v0.20.1`\n - `sigs.k8s.io/yaml v1.4.0`\n</details>\n</blockquote>\n</details>\n<details><summary>regex</summary>\n<blockquote>\n<details><summary>Makefile</summary>\n - `sigs.k8s.io/kustomize/kustomize/v5 v5.6.0`\n</details>\n<details><summary>Makefile</summary>\n - `sigs.k8s.io/controller-tools v0.16.5`\n</details>\n<details><summary>Makefile</summary>\n - `github.com/goreleaser/goreleaser v2.6.1`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `kubernetes-csi/external-snapshotter v8.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-snapshotter v8.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-provisioner v5.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-resizer v1.13.1`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-attacher v4.8.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/csi-driver-host-path v1.15.0`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `rook/rook v1.16.3`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `kubernetes-sigs/kind v0.26.0`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `kubernetes-sigs/kind v0.26.0`\n</details>\n<details><summary>hack/e2e/run-e2e-kind.sh</summary>\n - `kindest/node v1.32.1`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kindest/node v1.32.1`\n</details>\n<details><summary>hack/e2e/run-e2e-k3d.sh</summary>\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n</details>\n<details><summary>Makefile</summary>\n - `jonasbn/github-action-spellcheck 0.46.0`\n</details>\n<details><summary>Makefile</summary>\n - `getwoke/woke 0.19.0`\n</details>\n<details><summary>Makefile</summary>\n - `operator-framework/operator-sdk v1.39.1`\n</details>\n<details><summary>Makefile</summary>\n - `operator-framework/operator-registry v1.50.0`\n</details>\n<details><summary>Makefile</summary>\n - `redhat-openshift-ecosystem/openshift-preflight 1.11.1`\n</details>\n<details><summary>config/olm-scorecard/patches/basic.config.yaml</summary>\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n</details>\n<details><summary>config/olm-scorecard/patches/olm.config.yaml</summary>\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n</details>\n<details><summary>pkg/specs/pgbouncer/deployments.go</summary>\n - `ghcr.io/cloudnative-pg/pgbouncer 1.24.0`\n</details>\n<details><summary>.github/workflows/backport.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/codeql-analysis.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/release-publish.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `golangci/golangci-lint 1.63.4`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `vmware-tanzu/velero 1.15.1`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `vmware-tanzu/velero-plugin-for-aws 1.11.1`\n</details>\n</blockquote>\n</details>\n</blockquote>\n</details>\n<details><summary>Branch release-1.24</summary>\n<blockquote>\n<details><summary>devcontainer</summary>\n<blockquote>\n<details><summary>.devcontainer/devcontainer.json</summary>\n - `mcr.microsoft.com/devcontainers/go 1-bookworm`\n - `ghcr.io/devcontainers/features/docker-in-docker 2`\n - `ghcr.io/rio/features/k3d 1`\n - `ghcr.io/mpriscella/features/kind 1`\n - `ghcr.io/rjfmachado/devcontainer-features/cloud-native 1`\n - `ghcr.io/guiyomh/features/golangci-lint 0`\n - `ghcr.io/devcontainers-contrib/features/kubectx-kubens 1`\n - `ghcr.io/dhoeric/features/stern 1`\n</details>\n</blockquote>\n</details>\n<details><summary>dockerfile</summary>\n<blockquote>\n<details><summary>Dockerfile</summary>\n</details>\n<details><summary>Dockerfile-ubi8</summary>\n</details>\n<details><summary>Dockerfile-ubi9</summary>\n</details>\n</blockquote>\n</details>\n<details><summary>github-actions</summary>\n<blockquote>\n<details><summary>.github/workflows/backport.yml</summary>\n - `actions-ecosystem/action-add-labels v1`\n - `peter-evans/create-or-update-comment v4`\n - `actions-ecosystem/action-remove-labels v1`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `dacbd/create-issue-action v2`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/chatops.yml</summary>\n - `actions-cool/check-user-permission v2`\n - `actions-ecosystem/action-add-labels v1.1.3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/codeql-analysis.yml</summary>\n - `fkirc/skip-duplicate-actions v5.3.1`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `github/codeql-action v3`\n - `github/codeql-action v3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `benc-uk/workflow-dispatch v1`\n - `xt0rted/slash-command-action v2`\n - `xt0rted/pull-request-comment-branch v3`\n - `peter-evans/create-or-update-comment v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `goreleaser/goreleaser-action v6`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/upload-artifact v4`\n - `goreleaser/goreleaser-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `ad-m/github-push-action v0.8.0`\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `azure/login v2.2.0`\n - `nick-fields/retry v3`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `docker/login-action v3`\n - `azure/login v2.2.0`\n - `azure/setup-kubectl v4`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `azure/login v2.2.0`\n - `nick-fields/retry v3`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `aws-actions/configure-aws-credentials v4`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `google-github-actions/auth v2`\n - `google-github-actions/setup-gcloud v2`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `redhat-actions/openshift-tools-installer v1`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/download-artifact v4`\n - `cloudnative-pg/ciclops v1.3.1`\n - `actions/upload-artifact v4`\n - `rtCamp/action-slack-notify v2`\n - `actions-ecosystem/action-add-labels v1.1.3`\n - `actions-ecosystem/action-remove-labels v1.3.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `benc-uk/workflow-dispatch v1`\n - `fkirc/skip-duplicate-actions v5.3.1`\n - `actions/checkout v4`\n - `dorny/paths-filter v3.0.2`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `golangci/golangci-lint-action v6`\n - `actions/checkout v4`\n - `golang/govulncheck-action v1`\n - `actions/checkout v4`\n - `ludeeus/action-shellcheck 2.0.0`\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `goreleaser/goreleaser-action v6`\n - `rtCamp/action-slack-notify v2`\n - `goreleaser/goreleaser-action v6`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `github/codeql-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `docker/setup-qemu-action v3`\n - `actions/setup-go v5`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `helm/kind-action v1.12.0`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `redhat-actions/podman-login v1`\n - `actions/download-artifact v4`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/k8s-versions-check.yml</summary>\n - `actions/checkout v4`\n - `azure/login v2.2.0`\n - `google-github-actions/auth v2`\n - `google-github-actions/setup-gcloud v2`\n - `frenck/action-setup-yq v1`\n - `peter-evans/create-pull-request v7`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/latest-postgres-version-check.yml</summary>\n - `actions/checkout v4`\n - `actions/setup-python v5`\n - `peter-evans/create-pull-request v7`\n - `peter-evans/create-pull-request v7`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/pr_verify_linked_issue.yml</summary>\n - `hattan/verify-linked-issue-action v1.1.5`\n</details>\n<details><summary>.github/workflows/registry-clean.yml</summary>\n - `snok/container-retention-policy v3.0.0`\n - `snok/container-retention-policy v3.0.0`\n</details>\n<details><summary>.github/workflows/release-pr.yml</summary>\n - `actions/checkout v4`\n - `repo-sync/pull-request v2.12`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/release-publish.yml</summary>\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `softprops/action-gh-release v2`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `crazy-max/ghaction-import-gpg v6`\n - `goreleaser/goreleaser-action v6`\n - `rajatjindal/krew-release-bot v0.0.47`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `docker/setup-qemu-action v3`\n - `actions/setup-go v5`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `peter-evans/create-pull-request v7`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `ad-m/github-push-action v0.8.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/release-tag.yml</summary>\n - `actions/checkout v4`\n - `christophebedard/tag-version-commit v1.7.0`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/require-labels.yml</summary>\n - `agilepathway/pull-request-label-checker v1.6.61`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/snyk.yml</summary>\n - `actions/checkout v4`\n - `snyk/actions 0.4.0`\n - `github/codeql-action v3`\n - `snyk/actions 0.4.0`\n - `github/codeql-action v3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/spellcheck.yml</summary>\n - `actions/checkout v4`\n - `get-woke/woke-action v0`\n - `actions/checkout v4`\n - `rojopolis/spellcheck-github-actions 0.46.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/sync-api.yml</summary>\n - `peter-evans/repository-dispatch v3`\n</details>\n</blockquote>\n</details>\n<details><summary>gomod</summary>\n<blockquote>\n<details><summary>go.mod</summary>\n - `go 1.23.5`\n - `github.com/DATA-DOG/go-sqlmock v1.5.2`\n - `github.com/Masterminds/semver/v3 v3.3.1`\n - `github.com/avast/retry-go/v4 v4.6.0`\n - `github.com/blang/semver v3.5.1+incompatible`\n - `github.com/cheynewallace/tabby v1.1.1`\n - `github.com/cloudnative-pg/barman-cloud v0.0.0-20250104195650-c1472628b450@c1472628b450`\n - `github.com/cloudnative-pg/cnpg-i v0.0.0-20241224161104-7e2cfa59debc@7e2cfa59debc`\n - `github.com/cloudnative-pg/machinery v0.0.0-20250102082645-95c37fe624d0@95c37fe624d0`\n - `github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc@d8f796af33cc`\n - `github.com/evanphx/json-patch/v5 v5.9.11`\n - `github.com/go-logr/logr v1.4.2`\n - `github.com/google/shlex v0.0.0-20191202100458-e7afc7fbc510@e7afc7fbc510`\n - `github.com/grpc-ecosystem/go-grpc-middleware/v2 v2.2.0`\n - `github.com/jackc/pgx/v5 v5.7.2`\n - `github.com/jackc/puddle/v2 v2.2.2`\n - `github.com/kballard/go-shellquote v0.0.0-20180428030007-95032a82bc51@95032a82bc51`\n - `github.com/kubernetes-csi/external-snapshotter/client/v8 v8.2.0`\n - `github.com/lib/pq v1.10.9`\n - `github.com/logrusorgru/aurora/v4 v4.0.0`\n - `github.com/mitchellh/go-ps v1.0.0`\n - `github.com/onsi/ginkgo/v2 v2.22.2`\n - `github.com/onsi/gomega v1.36.2`\n - `github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring v0.80.0`\n - `github.com/prometheus/client_golang v1.20.5`\n - `github.com/robfig/cron v1.2.0`\n - `github.com/sethvargo/go-password v0.3.1`\n - `github.com/spf13/cobra v1.8.1`\n - `github.com/stern/stern v1.32.0`\n - `github.com/thoas/go-funk v0.9.3`\n - `go.uber.org/atomic v1.11.0`\n - `go.uber.org/multierr v1.11.0`\n - `go.uber.org/zap v1.27.0`\n - `golang.org/x/term v0.29.0`\n - `google.golang.org/grpc v1.70.0`\n - `gopkg.in/yaml.v3 v3.0.1`\n - `k8s.io/api v0.32.1`\n - `k8s.io/apiextensions-apiserver v0.32.1`\n - `k8s.io/apimachinery v0.32.1`\n - `k8s.io/cli-runtime v0.32.1`\n - `k8s.io/client-go v0.32.1`\n - `k8s.io/utils v0.0.0-20241210054802-24370beab758@24370beab758`\n - `sigs.k8s.io/controller-runtime v0.20.1`\n - `sigs.k8s.io/yaml v1.4.0`\n</details>\n</blockquote>\n</details>\n<details><summary>regex</summary>\n<blockquote>\n<details><summary>Makefile</summary>\n - `sigs.k8s.io/kustomize/kustomize/v5 v5.6.0`\n</details>\n<details><summary>Makefile</summary>\n - `sigs.k8s.io/controller-tools v0.16.5`\n</details>\n<details><summary>Makefile</summary>\n - `github.com/goreleaser/goreleaser v2.6.1`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `kubernetes-csi/external-snapshotter v8.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-snapshotter v8.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-provisioner v5.2.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-resizer v1.13.1`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/external-attacher v4.8.0`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kubernetes-csi/csi-driver-host-path v1.15.0`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `rook/rook v1.16.3`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `kubernetes-sigs/kind v0.26.0`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `kubernetes-sigs/kind v0.26.0`\n</details>\n<details><summary>hack/e2e/run-e2e-kind.sh</summary>\n - `kindest/node v1.32.1`\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n - `kindest/node v1.32.1`\n</details>\n<details><summary>hack/e2e/run-e2e-k3d.sh</summary>\n</details>\n<details><summary>hack/setup-cluster.sh</summary>\n</details>\n<details><summary>Makefile</summary>\n - `jonasbn/github-action-spellcheck 0.46.0`\n</details>\n<details><summary>Makefile</summary>\n - `getwoke/woke 0.19.0`\n</details>\n<details><summary>Makefile</summary>\n - `operator-framework/operator-sdk v1.39.1`\n</details>\n<details><summary>Makefile</summary>\n - `operator-framework/operator-registry v1.50.0`\n</details>\n<details><summary>Makefile</summary>\n - `redhat-openshift-ecosystem/openshift-preflight 1.11.1`\n</details>\n<details><summary>config/olm-scorecard/patches/basic.config.yaml</summary>\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n</details>\n<details><summary>config/olm-scorecard/patches/olm.config.yaml</summary>\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n - `quay.io/operator-framework/scorecard-test v1.39.1`\n</details>\n<details><summary>pkg/specs/pgbouncer/deployments.go</summary>\n - `ghcr.io/cloudnative-pg/pgbouncer 1.24.0`\n</details>\n<details><summary>.github/workflows/backport.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/codeql-analysis.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/release-publish.yml</summary>\n - `golang 1.23`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `golangci/golangci-lint 1.63.4`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `vmware-tanzu/velero 1.15.1`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `vmware-tanzu/velero-plugin-for-aws 1.11.1`\n</details>\n</blockquote>\n</details>\n</blockquote>\n</details>\n<details><summary>Branch release-1.25</summary>\n<blockquote>\n<details><summary>devcontainer</summary>\n<blockquote>\n<details><summary>.devcontainer/devcontainer.json</summary>\n - `mcr.microsoft.com/devcontainers/go 1-bookworm`\n - `ghcr.io/devcontainers/features/docker-in-docker 2`\n - `ghcr.io/rio/features/k3d 1`\n - `ghcr.io/mpriscella/features/kind 1`\n - `ghcr.io/rjfmachado/devcontainer-features/cloud-native 1`\n - `ghcr.io/guiyomh/features/golangci-lint 0`\n - `ghcr.io/devcontainers-contrib/features/kubectx-kubens 1`\n - `ghcr.io/dhoeric/features/stern 1`\n</details>\n</blockquote>\n</details>\n<details><summary>dockerfile</summary>\n<blockquote>\n<details><summary>Dockerfile</summary>\n</details>\n<details><summary>Dockerfile-ubi8</summary>\n</details>\n<details><summary>Dockerfile-ubi9</summary>\n</details>\n</blockquote>\n</details>\n<details><summary>github-actions</summary>\n<blockquote>\n<details><summary>.github/workflows/backport.yml</summary>\n - `actions-ecosystem/action-add-labels v1`\n - `peter-evans/create-or-update-comment v4`\n - `actions-ecosystem/action-remove-labels v1`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `dacbd/create-issue-action v2`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/chatops.yml</summary>\n - `actions-cool/check-user-permission v2`\n - `actions-ecosystem/action-add-labels v1.1.3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/codeql-analysis.yml</summary>\n - `fkirc/skip-duplicate-actions v5.3.1`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `github/codeql-action v3`\n - `github/codeql-action v3`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/continuous-delivery.yml</summary>\n - `benc-uk/workflow-dispatch v1`\n - `xt0rted/slash-command-action v2`\n - `xt0rted/pull-request-comment-branch v3`\n - `peter-evans/create-or-update-comment v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `goreleaser/goreleaser-action v6`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/upload-artifact v4`\n - `goreleaser/goreleaser-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `ad-m/github-push-action v0.8.0`\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `azure/login v2.2.0`\n - `nick-fields/retry v3`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `docker/login-action v3`\n - `azure/login v2.2.0`\n - `azure/setup-kubectl v4`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `azure/login v2.2.0`\n - `nick-fields/retry v3`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `aws-actions/configure-aws-credentials v4`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `nick-fields/retry v3`\n - `nick-fields/retry v3`\n - `google-github-actions/auth v2`\n - `google-github-actions/setup-gcloud v2`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `redhat-actions/openshift-tools-installer v1`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/upload-artifact v4`\n - `actions/download-artifact v4`\n - `cloudnative-pg/ciclops v1.3.1`\n - `actions/upload-artifact v4`\n - `rtCamp/action-slack-notify v2`\n - `actions-ecosystem/action-add-labels v1.1.3`\n - `actions-ecosystem/action-remove-labels v1.3.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/continuous-integration.yml</summary>\n - `benc-uk/workflow-dispatch v1`\n - `fkirc/skip-duplicate-actions v5.3.1`\n - `actions/checkout v4`\n - `dorny/paths-filter v3.0.2`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `golangci/golangci-lint-action v6`\n - `actions/checkout v4`\n - `golang/govulncheck-action v1`\n - `actions/checkout v4`\n - `ludeeus/action-shellcheck 2.0.0`\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `goreleaser/goreleaser-action v6`\n - `rtCamp/action-slack-notify v2`\n - `goreleaser/goreleaser-action v6`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `docker/build-push-action v6`\n - `erzz/dockle-action v1`\n - `github/codeql-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `docker/setup-qemu-action v3`\n - `actions/setup-go v5`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `helm/kind-action v1.12.0`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/setup-go v5`\n - `actions/checkout v4`\n - `redhat-actions/podman-login v1`\n - `actions/download-artifact v4`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/k8s-versions-check.yml</summary>\n - `actions/checkout v4`\n - `azure/login v2.2.0`\n - `google-github-actions/auth v2`\n - `google-github-actions/setup-gcloud v2`\n - `frenck/action-setup-yq v1`\n - `peter-evans/create-pull-request v7`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/latest-postgres-version-check.yml</summary>\n - `actions/checkout v4`\n - `actions/setup-python v5`\n - `peter-evans/create-pull-request v7`\n - `peter-evans/create-pull-request v7`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/pr_verify_linked_issue.yml</summary>\n - `hattan/verify-linked-issue-action v1.1.5`\n</details>\n<details><summary>.github/workflows/registry-clean.yml</summary>\n - `snok/container-retention-policy v3.0.0`\n - `snok/container-retention-policy v3.0.0`\n</details>\n<details><summary>.github/workflows/release-pr.yml</summary>\n - `actions/checkout v4`\n - `repo-sync/pull-request v2.12`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/release-publish.yml</summary>\n - `actions/checkout v4`\n - `actions/checkout v4`\n - `softprops/action-gh-release v2`\n - `actions/checkout v4`\n - `actions/setup-go v5`\n - `crazy-max/ghaction-import-gpg v6`\n - `goreleaser/goreleaser-action v6`\n - `rajatjindal/krew-release-bot v0.0.47`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/metadata-action v5`\n - `docker/setup-qemu-action v3`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `docker/build-push-action v6`\n - `actions/checkout v4`\n - `docker/setup-qemu-action v3`\n - `actions/setup-go v5`\n - `docker/setup-buildx-action v3`\n - `docker/login-action v3`\n - `actions/upload-artifact v4`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `peter-evans/create-pull-request v7`\n - `actions/checkout v4`\n - `actions/download-artifact v4`\n - `ad-m/github-push-action v0.8.0`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/release-tag.yml</summary>\n - `actions/checkout v4`\n - `christophebedard/tag-version-commit v1.7.0`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/require-labels.yml</summary>\n - `agilepathway/pull-request-label-checker v1.6.61`\n - `ubuntu 24.04`\n</details>\n<details><summary>.github/workflows/snyk.yml</summary>\n - `actions/checkout v4`\n - `snyk/actions 0.4.0`\n - `github/codeql-action v3`\n - `snyk/actions 0.4.0`\n - `github/codeql-action v3`\n - `ubuntu 24.04`\n</details>\n</blockquote>\n</details>\n</blockquote>\n</details>\n---\n- [ ] <!-- manual job -->Check this box to trigger a request for Renovate to run again on this repository"
    },
    {
        "title": "add an E2E test case that takes backups of a single instance PG cluster",
        "id": 1574627366,
        "state": "open",
        "first": "In order to prevent regressions like https://github.com/cloudnative-pg/cloudnative-pg/issues/1407, let's add an E2E test that uses a Single Instance PG cluster and make a backup / restore.",
        "messages": "In order to prevent regressions like https://github.com/cloudnative-pg/cloudnative-pg/issues/1407, let's add an E2E test that uses a Single Instance PG cluster and make a backup / restore."
    },
    {
        "title": "Introduce support for auth_dbname in pgbouncer",
        "id": 1566309984,
        "state": "open",
        "first": "PgBouncer has introduced the patch that we submitted some time ago about the `auth_dbname`:\r\nhttps://github.com/pgbouncer/pgbouncer/commit/0ba795f063da121ad7aa3d63699058e85fd3c382\r\nThe introduction of its support in the operator might simplify the integration with pgbouncer as well as reduce the workload on the reconciler.",
        "messages": "PgBouncer has introduced the patch that we submitted some time ago about the `auth_dbname`:\r\nhttps://github.com/pgbouncer/pgbouncer/commit/0ba795f063da121ad7aa3d63699058e85fd3c382\r\nThe introduction of its support in the operator might simplify the integration with pgbouncer as well as reduce the workload on the reconciler.@phisco any news? ;)\r\nThis could remove tech debt in CNPG.\n---\non it, @gbartolini \ud83d\ude09\n---\nSo, currently we create a dedicated role, then for all the accessible databases we create the function and grant that role access to it. Are you suggesting we create the role and the function only in an hardcoded database, e.g. postgres, and hardcode the auth_dbname to that? Should we let users set or unset auth_dbname if needed? \ud83e\udd14 @gbartolini\n---\nBasically, with auth_dbname, we no longer need the function in the database.\r\nAll you need to do is define an entry in the [databases section](https://www.pgbouncer.org/config.html#section-databases). We can use the same name in every deployment (a reserved one). Then, specify the same name in `auth_dbname`.\r\nThis database should point to the `postgres` database in the cluster.\r\nWe should use the same user `cnpg_pooler_pgbouncer` to connect to the cluster. Instead of creating the function, all we need to ensure is that the `cnpg_pooler_pgbouncer` user has the following permissions:\r\n```sql\r\nGRANT SELECT (usename, passwd) ON \"pg_catalog\".\"pg_shadow\" TO \"cnpg_pooler_pgbouncer\"\r\n```\r\n(I have not run the above query, please verify it).\n---\nIn my opinion, in our implementation we don't allow any configuration of auth_dbname.\n---\nI opted for adding a field `AutDBName`, setting it results in the automatic configuration not being applied, same as we do for `AuthQuery` and `AuthQuerySecret`."
    },
    {
        "title": "Bootstrapping a replica cluster from a non-cnpg cluster",
        "id": 1548829603,
        "state": "open",
        "first": "In order to migrate a non-cnpg cluster to cnpg, I'm attempting to create a cnpg replica cluster that I bootstrap using pg_basebackup mode from a the original, non-cnpg, cluster.\r\nI'm not sure if this is a supported use case? The symptoms are as follow.\r\nThe basebackup works fine but then the designated primary does not start with `FATAL:  could not create lock file \\\"/var/run/postgresql/.s.PGSQL.5432.lock\\\": No such file or directory`.\r\nThe `postgresql.conf` in the restored cluster is the one from the original cluster.\r\nHowever, while `custom.conf` is present, `postgresql.conf` misses the `include \"custom.conf\"` line.\r\nThis seems to explain the error.\r\nI tried adding the include line in postgresql.conf manually and succeded starting the primary, however it did not start as a replica (although `posgresql.auto.conf` seems to be correctly setup for replication).",
        "messages": "In order to migrate a non-cnpg cluster to cnpg, I'm attempting to create a cnpg replica cluster that I bootstrap using pg_basebackup mode from a the original, non-cnpg, cluster.\r\nI'm not sure if this is a supported use case? The symptoms are as follow.\r\nThe basebackup works fine but then the designated primary does not start with `FATAL:  could not create lock file \\\"/var/run/postgresql/.s.PGSQL.5432.lock\\\": No such file or directory`.\r\nThe `postgresql.conf` in the restored cluster is the one from the original cluster.\r\nHowever, while `custom.conf` is present, `postgresql.conf` misses the `include \"custom.conf\"` line.\r\nThis seems to explain the error.\r\nI tried adding the include line in postgresql.conf manually and succeded starting the primary, however it did not start as a replica (although `posgresql.auto.conf` seems to be correctly setup for replication).Make sure that the Postgres configuration options you use are compatible with CloudNativePG.\r\nRegarding the replica, did you actually activate it? You need:\r\n```yaml\r\n  replica:\r\n    enabled: true\r\n    source: <NAME_OF_EXTERNAL_CLUSTER>\r\n```\n---\n@gbartolini thanks for the response. \r\nYes the replica `enabled` and `source` are correctly defined in the new cluster resource, and they are reflected in `postgresql.auto.conf` in the cnpg volume after basebackup.\r\nI have compared the postgres.conf files with a running cnpg cluster with a similar config and the only thing that stands out is that `custom.conf` is not included at the end, so the cnpg options are not applied.\n---\nI have run into the same issue. Trying to bootstrap from a non cnpg postgres and the initial `pg_basebackup` works fine, but then the cnpg pod starts crash looping with the error message:\r\n```\r\n{\"level\":\"info\",\"ts\":\"2023-07-12T08:29:01Z\",\"logger\":\"postgres\",\"msg\":\"postgres: could not access the server configuration fi\r\nle \\\"/var/lib/postgresql/data/pgdata/postgresql.conf\\\": No such file or directory\",\"pipe\":\"stderr\",\"logging_pod\":\"cnpg-1\"}\r\n{\"level\":\"info\",\"ts\":\"2023-07-12T08:29:01Z\",\"msg\":\"Extracting pg_controldata information\",\"logging_pod\":\"cnpg-1\",\"reason\":\"po\r\nstmaster has exited\"}\r\n{\"level\":\"error\",\"ts\":\"2023-07-12T08:29:01Z\",\"msg\":\"PostgreSQL process exited with errors\",\"logging_pod\":\"cnpg-1\",\"error\":\"ex\r\nit status 2\",\"stacktrace\":\"github.com/cloudnative-pg/cloudnative-pg/pkg/management/log.(*logger).Error\\n\\tpkg/management/log/\r\nlog.go:127\\ngithub.com/cloudnative-pg/cloudnative-pg/internal/cmd/manager/instance/run/lifecycle.(*PostgresLifecycle).Start\\n\r\n\\tinternal/cmd/manager/instance/run/lifecycle/lifecycle.go:99\\nsigs.k8s.io/controller-runtime/pkg/manager.(*runnableGroup).re\r\nconcile.func1\\n\\tpkg/mod/sigs.k8s.io/controller-runtime@v0.14.2/pkg/manager/runnable_group.go:219\"}\r\n```\r\nI also have `replica.enabled=true` (if I put this to `false` then everything is fine, but ofc I dont get streaming replication then).\r\n@gbartolini can you elaborate what are the conditions to be compatible?\r\n> Make sure that the Postgres configuration options you use are compatible with CloudNativePG.\r\n@sbidoul Were you able to find a fix for this?\n---\nAlso found this [discussion](https://github.com/cloudnative-pg/cloudnative-pg/discussions/2331) where a similar issue is described.\n---\nEncountered the same issue as @m33li5, any tips appreciated ;)\n---\nFor anyone wondering we've been able to make it work but your source database cluster/instance needs to match 1 for 1 on some things with how CNPG operates :\r\n- Your running `postgresql.conf` or the `postgresql.conf` file you want your new CNPG to use must be located inside $PGDATA (Not sure of that but it seems to be the case)\r\n- It must have directives including `custom.conf` and `override.conf` (override.conf is important, if it's not included replication won't start)\r\n- The source and target max_wal_senders must be the same\r\n- The `unix_socket_directories` must be \"/controller/run\" (It should normally be set by custom.conf if I remember correctly)\n---\nThanks @Alan-pad your procedure works fine!\r\nIn order to ease the process for other users, here is the `postgresql.conf` **dummy** file:\r\n```\r\nmax_wal_senders = 5\r\ninclude 'custom.conf'\r\ninclude 'override.conf'\r\nunix_socket_directories = '/controller/run'\r\n```\n---\nAlso, for anyone that doesn't know: cnpg needs `streaming_replica` ROLE to exist for it's own replication to work. If anyone needs help with using Patroni as the old cluster, hit me up.\n---\n> > Also, for anyone that doesn't know: cnpg needs `streaming_replica` ROLE to exist for it's own replication to work. If anyone needs help with using Patroni as the old cluster, hit me up.\r\n> \r\n> Hi @aleksasiriski , I am facing an collision error `Error: while configuring restored instance: could not create ApplicationDatabase: ERROR: template database \"template1\" has a collation version mismatch (SQLSTATE XX000)` while migrating the data from zalando to cnpg . Could you pls take a look into this.\r\nTry making it a replica cluster first and after that succeeds disable replica mode and it will become an independant cluster. I've created a simple [gist](https://gist.github.com/aleksasiriski/f85070b38fee370b36e6075f1c0e6136) with notes on how I got it working in replica mode, and after disabling the replica mode it continued to work.\n---\n> * The source and target max_wal_senders must be the same\r\nAlso, `max_connections` must be the same\n---\nI tried to place `postgresql.conf` in the pgdata directory of the source postgres server; pgbasebackup finishes without any errors, but the actual replica cluster never starts.\r\n```yaml\r\napiVersion: postgresql.cnpg.io/v1\r\nkind: Cluster\r\nmetadata:\r\n  name: pg\r\n  namespace: fastcup\r\nspec:\r\n  instances: 1\r\n  imageName: ghcr.io/cloudnative-pg/postgresql:17.0\r\n  bootstrap:\r\n    pg_basebackup:\r\n      source: source-db\r\n  replica:\r\n    enabled: true\r\n    source: source-db\r\n  storage:\r\n    size: 1500Gi\r\n    storageClass: nvme-lvm-local\r\n  externalClusters:\r\n    - name: source-db\r\n      connectionParameters:\r\n        host: 1.2.3.4\r\n        user: replicate\r\n      password:\r\n        name: source-db-replica-user\r\n        key: password\r\n  postgresql:\r\n    parameters:\r\n      max_connections: \"200\"\r\n      max_wal_senders: \"10\"\r\n```\r\n```\r\nxlog min recovery request 19173/5D280898 is past current point 19166/499F7A18\",\"context\":\"writing block 3 of relation base/16403/4840307_vm\\nWAL redo at 19166/499F6038 for Btree/INSERT_LEAF: off: 209; blkref #0: rel 1663/16403/1998418, blk 32061 FPW\r\nfailed to connect to `user=postgres database=postgres`: /controller/run/.s.PGSQL.5432 (/controller/run): server error: FATAL: the database system is not yet accepting connections (SQLSTATE 57P03)\r\n\"message\":\"the database system is not yet accepting connections\",\"detail\":\"Consistent recovery state has not been yet reached.\"\r\ntried restoring WALs, but no backup was configured\r\ntried restoring WALs, but no backup was configured\r\ntried restoring WALs, but no backup was configured\r\ntried restoring WALs, but no backup was configured\r\n```\r\nWhat am I doing wrong?\n---\nnot stale"
    },
    {
        "title": "[Feature] Support odyssey - Scalable PostgreSQL connection pooler",
        "id": 1367861690,
        "state": "open",
        "first": "Hello! Thanks for cloudnative-pg\r\nPlease add to roadmap support odyssey - Scalable PostgreSQL connection pooler",
        "messages": "Hello! Thanks for cloudnative-pg\r\nPlease add to roadmap support odyssey - Scalable PostgreSQL connection poolerHi, thanks for the request, could you expand a little bit more? As you know, cloudnative-pg currently supports pgbouncer, what would odyssey add in your opinion? Thanks!\n---\nhttps://github.com/yandex/odyssey\r\nAdvanced multi-threaded PostgreSQL connection pooler and request router.\r\nhttps://github.com/yandex/odyssey/releases/tag/1.3\r\nSupport for protocol-level prepared statements in transaction pooling mode. You can enable [pool_reserve_prepared_statement](https://github.com/yandex/odyssey/blob/136dcca1a4fa8f92094b86132133272bbc444b2d/config-examples/odyssey-dev.conf#L87) for a database. For client sessions issuing prepared statements Odyssey will automatically prepare those statements in database processes when necessary.\n---\nHello. Any updates?\r\nWe use Yandex Odyssey; it is much better than pgbouncer.\n---\nnot stale\n---\nAny news?"
    },
    {
        "title": "Maintenance windows",
        "id": 1200446447,
        "state": "open",
        "first": "An initial implementation of maintenance windows is to introduce their specification on a weekly basis. In the future we might provide more control, including specification of an actual day.\nMaintenance windows are enabled only for the \u201cunsupervised\u201d primary update strategy, with \u201crestart\u201d method option. When a maintenance window is defined in a cluster, an update of the image is \u201cpostponed\u201d to the first available maintenance window, which will trigger a rollout update.\nA maintenance window has a minimal length of \"w\" hours (initially 6), in which activities cannot start any later than \"w - s\" hours, where \"s\" is expressed in hours (by default 3). So, for example, with w=6h and s=3, activities can only start in the first 3 hours of the maintenance window.\nMaintenance windows can be specified as an array:\n```\nmaintenanceWindows:\n  - dow: 0, 6\n    start_at: 02:00\n    end_at: 08:00\n```\nOr:\n```\nmaintenanceWindows:\n  - dow: 0\n    start_at: 02:00\n    end_at: 08:00\n  - dow: 6\n    start_at: 03:00\n    end_at: 09:30\n```",
        "messages": "An initial implementation of maintenance windows is to introduce their specification on a weekly basis. In the future we might provide more control, including specification of an actual day.\nMaintenance windows are enabled only for the \u201cunsupervised\u201d primary update strategy, with \u201crestart\u201d method option. When a maintenance window is defined in a cluster, an update of the image is \u201cpostponed\u201d to the first available maintenance window, which will trigger a rollout update.\nA maintenance window has a minimal length of \"w\" hours (initially 6), in which activities cannot start any later than \"w - s\" hours, where \"s\" is expressed in hours (by default 3). So, for example, with w=6h and s=3, activities can only start in the first 3 hours of the maintenance window.\nMaintenance windows can be specified as an array:\n```\nmaintenanceWindows:\n  - dow: 0, 6\n    start_at: 02:00\n    end_at: 08:00\n```\nOr:\n```\nmaintenanceWindows:\n  - dow: 0\n    start_at: 02:00\n    end_at: 08:00\n  - dow: 6\n    start_at: 03:00\n    end_at: 09:30\n```This should be depending on #12"
    }
]