import requests
import json
import argparse
import time

owner = "cloudnative-pg"
repo = "cloudnative-pg"

# Add the User-Agent and Authorization headers for GitHub API requests
headers = {
    'User-Agent': 'Michkr123',  # Replace with your GitHub username or application name
    'Authorization': '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'  # Replace 'token here' with your GitHub personal access token
}

def check_rate_limit():
    url = 'https://api.github.com/rate_limit'
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        data = response.json()
        remaining = data['resources']['core']['remaining']
        reset_time = data['resources']['core']['reset']
        if remaining == 0:
            reset_timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(reset_time))
            print(f"Rate limit exceeded. Try again at {reset_timestamp}.")
        return remaining
    else:
        print(f"Error checking rate limit: {response.status_code}")
        return 0

def fetch_issues():
    url = f'https://api.github.com/repos/{owner}/{repo}/issues'
    issues = []
    page = 1
    
    while True:
        remaining = check_rate_limit()
        if remaining == 0:
            break

        response = requests.get(url, params={'page': page, 'per_page': 100}, headers=headers)
        if response.status_code != 200:
            print(f"Error fetching issues: {response.status_code}")
            break
        
        data = response.json()
        if not data:
            break
        
        issues.extend(data)
        page += 1
        
    return issues

def fetch_comments(issue_id):
    url = f'https://api.github.com/repos/{owner}/{repo}/issues/{issue_id}/comments'
    comments = []
    page = 1
    
    while True:
        remaining = check_rate_limit()
        if remaining == 0:
            break
        
        response = requests.get(url, params={'page': page, 'per_page': 100}, headers=headers)
        if response.status_code == 404:
            print(f"No comments for issue {issue_id}.")
            break
        elif response.status_code != 200:
            print(f"Error fetching comments for issue {issue_id}: {response.status_code}")
            break
        
        data = response.json()
        if not data:
            break
        
        comments.extend(data)
        page += 1
    
    return comments

def process_issue(issue):
    # Extract first issue message
    first_message = issue.get('body', '').strip() if issue.get('body') else ''
    
    # Fetch comments (you can remove the fetch_comments call if the issues have few comments and you want to cache this)
    comments = fetch_comments(issue['id'])
    
    # Remove robot comments (e.g., messages from GitHub bots)
    messages = [comment['body'].strip() for comment in comments if 'bot' not in comment.get('user', {}).get('login', '')]
    
    # Determine the issue state
    state = 'no reaction'  # Default state if no valid comments
    if issue['state'] in ['open', 'closed']:
        state = issue['state']
    
    # Create the resulting JSON object for the issue
    result = {
        "title": issue.get('title', ''),
        "id": issue['id'],
        "state": state,
        "first": first_message,
        "messages": "\n---\n".join([first_message] + messages)
    }
    
    return result

def save_to_json(issues, output_file):
    with open(output_file, 'w') as f:
        json.dump(issues, f, indent=2)

def main():
    parser = argparse.ArgumentParser(description="Fetch issues and comments from GitHub and format them into Markdown.")
    parser.add_argument('--output', type=str, required=True, help="Output file to store the JSON data.")
    args = parser.parse_args()
    
    issues = fetch_issues()
    processed_issues = [process_issue(issue) for issue in issues]
    
    save_to_json(processed_issues, args.output)
    print(f"Processed {len(processed_issues)} issues and saved to {args.output}")

if __name__ == "__main__":
    main()
